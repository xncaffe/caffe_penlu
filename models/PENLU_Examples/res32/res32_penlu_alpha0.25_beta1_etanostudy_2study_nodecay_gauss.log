I1006 11:25:16.855927  2824 caffe.cpp:218] Using GPUs 0
I1006 11:25:16.881229  2824 caffe.cpp:223] GPU 0: GeForce GTX 1080
I1006 11:25:17.107084  2824 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 100000
snapshot_prefix: "xn/PENLU/snapshot/resnet/res32/res32_penlu_alpha0.25_beta1_etanostudy_2study_nodecay_gauss"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/PENLU/neural/resnet/res32/res32_penlu_gauss.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 40000
stepvalue: 80000
I1006 11:25:17.107218  2824 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/PENLU/neural/resnet/res32/res32_penlu_gauss.prototxt
I1006 11:25:17.109566  2824 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res32/res32_penlu_gauss.prototxt
I1006 11:25:17.109576  2824 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1006 11:25:17.109756  2824 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I1006 11:25:17.109838  2824 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I1006 11:25:17.110540  2824 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution16"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: 
I1006 11:25:17.111028  2824 layer_factory.hpp:77] Creating layer Data1
I1006 11:25:17.111104  2824 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb
I1006 11:25:17.111124  2824 net.cpp:84] Creating Layer Data1
I1006 11:25:17.111129  2824 net.cpp:380] Data1 -> Data1
I1006 11:25:17.111143  2824 net.cpp:380] Data1 -> Data2
I1006 11:25:17.111152  2824 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I1006 11:25:17.112538  2824 data_layer.cpp:45] output data size: 100,3,28,28
I1006 11:25:17.114805  2824 net.cpp:122] Setting up Data1
I1006 11:25:17.114819  2824 net.cpp:129] Top shape: 100 3 28 28 (235200)
I1006 11:25:17.114822  2824 net.cpp:129] Top shape: 100 (100)
I1006 11:25:17.114825  2824 net.cpp:137] Memory required for data: 941200
I1006 11:25:17.114830  2824 layer_factory.hpp:77] Creating layer Convolution1
I1006 11:25:17.114848  2824 net.cpp:84] Creating Layer Convolution1
I1006 11:25:17.114852  2824 net.cpp:406] Convolution1 <- Data1
I1006 11:25:17.114861  2824 net.cpp:380] Convolution1 -> Convolution1
I1006 11:25:17.261265  2824 net.cpp:122] Setting up Convolution1
I1006 11:25:17.261288  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.261291  2824 net.cpp:137] Memory required for data: 5958800
I1006 11:25:17.261307  2824 layer_factory.hpp:77] Creating layer BatchNorm1
I1006 11:25:17.261327  2824 net.cpp:84] Creating Layer BatchNorm1
I1006 11:25:17.261350  2824 net.cpp:406] BatchNorm1 <- Convolution1
I1006 11:25:17.261366  2824 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1006 11:25:17.261519  2824 net.cpp:122] Setting up BatchNorm1
I1006 11:25:17.261525  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.261528  2824 net.cpp:137] Memory required for data: 10976400
I1006 11:25:17.261535  2824 layer_factory.hpp:77] Creating layer Scale1
I1006 11:25:17.261554  2824 net.cpp:84] Creating Layer Scale1
I1006 11:25:17.261557  2824 net.cpp:406] Scale1 <- Convolution1
I1006 11:25:17.261560  2824 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1006 11:25:17.261622  2824 layer_factory.hpp:77] Creating layer Scale1
I1006 11:25:17.261735  2824 net.cpp:122] Setting up Scale1
I1006 11:25:17.261740  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.261742  2824 net.cpp:137] Memory required for data: 15994000
I1006 11:25:17.261757  2824 layer_factory.hpp:77] Creating layer penlu1
I1006 11:25:17.261767  2824 net.cpp:84] Creating Layer penlu1
I1006 11:25:17.261770  2824 net.cpp:406] penlu1 <- Convolution1
I1006 11:25:17.261775  2824 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1006 11:25:17.262377  2824 net.cpp:122] Setting up penlu1
I1006 11:25:17.262387  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.262389  2824 net.cpp:137] Memory required for data: 21011600
I1006 11:25:17.262397  2824 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1006 11:25:17.262401  2824 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1006 11:25:17.262403  2824 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1006 11:25:17.262408  2824 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1006 11:25:17.262423  2824 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1006 11:25:17.262445  2824 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1006 11:25:17.262450  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.262462  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.262465  2824 net.cpp:137] Memory required for data: 31046800
I1006 11:25:17.262467  2824 layer_factory.hpp:77] Creating layer Convolution2
I1006 11:25:17.262485  2824 net.cpp:84] Creating Layer Convolution2
I1006 11:25:17.262487  2824 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1006 11:25:17.262491  2824 net.cpp:380] Convolution2 -> Convolution2
I1006 11:25:17.263356  2824 net.cpp:122] Setting up Convolution2
I1006 11:25:17.263366  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.263370  2824 net.cpp:137] Memory required for data: 36064400
I1006 11:25:17.263383  2824 layer_factory.hpp:77] Creating layer BatchNorm2
I1006 11:25:17.263389  2824 net.cpp:84] Creating Layer BatchNorm2
I1006 11:25:17.263391  2824 net.cpp:406] BatchNorm2 <- Convolution2
I1006 11:25:17.263396  2824 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1006 11:25:17.263517  2824 net.cpp:122] Setting up BatchNorm2
I1006 11:25:17.263522  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.263525  2824 net.cpp:137] Memory required for data: 41082000
I1006 11:25:17.263540  2824 layer_factory.hpp:77] Creating layer Scale2
I1006 11:25:17.263545  2824 net.cpp:84] Creating Layer Scale2
I1006 11:25:17.263548  2824 net.cpp:406] Scale2 <- Convolution2
I1006 11:25:17.263552  2824 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1006 11:25:17.263576  2824 layer_factory.hpp:77] Creating layer Scale2
I1006 11:25:17.263672  2824 net.cpp:122] Setting up Scale2
I1006 11:25:17.263677  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.263679  2824 net.cpp:137] Memory required for data: 46099600
I1006 11:25:17.263695  2824 layer_factory.hpp:77] Creating layer penlu2
I1006 11:25:17.263701  2824 net.cpp:84] Creating Layer penlu2
I1006 11:25:17.263703  2824 net.cpp:406] penlu2 <- Convolution2
I1006 11:25:17.263707  2824 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1006 11:25:17.263808  2824 net.cpp:122] Setting up penlu2
I1006 11:25:17.263819  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.263823  2824 net.cpp:137] Memory required for data: 51117200
I1006 11:25:17.263828  2824 layer_factory.hpp:77] Creating layer Convolution3
I1006 11:25:17.263834  2824 net.cpp:84] Creating Layer Convolution3
I1006 11:25:17.263837  2824 net.cpp:406] Convolution3 <- Convolution2
I1006 11:25:17.263841  2824 net.cpp:380] Convolution3 -> Convolution3
I1006 11:25:17.264660  2824 net.cpp:122] Setting up Convolution3
I1006 11:25:17.264670  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.264673  2824 net.cpp:137] Memory required for data: 56134800
I1006 11:25:17.264678  2824 layer_factory.hpp:77] Creating layer BatchNorm3
I1006 11:25:17.264683  2824 net.cpp:84] Creating Layer BatchNorm3
I1006 11:25:17.264686  2824 net.cpp:406] BatchNorm3 <- Convolution3
I1006 11:25:17.264690  2824 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1006 11:25:17.264804  2824 net.cpp:122] Setting up BatchNorm3
I1006 11:25:17.264811  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.264812  2824 net.cpp:137] Memory required for data: 61152400
I1006 11:25:17.264817  2824 layer_factory.hpp:77] Creating layer Scale3
I1006 11:25:17.264822  2824 net.cpp:84] Creating Layer Scale3
I1006 11:25:17.264825  2824 net.cpp:406] Scale3 <- Convolution3
I1006 11:25:17.264828  2824 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1006 11:25:17.264852  2824 layer_factory.hpp:77] Creating layer Scale3
I1006 11:25:17.264919  2824 net.cpp:122] Setting up Scale3
I1006 11:25:17.264922  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.264925  2824 net.cpp:137] Memory required for data: 66170000
I1006 11:25:17.264930  2824 layer_factory.hpp:77] Creating layer Eltwise1
I1006 11:25:17.264933  2824 net.cpp:84] Creating Layer Eltwise1
I1006 11:25:17.264936  2824 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1006 11:25:17.264940  2824 net.cpp:406] Eltwise1 <- Convolution3
I1006 11:25:17.264942  2824 net.cpp:380] Eltwise1 -> Eltwise1
I1006 11:25:17.264958  2824 net.cpp:122] Setting up Eltwise1
I1006 11:25:17.264962  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.264964  2824 net.cpp:137] Memory required for data: 71187600
I1006 11:25:17.264966  2824 layer_factory.hpp:77] Creating layer penlu3
I1006 11:25:17.264971  2824 net.cpp:84] Creating Layer penlu3
I1006 11:25:17.264974  2824 net.cpp:406] penlu3 <- Eltwise1
I1006 11:25:17.264977  2824 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1006 11:25:17.265069  2824 net.cpp:122] Setting up penlu3
I1006 11:25:17.265074  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.265077  2824 net.cpp:137] Memory required for data: 76205200
I1006 11:25:17.265081  2824 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1006 11:25:17.265085  2824 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1006 11:25:17.265089  2824 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1006 11:25:17.265091  2824 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1006 11:25:17.265096  2824 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1006 11:25:17.265116  2824 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1006 11:25:17.265120  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.265123  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.265125  2824 net.cpp:137] Memory required for data: 86240400
I1006 11:25:17.265128  2824 layer_factory.hpp:77] Creating layer Convolution4
I1006 11:25:17.265136  2824 net.cpp:84] Creating Layer Convolution4
I1006 11:25:17.265138  2824 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1006 11:25:17.265141  2824 net.cpp:380] Convolution4 -> Convolution4
I1006 11:25:17.265956  2824 net.cpp:122] Setting up Convolution4
I1006 11:25:17.265965  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.265969  2824 net.cpp:137] Memory required for data: 91258000
I1006 11:25:17.265974  2824 layer_factory.hpp:77] Creating layer BatchNorm4
I1006 11:25:17.265980  2824 net.cpp:84] Creating Layer BatchNorm4
I1006 11:25:17.265988  2824 net.cpp:406] BatchNorm4 <- Convolution4
I1006 11:25:17.265993  2824 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1006 11:25:17.266105  2824 net.cpp:122] Setting up BatchNorm4
I1006 11:25:17.266111  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.266113  2824 net.cpp:137] Memory required for data: 96275600
I1006 11:25:17.266121  2824 layer_factory.hpp:77] Creating layer Scale4
I1006 11:25:17.266125  2824 net.cpp:84] Creating Layer Scale4
I1006 11:25:17.266129  2824 net.cpp:406] Scale4 <- Convolution4
I1006 11:25:17.266132  2824 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1006 11:25:17.266155  2824 layer_factory.hpp:77] Creating layer Scale4
I1006 11:25:17.266222  2824 net.cpp:122] Setting up Scale4
I1006 11:25:17.266225  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.266228  2824 net.cpp:137] Memory required for data: 101293200
I1006 11:25:17.266232  2824 layer_factory.hpp:77] Creating layer penlu4
I1006 11:25:17.266237  2824 net.cpp:84] Creating Layer penlu4
I1006 11:25:17.266242  2824 net.cpp:406] penlu4 <- Convolution4
I1006 11:25:17.266244  2824 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1006 11:25:17.266333  2824 net.cpp:122] Setting up penlu4
I1006 11:25:17.266338  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.266341  2824 net.cpp:137] Memory required for data: 106310800
I1006 11:25:17.266345  2824 layer_factory.hpp:77] Creating layer Convolution5
I1006 11:25:17.266352  2824 net.cpp:84] Creating Layer Convolution5
I1006 11:25:17.266355  2824 net.cpp:406] Convolution5 <- Convolution4
I1006 11:25:17.266360  2824 net.cpp:380] Convolution5 -> Convolution5
I1006 11:25:17.267187  2824 net.cpp:122] Setting up Convolution5
I1006 11:25:17.267197  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.267210  2824 net.cpp:137] Memory required for data: 111328400
I1006 11:25:17.267215  2824 layer_factory.hpp:77] Creating layer BatchNorm5
I1006 11:25:17.267220  2824 net.cpp:84] Creating Layer BatchNorm5
I1006 11:25:17.267223  2824 net.cpp:406] BatchNorm5 <- Convolution5
I1006 11:25:17.267226  2824 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1006 11:25:17.267340  2824 net.cpp:122] Setting up BatchNorm5
I1006 11:25:17.267345  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.267349  2824 net.cpp:137] Memory required for data: 116346000
I1006 11:25:17.267352  2824 layer_factory.hpp:77] Creating layer Scale5
I1006 11:25:17.267357  2824 net.cpp:84] Creating Layer Scale5
I1006 11:25:17.267360  2824 net.cpp:406] Scale5 <- Convolution5
I1006 11:25:17.267364  2824 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1006 11:25:17.267386  2824 layer_factory.hpp:77] Creating layer Scale5
I1006 11:25:17.267453  2824 net.cpp:122] Setting up Scale5
I1006 11:25:17.267458  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.267460  2824 net.cpp:137] Memory required for data: 121363600
I1006 11:25:17.267464  2824 layer_factory.hpp:77] Creating layer Eltwise2
I1006 11:25:17.267468  2824 net.cpp:84] Creating Layer Eltwise2
I1006 11:25:17.267472  2824 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1006 11:25:17.267474  2824 net.cpp:406] Eltwise2 <- Convolution5
I1006 11:25:17.267477  2824 net.cpp:380] Eltwise2 -> Eltwise2
I1006 11:25:17.267491  2824 net.cpp:122] Setting up Eltwise2
I1006 11:25:17.267495  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.267498  2824 net.cpp:137] Memory required for data: 126381200
I1006 11:25:17.267500  2824 layer_factory.hpp:77] Creating layer penlu5
I1006 11:25:17.267504  2824 net.cpp:84] Creating Layer penlu5
I1006 11:25:17.267508  2824 net.cpp:406] penlu5 <- Eltwise2
I1006 11:25:17.267511  2824 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1006 11:25:17.267604  2824 net.cpp:122] Setting up penlu5
I1006 11:25:17.267609  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.267611  2824 net.cpp:137] Memory required for data: 131398800
I1006 11:25:17.267616  2824 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1006 11:25:17.267627  2824 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1006 11:25:17.267630  2824 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1006 11:25:17.267633  2824 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1006 11:25:17.267637  2824 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1006 11:25:17.267659  2824 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1006 11:25:17.267663  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.267666  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.267668  2824 net.cpp:137] Memory required for data: 141434000
I1006 11:25:17.267671  2824 layer_factory.hpp:77] Creating layer Convolution6
I1006 11:25:17.267678  2824 net.cpp:84] Creating Layer Convolution6
I1006 11:25:17.267680  2824 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1006 11:25:17.267683  2824 net.cpp:380] Convolution6 -> Convolution6
I1006 11:25:17.268494  2824 net.cpp:122] Setting up Convolution6
I1006 11:25:17.268504  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.268507  2824 net.cpp:137] Memory required for data: 146451600
I1006 11:25:17.268512  2824 layer_factory.hpp:77] Creating layer BatchNorm6
I1006 11:25:17.268517  2824 net.cpp:84] Creating Layer BatchNorm6
I1006 11:25:17.268520  2824 net.cpp:406] BatchNorm6 <- Convolution6
I1006 11:25:17.268524  2824 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1006 11:25:17.268637  2824 net.cpp:122] Setting up BatchNorm6
I1006 11:25:17.268642  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.268646  2824 net.cpp:137] Memory required for data: 151469200
I1006 11:25:17.268651  2824 layer_factory.hpp:77] Creating layer Scale6
I1006 11:25:17.268654  2824 net.cpp:84] Creating Layer Scale6
I1006 11:25:17.268657  2824 net.cpp:406] Scale6 <- Convolution6
I1006 11:25:17.268661  2824 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1006 11:25:17.268683  2824 layer_factory.hpp:77] Creating layer Scale6
I1006 11:25:17.268751  2824 net.cpp:122] Setting up Scale6
I1006 11:25:17.268756  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.268759  2824 net.cpp:137] Memory required for data: 156486800
I1006 11:25:17.268762  2824 layer_factory.hpp:77] Creating layer penlu6
I1006 11:25:17.268769  2824 net.cpp:84] Creating Layer penlu6
I1006 11:25:17.268771  2824 net.cpp:406] penlu6 <- Convolution6
I1006 11:25:17.268774  2824 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1006 11:25:17.268867  2824 net.cpp:122] Setting up penlu6
I1006 11:25:17.268872  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.268874  2824 net.cpp:137] Memory required for data: 161504400
I1006 11:25:17.268878  2824 layer_factory.hpp:77] Creating layer Convolution7
I1006 11:25:17.268884  2824 net.cpp:84] Creating Layer Convolution7
I1006 11:25:17.268888  2824 net.cpp:406] Convolution7 <- Convolution6
I1006 11:25:17.268893  2824 net.cpp:380] Convolution7 -> Convolution7
I1006 11:25:17.269383  2824 net.cpp:122] Setting up Convolution7
I1006 11:25:17.269392  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.269395  2824 net.cpp:137] Memory required for data: 166522000
I1006 11:25:17.269399  2824 layer_factory.hpp:77] Creating layer BatchNorm7
I1006 11:25:17.269403  2824 net.cpp:84] Creating Layer BatchNorm7
I1006 11:25:17.269407  2824 net.cpp:406] BatchNorm7 <- Convolution7
I1006 11:25:17.269410  2824 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1006 11:25:17.269524  2824 net.cpp:122] Setting up BatchNorm7
I1006 11:25:17.269529  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.269531  2824 net.cpp:137] Memory required for data: 171539600
I1006 11:25:17.269541  2824 layer_factory.hpp:77] Creating layer Scale7
I1006 11:25:17.269546  2824 net.cpp:84] Creating Layer Scale7
I1006 11:25:17.269549  2824 net.cpp:406] Scale7 <- Convolution7
I1006 11:25:17.269552  2824 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1006 11:25:17.269575  2824 layer_factory.hpp:77] Creating layer Scale7
I1006 11:25:17.269642  2824 net.cpp:122] Setting up Scale7
I1006 11:25:17.269654  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.269655  2824 net.cpp:137] Memory required for data: 176557200
I1006 11:25:17.269659  2824 layer_factory.hpp:77] Creating layer Eltwise3
I1006 11:25:17.269665  2824 net.cpp:84] Creating Layer Eltwise3
I1006 11:25:17.269668  2824 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1006 11:25:17.269671  2824 net.cpp:406] Eltwise3 <- Convolution7
I1006 11:25:17.269675  2824 net.cpp:380] Eltwise3 -> Eltwise3
I1006 11:25:17.269690  2824 net.cpp:122] Setting up Eltwise3
I1006 11:25:17.269693  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.269696  2824 net.cpp:137] Memory required for data: 181574800
I1006 11:25:17.269698  2824 layer_factory.hpp:77] Creating layer penlu7
I1006 11:25:17.269703  2824 net.cpp:84] Creating Layer penlu7
I1006 11:25:17.269706  2824 net.cpp:406] penlu7 <- Eltwise3
I1006 11:25:17.269709  2824 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1006 11:25:17.269804  2824 net.cpp:122] Setting up penlu7
I1006 11:25:17.269807  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.269810  2824 net.cpp:137] Memory required for data: 186592400
I1006 11:25:17.269814  2824 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1006 11:25:17.269819  2824 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1006 11:25:17.269821  2824 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1006 11:25:17.269824  2824 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1006 11:25:17.269829  2824 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1006 11:25:17.269848  2824 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1006 11:25:17.269853  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.269856  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.269858  2824 net.cpp:137] Memory required for data: 196627600
I1006 11:25:17.269860  2824 layer_factory.hpp:77] Creating layer Convolution8
I1006 11:25:17.269866  2824 net.cpp:84] Creating Layer Convolution8
I1006 11:25:17.269870  2824 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1006 11:25:17.269873  2824 net.cpp:380] Convolution8 -> Convolution8
I1006 11:25:17.270671  2824 net.cpp:122] Setting up Convolution8
I1006 11:25:17.270681  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.270684  2824 net.cpp:137] Memory required for data: 201645200
I1006 11:25:17.270689  2824 layer_factory.hpp:77] Creating layer BatchNorm8
I1006 11:25:17.270694  2824 net.cpp:84] Creating Layer BatchNorm8
I1006 11:25:17.270697  2824 net.cpp:406] BatchNorm8 <- Convolution8
I1006 11:25:17.270700  2824 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1006 11:25:17.270815  2824 net.cpp:122] Setting up BatchNorm8
I1006 11:25:17.270820  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.270823  2824 net.cpp:137] Memory required for data: 206662800
I1006 11:25:17.270828  2824 layer_factory.hpp:77] Creating layer Scale8
I1006 11:25:17.270833  2824 net.cpp:84] Creating Layer Scale8
I1006 11:25:17.270835  2824 net.cpp:406] Scale8 <- Convolution8
I1006 11:25:17.270838  2824 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1006 11:25:17.270861  2824 layer_factory.hpp:77] Creating layer Scale8
I1006 11:25:17.270928  2824 net.cpp:122] Setting up Scale8
I1006 11:25:17.270933  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.270936  2824 net.cpp:137] Memory required for data: 211680400
I1006 11:25:17.270939  2824 layer_factory.hpp:77] Creating layer penlu8
I1006 11:25:17.270944  2824 net.cpp:84] Creating Layer penlu8
I1006 11:25:17.270947  2824 net.cpp:406] penlu8 <- Convolution8
I1006 11:25:17.270951  2824 net.cpp:367] penlu8 -> Convolution8 (in-place)
I1006 11:25:17.271042  2824 net.cpp:122] Setting up penlu8
I1006 11:25:17.271047  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.271050  2824 net.cpp:137] Memory required for data: 216698000
I1006 11:25:17.271054  2824 layer_factory.hpp:77] Creating layer Convolution9
I1006 11:25:17.271067  2824 net.cpp:84] Creating Layer Convolution9
I1006 11:25:17.271070  2824 net.cpp:406] Convolution9 <- Convolution8
I1006 11:25:17.271075  2824 net.cpp:380] Convolution9 -> Convolution9
I1006 11:25:17.271936  2824 net.cpp:122] Setting up Convolution9
I1006 11:25:17.271946  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.271950  2824 net.cpp:137] Memory required for data: 221715600
I1006 11:25:17.271955  2824 layer_factory.hpp:77] Creating layer BatchNorm9
I1006 11:25:17.271960  2824 net.cpp:84] Creating Layer BatchNorm9
I1006 11:25:17.271963  2824 net.cpp:406] BatchNorm9 <- Convolution9
I1006 11:25:17.271967  2824 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1006 11:25:17.272085  2824 net.cpp:122] Setting up BatchNorm9
I1006 11:25:17.272091  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.272094  2824 net.cpp:137] Memory required for data: 226733200
I1006 11:25:17.272099  2824 layer_factory.hpp:77] Creating layer Scale9
I1006 11:25:17.272104  2824 net.cpp:84] Creating Layer Scale9
I1006 11:25:17.272106  2824 net.cpp:406] Scale9 <- Convolution9
I1006 11:25:17.272109  2824 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1006 11:25:17.272133  2824 layer_factory.hpp:77] Creating layer Scale9
I1006 11:25:17.272204  2824 net.cpp:122] Setting up Scale9
I1006 11:25:17.272209  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.272212  2824 net.cpp:137] Memory required for data: 231750800
I1006 11:25:17.272215  2824 layer_factory.hpp:77] Creating layer Eltwise4
I1006 11:25:17.272219  2824 net.cpp:84] Creating Layer Eltwise4
I1006 11:25:17.272222  2824 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I1006 11:25:17.272225  2824 net.cpp:406] Eltwise4 <- Convolution9
I1006 11:25:17.272228  2824 net.cpp:380] Eltwise4 -> Eltwise4
I1006 11:25:17.272243  2824 net.cpp:122] Setting up Eltwise4
I1006 11:25:17.272246  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.272249  2824 net.cpp:137] Memory required for data: 236768400
I1006 11:25:17.272251  2824 layer_factory.hpp:77] Creating layer penlu9
I1006 11:25:17.272256  2824 net.cpp:84] Creating Layer penlu9
I1006 11:25:17.272258  2824 net.cpp:406] penlu9 <- Eltwise4
I1006 11:25:17.272263  2824 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1006 11:25:17.272359  2824 net.cpp:122] Setting up penlu9
I1006 11:25:17.272364  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.272367  2824 net.cpp:137] Memory required for data: 241786000
I1006 11:25:17.272372  2824 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1006 11:25:17.272375  2824 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1006 11:25:17.272378  2824 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1006 11:25:17.272382  2824 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1006 11:25:17.272387  2824 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1006 11:25:17.272408  2824 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1006 11:25:17.272411  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.272414  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.272416  2824 net.cpp:137] Memory required for data: 251821200
I1006 11:25:17.272418  2824 layer_factory.hpp:77] Creating layer Convolution10
I1006 11:25:17.272424  2824 net.cpp:84] Creating Layer Convolution10
I1006 11:25:17.272428  2824 net.cpp:406] Convolution10 <- Eltwise4_penlu9_0_split_0
I1006 11:25:17.272431  2824 net.cpp:380] Convolution10 -> Convolution10
I1006 11:25:17.273376  2824 net.cpp:122] Setting up Convolution10
I1006 11:25:17.273386  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.273389  2824 net.cpp:137] Memory required for data: 256838800
I1006 11:25:17.273404  2824 layer_factory.hpp:77] Creating layer BatchNorm10
I1006 11:25:17.273411  2824 net.cpp:84] Creating Layer BatchNorm10
I1006 11:25:17.273413  2824 net.cpp:406] BatchNorm10 <- Convolution10
I1006 11:25:17.273417  2824 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1006 11:25:17.273545  2824 net.cpp:122] Setting up BatchNorm10
I1006 11:25:17.273556  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.273560  2824 net.cpp:137] Memory required for data: 261856400
I1006 11:25:17.273574  2824 layer_factory.hpp:77] Creating layer Scale10
I1006 11:25:17.273581  2824 net.cpp:84] Creating Layer Scale10
I1006 11:25:17.273583  2824 net.cpp:406] Scale10 <- Convolution10
I1006 11:25:17.273586  2824 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1006 11:25:17.273622  2824 layer_factory.hpp:77] Creating layer Scale10
I1006 11:25:17.273701  2824 net.cpp:122] Setting up Scale10
I1006 11:25:17.273706  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.273710  2824 net.cpp:137] Memory required for data: 266874000
I1006 11:25:17.273713  2824 layer_factory.hpp:77] Creating layer penlu10
I1006 11:25:17.273720  2824 net.cpp:84] Creating Layer penlu10
I1006 11:25:17.273722  2824 net.cpp:406] penlu10 <- Convolution10
I1006 11:25:17.273725  2824 net.cpp:367] penlu10 -> Convolution10 (in-place)
I1006 11:25:17.273826  2824 net.cpp:122] Setting up penlu10
I1006 11:25:17.273831  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.273833  2824 net.cpp:137] Memory required for data: 271891600
I1006 11:25:17.273838  2824 layer_factory.hpp:77] Creating layer Convolution11
I1006 11:25:17.273844  2824 net.cpp:84] Creating Layer Convolution11
I1006 11:25:17.273847  2824 net.cpp:406] Convolution11 <- Convolution10
I1006 11:25:17.273851  2824 net.cpp:380] Convolution11 -> Convolution11
I1006 11:25:17.274790  2824 net.cpp:122] Setting up Convolution11
I1006 11:25:17.274801  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.274803  2824 net.cpp:137] Memory required for data: 276909200
I1006 11:25:17.274818  2824 layer_factory.hpp:77] Creating layer BatchNorm11
I1006 11:25:17.274826  2824 net.cpp:84] Creating Layer BatchNorm11
I1006 11:25:17.274828  2824 net.cpp:406] BatchNorm11 <- Convolution11
I1006 11:25:17.274832  2824 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1006 11:25:17.274986  2824 net.cpp:122] Setting up BatchNorm11
I1006 11:25:17.274991  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.274993  2824 net.cpp:137] Memory required for data: 281926800
I1006 11:25:17.275007  2824 layer_factory.hpp:77] Creating layer Scale11
I1006 11:25:17.275012  2824 net.cpp:84] Creating Layer Scale11
I1006 11:25:17.275015  2824 net.cpp:406] Scale11 <- Convolution11
I1006 11:25:17.275019  2824 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1006 11:25:17.275044  2824 layer_factory.hpp:77] Creating layer Scale11
I1006 11:25:17.275116  2824 net.cpp:122] Setting up Scale11
I1006 11:25:17.275121  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.275125  2824 net.cpp:137] Memory required for data: 286944400
I1006 11:25:17.275128  2824 layer_factory.hpp:77] Creating layer Eltwise5
I1006 11:25:17.275133  2824 net.cpp:84] Creating Layer Eltwise5
I1006 11:25:17.275136  2824 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1006 11:25:17.275140  2824 net.cpp:406] Eltwise5 <- Convolution11
I1006 11:25:17.275143  2824 net.cpp:380] Eltwise5 -> Eltwise5
I1006 11:25:17.275158  2824 net.cpp:122] Setting up Eltwise5
I1006 11:25:17.275173  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.275177  2824 net.cpp:137] Memory required for data: 291962000
I1006 11:25:17.275179  2824 layer_factory.hpp:77] Creating layer penlu11
I1006 11:25:17.275184  2824 net.cpp:84] Creating Layer penlu11
I1006 11:25:17.275187  2824 net.cpp:406] penlu11 <- Eltwise5
I1006 11:25:17.275192  2824 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1006 11:25:17.275297  2824 net.cpp:122] Setting up penlu11
I1006 11:25:17.275302  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.275305  2824 net.cpp:137] Memory required for data: 296979600
I1006 11:25:17.275310  2824 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1006 11:25:17.275315  2824 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1006 11:25:17.275317  2824 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1006 11:25:17.275328  2824 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1006 11:25:17.275334  2824 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1006 11:25:17.275357  2824 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1006 11:25:17.275362  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.275367  2824 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1006 11:25:17.275369  2824 net.cpp:137] Memory required for data: 307014800
I1006 11:25:17.275372  2824 layer_factory.hpp:77] Creating layer Convolution12
I1006 11:25:17.275377  2824 net.cpp:84] Creating Layer Convolution12
I1006 11:25:17.275382  2824 net.cpp:406] Convolution12 <- Eltwise5_penlu11_0_split_0
I1006 11:25:17.275385  2824 net.cpp:380] Convolution12 -> Convolution12
I1006 11:25:17.276526  2824 net.cpp:122] Setting up Convolution12
I1006 11:25:17.276537  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.276540  2824 net.cpp:137] Memory required for data: 309523600
I1006 11:25:17.276546  2824 layer_factory.hpp:77] Creating layer BatchNorm12
I1006 11:25:17.276551  2824 net.cpp:84] Creating Layer BatchNorm12
I1006 11:25:17.276556  2824 net.cpp:406] BatchNorm12 <- Convolution12
I1006 11:25:17.276559  2824 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1006 11:25:17.276687  2824 net.cpp:122] Setting up BatchNorm12
I1006 11:25:17.276693  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.276696  2824 net.cpp:137] Memory required for data: 312032400
I1006 11:25:17.276701  2824 layer_factory.hpp:77] Creating layer Scale12
I1006 11:25:17.276706  2824 net.cpp:84] Creating Layer Scale12
I1006 11:25:17.276710  2824 net.cpp:406] Scale12 <- Convolution12
I1006 11:25:17.276712  2824 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1006 11:25:17.276738  2824 layer_factory.hpp:77] Creating layer Scale12
I1006 11:25:17.276808  2824 net.cpp:122] Setting up Scale12
I1006 11:25:17.276813  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.276815  2824 net.cpp:137] Memory required for data: 314541200
I1006 11:25:17.276819  2824 layer_factory.hpp:77] Creating layer Convolution13
I1006 11:25:17.276826  2824 net.cpp:84] Creating Layer Convolution13
I1006 11:25:17.276830  2824 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_1
I1006 11:25:17.276834  2824 net.cpp:380] Convolution13 -> Convolution13
I1006 11:25:17.278060  2824 net.cpp:122] Setting up Convolution13
I1006 11:25:17.278070  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.278074  2824 net.cpp:137] Memory required for data: 317050000
I1006 11:25:17.278079  2824 layer_factory.hpp:77] Creating layer BatchNorm13
I1006 11:25:17.278084  2824 net.cpp:84] Creating Layer BatchNorm13
I1006 11:25:17.278087  2824 net.cpp:406] BatchNorm13 <- Convolution13
I1006 11:25:17.278091  2824 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1006 11:25:17.278218  2824 net.cpp:122] Setting up BatchNorm13
I1006 11:25:17.278224  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.278228  2824 net.cpp:137] Memory required for data: 319558800
I1006 11:25:17.278232  2824 layer_factory.hpp:77] Creating layer Scale13
I1006 11:25:17.278237  2824 net.cpp:84] Creating Layer Scale13
I1006 11:25:17.278240  2824 net.cpp:406] Scale13 <- Convolution13
I1006 11:25:17.278244  2824 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1006 11:25:17.278270  2824 layer_factory.hpp:77] Creating layer Scale13
I1006 11:25:17.278340  2824 net.cpp:122] Setting up Scale13
I1006 11:25:17.278345  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.278348  2824 net.cpp:137] Memory required for data: 322067600
I1006 11:25:17.278353  2824 layer_factory.hpp:77] Creating layer penlu12
I1006 11:25:17.278358  2824 net.cpp:84] Creating Layer penlu12
I1006 11:25:17.278362  2824 net.cpp:406] penlu12 <- Convolution13
I1006 11:25:17.278365  2824 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1006 11:25:17.278466  2824 net.cpp:122] Setting up penlu12
I1006 11:25:17.278471  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.278481  2824 net.cpp:137] Memory required for data: 324576400
I1006 11:25:17.278486  2824 layer_factory.hpp:77] Creating layer Convolution14
I1006 11:25:17.278492  2824 net.cpp:84] Creating Layer Convolution14
I1006 11:25:17.278496  2824 net.cpp:406] Convolution14 <- Convolution13
I1006 11:25:17.278501  2824 net.cpp:380] Convolution14 -> Convolution14
I1006 11:25:17.279533  2824 net.cpp:122] Setting up Convolution14
I1006 11:25:17.279543  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.279547  2824 net.cpp:137] Memory required for data: 327085200
I1006 11:25:17.279561  2824 layer_factory.hpp:77] Creating layer BatchNorm14
I1006 11:25:17.279568  2824 net.cpp:84] Creating Layer BatchNorm14
I1006 11:25:17.279572  2824 net.cpp:406] BatchNorm14 <- Convolution14
I1006 11:25:17.279577  2824 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1006 11:25:17.279701  2824 net.cpp:122] Setting up BatchNorm14
I1006 11:25:17.279706  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.279709  2824 net.cpp:137] Memory required for data: 329594000
I1006 11:25:17.279716  2824 layer_factory.hpp:77] Creating layer Scale14
I1006 11:25:17.279721  2824 net.cpp:84] Creating Layer Scale14
I1006 11:25:17.279723  2824 net.cpp:406] Scale14 <- Convolution14
I1006 11:25:17.279727  2824 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1006 11:25:17.279752  2824 layer_factory.hpp:77] Creating layer Scale14
I1006 11:25:17.279824  2824 net.cpp:122] Setting up Scale14
I1006 11:25:17.279829  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.279831  2824 net.cpp:137] Memory required for data: 332102800
I1006 11:25:17.279836  2824 layer_factory.hpp:77] Creating layer Eltwise6
I1006 11:25:17.279840  2824 net.cpp:84] Creating Layer Eltwise6
I1006 11:25:17.279844  2824 net.cpp:406] Eltwise6 <- Convolution12
I1006 11:25:17.279846  2824 net.cpp:406] Eltwise6 <- Convolution14
I1006 11:25:17.279850  2824 net.cpp:380] Eltwise6 -> Eltwise6
I1006 11:25:17.279865  2824 net.cpp:122] Setting up Eltwise6
I1006 11:25:17.279868  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.279870  2824 net.cpp:137] Memory required for data: 334611600
I1006 11:25:17.279872  2824 layer_factory.hpp:77] Creating layer penlu13
I1006 11:25:17.279877  2824 net.cpp:84] Creating Layer penlu13
I1006 11:25:17.279881  2824 net.cpp:406] penlu13 <- Eltwise6
I1006 11:25:17.279883  2824 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1006 11:25:17.279983  2824 net.cpp:122] Setting up penlu13
I1006 11:25:17.279988  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.279989  2824 net.cpp:137] Memory required for data: 337120400
I1006 11:25:17.279994  2824 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1006 11:25:17.279997  2824 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1006 11:25:17.280000  2824 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1006 11:25:17.280004  2824 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1006 11:25:17.280007  2824 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1006 11:25:17.280028  2824 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1006 11:25:17.280032  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.280035  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.280037  2824 net.cpp:137] Memory required for data: 342138000
I1006 11:25:17.280040  2824 layer_factory.hpp:77] Creating layer Convolution15
I1006 11:25:17.280045  2824 net.cpp:84] Creating Layer Convolution15
I1006 11:25:17.280048  2824 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1006 11:25:17.280052  2824 net.cpp:380] Convolution15 -> Convolution15
I1006 11:25:17.281059  2824 net.cpp:122] Setting up Convolution15
I1006 11:25:17.281069  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.281071  2824 net.cpp:137] Memory required for data: 344646800
I1006 11:25:17.281075  2824 layer_factory.hpp:77] Creating layer BatchNorm15
I1006 11:25:17.281080  2824 net.cpp:84] Creating Layer BatchNorm15
I1006 11:25:17.281090  2824 net.cpp:406] BatchNorm15 <- Convolution15
I1006 11:25:17.281095  2824 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1006 11:25:17.281217  2824 net.cpp:122] Setting up BatchNorm15
I1006 11:25:17.281222  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.281224  2824 net.cpp:137] Memory required for data: 347155600
I1006 11:25:17.281229  2824 layer_factory.hpp:77] Creating layer Scale15
I1006 11:25:17.281234  2824 net.cpp:84] Creating Layer Scale15
I1006 11:25:17.281236  2824 net.cpp:406] Scale15 <- Convolution15
I1006 11:25:17.281239  2824 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1006 11:25:17.281263  2824 layer_factory.hpp:77] Creating layer Scale15
I1006 11:25:17.281332  2824 net.cpp:122] Setting up Scale15
I1006 11:25:17.281337  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.281339  2824 net.cpp:137] Memory required for data: 349664400
I1006 11:25:17.281343  2824 layer_factory.hpp:77] Creating layer penlu14
I1006 11:25:17.281348  2824 net.cpp:84] Creating Layer penlu14
I1006 11:25:17.281350  2824 net.cpp:406] penlu14 <- Convolution15
I1006 11:25:17.281354  2824 net.cpp:367] penlu14 -> Convolution15 (in-place)
I1006 11:25:17.281452  2824 net.cpp:122] Setting up penlu14
I1006 11:25:17.281456  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.281460  2824 net.cpp:137] Memory required for data: 352173200
I1006 11:25:17.281463  2824 layer_factory.hpp:77] Creating layer Convolution16
I1006 11:25:17.281469  2824 net.cpp:84] Creating Layer Convolution16
I1006 11:25:17.281471  2824 net.cpp:406] Convolution16 <- Convolution15
I1006 11:25:17.281476  2824 net.cpp:380] Convolution16 -> Convolution16
I1006 11:25:17.282485  2824 net.cpp:122] Setting up Convolution16
I1006 11:25:17.282495  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.282496  2824 net.cpp:137] Memory required for data: 354682000
I1006 11:25:17.282502  2824 layer_factory.hpp:77] Creating layer BatchNorm16
I1006 11:25:17.282507  2824 net.cpp:84] Creating Layer BatchNorm16
I1006 11:25:17.282510  2824 net.cpp:406] BatchNorm16 <- Convolution16
I1006 11:25:17.282513  2824 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1006 11:25:17.282636  2824 net.cpp:122] Setting up BatchNorm16
I1006 11:25:17.282640  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.282642  2824 net.cpp:137] Memory required for data: 357190800
I1006 11:25:17.282647  2824 layer_factory.hpp:77] Creating layer Scale16
I1006 11:25:17.282651  2824 net.cpp:84] Creating Layer Scale16
I1006 11:25:17.282655  2824 net.cpp:406] Scale16 <- Convolution16
I1006 11:25:17.282657  2824 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1006 11:25:17.282681  2824 layer_factory.hpp:77] Creating layer Scale16
I1006 11:25:17.282752  2824 net.cpp:122] Setting up Scale16
I1006 11:25:17.282757  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.282758  2824 net.cpp:137] Memory required for data: 359699600
I1006 11:25:17.282763  2824 layer_factory.hpp:77] Creating layer Eltwise7
I1006 11:25:17.282766  2824 net.cpp:84] Creating Layer Eltwise7
I1006 11:25:17.282768  2824 net.cpp:406] Eltwise7 <- Eltwise6_penlu13_0_split_1
I1006 11:25:17.282771  2824 net.cpp:406] Eltwise7 <- Convolution16
I1006 11:25:17.282775  2824 net.cpp:380] Eltwise7 -> Eltwise7
I1006 11:25:17.282789  2824 net.cpp:122] Setting up Eltwise7
I1006 11:25:17.282793  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.282795  2824 net.cpp:137] Memory required for data: 362208400
I1006 11:25:17.282797  2824 layer_factory.hpp:77] Creating layer penlu15
I1006 11:25:17.282802  2824 net.cpp:84] Creating Layer penlu15
I1006 11:25:17.282804  2824 net.cpp:406] penlu15 <- Eltwise7
I1006 11:25:17.282809  2824 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1006 11:25:17.282908  2824 net.cpp:122] Setting up penlu15
I1006 11:25:17.282912  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.282914  2824 net.cpp:137] Memory required for data: 364717200
I1006 11:25:17.282919  2824 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1006 11:25:17.282929  2824 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1006 11:25:17.282932  2824 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1006 11:25:17.282935  2824 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1006 11:25:17.282939  2824 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1006 11:25:17.282963  2824 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1006 11:25:17.282966  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.282969  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.282971  2824 net.cpp:137] Memory required for data: 369734800
I1006 11:25:17.282974  2824 layer_factory.hpp:77] Creating layer Convolution17
I1006 11:25:17.282980  2824 net.cpp:84] Creating Layer Convolution17
I1006 11:25:17.282982  2824 net.cpp:406] Convolution17 <- Eltwise7_penlu15_0_split_0
I1006 11:25:17.282986  2824 net.cpp:380] Convolution17 -> Convolution17
I1006 11:25:17.283681  2824 net.cpp:122] Setting up Convolution17
I1006 11:25:17.283689  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.283691  2824 net.cpp:137] Memory required for data: 372243600
I1006 11:25:17.283695  2824 layer_factory.hpp:77] Creating layer BatchNorm17
I1006 11:25:17.283699  2824 net.cpp:84] Creating Layer BatchNorm17
I1006 11:25:17.283702  2824 net.cpp:406] BatchNorm17 <- Convolution17
I1006 11:25:17.283706  2824 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1006 11:25:17.283823  2824 net.cpp:122] Setting up BatchNorm17
I1006 11:25:17.283828  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.283829  2824 net.cpp:137] Memory required for data: 374752400
I1006 11:25:17.283834  2824 layer_factory.hpp:77] Creating layer Scale17
I1006 11:25:17.283838  2824 net.cpp:84] Creating Layer Scale17
I1006 11:25:17.283840  2824 net.cpp:406] Scale17 <- Convolution17
I1006 11:25:17.283844  2824 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1006 11:25:17.283867  2824 layer_factory.hpp:77] Creating layer Scale17
I1006 11:25:17.283956  2824 net.cpp:122] Setting up Scale17
I1006 11:25:17.283959  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.283962  2824 net.cpp:137] Memory required for data: 377261200
I1006 11:25:17.283965  2824 layer_factory.hpp:77] Creating layer penlu16
I1006 11:25:17.283970  2824 net.cpp:84] Creating Layer penlu16
I1006 11:25:17.283973  2824 net.cpp:406] penlu16 <- Convolution17
I1006 11:25:17.283977  2824 net.cpp:367] penlu16 -> Convolution17 (in-place)
I1006 11:25:17.284072  2824 net.cpp:122] Setting up penlu16
I1006 11:25:17.284076  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.284078  2824 net.cpp:137] Memory required for data: 379770000
I1006 11:25:17.284082  2824 layer_factory.hpp:77] Creating layer Convolution18
I1006 11:25:17.284088  2824 net.cpp:84] Creating Layer Convolution18
I1006 11:25:17.284090  2824 net.cpp:406] Convolution18 <- Convolution17
I1006 11:25:17.284095  2824 net.cpp:380] Convolution18 -> Convolution18
I1006 11:25:17.285079  2824 net.cpp:122] Setting up Convolution18
I1006 11:25:17.285086  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.285089  2824 net.cpp:137] Memory required for data: 382278800
I1006 11:25:17.285094  2824 layer_factory.hpp:77] Creating layer BatchNorm18
I1006 11:25:17.285099  2824 net.cpp:84] Creating Layer BatchNorm18
I1006 11:25:17.285101  2824 net.cpp:406] BatchNorm18 <- Convolution18
I1006 11:25:17.285105  2824 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1006 11:25:17.285224  2824 net.cpp:122] Setting up BatchNorm18
I1006 11:25:17.285228  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.285230  2824 net.cpp:137] Memory required for data: 384787600
I1006 11:25:17.285235  2824 layer_factory.hpp:77] Creating layer Scale18
I1006 11:25:17.285239  2824 net.cpp:84] Creating Layer Scale18
I1006 11:25:17.285241  2824 net.cpp:406] Scale18 <- Convolution18
I1006 11:25:17.285244  2824 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1006 11:25:17.285276  2824 layer_factory.hpp:77] Creating layer Scale18
I1006 11:25:17.285346  2824 net.cpp:122] Setting up Scale18
I1006 11:25:17.285349  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.285351  2824 net.cpp:137] Memory required for data: 387296400
I1006 11:25:17.285356  2824 layer_factory.hpp:77] Creating layer Eltwise8
I1006 11:25:17.285359  2824 net.cpp:84] Creating Layer Eltwise8
I1006 11:25:17.285362  2824 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1006 11:25:17.285365  2824 net.cpp:406] Eltwise8 <- Convolution18
I1006 11:25:17.285368  2824 net.cpp:380] Eltwise8 -> Eltwise8
I1006 11:25:17.285382  2824 net.cpp:122] Setting up Eltwise8
I1006 11:25:17.285385  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.285387  2824 net.cpp:137] Memory required for data: 389805200
I1006 11:25:17.285389  2824 layer_factory.hpp:77] Creating layer penlu17
I1006 11:25:17.285394  2824 net.cpp:84] Creating Layer penlu17
I1006 11:25:17.285398  2824 net.cpp:406] penlu17 <- Eltwise8
I1006 11:25:17.285400  2824 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1006 11:25:17.285496  2824 net.cpp:122] Setting up penlu17
I1006 11:25:17.285501  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.285503  2824 net.cpp:137] Memory required for data: 392314000
I1006 11:25:17.285507  2824 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1006 11:25:17.285511  2824 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1006 11:25:17.285513  2824 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1006 11:25:17.285516  2824 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1006 11:25:17.285521  2824 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1006 11:25:17.285542  2824 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1006 11:25:17.285544  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.285547  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.285549  2824 net.cpp:137] Memory required for data: 397331600
I1006 11:25:17.285552  2824 layer_factory.hpp:77] Creating layer Convolution19
I1006 11:25:17.285557  2824 net.cpp:84] Creating Layer Convolution19
I1006 11:25:17.285560  2824 net.cpp:406] Convolution19 <- Eltwise8_penlu17_0_split_0
I1006 11:25:17.285564  2824 net.cpp:380] Convolution19 -> Convolution19
I1006 11:25:17.286870  2824 net.cpp:122] Setting up Convolution19
I1006 11:25:17.286880  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.286882  2824 net.cpp:137] Memory required for data: 399840400
I1006 11:25:17.286887  2824 layer_factory.hpp:77] Creating layer BatchNorm19
I1006 11:25:17.286891  2824 net.cpp:84] Creating Layer BatchNorm19
I1006 11:25:17.286895  2824 net.cpp:406] BatchNorm19 <- Convolution19
I1006 11:25:17.286898  2824 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1006 11:25:17.287034  2824 net.cpp:122] Setting up BatchNorm19
I1006 11:25:17.287039  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.287040  2824 net.cpp:137] Memory required for data: 402349200
I1006 11:25:17.287045  2824 layer_factory.hpp:77] Creating layer Scale19
I1006 11:25:17.287050  2824 net.cpp:84] Creating Layer Scale19
I1006 11:25:17.287052  2824 net.cpp:406] Scale19 <- Convolution19
I1006 11:25:17.287055  2824 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1006 11:25:17.287081  2824 layer_factory.hpp:77] Creating layer Scale19
I1006 11:25:17.287159  2824 net.cpp:122] Setting up Scale19
I1006 11:25:17.287168  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.287173  2824 net.cpp:137] Memory required for data: 404858000
I1006 11:25:17.287176  2824 layer_factory.hpp:77] Creating layer penlu18
I1006 11:25:17.287181  2824 net.cpp:84] Creating Layer penlu18
I1006 11:25:17.287184  2824 net.cpp:406] penlu18 <- Convolution19
I1006 11:25:17.287187  2824 net.cpp:367] penlu18 -> Convolution19 (in-place)
I1006 11:25:17.287286  2824 net.cpp:122] Setting up penlu18
I1006 11:25:17.287289  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.287292  2824 net.cpp:137] Memory required for data: 407366800
I1006 11:25:17.287303  2824 layer_factory.hpp:77] Creating layer Convolution20
I1006 11:25:17.287310  2824 net.cpp:84] Creating Layer Convolution20
I1006 11:25:17.287312  2824 net.cpp:406] Convolution20 <- Convolution19
I1006 11:25:17.287317  2824 net.cpp:380] Convolution20 -> Convolution20
I1006 11:25:17.288362  2824 net.cpp:122] Setting up Convolution20
I1006 11:25:17.288374  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.288379  2824 net.cpp:137] Memory required for data: 409875600
I1006 11:25:17.288388  2824 layer_factory.hpp:77] Creating layer BatchNorm20
I1006 11:25:17.288395  2824 net.cpp:84] Creating Layer BatchNorm20
I1006 11:25:17.288399  2824 net.cpp:406] BatchNorm20 <- Convolution20
I1006 11:25:17.288405  2824 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1006 11:25:17.288554  2824 net.cpp:122] Setting up BatchNorm20
I1006 11:25:17.288561  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.288563  2824 net.cpp:137] Memory required for data: 412384400
I1006 11:25:17.288569  2824 layer_factory.hpp:77] Creating layer Scale20
I1006 11:25:17.288573  2824 net.cpp:84] Creating Layer Scale20
I1006 11:25:17.288576  2824 net.cpp:406] Scale20 <- Convolution20
I1006 11:25:17.288579  2824 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1006 11:25:17.288619  2824 layer_factory.hpp:77] Creating layer Scale20
I1006 11:25:17.288707  2824 net.cpp:122] Setting up Scale20
I1006 11:25:17.288712  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.288715  2824 net.cpp:137] Memory required for data: 414893200
I1006 11:25:17.288718  2824 layer_factory.hpp:77] Creating layer Eltwise9
I1006 11:25:17.288722  2824 net.cpp:84] Creating Layer Eltwise9
I1006 11:25:17.288725  2824 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1006 11:25:17.288728  2824 net.cpp:406] Eltwise9 <- Convolution20
I1006 11:25:17.288731  2824 net.cpp:380] Eltwise9 -> Eltwise9
I1006 11:25:17.288746  2824 net.cpp:122] Setting up Eltwise9
I1006 11:25:17.288749  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.288751  2824 net.cpp:137] Memory required for data: 417402000
I1006 11:25:17.288754  2824 layer_factory.hpp:77] Creating layer penlu19
I1006 11:25:17.288758  2824 net.cpp:84] Creating Layer penlu19
I1006 11:25:17.288761  2824 net.cpp:406] penlu19 <- Eltwise9
I1006 11:25:17.288764  2824 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1006 11:25:17.288861  2824 net.cpp:122] Setting up penlu19
I1006 11:25:17.288864  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.288866  2824 net.cpp:137] Memory required for data: 419910800
I1006 11:25:17.288872  2824 layer_factory.hpp:77] Creating layer Eltwise9_penlu19_0_split
I1006 11:25:17.288874  2824 net.cpp:84] Creating Layer Eltwise9_penlu19_0_split
I1006 11:25:17.288877  2824 net.cpp:406] Eltwise9_penlu19_0_split <- Eltwise9
I1006 11:25:17.288880  2824 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_0
I1006 11:25:17.288884  2824 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_1
I1006 11:25:17.288904  2824 net.cpp:122] Setting up Eltwise9_penlu19_0_split
I1006 11:25:17.288908  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.288911  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.288913  2824 net.cpp:137] Memory required for data: 424928400
I1006 11:25:17.288915  2824 layer_factory.hpp:77] Creating layer Convolution21
I1006 11:25:17.288920  2824 net.cpp:84] Creating Layer Convolution21
I1006 11:25:17.288923  2824 net.cpp:406] Convolution21 <- Eltwise9_penlu19_0_split_0
I1006 11:25:17.288928  2824 net.cpp:380] Convolution21 -> Convolution21
I1006 11:25:17.290343  2824 net.cpp:122] Setting up Convolution21
I1006 11:25:17.290352  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.290355  2824 net.cpp:137] Memory required for data: 427437200
I1006 11:25:17.290359  2824 layer_factory.hpp:77] Creating layer BatchNorm21
I1006 11:25:17.290364  2824 net.cpp:84] Creating Layer BatchNorm21
I1006 11:25:17.290374  2824 net.cpp:406] BatchNorm21 <- Convolution21
I1006 11:25:17.290377  2824 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1006 11:25:17.290504  2824 net.cpp:122] Setting up BatchNorm21
I1006 11:25:17.290508  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.290511  2824 net.cpp:137] Memory required for data: 429946000
I1006 11:25:17.290515  2824 layer_factory.hpp:77] Creating layer Scale21
I1006 11:25:17.290519  2824 net.cpp:84] Creating Layer Scale21
I1006 11:25:17.290522  2824 net.cpp:406] Scale21 <- Convolution21
I1006 11:25:17.290525  2824 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1006 11:25:17.290549  2824 layer_factory.hpp:77] Creating layer Scale21
I1006 11:25:17.290619  2824 net.cpp:122] Setting up Scale21
I1006 11:25:17.290623  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.290626  2824 net.cpp:137] Memory required for data: 432454800
I1006 11:25:17.290629  2824 layer_factory.hpp:77] Creating layer penlu20
I1006 11:25:17.290634  2824 net.cpp:84] Creating Layer penlu20
I1006 11:25:17.290637  2824 net.cpp:406] penlu20 <- Convolution21
I1006 11:25:17.290640  2824 net.cpp:367] penlu20 -> Convolution21 (in-place)
I1006 11:25:17.290740  2824 net.cpp:122] Setting up penlu20
I1006 11:25:17.290745  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.290746  2824 net.cpp:137] Memory required for data: 434963600
I1006 11:25:17.290750  2824 layer_factory.hpp:77] Creating layer Convolution22
I1006 11:25:17.290756  2824 net.cpp:84] Creating Layer Convolution22
I1006 11:25:17.290760  2824 net.cpp:406] Convolution22 <- Convolution21
I1006 11:25:17.290763  2824 net.cpp:380] Convolution22 -> Convolution22
I1006 11:25:17.291831  2824 net.cpp:122] Setting up Convolution22
I1006 11:25:17.291841  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.291843  2824 net.cpp:137] Memory required for data: 437472400
I1006 11:25:17.291847  2824 layer_factory.hpp:77] Creating layer BatchNorm22
I1006 11:25:17.291852  2824 net.cpp:84] Creating Layer BatchNorm22
I1006 11:25:17.291856  2824 net.cpp:406] BatchNorm22 <- Convolution22
I1006 11:25:17.291867  2824 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I1006 11:25:17.291990  2824 net.cpp:122] Setting up BatchNorm22
I1006 11:25:17.291993  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.291996  2824 net.cpp:137] Memory required for data: 439981200
I1006 11:25:17.292001  2824 layer_factory.hpp:77] Creating layer Scale22
I1006 11:25:17.292004  2824 net.cpp:84] Creating Layer Scale22
I1006 11:25:17.292007  2824 net.cpp:406] Scale22 <- Convolution22
I1006 11:25:17.292011  2824 net.cpp:367] Scale22 -> Convolution22 (in-place)
I1006 11:25:17.292033  2824 layer_factory.hpp:77] Creating layer Scale22
I1006 11:25:17.292102  2824 net.cpp:122] Setting up Scale22
I1006 11:25:17.292106  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.292109  2824 net.cpp:137] Memory required for data: 442490000
I1006 11:25:17.292112  2824 layer_factory.hpp:77] Creating layer Eltwise10
I1006 11:25:17.292116  2824 net.cpp:84] Creating Layer Eltwise10
I1006 11:25:17.292119  2824 net.cpp:406] Eltwise10 <- Eltwise9_penlu19_0_split_1
I1006 11:25:17.292121  2824 net.cpp:406] Eltwise10 <- Convolution22
I1006 11:25:17.292124  2824 net.cpp:380] Eltwise10 -> Eltwise10
I1006 11:25:17.292138  2824 net.cpp:122] Setting up Eltwise10
I1006 11:25:17.292142  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.292145  2824 net.cpp:137] Memory required for data: 444998800
I1006 11:25:17.292146  2824 layer_factory.hpp:77] Creating layer penlu21
I1006 11:25:17.292151  2824 net.cpp:84] Creating Layer penlu21
I1006 11:25:17.292153  2824 net.cpp:406] penlu21 <- Eltwise10
I1006 11:25:17.292156  2824 net.cpp:367] penlu21 -> Eltwise10 (in-place)
I1006 11:25:17.292253  2824 net.cpp:122] Setting up penlu21
I1006 11:25:17.292258  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.292259  2824 net.cpp:137] Memory required for data: 447507600
I1006 11:25:17.292263  2824 layer_factory.hpp:77] Creating layer Eltwise10_penlu21_0_split
I1006 11:25:17.292274  2824 net.cpp:84] Creating Layer Eltwise10_penlu21_0_split
I1006 11:25:17.292276  2824 net.cpp:406] Eltwise10_penlu21_0_split <- Eltwise10
I1006 11:25:17.292280  2824 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_0
I1006 11:25:17.292284  2824 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_1
I1006 11:25:17.292306  2824 net.cpp:122] Setting up Eltwise10_penlu21_0_split
I1006 11:25:17.292310  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.292313  2824 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1006 11:25:17.292315  2824 net.cpp:137] Memory required for data: 452525200
I1006 11:25:17.292317  2824 layer_factory.hpp:77] Creating layer Convolution23
I1006 11:25:17.292323  2824 net.cpp:84] Creating Layer Convolution23
I1006 11:25:17.292325  2824 net.cpp:406] Convolution23 <- Eltwise10_penlu21_0_split_0
I1006 11:25:17.292330  2824 net.cpp:380] Convolution23 -> Convolution23
I1006 11:25:17.293153  2824 net.cpp:122] Setting up Convolution23
I1006 11:25:17.293161  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.293164  2824 net.cpp:137] Memory required for data: 453779600
I1006 11:25:17.293169  2824 layer_factory.hpp:77] Creating layer BatchNorm23
I1006 11:25:17.293174  2824 net.cpp:84] Creating Layer BatchNorm23
I1006 11:25:17.293175  2824 net.cpp:406] BatchNorm23 <- Convolution23
I1006 11:25:17.293179  2824 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I1006 11:25:17.293298  2824 net.cpp:122] Setting up BatchNorm23
I1006 11:25:17.293303  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.293304  2824 net.cpp:137] Memory required for data: 455034000
I1006 11:25:17.293309  2824 layer_factory.hpp:77] Creating layer Scale23
I1006 11:25:17.293313  2824 net.cpp:84] Creating Layer Scale23
I1006 11:25:17.293315  2824 net.cpp:406] Scale23 <- Convolution23
I1006 11:25:17.293318  2824 net.cpp:367] Scale23 -> Convolution23 (in-place)
I1006 11:25:17.293342  2824 layer_factory.hpp:77] Creating layer Scale23
I1006 11:25:17.293411  2824 net.cpp:122] Setting up Scale23
I1006 11:25:17.293414  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.293416  2824 net.cpp:137] Memory required for data: 456288400
I1006 11:25:17.293421  2824 layer_factory.hpp:77] Creating layer Convolution24
I1006 11:25:17.293426  2824 net.cpp:84] Creating Layer Convolution24
I1006 11:25:17.293428  2824 net.cpp:406] Convolution24 <- Eltwise10_penlu21_0_split_1
I1006 11:25:17.293432  2824 net.cpp:380] Convolution24 -> Convolution24
I1006 11:25:17.295104  2824 net.cpp:122] Setting up Convolution24
I1006 11:25:17.295112  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.295115  2824 net.cpp:137] Memory required for data: 457542800
I1006 11:25:17.295120  2824 layer_factory.hpp:77] Creating layer BatchNorm24
I1006 11:25:17.295125  2824 net.cpp:84] Creating Layer BatchNorm24
I1006 11:25:17.295127  2824 net.cpp:406] BatchNorm24 <- Convolution24
I1006 11:25:17.295130  2824 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I1006 11:25:17.295289  2824 net.cpp:122] Setting up BatchNorm24
I1006 11:25:17.295295  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.295297  2824 net.cpp:137] Memory required for data: 458797200
I1006 11:25:17.295301  2824 layer_factory.hpp:77] Creating layer Scale24
I1006 11:25:17.295306  2824 net.cpp:84] Creating Layer Scale24
I1006 11:25:17.295308  2824 net.cpp:406] Scale24 <- Convolution24
I1006 11:25:17.295311  2824 net.cpp:367] Scale24 -> Convolution24 (in-place)
I1006 11:25:17.295336  2824 layer_factory.hpp:77] Creating layer Scale24
I1006 11:25:17.295405  2824 net.cpp:122] Setting up Scale24
I1006 11:25:17.295409  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.295411  2824 net.cpp:137] Memory required for data: 460051600
I1006 11:25:17.295415  2824 layer_factory.hpp:77] Creating layer penlu22
I1006 11:25:17.295420  2824 net.cpp:84] Creating Layer penlu22
I1006 11:25:17.295423  2824 net.cpp:406] penlu22 <- Convolution24
I1006 11:25:17.295426  2824 net.cpp:367] penlu22 -> Convolution24 (in-place)
I1006 11:25:17.295532  2824 net.cpp:122] Setting up penlu22
I1006 11:25:17.295536  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.295539  2824 net.cpp:137] Memory required for data: 461306000
I1006 11:25:17.295543  2824 layer_factory.hpp:77] Creating layer Convolution25
I1006 11:25:17.295549  2824 net.cpp:84] Creating Layer Convolution25
I1006 11:25:17.295552  2824 net.cpp:406] Convolution25 <- Convolution24
I1006 11:25:17.295557  2824 net.cpp:380] Convolution25 -> Convolution25
I1006 11:25:17.297446  2824 net.cpp:122] Setting up Convolution25
I1006 11:25:17.297453  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.297456  2824 net.cpp:137] Memory required for data: 462560400
I1006 11:25:17.297461  2824 layer_factory.hpp:77] Creating layer BatchNorm25
I1006 11:25:17.297466  2824 net.cpp:84] Creating Layer BatchNorm25
I1006 11:25:17.297468  2824 net.cpp:406] BatchNorm25 <- Convolution25
I1006 11:25:17.297472  2824 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I1006 11:25:17.297596  2824 net.cpp:122] Setting up BatchNorm25
I1006 11:25:17.297601  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.297603  2824 net.cpp:137] Memory required for data: 463814800
I1006 11:25:17.297608  2824 layer_factory.hpp:77] Creating layer Scale25
I1006 11:25:17.297612  2824 net.cpp:84] Creating Layer Scale25
I1006 11:25:17.297614  2824 net.cpp:406] Scale25 <- Convolution25
I1006 11:25:17.297617  2824 net.cpp:367] Scale25 -> Convolution25 (in-place)
I1006 11:25:17.297642  2824 layer_factory.hpp:77] Creating layer Scale25
I1006 11:25:17.297711  2824 net.cpp:122] Setting up Scale25
I1006 11:25:17.297716  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.297719  2824 net.cpp:137] Memory required for data: 465069200
I1006 11:25:17.297722  2824 layer_factory.hpp:77] Creating layer Eltwise11
I1006 11:25:17.297726  2824 net.cpp:84] Creating Layer Eltwise11
I1006 11:25:17.297729  2824 net.cpp:406] Eltwise11 <- Convolution23
I1006 11:25:17.297731  2824 net.cpp:406] Eltwise11 <- Convolution25
I1006 11:25:17.297734  2824 net.cpp:380] Eltwise11 -> Eltwise11
I1006 11:25:17.297749  2824 net.cpp:122] Setting up Eltwise11
I1006 11:25:17.297754  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.297755  2824 net.cpp:137] Memory required for data: 466323600
I1006 11:25:17.297757  2824 layer_factory.hpp:77] Creating layer penlu23
I1006 11:25:17.297762  2824 net.cpp:84] Creating Layer penlu23
I1006 11:25:17.297765  2824 net.cpp:406] penlu23 <- Eltwise11
I1006 11:25:17.297767  2824 net.cpp:367] penlu23 -> Eltwise11 (in-place)
I1006 11:25:17.297866  2824 net.cpp:122] Setting up penlu23
I1006 11:25:17.297870  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.297873  2824 net.cpp:137] Memory required for data: 467578000
I1006 11:25:17.297878  2824 layer_factory.hpp:77] Creating layer Eltwise11_penlu23_0_split
I1006 11:25:17.297880  2824 net.cpp:84] Creating Layer Eltwise11_penlu23_0_split
I1006 11:25:17.297883  2824 net.cpp:406] Eltwise11_penlu23_0_split <- Eltwise11
I1006 11:25:17.297886  2824 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_0
I1006 11:25:17.297890  2824 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_1
I1006 11:25:17.297911  2824 net.cpp:122] Setting up Eltwise11_penlu23_0_split
I1006 11:25:17.297914  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.297917  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.297919  2824 net.cpp:137] Memory required for data: 470086800
I1006 11:25:17.297921  2824 layer_factory.hpp:77] Creating layer Convolution26
I1006 11:25:17.297927  2824 net.cpp:84] Creating Layer Convolution26
I1006 11:25:17.297930  2824 net.cpp:406] Convolution26 <- Eltwise11_penlu23_0_split_0
I1006 11:25:17.297935  2824 net.cpp:380] Convolution26 -> Convolution26
I1006 11:25:17.299536  2824 net.cpp:122] Setting up Convolution26
I1006 11:25:17.299545  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.299547  2824 net.cpp:137] Memory required for data: 471341200
I1006 11:25:17.299558  2824 layer_factory.hpp:77] Creating layer BatchNorm26
I1006 11:25:17.299563  2824 net.cpp:84] Creating Layer BatchNorm26
I1006 11:25:17.299566  2824 net.cpp:406] BatchNorm26 <- Convolution26
I1006 11:25:17.299569  2824 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I1006 11:25:17.299691  2824 net.cpp:122] Setting up BatchNorm26
I1006 11:25:17.299695  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.299698  2824 net.cpp:137] Memory required for data: 472595600
I1006 11:25:17.299702  2824 layer_factory.hpp:77] Creating layer Scale26
I1006 11:25:17.299706  2824 net.cpp:84] Creating Layer Scale26
I1006 11:25:17.299710  2824 net.cpp:406] Scale26 <- Convolution26
I1006 11:25:17.299712  2824 net.cpp:367] Scale26 -> Convolution26 (in-place)
I1006 11:25:17.299736  2824 layer_factory.hpp:77] Creating layer Scale26
I1006 11:25:17.299804  2824 net.cpp:122] Setting up Scale26
I1006 11:25:17.299808  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.299811  2824 net.cpp:137] Memory required for data: 473850000
I1006 11:25:17.299814  2824 layer_factory.hpp:77] Creating layer penlu24
I1006 11:25:17.299819  2824 net.cpp:84] Creating Layer penlu24
I1006 11:25:17.299823  2824 net.cpp:406] penlu24 <- Convolution26
I1006 11:25:17.299825  2824 net.cpp:367] penlu24 -> Convolution26 (in-place)
I1006 11:25:17.299923  2824 net.cpp:122] Setting up penlu24
I1006 11:25:17.299927  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.299929  2824 net.cpp:137] Memory required for data: 475104400
I1006 11:25:17.299933  2824 layer_factory.hpp:77] Creating layer Convolution27
I1006 11:25:17.299940  2824 net.cpp:84] Creating Layer Convolution27
I1006 11:25:17.299942  2824 net.cpp:406] Convolution27 <- Convolution26
I1006 11:25:17.299947  2824 net.cpp:380] Convolution27 -> Convolution27
I1006 11:25:17.301501  2824 net.cpp:122] Setting up Convolution27
I1006 11:25:17.301509  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.301512  2824 net.cpp:137] Memory required for data: 476358800
I1006 11:25:17.301517  2824 layer_factory.hpp:77] Creating layer BatchNorm27
I1006 11:25:17.301522  2824 net.cpp:84] Creating Layer BatchNorm27
I1006 11:25:17.301524  2824 net.cpp:406] BatchNorm27 <- Convolution27
I1006 11:25:17.301528  2824 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I1006 11:25:17.301654  2824 net.cpp:122] Setting up BatchNorm27
I1006 11:25:17.301658  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.301661  2824 net.cpp:137] Memory required for data: 477613200
I1006 11:25:17.301681  2824 layer_factory.hpp:77] Creating layer Scale27
I1006 11:25:17.301692  2824 net.cpp:84] Creating Layer Scale27
I1006 11:25:17.301693  2824 net.cpp:406] Scale27 <- Convolution27
I1006 11:25:17.301697  2824 net.cpp:367] Scale27 -> Convolution27 (in-place)
I1006 11:25:17.301723  2824 layer_factory.hpp:77] Creating layer Scale27
I1006 11:25:17.301793  2824 net.cpp:122] Setting up Scale27
I1006 11:25:17.301797  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.301800  2824 net.cpp:137] Memory required for data: 478867600
I1006 11:25:17.301803  2824 layer_factory.hpp:77] Creating layer Eltwise12
I1006 11:25:17.301807  2824 net.cpp:84] Creating Layer Eltwise12
I1006 11:25:17.301810  2824 net.cpp:406] Eltwise12 <- Eltwise11_penlu23_0_split_1
I1006 11:25:17.301812  2824 net.cpp:406] Eltwise12 <- Convolution27
I1006 11:25:17.301816  2824 net.cpp:380] Eltwise12 -> Eltwise12
I1006 11:25:17.301834  2824 net.cpp:122] Setting up Eltwise12
I1006 11:25:17.301838  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.301841  2824 net.cpp:137] Memory required for data: 480122000
I1006 11:25:17.301842  2824 layer_factory.hpp:77] Creating layer penlu25
I1006 11:25:17.301847  2824 net.cpp:84] Creating Layer penlu25
I1006 11:25:17.301849  2824 net.cpp:406] penlu25 <- Eltwise12
I1006 11:25:17.301853  2824 net.cpp:367] penlu25 -> Eltwise12 (in-place)
I1006 11:25:17.301954  2824 net.cpp:122] Setting up penlu25
I1006 11:25:17.301957  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.301965  2824 net.cpp:137] Memory required for data: 481376400
I1006 11:25:17.301970  2824 layer_factory.hpp:77] Creating layer Eltwise12_penlu25_0_split
I1006 11:25:17.301973  2824 net.cpp:84] Creating Layer Eltwise12_penlu25_0_split
I1006 11:25:17.301975  2824 net.cpp:406] Eltwise12_penlu25_0_split <- Eltwise12
I1006 11:25:17.301980  2824 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_0
I1006 11:25:17.301983  2824 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_1
I1006 11:25:17.302006  2824 net.cpp:122] Setting up Eltwise12_penlu25_0_split
I1006 11:25:17.302009  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.302012  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.302014  2824 net.cpp:137] Memory required for data: 483885200
I1006 11:25:17.302016  2824 layer_factory.hpp:77] Creating layer Convolution28
I1006 11:25:17.302022  2824 net.cpp:84] Creating Layer Convolution28
I1006 11:25:17.302024  2824 net.cpp:406] Convolution28 <- Eltwise12_penlu25_0_split_0
I1006 11:25:17.302028  2824 net.cpp:380] Convolution28 -> Convolution28
I1006 11:25:17.303622  2824 net.cpp:122] Setting up Convolution28
I1006 11:25:17.303632  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.303634  2824 net.cpp:137] Memory required for data: 485139600
I1006 11:25:17.303638  2824 layer_factory.hpp:77] Creating layer BatchNorm28
I1006 11:25:17.303643  2824 net.cpp:84] Creating Layer BatchNorm28
I1006 11:25:17.303647  2824 net.cpp:406] BatchNorm28 <- Convolution28
I1006 11:25:17.303650  2824 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I1006 11:25:17.303777  2824 net.cpp:122] Setting up BatchNorm28
I1006 11:25:17.303781  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.303783  2824 net.cpp:137] Memory required for data: 486394000
I1006 11:25:17.303788  2824 layer_factory.hpp:77] Creating layer Scale28
I1006 11:25:17.303792  2824 net.cpp:84] Creating Layer Scale28
I1006 11:25:17.303794  2824 net.cpp:406] Scale28 <- Convolution28
I1006 11:25:17.303797  2824 net.cpp:367] Scale28 -> Convolution28 (in-place)
I1006 11:25:17.303822  2824 layer_factory.hpp:77] Creating layer Scale28
I1006 11:25:17.303894  2824 net.cpp:122] Setting up Scale28
I1006 11:25:17.303897  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.303900  2824 net.cpp:137] Memory required for data: 487648400
I1006 11:25:17.303903  2824 layer_factory.hpp:77] Creating layer penlu26
I1006 11:25:17.303908  2824 net.cpp:84] Creating Layer penlu26
I1006 11:25:17.303910  2824 net.cpp:406] penlu26 <- Convolution28
I1006 11:25:17.303915  2824 net.cpp:367] penlu26 -> Convolution28 (in-place)
I1006 11:25:17.304018  2824 net.cpp:122] Setting up penlu26
I1006 11:25:17.304021  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.304023  2824 net.cpp:137] Memory required for data: 488902800
I1006 11:25:17.304028  2824 layer_factory.hpp:77] Creating layer Convolution29
I1006 11:25:17.304034  2824 net.cpp:84] Creating Layer Convolution29
I1006 11:25:17.304036  2824 net.cpp:406] Convolution29 <- Convolution28
I1006 11:25:17.304040  2824 net.cpp:380] Convolution29 -> Convolution29
I1006 11:25:17.305923  2824 net.cpp:122] Setting up Convolution29
I1006 11:25:17.305932  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.305935  2824 net.cpp:137] Memory required for data: 490157200
I1006 11:25:17.305939  2824 layer_factory.hpp:77] Creating layer BatchNorm29
I1006 11:25:17.305944  2824 net.cpp:84] Creating Layer BatchNorm29
I1006 11:25:17.305946  2824 net.cpp:406] BatchNorm29 <- Convolution29
I1006 11:25:17.305950  2824 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I1006 11:25:17.306080  2824 net.cpp:122] Setting up BatchNorm29
I1006 11:25:17.306084  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.306087  2824 net.cpp:137] Memory required for data: 491411600
I1006 11:25:17.306092  2824 layer_factory.hpp:77] Creating layer Scale29
I1006 11:25:17.306095  2824 net.cpp:84] Creating Layer Scale29
I1006 11:25:17.306104  2824 net.cpp:406] Scale29 <- Convolution29
I1006 11:25:17.306107  2824 net.cpp:367] Scale29 -> Convolution29 (in-place)
I1006 11:25:17.306134  2824 layer_factory.hpp:77] Creating layer Scale29
I1006 11:25:17.306205  2824 net.cpp:122] Setting up Scale29
I1006 11:25:17.306210  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.306212  2824 net.cpp:137] Memory required for data: 492666000
I1006 11:25:17.306216  2824 layer_factory.hpp:77] Creating layer Eltwise13
I1006 11:25:17.306219  2824 net.cpp:84] Creating Layer Eltwise13
I1006 11:25:17.306222  2824 net.cpp:406] Eltwise13 <- Eltwise12_penlu25_0_split_1
I1006 11:25:17.306226  2824 net.cpp:406] Eltwise13 <- Convolution29
I1006 11:25:17.306228  2824 net.cpp:380] Eltwise13 -> Eltwise13
I1006 11:25:17.306243  2824 net.cpp:122] Setting up Eltwise13
I1006 11:25:17.306247  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.306249  2824 net.cpp:137] Memory required for data: 493920400
I1006 11:25:17.306252  2824 layer_factory.hpp:77] Creating layer penlu27
I1006 11:25:17.306257  2824 net.cpp:84] Creating Layer penlu27
I1006 11:25:17.306258  2824 net.cpp:406] penlu27 <- Eltwise13
I1006 11:25:17.306262  2824 net.cpp:367] penlu27 -> Eltwise13 (in-place)
I1006 11:25:17.306363  2824 net.cpp:122] Setting up penlu27
I1006 11:25:17.306367  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.306370  2824 net.cpp:137] Memory required for data: 495174800
I1006 11:25:17.306375  2824 layer_factory.hpp:77] Creating layer Eltwise13_penlu27_0_split
I1006 11:25:17.306377  2824 net.cpp:84] Creating Layer Eltwise13_penlu27_0_split
I1006 11:25:17.306380  2824 net.cpp:406] Eltwise13_penlu27_0_split <- Eltwise13
I1006 11:25:17.306383  2824 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_0
I1006 11:25:17.306387  2824 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_1
I1006 11:25:17.306408  2824 net.cpp:122] Setting up Eltwise13_penlu27_0_split
I1006 11:25:17.306412  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.306416  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.306417  2824 net.cpp:137] Memory required for data: 497683600
I1006 11:25:17.306419  2824 layer_factory.hpp:77] Creating layer Convolution30
I1006 11:25:17.306426  2824 net.cpp:84] Creating Layer Convolution30
I1006 11:25:17.306427  2824 net.cpp:406] Convolution30 <- Eltwise13_penlu27_0_split_0
I1006 11:25:17.306432  2824 net.cpp:380] Convolution30 -> Convolution30
I1006 11:25:17.308045  2824 net.cpp:122] Setting up Convolution30
I1006 11:25:17.308054  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.308058  2824 net.cpp:137] Memory required for data: 498938000
I1006 11:25:17.308061  2824 layer_factory.hpp:77] Creating layer BatchNorm30
I1006 11:25:17.308066  2824 net.cpp:84] Creating Layer BatchNorm30
I1006 11:25:17.308069  2824 net.cpp:406] BatchNorm30 <- Convolution30
I1006 11:25:17.308073  2824 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I1006 11:25:17.308199  2824 net.cpp:122] Setting up BatchNorm30
I1006 11:25:17.308204  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.308207  2824 net.cpp:137] Memory required for data: 500192400
I1006 11:25:17.308212  2824 layer_factory.hpp:77] Creating layer Scale30
I1006 11:25:17.308215  2824 net.cpp:84] Creating Layer Scale30
I1006 11:25:17.308218  2824 net.cpp:406] Scale30 <- Convolution30
I1006 11:25:17.308220  2824 net.cpp:367] Scale30 -> Convolution30 (in-place)
I1006 11:25:17.308245  2824 layer_factory.hpp:77] Creating layer Scale30
I1006 11:25:17.308317  2824 net.cpp:122] Setting up Scale30
I1006 11:25:17.308322  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.308324  2824 net.cpp:137] Memory required for data: 501446800
I1006 11:25:17.308328  2824 layer_factory.hpp:77] Creating layer penlu28
I1006 11:25:17.308333  2824 net.cpp:84] Creating Layer penlu28
I1006 11:25:17.308336  2824 net.cpp:406] penlu28 <- Convolution30
I1006 11:25:17.308338  2824 net.cpp:367] penlu28 -> Convolution30 (in-place)
I1006 11:25:17.308452  2824 net.cpp:122] Setting up penlu28
I1006 11:25:17.308456  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.308459  2824 net.cpp:137] Memory required for data: 502701200
I1006 11:25:17.308464  2824 layer_factory.hpp:77] Creating layer Convolution31
I1006 11:25:17.308470  2824 net.cpp:84] Creating Layer Convolution31
I1006 11:25:17.308471  2824 net.cpp:406] Convolution31 <- Convolution30
I1006 11:25:17.308475  2824 net.cpp:380] Convolution31 -> Convolution31
I1006 11:25:17.310377  2824 net.cpp:122] Setting up Convolution31
I1006 11:25:17.310385  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.310389  2824 net.cpp:137] Memory required for data: 503955600
I1006 11:25:17.310394  2824 layer_factory.hpp:77] Creating layer BatchNorm31
I1006 11:25:17.310397  2824 net.cpp:84] Creating Layer BatchNorm31
I1006 11:25:17.310400  2824 net.cpp:406] BatchNorm31 <- Convolution31
I1006 11:25:17.310405  2824 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I1006 11:25:17.310531  2824 net.cpp:122] Setting up BatchNorm31
I1006 11:25:17.310535  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.310537  2824 net.cpp:137] Memory required for data: 505210000
I1006 11:25:17.310542  2824 layer_factory.hpp:77] Creating layer Scale31
I1006 11:25:17.310546  2824 net.cpp:84] Creating Layer Scale31
I1006 11:25:17.310549  2824 net.cpp:406] Scale31 <- Convolution31
I1006 11:25:17.310551  2824 net.cpp:367] Scale31 -> Convolution31 (in-place)
I1006 11:25:17.310577  2824 layer_factory.hpp:77] Creating layer Scale31
I1006 11:25:17.310648  2824 net.cpp:122] Setting up Scale31
I1006 11:25:17.310652  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.310654  2824 net.cpp:137] Memory required for data: 506464400
I1006 11:25:17.310658  2824 layer_factory.hpp:77] Creating layer Eltwise14
I1006 11:25:17.310662  2824 net.cpp:84] Creating Layer Eltwise14
I1006 11:25:17.310664  2824 net.cpp:406] Eltwise14 <- Eltwise13_penlu27_0_split_1
I1006 11:25:17.310667  2824 net.cpp:406] Eltwise14 <- Convolution31
I1006 11:25:17.310670  2824 net.cpp:380] Eltwise14 -> Eltwise14
I1006 11:25:17.310685  2824 net.cpp:122] Setting up Eltwise14
I1006 11:25:17.310689  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.310691  2824 net.cpp:137] Memory required for data: 507718800
I1006 11:25:17.310693  2824 layer_factory.hpp:77] Creating layer penlu29
I1006 11:25:17.310698  2824 net.cpp:84] Creating Layer penlu29
I1006 11:25:17.310700  2824 net.cpp:406] penlu29 <- Eltwise14
I1006 11:25:17.310704  2824 net.cpp:367] penlu29 -> Eltwise14 (in-place)
I1006 11:25:17.310806  2824 net.cpp:122] Setting up penlu29
I1006 11:25:17.310809  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.310811  2824 net.cpp:137] Memory required for data: 508973200
I1006 11:25:17.310816  2824 layer_factory.hpp:77] Creating layer Eltwise14_penlu29_0_split
I1006 11:25:17.310818  2824 net.cpp:84] Creating Layer Eltwise14_penlu29_0_split
I1006 11:25:17.310820  2824 net.cpp:406] Eltwise14_penlu29_0_split <- Eltwise14
I1006 11:25:17.310824  2824 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_0
I1006 11:25:17.310828  2824 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_1
I1006 11:25:17.310849  2824 net.cpp:122] Setting up Eltwise14_penlu29_0_split
I1006 11:25:17.310853  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.310856  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.310858  2824 net.cpp:137] Memory required for data: 511482000
I1006 11:25:17.310860  2824 layer_factory.hpp:77] Creating layer Convolution32
I1006 11:25:17.310866  2824 net.cpp:84] Creating Layer Convolution32
I1006 11:25:17.310868  2824 net.cpp:406] Convolution32 <- Eltwise14_penlu29_0_split_0
I1006 11:25:17.310873  2824 net.cpp:380] Convolution32 -> Convolution32
I1006 11:25:17.312510  2824 net.cpp:122] Setting up Convolution32
I1006 11:25:17.312520  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.312522  2824 net.cpp:137] Memory required for data: 512736400
I1006 11:25:17.312533  2824 layer_factory.hpp:77] Creating layer BatchNorm32
I1006 11:25:17.312538  2824 net.cpp:84] Creating Layer BatchNorm32
I1006 11:25:17.312541  2824 net.cpp:406] BatchNorm32 <- Convolution32
I1006 11:25:17.312544  2824 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I1006 11:25:17.312676  2824 net.cpp:122] Setting up BatchNorm32
I1006 11:25:17.312680  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.312683  2824 net.cpp:137] Memory required for data: 513990800
I1006 11:25:17.312687  2824 layer_factory.hpp:77] Creating layer Scale32
I1006 11:25:17.312691  2824 net.cpp:84] Creating Layer Scale32
I1006 11:25:17.312695  2824 net.cpp:406] Scale32 <- Convolution32
I1006 11:25:17.312697  2824 net.cpp:367] Scale32 -> Convolution32 (in-place)
I1006 11:25:17.312722  2824 layer_factory.hpp:77] Creating layer Scale32
I1006 11:25:17.312796  2824 net.cpp:122] Setting up Scale32
I1006 11:25:17.312800  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.312803  2824 net.cpp:137] Memory required for data: 515245200
I1006 11:25:17.312806  2824 layer_factory.hpp:77] Creating layer penlu30
I1006 11:25:17.312811  2824 net.cpp:84] Creating Layer penlu30
I1006 11:25:17.312814  2824 net.cpp:406] penlu30 <- Convolution32
I1006 11:25:17.312818  2824 net.cpp:367] penlu30 -> Convolution32 (in-place)
I1006 11:25:17.312922  2824 net.cpp:122] Setting up penlu30
I1006 11:25:17.312925  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.312928  2824 net.cpp:137] Memory required for data: 516499600
I1006 11:25:17.312932  2824 layer_factory.hpp:77] Creating layer Convolution33
I1006 11:25:17.312938  2824 net.cpp:84] Creating Layer Convolution33
I1006 11:25:17.312942  2824 net.cpp:406] Convolution33 <- Convolution32
I1006 11:25:17.312945  2824 net.cpp:380] Convolution33 -> Convolution33
I1006 11:25:17.314848  2824 net.cpp:122] Setting up Convolution33
I1006 11:25:17.314857  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.314860  2824 net.cpp:137] Memory required for data: 517754000
I1006 11:25:17.314864  2824 layer_factory.hpp:77] Creating layer BatchNorm33
I1006 11:25:17.314868  2824 net.cpp:84] Creating Layer BatchNorm33
I1006 11:25:17.314872  2824 net.cpp:406] BatchNorm33 <- Convolution33
I1006 11:25:17.314875  2824 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I1006 11:25:17.315006  2824 net.cpp:122] Setting up BatchNorm33
I1006 11:25:17.315009  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.315012  2824 net.cpp:137] Memory required for data: 519008400
I1006 11:25:17.315016  2824 layer_factory.hpp:77] Creating layer Scale33
I1006 11:25:17.315021  2824 net.cpp:84] Creating Layer Scale33
I1006 11:25:17.315022  2824 net.cpp:406] Scale33 <- Convolution33
I1006 11:25:17.315026  2824 net.cpp:367] Scale33 -> Convolution33 (in-place)
I1006 11:25:17.315052  2824 layer_factory.hpp:77] Creating layer Scale33
I1006 11:25:17.315124  2824 net.cpp:122] Setting up Scale33
I1006 11:25:17.315127  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.315129  2824 net.cpp:137] Memory required for data: 520262800
I1006 11:25:17.315134  2824 layer_factory.hpp:77] Creating layer Eltwise15
I1006 11:25:17.315137  2824 net.cpp:84] Creating Layer Eltwise15
I1006 11:25:17.315140  2824 net.cpp:406] Eltwise15 <- Eltwise14_penlu29_0_split_1
I1006 11:25:17.315142  2824 net.cpp:406] Eltwise15 <- Convolution33
I1006 11:25:17.315146  2824 net.cpp:380] Eltwise15 -> Eltwise15
I1006 11:25:17.315160  2824 net.cpp:122] Setting up Eltwise15
I1006 11:25:17.315179  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.315182  2824 net.cpp:137] Memory required for data: 521517200
I1006 11:25:17.315186  2824 layer_factory.hpp:77] Creating layer penlu31
I1006 11:25:17.315189  2824 net.cpp:84] Creating Layer penlu31
I1006 11:25:17.315192  2824 net.cpp:406] penlu31 <- Eltwise15
I1006 11:25:17.315196  2824 net.cpp:367] penlu31 -> Eltwise15 (in-place)
I1006 11:25:17.315309  2824 net.cpp:122] Setting up penlu31
I1006 11:25:17.315312  2824 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1006 11:25:17.315320  2824 net.cpp:137] Memory required for data: 522771600
I1006 11:25:17.315325  2824 layer_factory.hpp:77] Creating layer Pooling1
I1006 11:25:17.315330  2824 net.cpp:84] Creating Layer Pooling1
I1006 11:25:17.315332  2824 net.cpp:406] Pooling1 <- Eltwise15
I1006 11:25:17.315335  2824 net.cpp:380] Pooling1 -> Pooling1
I1006 11:25:17.315476  2824 net.cpp:122] Setting up Pooling1
I1006 11:25:17.315482  2824 net.cpp:129] Top shape: 100 64 1 1 (6400)
I1006 11:25:17.315485  2824 net.cpp:137] Memory required for data: 522797200
I1006 11:25:17.315486  2824 layer_factory.hpp:77] Creating layer InnerProduct1
I1006 11:25:17.315495  2824 net.cpp:84] Creating Layer InnerProduct1
I1006 11:25:17.315498  2824 net.cpp:406] InnerProduct1 <- Pooling1
I1006 11:25:17.315502  2824 net.cpp:380] InnerProduct1 -> InnerProduct1
I1006 11:25:17.315603  2824 net.cpp:122] Setting up InnerProduct1
I1006 11:25:17.315608  2824 net.cpp:129] Top shape: 100 10 (1000)
I1006 11:25:17.315609  2824 net.cpp:137] Memory required for data: 522801200
I1006 11:25:17.315613  2824 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1006 11:25:17.315618  2824 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1006 11:25:17.315620  2824 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1006 11:25:17.315623  2824 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1006 11:25:17.315626  2824 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1006 11:25:17.315634  2824 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1006 11:25:17.315804  2824 net.cpp:122] Setting up SoftmaxWithLoss1
I1006 11:25:17.315809  2824 net.cpp:129] Top shape: (1)
I1006 11:25:17.315812  2824 net.cpp:132]     with loss weight 1
I1006 11:25:17.315824  2824 net.cpp:137] Memory required for data: 522801204
I1006 11:25:17.315827  2824 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1006 11:25:17.315829  2824 net.cpp:198] InnerProduct1 needs backward computation.
I1006 11:25:17.315831  2824 net.cpp:198] Pooling1 needs backward computation.
I1006 11:25:17.315834  2824 net.cpp:198] penlu31 needs backward computation.
I1006 11:25:17.315836  2824 net.cpp:198] Eltwise15 needs backward computation.
I1006 11:25:17.315838  2824 net.cpp:198] Scale33 needs backward computation.
I1006 11:25:17.315840  2824 net.cpp:198] BatchNorm33 needs backward computation.
I1006 11:25:17.315842  2824 net.cpp:198] Convolution33 needs backward computation.
I1006 11:25:17.315845  2824 net.cpp:198] penlu30 needs backward computation.
I1006 11:25:17.315847  2824 net.cpp:198] Scale32 needs backward computation.
I1006 11:25:17.315850  2824 net.cpp:198] BatchNorm32 needs backward computation.
I1006 11:25:17.315851  2824 net.cpp:198] Convolution32 needs backward computation.
I1006 11:25:17.315853  2824 net.cpp:198] Eltwise14_penlu29_0_split needs backward computation.
I1006 11:25:17.315855  2824 net.cpp:198] penlu29 needs backward computation.
I1006 11:25:17.315857  2824 net.cpp:198] Eltwise14 needs backward computation.
I1006 11:25:17.315860  2824 net.cpp:198] Scale31 needs backward computation.
I1006 11:25:17.315862  2824 net.cpp:198] BatchNorm31 needs backward computation.
I1006 11:25:17.315865  2824 net.cpp:198] Convolution31 needs backward computation.
I1006 11:25:17.315866  2824 net.cpp:198] penlu28 needs backward computation.
I1006 11:25:17.315868  2824 net.cpp:198] Scale30 needs backward computation.
I1006 11:25:17.315871  2824 net.cpp:198] BatchNorm30 needs backward computation.
I1006 11:25:17.315872  2824 net.cpp:198] Convolution30 needs backward computation.
I1006 11:25:17.315874  2824 net.cpp:198] Eltwise13_penlu27_0_split needs backward computation.
I1006 11:25:17.315876  2824 net.cpp:198] penlu27 needs backward computation.
I1006 11:25:17.315878  2824 net.cpp:198] Eltwise13 needs backward computation.
I1006 11:25:17.315881  2824 net.cpp:198] Scale29 needs backward computation.
I1006 11:25:17.315883  2824 net.cpp:198] BatchNorm29 needs backward computation.
I1006 11:25:17.315886  2824 net.cpp:198] Convolution29 needs backward computation.
I1006 11:25:17.315887  2824 net.cpp:198] penlu26 needs backward computation.
I1006 11:25:17.315896  2824 net.cpp:198] Scale28 needs backward computation.
I1006 11:25:17.315898  2824 net.cpp:198] BatchNorm28 needs backward computation.
I1006 11:25:17.315901  2824 net.cpp:198] Convolution28 needs backward computation.
I1006 11:25:17.315903  2824 net.cpp:198] Eltwise12_penlu25_0_split needs backward computation.
I1006 11:25:17.315906  2824 net.cpp:198] penlu25 needs backward computation.
I1006 11:25:17.315907  2824 net.cpp:198] Eltwise12 needs backward computation.
I1006 11:25:17.315910  2824 net.cpp:198] Scale27 needs backward computation.
I1006 11:25:17.315912  2824 net.cpp:198] BatchNorm27 needs backward computation.
I1006 11:25:17.315914  2824 net.cpp:198] Convolution27 needs backward computation.
I1006 11:25:17.315917  2824 net.cpp:198] penlu24 needs backward computation.
I1006 11:25:17.315918  2824 net.cpp:198] Scale26 needs backward computation.
I1006 11:25:17.315920  2824 net.cpp:198] BatchNorm26 needs backward computation.
I1006 11:25:17.315922  2824 net.cpp:198] Convolution26 needs backward computation.
I1006 11:25:17.315924  2824 net.cpp:198] Eltwise11_penlu23_0_split needs backward computation.
I1006 11:25:17.315927  2824 net.cpp:198] penlu23 needs backward computation.
I1006 11:25:17.315929  2824 net.cpp:198] Eltwise11 needs backward computation.
I1006 11:25:17.315932  2824 net.cpp:198] Scale25 needs backward computation.
I1006 11:25:17.315933  2824 net.cpp:198] BatchNorm25 needs backward computation.
I1006 11:25:17.315935  2824 net.cpp:198] Convolution25 needs backward computation.
I1006 11:25:17.315938  2824 net.cpp:198] penlu22 needs backward computation.
I1006 11:25:17.315940  2824 net.cpp:198] Scale24 needs backward computation.
I1006 11:25:17.315943  2824 net.cpp:198] BatchNorm24 needs backward computation.
I1006 11:25:17.315944  2824 net.cpp:198] Convolution24 needs backward computation.
I1006 11:25:17.315946  2824 net.cpp:198] Scale23 needs backward computation.
I1006 11:25:17.315948  2824 net.cpp:198] BatchNorm23 needs backward computation.
I1006 11:25:17.315950  2824 net.cpp:198] Convolution23 needs backward computation.
I1006 11:25:17.315953  2824 net.cpp:198] Eltwise10_penlu21_0_split needs backward computation.
I1006 11:25:17.315955  2824 net.cpp:198] penlu21 needs backward computation.
I1006 11:25:17.315958  2824 net.cpp:198] Eltwise10 needs backward computation.
I1006 11:25:17.315959  2824 net.cpp:198] Scale22 needs backward computation.
I1006 11:25:17.315961  2824 net.cpp:198] BatchNorm22 needs backward computation.
I1006 11:25:17.315964  2824 net.cpp:198] Convolution22 needs backward computation.
I1006 11:25:17.315966  2824 net.cpp:198] penlu20 needs backward computation.
I1006 11:25:17.315968  2824 net.cpp:198] Scale21 needs backward computation.
I1006 11:25:17.315970  2824 net.cpp:198] BatchNorm21 needs backward computation.
I1006 11:25:17.315973  2824 net.cpp:198] Convolution21 needs backward computation.
I1006 11:25:17.315975  2824 net.cpp:198] Eltwise9_penlu19_0_split needs backward computation.
I1006 11:25:17.315978  2824 net.cpp:198] penlu19 needs backward computation.
I1006 11:25:17.315980  2824 net.cpp:198] Eltwise9 needs backward computation.
I1006 11:25:17.315982  2824 net.cpp:198] Scale20 needs backward computation.
I1006 11:25:17.315984  2824 net.cpp:198] BatchNorm20 needs backward computation.
I1006 11:25:17.315986  2824 net.cpp:198] Convolution20 needs backward computation.
I1006 11:25:17.315989  2824 net.cpp:198] penlu18 needs backward computation.
I1006 11:25:17.315991  2824 net.cpp:198] Scale19 needs backward computation.
I1006 11:25:17.315994  2824 net.cpp:198] BatchNorm19 needs backward computation.
I1006 11:25:17.315995  2824 net.cpp:198] Convolution19 needs backward computation.
I1006 11:25:17.315999  2824 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1006 11:25:17.316000  2824 net.cpp:198] penlu17 needs backward computation.
I1006 11:25:17.316004  2824 net.cpp:198] Eltwise8 needs backward computation.
I1006 11:25:17.316005  2824 net.cpp:198] Scale18 needs backward computation.
I1006 11:25:17.316007  2824 net.cpp:198] BatchNorm18 needs backward computation.
I1006 11:25:17.316013  2824 net.cpp:198] Convolution18 needs backward computation.
I1006 11:25:17.316015  2824 net.cpp:198] penlu16 needs backward computation.
I1006 11:25:17.316017  2824 net.cpp:198] Scale17 needs backward computation.
I1006 11:25:17.316020  2824 net.cpp:198] BatchNorm17 needs backward computation.
I1006 11:25:17.316022  2824 net.cpp:198] Convolution17 needs backward computation.
I1006 11:25:17.316025  2824 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1006 11:25:17.316027  2824 net.cpp:198] penlu15 needs backward computation.
I1006 11:25:17.316030  2824 net.cpp:198] Eltwise7 needs backward computation.
I1006 11:25:17.316031  2824 net.cpp:198] Scale16 needs backward computation.
I1006 11:25:17.316035  2824 net.cpp:198] BatchNorm16 needs backward computation.
I1006 11:25:17.316036  2824 net.cpp:198] Convolution16 needs backward computation.
I1006 11:25:17.316038  2824 net.cpp:198] penlu14 needs backward computation.
I1006 11:25:17.316040  2824 net.cpp:198] Scale15 needs backward computation.
I1006 11:25:17.316043  2824 net.cpp:198] BatchNorm15 needs backward computation.
I1006 11:25:17.316045  2824 net.cpp:198] Convolution15 needs backward computation.
I1006 11:25:17.316047  2824 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1006 11:25:17.316051  2824 net.cpp:198] penlu13 needs backward computation.
I1006 11:25:17.316052  2824 net.cpp:198] Eltwise6 needs backward computation.
I1006 11:25:17.316056  2824 net.cpp:198] Scale14 needs backward computation.
I1006 11:25:17.316057  2824 net.cpp:198] BatchNorm14 needs backward computation.
I1006 11:25:17.316059  2824 net.cpp:198] Convolution14 needs backward computation.
I1006 11:25:17.316062  2824 net.cpp:198] penlu12 needs backward computation.
I1006 11:25:17.316064  2824 net.cpp:198] Scale13 needs backward computation.
I1006 11:25:17.316066  2824 net.cpp:198] BatchNorm13 needs backward computation.
I1006 11:25:17.316068  2824 net.cpp:198] Convolution13 needs backward computation.
I1006 11:25:17.316071  2824 net.cpp:198] Scale12 needs backward computation.
I1006 11:25:17.316073  2824 net.cpp:198] BatchNorm12 needs backward computation.
I1006 11:25:17.316076  2824 net.cpp:198] Convolution12 needs backward computation.
I1006 11:25:17.316077  2824 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1006 11:25:17.316081  2824 net.cpp:198] penlu11 needs backward computation.
I1006 11:25:17.316082  2824 net.cpp:198] Eltwise5 needs backward computation.
I1006 11:25:17.316085  2824 net.cpp:198] Scale11 needs backward computation.
I1006 11:25:17.316087  2824 net.cpp:198] BatchNorm11 needs backward computation.
I1006 11:25:17.316089  2824 net.cpp:198] Convolution11 needs backward computation.
I1006 11:25:17.316092  2824 net.cpp:198] penlu10 needs backward computation.
I1006 11:25:17.316094  2824 net.cpp:198] Scale10 needs backward computation.
I1006 11:25:17.316097  2824 net.cpp:198] BatchNorm10 needs backward computation.
I1006 11:25:17.316098  2824 net.cpp:198] Convolution10 needs backward computation.
I1006 11:25:17.316102  2824 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1006 11:25:17.316103  2824 net.cpp:198] penlu9 needs backward computation.
I1006 11:25:17.316105  2824 net.cpp:198] Eltwise4 needs backward computation.
I1006 11:25:17.316108  2824 net.cpp:198] Scale9 needs backward computation.
I1006 11:25:17.316112  2824 net.cpp:198] BatchNorm9 needs backward computation.
I1006 11:25:17.316113  2824 net.cpp:198] Convolution9 needs backward computation.
I1006 11:25:17.316115  2824 net.cpp:198] penlu8 needs backward computation.
I1006 11:25:17.316118  2824 net.cpp:198] Scale8 needs backward computation.
I1006 11:25:17.316120  2824 net.cpp:198] BatchNorm8 needs backward computation.
I1006 11:25:17.316123  2824 net.cpp:198] Convolution8 needs backward computation.
I1006 11:25:17.316125  2824 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1006 11:25:17.316128  2824 net.cpp:198] penlu7 needs backward computation.
I1006 11:25:17.316133  2824 net.cpp:198] Eltwise3 needs backward computation.
I1006 11:25:17.316135  2824 net.cpp:198] Scale7 needs backward computation.
I1006 11:25:17.316138  2824 net.cpp:198] BatchNorm7 needs backward computation.
I1006 11:25:17.316140  2824 net.cpp:198] Convolution7 needs backward computation.
I1006 11:25:17.316143  2824 net.cpp:198] penlu6 needs backward computation.
I1006 11:25:17.316144  2824 net.cpp:198] Scale6 needs backward computation.
I1006 11:25:17.316146  2824 net.cpp:198] BatchNorm6 needs backward computation.
I1006 11:25:17.316148  2824 net.cpp:198] Convolution6 needs backward computation.
I1006 11:25:17.316151  2824 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1006 11:25:17.316154  2824 net.cpp:198] penlu5 needs backward computation.
I1006 11:25:17.316156  2824 net.cpp:198] Eltwise2 needs backward computation.
I1006 11:25:17.316160  2824 net.cpp:198] Scale5 needs backward computation.
I1006 11:25:17.316164  2824 net.cpp:198] BatchNorm5 needs backward computation.
I1006 11:25:17.316165  2824 net.cpp:198] Convolution5 needs backward computation.
I1006 11:25:17.316169  2824 net.cpp:198] penlu4 needs backward computation.
I1006 11:25:17.316170  2824 net.cpp:198] Scale4 needs backward computation.
I1006 11:25:17.316172  2824 net.cpp:198] BatchNorm4 needs backward computation.
I1006 11:25:17.316174  2824 net.cpp:198] Convolution4 needs backward computation.
I1006 11:25:17.316177  2824 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1006 11:25:17.316179  2824 net.cpp:198] penlu3 needs backward computation.
I1006 11:25:17.316181  2824 net.cpp:198] Eltwise1 needs backward computation.
I1006 11:25:17.316184  2824 net.cpp:198] Scale3 needs backward computation.
I1006 11:25:17.316187  2824 net.cpp:198] BatchNorm3 needs backward computation.
I1006 11:25:17.316190  2824 net.cpp:198] Convolution3 needs backward computation.
I1006 11:25:17.316191  2824 net.cpp:198] penlu2 needs backward computation.
I1006 11:25:17.316195  2824 net.cpp:198] Scale2 needs backward computation.
I1006 11:25:17.316196  2824 net.cpp:198] BatchNorm2 needs backward computation.
I1006 11:25:17.316198  2824 net.cpp:198] Convolution2 needs backward computation.
I1006 11:25:17.316201  2824 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1006 11:25:17.316205  2824 net.cpp:198] penlu1 needs backward computation.
I1006 11:25:17.316206  2824 net.cpp:198] Scale1 needs backward computation.
I1006 11:25:17.316208  2824 net.cpp:198] BatchNorm1 needs backward computation.
I1006 11:25:17.316210  2824 net.cpp:198] Convolution1 needs backward computation.
I1006 11:25:17.316213  2824 net.cpp:200] Data1 does not need backward computation.
I1006 11:25:17.316215  2824 net.cpp:242] This network produces output SoftmaxWithLoss1
I1006 11:25:17.316263  2824 net.cpp:255] Network initialization done.
I1006 11:25:17.318975  2824 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res32/res32_penlu_gauss.prototxt
I1006 11:25:17.318984  2824 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1006 11:25:17.318989  2824 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/PENLU/neural/resnet/res32/res32_penlu_gauss.prototxt
I1006 11:25:17.319103  2824 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I1006 11:25:17.319911  2824 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution16"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bi
I1006 11:25:17.322101  2824 layer_factory.hpp:77] Creating layer Data1
I1006 11:25:17.349861  2824 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb
I1006 11:25:17.349875  2824 net.cpp:84] Creating Layer Data1
I1006 11:25:17.349880  2824 net.cpp:380] Data1 -> Data1
I1006 11:25:17.349887  2824 net.cpp:380] Data1 -> Data2
I1006 11:25:17.349894  2824 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I1006 11:25:17.350039  2824 data_layer.cpp:45] output data size: 100,3,32,32
I1006 11:25:17.354601  2824 net.cpp:122] Setting up Data1
I1006 11:25:17.354621  2824 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1006 11:25:17.354627  2824 net.cpp:129] Top shape: 100 (100)
I1006 11:25:17.354629  2824 net.cpp:137] Memory required for data: 1229200
I1006 11:25:17.354635  2824 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1006 11:25:17.354643  2824 net.cpp:84] Creating Layer Data2_Data1_1_split
I1006 11:25:17.354646  2824 net.cpp:406] Data2_Data1_1_split <- Data2
I1006 11:25:17.354650  2824 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1006 11:25:17.354657  2824 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1006 11:25:17.354722  2824 net.cpp:122] Setting up Data2_Data1_1_split
I1006 11:25:17.354727  2824 net.cpp:129] Top shape: 100 (100)
I1006 11:25:17.354729  2824 net.cpp:129] Top shape: 100 (100)
I1006 11:25:17.354732  2824 net.cpp:137] Memory required for data: 1230000
I1006 11:25:17.354734  2824 layer_factory.hpp:77] Creating layer Convolution1
I1006 11:25:17.354744  2824 net.cpp:84] Creating Layer Convolution1
I1006 11:25:17.354746  2824 net.cpp:406] Convolution1 <- Data1
I1006 11:25:17.354751  2824 net.cpp:380] Convolution1 -> Convolution1
I1006 11:25:17.355931  2824 net.cpp:122] Setting up Convolution1
I1006 11:25:17.355942  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.355943  2824 net.cpp:137] Memory required for data: 7783600
I1006 11:25:17.355952  2824 layer_factory.hpp:77] Creating layer BatchNorm1
I1006 11:25:17.355958  2824 net.cpp:84] Creating Layer BatchNorm1
I1006 11:25:17.355962  2824 net.cpp:406] BatchNorm1 <- Convolution1
I1006 11:25:17.355964  2824 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1006 11:25:17.356101  2824 net.cpp:122] Setting up BatchNorm1
I1006 11:25:17.356106  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.356108  2824 net.cpp:137] Memory required for data: 14337200
I1006 11:25:17.356124  2824 layer_factory.hpp:77] Creating layer Scale1
I1006 11:25:17.356130  2824 net.cpp:84] Creating Layer Scale1
I1006 11:25:17.356132  2824 net.cpp:406] Scale1 <- Convolution1
I1006 11:25:17.356135  2824 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1006 11:25:17.356166  2824 layer_factory.hpp:77] Creating layer Scale1
I1006 11:25:17.356243  2824 net.cpp:122] Setting up Scale1
I1006 11:25:17.356247  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.356250  2824 net.cpp:137] Memory required for data: 20890800
I1006 11:25:17.356253  2824 layer_factory.hpp:77] Creating layer penlu1
I1006 11:25:17.356263  2824 net.cpp:84] Creating Layer penlu1
I1006 11:25:17.356266  2824 net.cpp:406] penlu1 <- Convolution1
I1006 11:25:17.356271  2824 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1006 11:25:17.356387  2824 net.cpp:122] Setting up penlu1
I1006 11:25:17.356391  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.356410  2824 net.cpp:137] Memory required for data: 27444400
I1006 11:25:17.356416  2824 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1006 11:25:17.356420  2824 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1006 11:25:17.356422  2824 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1006 11:25:17.356426  2824 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1006 11:25:17.356431  2824 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1006 11:25:17.356456  2824 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1006 11:25:17.356459  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.356462  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.356465  2824 net.cpp:137] Memory required for data: 40551600
I1006 11:25:17.356468  2824 layer_factory.hpp:77] Creating layer Convolution2
I1006 11:25:17.356475  2824 net.cpp:84] Creating Layer Convolution2
I1006 11:25:17.356477  2824 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1006 11:25:17.356482  2824 net.cpp:380] Convolution2 -> Convolution2
I1006 11:25:17.357532  2824 net.cpp:122] Setting up Convolution2
I1006 11:25:17.357542  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.357544  2824 net.cpp:137] Memory required for data: 47105200
I1006 11:25:17.357549  2824 layer_factory.hpp:77] Creating layer BatchNorm2
I1006 11:25:17.357555  2824 net.cpp:84] Creating Layer BatchNorm2
I1006 11:25:17.357558  2824 net.cpp:406] BatchNorm2 <- Convolution2
I1006 11:25:17.357563  2824 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1006 11:25:17.357697  2824 net.cpp:122] Setting up BatchNorm2
I1006 11:25:17.357702  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.357704  2824 net.cpp:137] Memory required for data: 53658800
I1006 11:25:17.357709  2824 layer_factory.hpp:77] Creating layer Scale2
I1006 11:25:17.357713  2824 net.cpp:84] Creating Layer Scale2
I1006 11:25:17.357715  2824 net.cpp:406] Scale2 <- Convolution2
I1006 11:25:17.357719  2824 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1006 11:25:17.357745  2824 layer_factory.hpp:77] Creating layer Scale2
I1006 11:25:17.357820  2824 net.cpp:122] Setting up Scale2
I1006 11:25:17.357825  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.357827  2824 net.cpp:137] Memory required for data: 60212400
I1006 11:25:17.357832  2824 layer_factory.hpp:77] Creating layer penlu2
I1006 11:25:17.357839  2824 net.cpp:84] Creating Layer penlu2
I1006 11:25:17.357842  2824 net.cpp:406] penlu2 <- Convolution2
I1006 11:25:17.357846  2824 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1006 11:25:17.357961  2824 net.cpp:122] Setting up penlu2
I1006 11:25:17.357965  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.357969  2824 net.cpp:137] Memory required for data: 66766000
I1006 11:25:17.357972  2824 layer_factory.hpp:77] Creating layer Convolution3
I1006 11:25:17.357980  2824 net.cpp:84] Creating Layer Convolution3
I1006 11:25:17.357981  2824 net.cpp:406] Convolution3 <- Convolution2
I1006 11:25:17.357985  2824 net.cpp:380] Convolution3 -> Convolution3
I1006 11:25:17.359017  2824 net.cpp:122] Setting up Convolution3
I1006 11:25:17.359028  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.359031  2824 net.cpp:137] Memory required for data: 73319600
I1006 11:25:17.359035  2824 layer_factory.hpp:77] Creating layer BatchNorm3
I1006 11:25:17.359040  2824 net.cpp:84] Creating Layer BatchNorm3
I1006 11:25:17.359042  2824 net.cpp:406] BatchNorm3 <- Convolution3
I1006 11:25:17.359047  2824 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1006 11:25:17.359218  2824 net.cpp:122] Setting up BatchNorm3
I1006 11:25:17.359233  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.359236  2824 net.cpp:137] Memory required for data: 79873200
I1006 11:25:17.359241  2824 layer_factory.hpp:77] Creating layer Scale3
I1006 11:25:17.359246  2824 net.cpp:84] Creating Layer Scale3
I1006 11:25:17.359254  2824 net.cpp:406] Scale3 <- Convolution3
I1006 11:25:17.359258  2824 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1006 11:25:17.359287  2824 layer_factory.hpp:77] Creating layer Scale3
I1006 11:25:17.359361  2824 net.cpp:122] Setting up Scale3
I1006 11:25:17.359365  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.359367  2824 net.cpp:137] Memory required for data: 86426800
I1006 11:25:17.359371  2824 layer_factory.hpp:77] Creating layer Eltwise1
I1006 11:25:17.359376  2824 net.cpp:84] Creating Layer Eltwise1
I1006 11:25:17.359378  2824 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1006 11:25:17.359381  2824 net.cpp:406] Eltwise1 <- Convolution3
I1006 11:25:17.359386  2824 net.cpp:380] Eltwise1 -> Eltwise1
I1006 11:25:17.359403  2824 net.cpp:122] Setting up Eltwise1
I1006 11:25:17.359406  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.359408  2824 net.cpp:137] Memory required for data: 92980400
I1006 11:25:17.359414  2824 layer_factory.hpp:77] Creating layer penlu3
I1006 11:25:17.359419  2824 net.cpp:84] Creating Layer penlu3
I1006 11:25:17.359422  2824 net.cpp:406] penlu3 <- Eltwise1
I1006 11:25:17.359426  2824 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1006 11:25:17.359540  2824 net.cpp:122] Setting up penlu3
I1006 11:25:17.359545  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.359547  2824 net.cpp:137] Memory required for data: 99534000
I1006 11:25:17.359551  2824 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1006 11:25:17.359556  2824 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1006 11:25:17.359558  2824 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1006 11:25:17.359562  2824 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1006 11:25:17.359566  2824 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1006 11:25:17.359589  2824 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1006 11:25:17.359592  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.359596  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.359597  2824 net.cpp:137] Memory required for data: 112641200
I1006 11:25:17.359601  2824 layer_factory.hpp:77] Creating layer Convolution4
I1006 11:25:17.359606  2824 net.cpp:84] Creating Layer Convolution4
I1006 11:25:17.359609  2824 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1006 11:25:17.359612  2824 net.cpp:380] Convolution4 -> Convolution4
I1006 11:25:17.360669  2824 net.cpp:122] Setting up Convolution4
I1006 11:25:17.360677  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.360680  2824 net.cpp:137] Memory required for data: 119194800
I1006 11:25:17.360684  2824 layer_factory.hpp:77] Creating layer BatchNorm4
I1006 11:25:17.360692  2824 net.cpp:84] Creating Layer BatchNorm4
I1006 11:25:17.360694  2824 net.cpp:406] BatchNorm4 <- Convolution4
I1006 11:25:17.360698  2824 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1006 11:25:17.360833  2824 net.cpp:122] Setting up BatchNorm4
I1006 11:25:17.360837  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.360839  2824 net.cpp:137] Memory required for data: 125748400
I1006 11:25:17.360847  2824 layer_factory.hpp:77] Creating layer Scale4
I1006 11:25:17.360857  2824 net.cpp:84] Creating Layer Scale4
I1006 11:25:17.360859  2824 net.cpp:406] Scale4 <- Convolution4
I1006 11:25:17.360862  2824 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1006 11:25:17.360891  2824 layer_factory.hpp:77] Creating layer Scale4
I1006 11:25:17.360970  2824 net.cpp:122] Setting up Scale4
I1006 11:25:17.360973  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.360976  2824 net.cpp:137] Memory required for data: 132302000
I1006 11:25:17.360980  2824 layer_factory.hpp:77] Creating layer penlu4
I1006 11:25:17.360985  2824 net.cpp:84] Creating Layer penlu4
I1006 11:25:17.360987  2824 net.cpp:406] penlu4 <- Convolution4
I1006 11:25:17.360991  2824 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1006 11:25:17.361110  2824 net.cpp:122] Setting up penlu4
I1006 11:25:17.361121  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.361124  2824 net.cpp:137] Memory required for data: 138855600
I1006 11:25:17.361131  2824 layer_factory.hpp:77] Creating layer Convolution5
I1006 11:25:17.361138  2824 net.cpp:84] Creating Layer Convolution5
I1006 11:25:17.361140  2824 net.cpp:406] Convolution5 <- Convolution4
I1006 11:25:17.361145  2824 net.cpp:380] Convolution5 -> Convolution5
I1006 11:25:17.362082  2824 net.cpp:122] Setting up Convolution5
I1006 11:25:17.362090  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.362093  2824 net.cpp:137] Memory required for data: 145409200
I1006 11:25:17.362097  2824 layer_factory.hpp:77] Creating layer BatchNorm5
I1006 11:25:17.362102  2824 net.cpp:84] Creating Layer BatchNorm5
I1006 11:25:17.362104  2824 net.cpp:406] BatchNorm5 <- Convolution5
I1006 11:25:17.362109  2824 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1006 11:25:17.362244  2824 net.cpp:122] Setting up BatchNorm5
I1006 11:25:17.362249  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.362251  2824 net.cpp:137] Memory required for data: 151962800
I1006 11:25:17.362256  2824 layer_factory.hpp:77] Creating layer Scale5
I1006 11:25:17.362259  2824 net.cpp:84] Creating Layer Scale5
I1006 11:25:17.362262  2824 net.cpp:406] Scale5 <- Convolution5
I1006 11:25:17.362265  2824 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1006 11:25:17.362293  2824 layer_factory.hpp:77] Creating layer Scale5
I1006 11:25:17.362366  2824 net.cpp:122] Setting up Scale5
I1006 11:25:17.362370  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.362372  2824 net.cpp:137] Memory required for data: 158516400
I1006 11:25:17.362376  2824 layer_factory.hpp:77] Creating layer Eltwise2
I1006 11:25:17.362380  2824 net.cpp:84] Creating Layer Eltwise2
I1006 11:25:17.362383  2824 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1006 11:25:17.362385  2824 net.cpp:406] Eltwise2 <- Convolution5
I1006 11:25:17.362388  2824 net.cpp:380] Eltwise2 -> Eltwise2
I1006 11:25:17.362404  2824 net.cpp:122] Setting up Eltwise2
I1006 11:25:17.362408  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.362411  2824 net.cpp:137] Memory required for data: 165070000
I1006 11:25:17.362412  2824 layer_factory.hpp:77] Creating layer penlu5
I1006 11:25:17.362417  2824 net.cpp:84] Creating Layer penlu5
I1006 11:25:17.362419  2824 net.cpp:406] penlu5 <- Eltwise2
I1006 11:25:17.362423  2824 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1006 11:25:17.362540  2824 net.cpp:122] Setting up penlu5
I1006 11:25:17.362543  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.362545  2824 net.cpp:137] Memory required for data: 171623600
I1006 11:25:17.362550  2824 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1006 11:25:17.362553  2824 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1006 11:25:17.362555  2824 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1006 11:25:17.362560  2824 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1006 11:25:17.362563  2824 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1006 11:25:17.362588  2824 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1006 11:25:17.362592  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.362596  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.362597  2824 net.cpp:137] Memory required for data: 184730800
I1006 11:25:17.362599  2824 layer_factory.hpp:77] Creating layer Convolution6
I1006 11:25:17.362606  2824 net.cpp:84] Creating Layer Convolution6
I1006 11:25:17.362607  2824 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1006 11:25:17.362612  2824 net.cpp:380] Convolution6 -> Convolution6
I1006 11:25:17.363548  2824 net.cpp:122] Setting up Convolution6
I1006 11:25:17.363556  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.363559  2824 net.cpp:137] Memory required for data: 191284400
I1006 11:25:17.363562  2824 layer_factory.hpp:77] Creating layer BatchNorm6
I1006 11:25:17.363575  2824 net.cpp:84] Creating Layer BatchNorm6
I1006 11:25:17.363579  2824 net.cpp:406] BatchNorm6 <- Convolution6
I1006 11:25:17.363581  2824 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1006 11:25:17.363718  2824 net.cpp:122] Setting up BatchNorm6
I1006 11:25:17.363723  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.363724  2824 net.cpp:137] Memory required for data: 197838000
I1006 11:25:17.363729  2824 layer_factory.hpp:77] Creating layer Scale6
I1006 11:25:17.363734  2824 net.cpp:84] Creating Layer Scale6
I1006 11:25:17.363735  2824 net.cpp:406] Scale6 <- Convolution6
I1006 11:25:17.380340  2824 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1006 11:25:17.380388  2824 layer_factory.hpp:77] Creating layer Scale6
I1006 11:25:17.380479  2824 net.cpp:122] Setting up Scale6
I1006 11:25:17.380486  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.380487  2824 net.cpp:137] Memory required for data: 204391600
I1006 11:25:17.380492  2824 layer_factory.hpp:77] Creating layer penlu6
I1006 11:25:17.380498  2824 net.cpp:84] Creating Layer penlu6
I1006 11:25:17.380501  2824 net.cpp:406] penlu6 <- Convolution6
I1006 11:25:17.380506  2824 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1006 11:25:17.380633  2824 net.cpp:122] Setting up penlu6
I1006 11:25:17.380638  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.380640  2824 net.cpp:137] Memory required for data: 210945200
I1006 11:25:17.380645  2824 layer_factory.hpp:77] Creating layer Convolution7
I1006 11:25:17.380652  2824 net.cpp:84] Creating Layer Convolution7
I1006 11:25:17.380656  2824 net.cpp:406] Convolution7 <- Convolution6
I1006 11:25:17.380659  2824 net.cpp:380] Convolution7 -> Convolution7
I1006 11:25:17.381680  2824 net.cpp:122] Setting up Convolution7
I1006 11:25:17.381700  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.381701  2824 net.cpp:137] Memory required for data: 217498800
I1006 11:25:17.381706  2824 layer_factory.hpp:77] Creating layer BatchNorm7
I1006 11:25:17.381713  2824 net.cpp:84] Creating Layer BatchNorm7
I1006 11:25:17.381716  2824 net.cpp:406] BatchNorm7 <- Convolution7
I1006 11:25:17.381719  2824 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1006 11:25:17.381916  2824 net.cpp:122] Setting up BatchNorm7
I1006 11:25:17.381925  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.381929  2824 net.cpp:137] Memory required for data: 224052400
I1006 11:25:17.381945  2824 layer_factory.hpp:77] Creating layer Scale7
I1006 11:25:17.381953  2824 net.cpp:84] Creating Layer Scale7
I1006 11:25:17.381956  2824 net.cpp:406] Scale7 <- Convolution7
I1006 11:25:17.381961  2824 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1006 11:25:17.382004  2824 layer_factory.hpp:77] Creating layer Scale7
I1006 11:25:17.382105  2824 net.cpp:122] Setting up Scale7
I1006 11:25:17.382110  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.382112  2824 net.cpp:137] Memory required for data: 230606000
I1006 11:25:17.382127  2824 layer_factory.hpp:77] Creating layer Eltwise3
I1006 11:25:17.382130  2824 net.cpp:84] Creating Layer Eltwise3
I1006 11:25:17.382133  2824 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1006 11:25:17.382136  2824 net.cpp:406] Eltwise3 <- Convolution7
I1006 11:25:17.382149  2824 net.cpp:380] Eltwise3 -> Eltwise3
I1006 11:25:17.382179  2824 net.cpp:122] Setting up Eltwise3
I1006 11:25:17.382185  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.382189  2824 net.cpp:137] Memory required for data: 237159600
I1006 11:25:17.382192  2824 layer_factory.hpp:77] Creating layer penlu7
I1006 11:25:17.382199  2824 net.cpp:84] Creating Layer penlu7
I1006 11:25:17.382201  2824 net.cpp:406] penlu7 <- Eltwise3
I1006 11:25:17.382205  2824 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1006 11:25:17.382340  2824 net.cpp:122] Setting up penlu7
I1006 11:25:17.382345  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.382347  2824 net.cpp:137] Memory required for data: 243713200
I1006 11:25:17.382352  2824 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1006 11:25:17.382362  2824 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1006 11:25:17.382365  2824 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1006 11:25:17.382369  2824 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1006 11:25:17.382374  2824 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1006 11:25:17.382398  2824 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1006 11:25:17.382402  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.382405  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.382407  2824 net.cpp:137] Memory required for data: 256820400
I1006 11:25:17.382410  2824 layer_factory.hpp:77] Creating layer Convolution8
I1006 11:25:17.382416  2824 net.cpp:84] Creating Layer Convolution8
I1006 11:25:17.382418  2824 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1006 11:25:17.382422  2824 net.cpp:380] Convolution8 -> Convolution8
I1006 11:25:17.383481  2824 net.cpp:122] Setting up Convolution8
I1006 11:25:17.383491  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.383492  2824 net.cpp:137] Memory required for data: 263374000
I1006 11:25:17.383497  2824 layer_factory.hpp:77] Creating layer BatchNorm8
I1006 11:25:17.383502  2824 net.cpp:84] Creating Layer BatchNorm8
I1006 11:25:17.383505  2824 net.cpp:406] BatchNorm8 <- Convolution8
I1006 11:25:17.383508  2824 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1006 11:25:17.383646  2824 net.cpp:122] Setting up BatchNorm8
I1006 11:25:17.383649  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.383652  2824 net.cpp:137] Memory required for data: 269927600
I1006 11:25:17.383656  2824 layer_factory.hpp:77] Creating layer Scale8
I1006 11:25:17.383661  2824 net.cpp:84] Creating Layer Scale8
I1006 11:25:17.383663  2824 net.cpp:406] Scale8 <- Convolution8
I1006 11:25:17.383666  2824 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1006 11:25:17.383693  2824 layer_factory.hpp:77] Creating layer Scale8
I1006 11:25:17.383769  2824 net.cpp:122] Setting up Scale8
I1006 11:25:17.383774  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.383775  2824 net.cpp:137] Memory required for data: 276481200
I1006 11:25:17.383780  2824 layer_factory.hpp:77] Creating layer penlu8
I1006 11:25:17.383785  2824 net.cpp:84] Creating Layer penlu8
I1006 11:25:17.383786  2824 net.cpp:406] penlu8 <- Convolution8
I1006 11:25:17.383800  2824 net.cpp:367] penlu8 -> Convolution8 (in-place)
I1006 11:25:17.383996  2824 net.cpp:122] Setting up penlu8
I1006 11:25:17.384001  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.384002  2824 net.cpp:137] Memory required for data: 283034800
I1006 11:25:17.384007  2824 layer_factory.hpp:77] Creating layer Convolution9
I1006 11:25:17.384013  2824 net.cpp:84] Creating Layer Convolution9
I1006 11:25:17.384016  2824 net.cpp:406] Convolution9 <- Convolution8
I1006 11:25:17.384021  2824 net.cpp:380] Convolution9 -> Convolution9
I1006 11:25:17.385509  2824 net.cpp:122] Setting up Convolution9
I1006 11:25:17.385519  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.385521  2824 net.cpp:137] Memory required for data: 289588400
I1006 11:25:17.385525  2824 layer_factory.hpp:77] Creating layer BatchNorm9
I1006 11:25:17.385531  2824 net.cpp:84] Creating Layer BatchNorm9
I1006 11:25:17.385534  2824 net.cpp:406] BatchNorm9 <- Convolution9
I1006 11:25:17.385537  2824 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1006 11:25:17.385675  2824 net.cpp:122] Setting up BatchNorm9
I1006 11:25:17.385680  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.385682  2824 net.cpp:137] Memory required for data: 296142000
I1006 11:25:17.385686  2824 layer_factory.hpp:77] Creating layer Scale9
I1006 11:25:17.385690  2824 net.cpp:84] Creating Layer Scale9
I1006 11:25:17.385694  2824 net.cpp:406] Scale9 <- Convolution9
I1006 11:25:17.385697  2824 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1006 11:25:17.385725  2824 layer_factory.hpp:77] Creating layer Scale9
I1006 11:25:17.385809  2824 net.cpp:122] Setting up Scale9
I1006 11:25:17.385814  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.385817  2824 net.cpp:137] Memory required for data: 302695600
I1006 11:25:17.385820  2824 layer_factory.hpp:77] Creating layer Eltwise4
I1006 11:25:17.385825  2824 net.cpp:84] Creating Layer Eltwise4
I1006 11:25:17.385828  2824 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I1006 11:25:17.385830  2824 net.cpp:406] Eltwise4 <- Convolution9
I1006 11:25:17.385833  2824 net.cpp:380] Eltwise4 -> Eltwise4
I1006 11:25:17.385850  2824 net.cpp:122] Setting up Eltwise4
I1006 11:25:17.385854  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.385855  2824 net.cpp:137] Memory required for data: 309249200
I1006 11:25:17.385857  2824 layer_factory.hpp:77] Creating layer penlu9
I1006 11:25:17.385864  2824 net.cpp:84] Creating Layer penlu9
I1006 11:25:17.385865  2824 net.cpp:406] penlu9 <- Eltwise4
I1006 11:25:17.385869  2824 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1006 11:25:17.385987  2824 net.cpp:122] Setting up penlu9
I1006 11:25:17.385990  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.385993  2824 net.cpp:137] Memory required for data: 315802800
I1006 11:25:17.385996  2824 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1006 11:25:17.386000  2824 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1006 11:25:17.386003  2824 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1006 11:25:17.386006  2824 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1006 11:25:17.386009  2824 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1006 11:25:17.386034  2824 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1006 11:25:17.386036  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.386039  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.386041  2824 net.cpp:137] Memory required for data: 328910000
I1006 11:25:17.386044  2824 layer_factory.hpp:77] Creating layer Convolution10
I1006 11:25:17.386049  2824 net.cpp:84] Creating Layer Convolution10
I1006 11:25:17.386051  2824 net.cpp:406] Convolution10 <- Eltwise4_penlu9_0_split_0
I1006 11:25:17.386056  2824 net.cpp:380] Convolution10 -> Convolution10
I1006 11:25:17.386624  2824 net.cpp:122] Setting up Convolution10
I1006 11:25:17.386632  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.386636  2824 net.cpp:137] Memory required for data: 335463600
I1006 11:25:17.386639  2824 layer_factory.hpp:77] Creating layer BatchNorm10
I1006 11:25:17.386643  2824 net.cpp:84] Creating Layer BatchNorm10
I1006 11:25:17.386646  2824 net.cpp:406] BatchNorm10 <- Convolution10
I1006 11:25:17.386651  2824 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1006 11:25:17.386787  2824 net.cpp:122] Setting up BatchNorm10
I1006 11:25:17.386791  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.386793  2824 net.cpp:137] Memory required for data: 342017200
I1006 11:25:17.386798  2824 layer_factory.hpp:77] Creating layer Scale10
I1006 11:25:17.386801  2824 net.cpp:84] Creating Layer Scale10
I1006 11:25:17.386804  2824 net.cpp:406] Scale10 <- Convolution10
I1006 11:25:17.386807  2824 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1006 11:25:17.386833  2824 layer_factory.hpp:77] Creating layer Scale10
I1006 11:25:17.386910  2824 net.cpp:122] Setting up Scale10
I1006 11:25:17.386914  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.386916  2824 net.cpp:137] Memory required for data: 348570800
I1006 11:25:17.386920  2824 layer_factory.hpp:77] Creating layer penlu10
I1006 11:25:17.386925  2824 net.cpp:84] Creating Layer penlu10
I1006 11:25:17.386927  2824 net.cpp:406] penlu10 <- Convolution10
I1006 11:25:17.386931  2824 net.cpp:367] penlu10 -> Convolution10 (in-place)
I1006 11:25:17.387048  2824 net.cpp:122] Setting up penlu10
I1006 11:25:17.387051  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.387053  2824 net.cpp:137] Memory required for data: 355124400
I1006 11:25:17.387064  2824 layer_factory.hpp:77] Creating layer Convolution11
I1006 11:25:17.387070  2824 net.cpp:84] Creating Layer Convolution11
I1006 11:25:17.387073  2824 net.cpp:406] Convolution11 <- Convolution10
I1006 11:25:17.387076  2824 net.cpp:380] Convolution11 -> Convolution11
I1006 11:25:17.388000  2824 net.cpp:122] Setting up Convolution11
I1006 11:25:17.388008  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.388010  2824 net.cpp:137] Memory required for data: 361678000
I1006 11:25:17.388015  2824 layer_factory.hpp:77] Creating layer BatchNorm11
I1006 11:25:17.388020  2824 net.cpp:84] Creating Layer BatchNorm11
I1006 11:25:17.388023  2824 net.cpp:406] BatchNorm11 <- Convolution11
I1006 11:25:17.388027  2824 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1006 11:25:17.388164  2824 net.cpp:122] Setting up BatchNorm11
I1006 11:25:17.388170  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.388171  2824 net.cpp:137] Memory required for data: 368231600
I1006 11:25:17.388175  2824 layer_factory.hpp:77] Creating layer Scale11
I1006 11:25:17.388180  2824 net.cpp:84] Creating Layer Scale11
I1006 11:25:17.388181  2824 net.cpp:406] Scale11 <- Convolution11
I1006 11:25:17.388186  2824 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1006 11:25:17.388212  2824 layer_factory.hpp:77] Creating layer Scale11
I1006 11:25:17.388288  2824 net.cpp:122] Setting up Scale11
I1006 11:25:17.388293  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.388294  2824 net.cpp:137] Memory required for data: 374785200
I1006 11:25:17.388298  2824 layer_factory.hpp:77] Creating layer Eltwise5
I1006 11:25:17.388303  2824 net.cpp:84] Creating Layer Eltwise5
I1006 11:25:17.388304  2824 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1006 11:25:17.388308  2824 net.cpp:406] Eltwise5 <- Convolution11
I1006 11:25:17.388310  2824 net.cpp:380] Eltwise5 -> Eltwise5
I1006 11:25:17.388326  2824 net.cpp:122] Setting up Eltwise5
I1006 11:25:17.388329  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.388331  2824 net.cpp:137] Memory required for data: 381338800
I1006 11:25:17.388334  2824 layer_factory.hpp:77] Creating layer penlu11
I1006 11:25:17.388339  2824 net.cpp:84] Creating Layer penlu11
I1006 11:25:17.388341  2824 net.cpp:406] penlu11 <- Eltwise5
I1006 11:25:17.388345  2824 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1006 11:25:17.388463  2824 net.cpp:122] Setting up penlu11
I1006 11:25:17.388466  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.388468  2824 net.cpp:137] Memory required for data: 387892400
I1006 11:25:17.388473  2824 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1006 11:25:17.388476  2824 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1006 11:25:17.388478  2824 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1006 11:25:17.388481  2824 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1006 11:25:17.388485  2824 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1006 11:25:17.388509  2824 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1006 11:25:17.388512  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.388515  2824 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1006 11:25:17.388517  2824 net.cpp:137] Memory required for data: 400999600
I1006 11:25:17.388520  2824 layer_factory.hpp:77] Creating layer Convolution12
I1006 11:25:17.388525  2824 net.cpp:84] Creating Layer Convolution12
I1006 11:25:17.388526  2824 net.cpp:406] Convolution12 <- Eltwise5_penlu11_0_split_0
I1006 11:25:17.388531  2824 net.cpp:380] Convolution12 -> Convolution12
I1006 11:25:17.389397  2824 net.cpp:122] Setting up Convolution12
I1006 11:25:17.389406  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.389408  2824 net.cpp:137] Memory required for data: 404276400
I1006 11:25:17.389413  2824 layer_factory.hpp:77] Creating layer BatchNorm12
I1006 11:25:17.389418  2824 net.cpp:84] Creating Layer BatchNorm12
I1006 11:25:17.389420  2824 net.cpp:406] BatchNorm12 <- Convolution12
I1006 11:25:17.389430  2824 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1006 11:25:17.389566  2824 net.cpp:122] Setting up BatchNorm12
I1006 11:25:17.389571  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.389574  2824 net.cpp:137] Memory required for data: 407553200
I1006 11:25:17.389578  2824 layer_factory.hpp:77] Creating layer Scale12
I1006 11:25:17.389582  2824 net.cpp:84] Creating Layer Scale12
I1006 11:25:17.389585  2824 net.cpp:406] Scale12 <- Convolution12
I1006 11:25:17.389587  2824 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1006 11:25:17.411098  2824 layer_factory.hpp:77] Creating layer Scale12
I1006 11:25:17.411202  2824 net.cpp:122] Setting up Scale12
I1006 11:25:17.411209  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.411212  2824 net.cpp:137] Memory required for data: 410830000
I1006 11:25:17.411217  2824 layer_factory.hpp:77] Creating layer Convolution13
I1006 11:25:17.411224  2824 net.cpp:84] Creating Layer Convolution13
I1006 11:25:17.411227  2824 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_1
I1006 11:25:17.411232  2824 net.cpp:380] Convolution13 -> Convolution13
I1006 11:25:17.412303  2824 net.cpp:122] Setting up Convolution13
I1006 11:25:17.412312  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.412315  2824 net.cpp:137] Memory required for data: 414106800
I1006 11:25:17.412320  2824 layer_factory.hpp:77] Creating layer BatchNorm13
I1006 11:25:17.412326  2824 net.cpp:84] Creating Layer BatchNorm13
I1006 11:25:17.412329  2824 net.cpp:406] BatchNorm13 <- Convolution13
I1006 11:25:17.412333  2824 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1006 11:25:17.412523  2824 net.cpp:122] Setting up BatchNorm13
I1006 11:25:17.412533  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.412537  2824 net.cpp:137] Memory required for data: 417383600
I1006 11:25:17.412546  2824 layer_factory.hpp:77] Creating layer Scale13
I1006 11:25:17.412554  2824 net.cpp:84] Creating Layer Scale13
I1006 11:25:17.412557  2824 net.cpp:406] Scale13 <- Convolution13
I1006 11:25:17.412561  2824 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1006 11:25:17.412593  2824 layer_factory.hpp:77] Creating layer Scale13
I1006 11:25:17.412675  2824 net.cpp:122] Setting up Scale13
I1006 11:25:17.412680  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.412684  2824 net.cpp:137] Memory required for data: 420660400
I1006 11:25:17.412690  2824 layer_factory.hpp:77] Creating layer penlu12
I1006 11:25:17.412698  2824 net.cpp:84] Creating Layer penlu12
I1006 11:25:17.412703  2824 net.cpp:406] penlu12 <- Convolution13
I1006 11:25:17.412708  2824 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1006 11:25:17.412845  2824 net.cpp:122] Setting up penlu12
I1006 11:25:17.412852  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.412853  2824 net.cpp:137] Memory required for data: 423937200
I1006 11:25:17.412858  2824 layer_factory.hpp:77] Creating layer Convolution14
I1006 11:25:17.412868  2824 net.cpp:84] Creating Layer Convolution14
I1006 11:25:17.412871  2824 net.cpp:406] Convolution14 <- Convolution13
I1006 11:25:17.412875  2824 net.cpp:380] Convolution14 -> Convolution14
I1006 11:25:17.414067  2824 net.cpp:122] Setting up Convolution14
I1006 11:25:17.414074  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.414077  2824 net.cpp:137] Memory required for data: 427214000
I1006 11:25:17.414093  2824 layer_factory.hpp:77] Creating layer BatchNorm14
I1006 11:25:17.414098  2824 net.cpp:84] Creating Layer BatchNorm14
I1006 11:25:17.414100  2824 net.cpp:406] BatchNorm14 <- Convolution14
I1006 11:25:17.414104  2824 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1006 11:25:17.414253  2824 net.cpp:122] Setting up BatchNorm14
I1006 11:25:17.414258  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.414260  2824 net.cpp:137] Memory required for data: 430490800
I1006 11:25:17.414266  2824 layer_factory.hpp:77] Creating layer Scale14
I1006 11:25:17.414270  2824 net.cpp:84] Creating Layer Scale14
I1006 11:25:17.414280  2824 net.cpp:406] Scale14 <- Convolution14
I1006 11:25:17.414285  2824 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1006 11:25:17.414314  2824 layer_factory.hpp:77] Creating layer Scale14
I1006 11:25:17.414415  2824 net.cpp:122] Setting up Scale14
I1006 11:25:17.414419  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.414422  2824 net.cpp:137] Memory required for data: 433767600
I1006 11:25:17.414425  2824 layer_factory.hpp:77] Creating layer Eltwise6
I1006 11:25:17.414429  2824 net.cpp:84] Creating Layer Eltwise6
I1006 11:25:17.414433  2824 net.cpp:406] Eltwise6 <- Convolution12
I1006 11:25:17.414435  2824 net.cpp:406] Eltwise6 <- Convolution14
I1006 11:25:17.414438  2824 net.cpp:380] Eltwise6 -> Eltwise6
I1006 11:25:17.414453  2824 net.cpp:122] Setting up Eltwise6
I1006 11:25:17.414456  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.414458  2824 net.cpp:137] Memory required for data: 437044400
I1006 11:25:17.414460  2824 layer_factory.hpp:77] Creating layer penlu13
I1006 11:25:17.414465  2824 net.cpp:84] Creating Layer penlu13
I1006 11:25:17.414469  2824 net.cpp:406] penlu13 <- Eltwise6
I1006 11:25:17.414471  2824 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1006 11:25:17.414629  2824 net.cpp:122] Setting up penlu13
I1006 11:25:17.414634  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.414636  2824 net.cpp:137] Memory required for data: 440321200
I1006 11:25:17.414640  2824 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1006 11:25:17.414645  2824 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1006 11:25:17.414647  2824 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1006 11:25:17.414651  2824 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1006 11:25:17.414655  2824 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1006 11:25:17.414680  2824 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1006 11:25:17.414685  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.414686  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.414690  2824 net.cpp:137] Memory required for data: 446874800
I1006 11:25:17.414691  2824 layer_factory.hpp:77] Creating layer Convolution15
I1006 11:25:17.414696  2824 net.cpp:84] Creating Layer Convolution15
I1006 11:25:17.414700  2824 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1006 11:25:17.414705  2824 net.cpp:380] Convolution15 -> Convolution15
I1006 11:25:17.416148  2824 net.cpp:122] Setting up Convolution15
I1006 11:25:17.416157  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.416160  2824 net.cpp:137] Memory required for data: 450151600
I1006 11:25:17.416165  2824 layer_factory.hpp:77] Creating layer BatchNorm15
I1006 11:25:17.416170  2824 net.cpp:84] Creating Layer BatchNorm15
I1006 11:25:17.416173  2824 net.cpp:406] BatchNorm15 <- Convolution15
I1006 11:25:17.416177  2824 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1006 11:25:17.416324  2824 net.cpp:122] Setting up BatchNorm15
I1006 11:25:17.416329  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.416332  2824 net.cpp:137] Memory required for data: 453428400
I1006 11:25:17.416337  2824 layer_factory.hpp:77] Creating layer Scale15
I1006 11:25:17.416340  2824 net.cpp:84] Creating Layer Scale15
I1006 11:25:17.416343  2824 net.cpp:406] Scale15 <- Convolution15
I1006 11:25:17.416347  2824 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1006 11:25:17.416376  2824 layer_factory.hpp:77] Creating layer Scale15
I1006 11:25:17.416460  2824 net.cpp:122] Setting up Scale15
I1006 11:25:17.416465  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.416467  2824 net.cpp:137] Memory required for data: 456705200
I1006 11:25:17.416471  2824 layer_factory.hpp:77] Creating layer penlu14
I1006 11:25:17.416476  2824 net.cpp:84] Creating Layer penlu14
I1006 11:25:17.416478  2824 net.cpp:406] penlu14 <- Convolution15
I1006 11:25:17.416482  2824 net.cpp:367] penlu14 -> Convolution15 (in-place)
I1006 11:25:17.416599  2824 net.cpp:122] Setting up penlu14
I1006 11:25:17.416611  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.416613  2824 net.cpp:137] Memory required for data: 459982000
I1006 11:25:17.416618  2824 layer_factory.hpp:77] Creating layer Convolution16
I1006 11:25:17.416625  2824 net.cpp:84] Creating Layer Convolution16
I1006 11:25:17.416627  2824 net.cpp:406] Convolution16 <- Convolution15
I1006 11:25:17.416631  2824 net.cpp:380] Convolution16 -> Convolution16
I1006 11:25:17.418289  2824 net.cpp:122] Setting up Convolution16
I1006 11:25:17.418298  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.418301  2824 net.cpp:137] Memory required for data: 463258800
I1006 11:25:17.418305  2824 layer_factory.hpp:77] Creating layer BatchNorm16
I1006 11:25:17.418313  2824 net.cpp:84] Creating Layer BatchNorm16
I1006 11:25:17.418315  2824 net.cpp:406] BatchNorm16 <- Convolution16
I1006 11:25:17.418318  2824 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1006 11:25:17.418462  2824 net.cpp:122] Setting up BatchNorm16
I1006 11:25:17.418467  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.418469  2824 net.cpp:137] Memory required for data: 466535600
I1006 11:25:17.418474  2824 layer_factory.hpp:77] Creating layer Scale16
I1006 11:25:17.418478  2824 net.cpp:84] Creating Layer Scale16
I1006 11:25:17.418481  2824 net.cpp:406] Scale16 <- Convolution16
I1006 11:25:17.418484  2824 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1006 11:25:17.418514  2824 layer_factory.hpp:77] Creating layer Scale16
I1006 11:25:17.418596  2824 net.cpp:122] Setting up Scale16
I1006 11:25:17.418599  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.418602  2824 net.cpp:137] Memory required for data: 469812400
I1006 11:25:17.418606  2824 layer_factory.hpp:77] Creating layer Eltwise7
I1006 11:25:17.418611  2824 net.cpp:84] Creating Layer Eltwise7
I1006 11:25:17.418613  2824 net.cpp:406] Eltwise7 <- Eltwise6_penlu13_0_split_1
I1006 11:25:17.418617  2824 net.cpp:406] Eltwise7 <- Convolution16
I1006 11:25:17.418619  2824 net.cpp:380] Eltwise7 -> Eltwise7
I1006 11:25:17.418633  2824 net.cpp:122] Setting up Eltwise7
I1006 11:25:17.418637  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.418638  2824 net.cpp:137] Memory required for data: 473089200
I1006 11:25:17.418642  2824 layer_factory.hpp:77] Creating layer penlu15
I1006 11:25:17.418648  2824 net.cpp:84] Creating Layer penlu15
I1006 11:25:17.418649  2824 net.cpp:406] penlu15 <- Eltwise7
I1006 11:25:17.418653  2824 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1006 11:25:17.418771  2824 net.cpp:122] Setting up penlu15
I1006 11:25:17.418776  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.418778  2824 net.cpp:137] Memory required for data: 476366000
I1006 11:25:17.418783  2824 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1006 11:25:17.418787  2824 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1006 11:25:17.418789  2824 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1006 11:25:17.418793  2824 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1006 11:25:17.418797  2824 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1006 11:25:17.418823  2824 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1006 11:25:17.418826  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.418829  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.418831  2824 net.cpp:137] Memory required for data: 482919600
I1006 11:25:17.418833  2824 layer_factory.hpp:77] Creating layer Convolution17
I1006 11:25:17.418839  2824 net.cpp:84] Creating Layer Convolution17
I1006 11:25:17.418843  2824 net.cpp:406] Convolution17 <- Eltwise7_penlu15_0_split_0
I1006 11:25:17.418846  2824 net.cpp:380] Convolution17 -> Convolution17
I1006 11:25:17.419950  2824 net.cpp:122] Setting up Convolution17
I1006 11:25:17.419960  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.419961  2824 net.cpp:137] Memory required for data: 486196400
I1006 11:25:17.419965  2824 layer_factory.hpp:77] Creating layer BatchNorm17
I1006 11:25:17.419978  2824 net.cpp:84] Creating Layer BatchNorm17
I1006 11:25:17.419981  2824 net.cpp:406] BatchNorm17 <- Convolution17
I1006 11:25:17.419986  2824 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1006 11:25:17.420127  2824 net.cpp:122] Setting up BatchNorm17
I1006 11:25:17.420131  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.420133  2824 net.cpp:137] Memory required for data: 489473200
I1006 11:25:17.420138  2824 layer_factory.hpp:77] Creating layer Scale17
I1006 11:25:17.420143  2824 net.cpp:84] Creating Layer Scale17
I1006 11:25:17.420145  2824 net.cpp:406] Scale17 <- Convolution17
I1006 11:25:17.420150  2824 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1006 11:25:17.420177  2824 layer_factory.hpp:77] Creating layer Scale17
I1006 11:25:17.420258  2824 net.cpp:122] Setting up Scale17
I1006 11:25:17.420261  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.420264  2824 net.cpp:137] Memory required for data: 492750000
I1006 11:25:17.420267  2824 layer_factory.hpp:77] Creating layer penlu16
I1006 11:25:17.420272  2824 net.cpp:84] Creating Layer penlu16
I1006 11:25:17.420275  2824 net.cpp:406] penlu16 <- Convolution17
I1006 11:25:17.420279  2824 net.cpp:367] penlu16 -> Convolution17 (in-place)
I1006 11:25:17.420390  2824 net.cpp:122] Setting up penlu16
I1006 11:25:17.420395  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.420397  2824 net.cpp:137] Memory required for data: 496026800
I1006 11:25:17.420402  2824 layer_factory.hpp:77] Creating layer Convolution18
I1006 11:25:17.420408  2824 net.cpp:84] Creating Layer Convolution18
I1006 11:25:17.420409  2824 net.cpp:406] Convolution18 <- Convolution17
I1006 11:25:17.420414  2824 net.cpp:380] Convolution18 -> Convolution18
I1006 11:25:17.421499  2824 net.cpp:122] Setting up Convolution18
I1006 11:25:17.421506  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.421509  2824 net.cpp:137] Memory required for data: 499303600
I1006 11:25:17.421514  2824 layer_factory.hpp:77] Creating layer BatchNorm18
I1006 11:25:17.421519  2824 net.cpp:84] Creating Layer BatchNorm18
I1006 11:25:17.421521  2824 net.cpp:406] BatchNorm18 <- Convolution18
I1006 11:25:17.421525  2824 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1006 11:25:17.421664  2824 net.cpp:122] Setting up BatchNorm18
I1006 11:25:17.421669  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.421671  2824 net.cpp:137] Memory required for data: 502580400
I1006 11:25:17.421675  2824 layer_factory.hpp:77] Creating layer Scale18
I1006 11:25:17.421680  2824 net.cpp:84] Creating Layer Scale18
I1006 11:25:17.421682  2824 net.cpp:406] Scale18 <- Convolution18
I1006 11:25:17.421685  2824 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1006 11:25:17.421713  2824 layer_factory.hpp:77] Creating layer Scale18
I1006 11:25:17.421795  2824 net.cpp:122] Setting up Scale18
I1006 11:25:17.421799  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.421802  2824 net.cpp:137] Memory required for data: 505857200
I1006 11:25:17.421805  2824 layer_factory.hpp:77] Creating layer Eltwise8
I1006 11:25:17.421810  2824 net.cpp:84] Creating Layer Eltwise8
I1006 11:25:17.421813  2824 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1006 11:25:17.421815  2824 net.cpp:406] Eltwise8 <- Convolution18
I1006 11:25:17.421819  2824 net.cpp:380] Eltwise8 -> Eltwise8
I1006 11:25:17.421833  2824 net.cpp:122] Setting up Eltwise8
I1006 11:25:17.421835  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.421838  2824 net.cpp:137] Memory required for data: 509134000
I1006 11:25:17.421840  2824 layer_factory.hpp:77] Creating layer penlu17
I1006 11:25:17.421845  2824 net.cpp:84] Creating Layer penlu17
I1006 11:25:17.421849  2824 net.cpp:406] penlu17 <- Eltwise8
I1006 11:25:17.421851  2824 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1006 11:25:17.421965  2824 net.cpp:122] Setting up penlu17
I1006 11:25:17.421970  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.421978  2824 net.cpp:137] Memory required for data: 512410800
I1006 11:25:17.421983  2824 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1006 11:25:17.421986  2824 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1006 11:25:17.421988  2824 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1006 11:25:17.421991  2824 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1006 11:25:17.441459  2824 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1006 11:25:17.441504  2824 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1006 11:25:17.441509  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.441512  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.441515  2824 net.cpp:137] Memory required for data: 518964400
I1006 11:25:17.441517  2824 layer_factory.hpp:77] Creating layer Convolution19
I1006 11:25:17.441525  2824 net.cpp:84] Creating Layer Convolution19
I1006 11:25:17.441529  2824 net.cpp:406] Convolution19 <- Eltwise8_penlu17_0_split_0
I1006 11:25:17.441532  2824 net.cpp:380] Convolution19 -> Convolution19
I1006 11:25:17.442766  2824 net.cpp:122] Setting up Convolution19
I1006 11:25:17.442778  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.442783  2824 net.cpp:137] Memory required for data: 522241200
I1006 11:25:17.442790  2824 layer_factory.hpp:77] Creating layer BatchNorm19
I1006 11:25:17.442800  2824 net.cpp:84] Creating Layer BatchNorm19
I1006 11:25:17.442804  2824 net.cpp:406] BatchNorm19 <- Convolution19
I1006 11:25:17.442811  2824 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1006 11:25:17.442981  2824 net.cpp:122] Setting up BatchNorm19
I1006 11:25:17.442991  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.442994  2824 net.cpp:137] Memory required for data: 525518000
I1006 11:25:17.443001  2824 layer_factory.hpp:77] Creating layer Scale19
I1006 11:25:17.443006  2824 net.cpp:84] Creating Layer Scale19
I1006 11:25:17.443008  2824 net.cpp:406] Scale19 <- Convolution19
I1006 11:25:17.443013  2824 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1006 11:25:17.443059  2824 layer_factory.hpp:77] Creating layer Scale19
I1006 11:25:17.443173  2824 net.cpp:122] Setting up Scale19
I1006 11:25:17.443178  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.443181  2824 net.cpp:137] Memory required for data: 528794800
I1006 11:25:17.443186  2824 layer_factory.hpp:77] Creating layer penlu18
I1006 11:25:17.443190  2824 net.cpp:84] Creating Layer penlu18
I1006 11:25:17.443192  2824 net.cpp:406] penlu18 <- Convolution19
I1006 11:25:17.443197  2824 net.cpp:367] penlu18 -> Convolution19 (in-place)
I1006 11:25:17.443315  2824 net.cpp:122] Setting up penlu18
I1006 11:25:17.443320  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.443321  2824 net.cpp:137] Memory required for data: 532071600
I1006 11:25:17.443326  2824 layer_factory.hpp:77] Creating layer Convolution20
I1006 11:25:17.443333  2824 net.cpp:84] Creating Layer Convolution20
I1006 11:25:17.443336  2824 net.cpp:406] Convolution20 <- Convolution19
I1006 11:25:17.443341  2824 net.cpp:380] Convolution20 -> Convolution20
I1006 11:25:17.444104  2824 net.cpp:122] Setting up Convolution20
I1006 11:25:17.444113  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.444115  2824 net.cpp:137] Memory required for data: 535348400
I1006 11:25:17.444119  2824 layer_factory.hpp:77] Creating layer BatchNorm20
I1006 11:25:17.444124  2824 net.cpp:84] Creating Layer BatchNorm20
I1006 11:25:17.444126  2824 net.cpp:406] BatchNorm20 <- Convolution20
I1006 11:25:17.444130  2824 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1006 11:25:17.444278  2824 net.cpp:122] Setting up BatchNorm20
I1006 11:25:17.444283  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.444284  2824 net.cpp:137] Memory required for data: 538625200
I1006 11:25:17.444289  2824 layer_factory.hpp:77] Creating layer Scale20
I1006 11:25:17.444293  2824 net.cpp:84] Creating Layer Scale20
I1006 11:25:17.444295  2824 net.cpp:406] Scale20 <- Convolution20
I1006 11:25:17.444306  2824 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1006 11:25:17.444339  2824 layer_factory.hpp:77] Creating layer Scale20
I1006 11:25:17.444422  2824 net.cpp:122] Setting up Scale20
I1006 11:25:17.444427  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.444428  2824 net.cpp:137] Memory required for data: 541902000
I1006 11:25:17.444432  2824 layer_factory.hpp:77] Creating layer Eltwise9
I1006 11:25:17.444437  2824 net.cpp:84] Creating Layer Eltwise9
I1006 11:25:17.444440  2824 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1006 11:25:17.444443  2824 net.cpp:406] Eltwise9 <- Convolution20
I1006 11:25:17.444447  2824 net.cpp:380] Eltwise9 -> Eltwise9
I1006 11:25:17.444460  2824 net.cpp:122] Setting up Eltwise9
I1006 11:25:17.444463  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.444465  2824 net.cpp:137] Memory required for data: 545178800
I1006 11:25:17.444468  2824 layer_factory.hpp:77] Creating layer penlu19
I1006 11:25:17.444473  2824 net.cpp:84] Creating Layer penlu19
I1006 11:25:17.444476  2824 net.cpp:406] penlu19 <- Eltwise9
I1006 11:25:17.444480  2824 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1006 11:25:17.444602  2824 net.cpp:122] Setting up penlu19
I1006 11:25:17.444607  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.444609  2824 net.cpp:137] Memory required for data: 548455600
I1006 11:25:17.444613  2824 layer_factory.hpp:77] Creating layer Eltwise9_penlu19_0_split
I1006 11:25:17.444617  2824 net.cpp:84] Creating Layer Eltwise9_penlu19_0_split
I1006 11:25:17.444619  2824 net.cpp:406] Eltwise9_penlu19_0_split <- Eltwise9
I1006 11:25:17.444624  2824 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_0
I1006 11:25:17.444628  2824 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_1
I1006 11:25:17.444653  2824 net.cpp:122] Setting up Eltwise9_penlu19_0_split
I1006 11:25:17.444658  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.444660  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.444663  2824 net.cpp:137] Memory required for data: 555009200
I1006 11:25:17.444664  2824 layer_factory.hpp:77] Creating layer Convolution21
I1006 11:25:17.444671  2824 net.cpp:84] Creating Layer Convolution21
I1006 11:25:17.444674  2824 net.cpp:406] Convolution21 <- Eltwise9_penlu19_0_split_0
I1006 11:25:17.444677  2824 net.cpp:380] Convolution21 -> Convolution21
I1006 11:25:17.445785  2824 net.cpp:122] Setting up Convolution21
I1006 11:25:17.445793  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.445796  2824 net.cpp:137] Memory required for data: 558286000
I1006 11:25:17.445801  2824 layer_factory.hpp:77] Creating layer BatchNorm21
I1006 11:25:17.445806  2824 net.cpp:84] Creating Layer BatchNorm21
I1006 11:25:17.445808  2824 net.cpp:406] BatchNorm21 <- Convolution21
I1006 11:25:17.445812  2824 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1006 11:25:17.445955  2824 net.cpp:122] Setting up BatchNorm21
I1006 11:25:17.445960  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.445962  2824 net.cpp:137] Memory required for data: 561562800
I1006 11:25:17.445966  2824 layer_factory.hpp:77] Creating layer Scale21
I1006 11:25:17.445971  2824 net.cpp:84] Creating Layer Scale21
I1006 11:25:17.445973  2824 net.cpp:406] Scale21 <- Convolution21
I1006 11:25:17.445976  2824 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1006 11:25:17.446004  2824 layer_factory.hpp:77] Creating layer Scale21
I1006 11:25:17.446087  2824 net.cpp:122] Setting up Scale21
I1006 11:25:17.446092  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.446094  2824 net.cpp:137] Memory required for data: 564839600
I1006 11:25:17.446099  2824 layer_factory.hpp:77] Creating layer penlu20
I1006 11:25:17.446102  2824 net.cpp:84] Creating Layer penlu20
I1006 11:25:17.446105  2824 net.cpp:406] penlu20 <- Convolution21
I1006 11:25:17.446110  2824 net.cpp:367] penlu20 -> Convolution21 (in-place)
I1006 11:25:17.446225  2824 net.cpp:122] Setting up penlu20
I1006 11:25:17.446235  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.446238  2824 net.cpp:137] Memory required for data: 568116400
I1006 11:25:17.446243  2824 layer_factory.hpp:77] Creating layer Convolution22
I1006 11:25:17.446249  2824 net.cpp:84] Creating Layer Convolution22
I1006 11:25:17.446252  2824 net.cpp:406] Convolution22 <- Convolution21
I1006 11:25:17.446256  2824 net.cpp:380] Convolution22 -> Convolution22
I1006 11:25:17.447374  2824 net.cpp:122] Setting up Convolution22
I1006 11:25:17.447383  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.447386  2824 net.cpp:137] Memory required for data: 571393200
I1006 11:25:17.447391  2824 layer_factory.hpp:77] Creating layer BatchNorm22
I1006 11:25:17.447396  2824 net.cpp:84] Creating Layer BatchNorm22
I1006 11:25:17.447399  2824 net.cpp:406] BatchNorm22 <- Convolution22
I1006 11:25:17.447402  2824 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I1006 11:25:17.447547  2824 net.cpp:122] Setting up BatchNorm22
I1006 11:25:17.447552  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.447554  2824 net.cpp:137] Memory required for data: 574670000
I1006 11:25:17.447559  2824 layer_factory.hpp:77] Creating layer Scale22
I1006 11:25:17.447563  2824 net.cpp:84] Creating Layer Scale22
I1006 11:25:17.447566  2824 net.cpp:406] Scale22 <- Convolution22
I1006 11:25:17.447569  2824 net.cpp:367] Scale22 -> Convolution22 (in-place)
I1006 11:25:17.447597  2824 layer_factory.hpp:77] Creating layer Scale22
I1006 11:25:17.447680  2824 net.cpp:122] Setting up Scale22
I1006 11:25:17.447685  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.447686  2824 net.cpp:137] Memory required for data: 577946800
I1006 11:25:17.447690  2824 layer_factory.hpp:77] Creating layer Eltwise10
I1006 11:25:17.447695  2824 net.cpp:84] Creating Layer Eltwise10
I1006 11:25:17.447697  2824 net.cpp:406] Eltwise10 <- Eltwise9_penlu19_0_split_1
I1006 11:25:17.447700  2824 net.cpp:406] Eltwise10 <- Convolution22
I1006 11:25:17.447703  2824 net.cpp:380] Eltwise10 -> Eltwise10
I1006 11:25:17.447718  2824 net.cpp:122] Setting up Eltwise10
I1006 11:25:17.447721  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.447723  2824 net.cpp:137] Memory required for data: 581223600
I1006 11:25:17.447726  2824 layer_factory.hpp:77] Creating layer penlu21
I1006 11:25:17.447731  2824 net.cpp:84] Creating Layer penlu21
I1006 11:25:17.447732  2824 net.cpp:406] penlu21 <- Eltwise10
I1006 11:25:17.447736  2824 net.cpp:367] penlu21 -> Eltwise10 (in-place)
I1006 11:25:17.447855  2824 net.cpp:122] Setting up penlu21
I1006 11:25:17.447860  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.447861  2824 net.cpp:137] Memory required for data: 584500400
I1006 11:25:17.447865  2824 layer_factory.hpp:77] Creating layer Eltwise10_penlu21_0_split
I1006 11:25:17.447870  2824 net.cpp:84] Creating Layer Eltwise10_penlu21_0_split
I1006 11:25:17.447871  2824 net.cpp:406] Eltwise10_penlu21_0_split <- Eltwise10
I1006 11:25:17.447875  2824 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_0
I1006 11:25:17.447878  2824 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_1
I1006 11:25:17.447904  2824 net.cpp:122] Setting up Eltwise10_penlu21_0_split
I1006 11:25:17.447907  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.447911  2824 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1006 11:25:17.447912  2824 net.cpp:137] Memory required for data: 591054000
I1006 11:25:17.447914  2824 layer_factory.hpp:77] Creating layer Convolution23
I1006 11:25:17.447921  2824 net.cpp:84] Creating Layer Convolution23
I1006 11:25:17.447923  2824 net.cpp:406] Convolution23 <- Eltwise10_penlu21_0_split_0
I1006 11:25:17.447927  2824 net.cpp:380] Convolution23 -> Convolution23
I1006 11:25:17.448879  2824 net.cpp:122] Setting up Convolution23
I1006 11:25:17.448889  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.448890  2824 net.cpp:137] Memory required for data: 592692400
I1006 11:25:17.448895  2824 layer_factory.hpp:77] Creating layer BatchNorm23
I1006 11:25:17.448907  2824 net.cpp:84] Creating Layer BatchNorm23
I1006 11:25:17.448910  2824 net.cpp:406] BatchNorm23 <- Convolution23
I1006 11:25:17.448915  2824 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I1006 11:25:17.449064  2824 net.cpp:122] Setting up BatchNorm23
I1006 11:25:17.449069  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.449070  2824 net.cpp:137] Memory required for data: 594330800
I1006 11:25:17.449075  2824 layer_factory.hpp:77] Creating layer Scale23
I1006 11:25:17.449080  2824 net.cpp:84] Creating Layer Scale23
I1006 11:25:17.449084  2824 net.cpp:406] Scale23 <- Convolution23
I1006 11:25:17.449086  2824 net.cpp:367] Scale23 -> Convolution23 (in-place)
I1006 11:25:17.449115  2824 layer_factory.hpp:77] Creating layer Scale23
I1006 11:25:17.449199  2824 net.cpp:122] Setting up Scale23
I1006 11:25:17.449204  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.449206  2824 net.cpp:137] Memory required for data: 595969200
I1006 11:25:17.449209  2824 layer_factory.hpp:77] Creating layer Convolution24
I1006 11:25:17.449216  2824 net.cpp:84] Creating Layer Convolution24
I1006 11:25:17.449219  2824 net.cpp:406] Convolution24 <- Eltwise10_penlu21_0_split_1
I1006 11:25:17.449223  2824 net.cpp:380] Convolution24 -> Convolution24
I1006 11:25:17.450531  2824 net.cpp:122] Setting up Convolution24
I1006 11:25:17.450539  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.450542  2824 net.cpp:137] Memory required for data: 597607600
I1006 11:25:17.450546  2824 layer_factory.hpp:77] Creating layer BatchNorm24
I1006 11:25:17.450551  2824 net.cpp:84] Creating Layer BatchNorm24
I1006 11:25:17.450554  2824 net.cpp:406] BatchNorm24 <- Convolution24
I1006 11:25:17.450559  2824 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I1006 11:25:17.450706  2824 net.cpp:122] Setting up BatchNorm24
I1006 11:25:17.450711  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.450713  2824 net.cpp:137] Memory required for data: 599246000
I1006 11:25:17.450717  2824 layer_factory.hpp:77] Creating layer Scale24
I1006 11:25:17.450722  2824 net.cpp:84] Creating Layer Scale24
I1006 11:25:17.450724  2824 net.cpp:406] Scale24 <- Convolution24
I1006 11:25:17.450728  2824 net.cpp:367] Scale24 -> Convolution24 (in-place)
I1006 11:25:17.450757  2824 layer_factory.hpp:77] Creating layer Scale24
I1006 11:25:17.450840  2824 net.cpp:122] Setting up Scale24
I1006 11:25:17.450845  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.450847  2824 net.cpp:137] Memory required for data: 600884400
I1006 11:25:17.450850  2824 layer_factory.hpp:77] Creating layer penlu22
I1006 11:25:17.450856  2824 net.cpp:84] Creating Layer penlu22
I1006 11:25:17.450860  2824 net.cpp:406] penlu22 <- Convolution24
I1006 11:25:17.450862  2824 net.cpp:367] penlu22 -> Convolution24 (in-place)
I1006 11:25:17.450979  2824 net.cpp:122] Setting up penlu22
I1006 11:25:17.450984  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.450986  2824 net.cpp:137] Memory required for data: 602522800
I1006 11:25:17.450990  2824 layer_factory.hpp:77] Creating layer Convolution25
I1006 11:25:17.450999  2824 net.cpp:84] Creating Layer Convolution25
I1006 11:25:17.451001  2824 net.cpp:406] Convolution25 <- Convolution24
I1006 11:25:17.451004  2824 net.cpp:380] Convolution25 -> Convolution25
I1006 11:25:17.452744  2824 net.cpp:122] Setting up Convolution25
I1006 11:25:17.452752  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.452755  2824 net.cpp:137] Memory required for data: 604161200
I1006 11:25:17.452759  2824 layer_factory.hpp:77] Creating layer BatchNorm25
I1006 11:25:17.452765  2824 net.cpp:84] Creating Layer BatchNorm25
I1006 11:25:17.452767  2824 net.cpp:406] BatchNorm25 <- Convolution25
I1006 11:25:17.452772  2824 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I1006 11:25:17.452924  2824 net.cpp:122] Setting up BatchNorm25
I1006 11:25:17.452929  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.452931  2824 net.cpp:137] Memory required for data: 605799600
I1006 11:25:17.452942  2824 layer_factory.hpp:77] Creating layer Scale25
I1006 11:25:17.452947  2824 net.cpp:84] Creating Layer Scale25
I1006 11:25:17.452950  2824 net.cpp:406] Scale25 <- Convolution25
I1006 11:25:17.452953  2824 net.cpp:367] Scale25 -> Convolution25 (in-place)
I1006 11:25:17.472405  2824 layer_factory.hpp:77] Creating layer Scale25
I1006 11:25:17.472507  2824 net.cpp:122] Setting up Scale25
I1006 11:25:17.472513  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.472517  2824 net.cpp:137] Memory required for data: 607438000
I1006 11:25:17.472522  2824 layer_factory.hpp:77] Creating layer Eltwise11
I1006 11:25:17.472527  2824 net.cpp:84] Creating Layer Eltwise11
I1006 11:25:17.472528  2824 net.cpp:406] Eltwise11 <- Convolution23
I1006 11:25:17.472532  2824 net.cpp:406] Eltwise11 <- Convolution25
I1006 11:25:17.472535  2824 net.cpp:380] Eltwise11 -> Eltwise11
I1006 11:25:17.472555  2824 net.cpp:122] Setting up Eltwise11
I1006 11:25:17.472559  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.472561  2824 net.cpp:137] Memory required for data: 609076400
I1006 11:25:17.472563  2824 layer_factory.hpp:77] Creating layer penlu23
I1006 11:25:17.472569  2824 net.cpp:84] Creating Layer penlu23
I1006 11:25:17.472573  2824 net.cpp:406] penlu23 <- Eltwise11
I1006 11:25:17.472575  2824 net.cpp:367] penlu23 -> Eltwise11 (in-place)
I1006 11:25:17.472702  2824 net.cpp:122] Setting up penlu23
I1006 11:25:17.472707  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.472709  2824 net.cpp:137] Memory required for data: 610714800
I1006 11:25:17.472714  2824 layer_factory.hpp:77] Creating layer Eltwise11_penlu23_0_split
I1006 11:25:17.472718  2824 net.cpp:84] Creating Layer Eltwise11_penlu23_0_split
I1006 11:25:17.472720  2824 net.cpp:406] Eltwise11_penlu23_0_split <- Eltwise11
I1006 11:25:17.472723  2824 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_0
I1006 11:25:17.472728  2824 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_1
I1006 11:25:17.472756  2824 net.cpp:122] Setting up Eltwise11_penlu23_0_split
I1006 11:25:17.472760  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.472764  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.472766  2824 net.cpp:137] Memory required for data: 613991600
I1006 11:25:17.472769  2824 layer_factory.hpp:77] Creating layer Convolution26
I1006 11:25:17.472775  2824 net.cpp:84] Creating Layer Convolution26
I1006 11:25:17.472779  2824 net.cpp:406] Convolution26 <- Eltwise11_penlu23_0_split_0
I1006 11:25:17.472782  2824 net.cpp:380] Convolution26 -> Convolution26
I1006 11:25:17.474702  2824 net.cpp:122] Setting up Convolution26
I1006 11:25:17.474714  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.474715  2824 net.cpp:137] Memory required for data: 615630000
I1006 11:25:17.474720  2824 layer_factory.hpp:77] Creating layer BatchNorm26
I1006 11:25:17.474726  2824 net.cpp:84] Creating Layer BatchNorm26
I1006 11:25:17.474728  2824 net.cpp:406] BatchNorm26 <- Convolution26
I1006 11:25:17.474732  2824 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I1006 11:25:17.474882  2824 net.cpp:122] Setting up BatchNorm26
I1006 11:25:17.474886  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.474889  2824 net.cpp:137] Memory required for data: 617268400
I1006 11:25:17.474894  2824 layer_factory.hpp:77] Creating layer Scale26
I1006 11:25:17.474897  2824 net.cpp:84] Creating Layer Scale26
I1006 11:25:17.474900  2824 net.cpp:406] Scale26 <- Convolution26
I1006 11:25:17.474903  2824 net.cpp:367] Scale26 -> Convolution26 (in-place)
I1006 11:25:17.474934  2824 layer_factory.hpp:77] Creating layer Scale26
I1006 11:25:17.475019  2824 net.cpp:122] Setting up Scale26
I1006 11:25:17.475033  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.475035  2824 net.cpp:137] Memory required for data: 618906800
I1006 11:25:17.475039  2824 layer_factory.hpp:77] Creating layer penlu24
I1006 11:25:17.475045  2824 net.cpp:84] Creating Layer penlu24
I1006 11:25:17.475054  2824 net.cpp:406] penlu24 <- Convolution26
I1006 11:25:17.475059  2824 net.cpp:367] penlu24 -> Convolution26 (in-place)
I1006 11:25:17.475194  2824 net.cpp:122] Setting up penlu24
I1006 11:25:17.475200  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.475203  2824 net.cpp:137] Memory required for data: 620545200
I1006 11:25:17.475208  2824 layer_factory.hpp:77] Creating layer Convolution27
I1006 11:25:17.475216  2824 net.cpp:84] Creating Layer Convolution27
I1006 11:25:17.475219  2824 net.cpp:406] Convolution27 <- Convolution26
I1006 11:25:17.475224  2824 net.cpp:380] Convolution27 -> Convolution27
I1006 11:25:17.476963  2824 net.cpp:122] Setting up Convolution27
I1006 11:25:17.476971  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.476974  2824 net.cpp:137] Memory required for data: 622183600
I1006 11:25:17.476979  2824 layer_factory.hpp:77] Creating layer BatchNorm27
I1006 11:25:17.476994  2824 net.cpp:84] Creating Layer BatchNorm27
I1006 11:25:17.476996  2824 net.cpp:406] BatchNorm27 <- Convolution27
I1006 11:25:17.477000  2824 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I1006 11:25:17.477151  2824 net.cpp:122] Setting up BatchNorm27
I1006 11:25:17.477156  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.477159  2824 net.cpp:137] Memory required for data: 623822000
I1006 11:25:17.477182  2824 layer_factory.hpp:77] Creating layer Scale27
I1006 11:25:17.477186  2824 net.cpp:84] Creating Layer Scale27
I1006 11:25:17.477190  2824 net.cpp:406] Scale27 <- Convolution27
I1006 11:25:17.477192  2824 net.cpp:367] Scale27 -> Convolution27 (in-place)
I1006 11:25:17.477224  2824 layer_factory.hpp:77] Creating layer Scale27
I1006 11:25:17.477309  2824 net.cpp:122] Setting up Scale27
I1006 11:25:17.477314  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.477316  2824 net.cpp:137] Memory required for data: 625460400
I1006 11:25:17.477320  2824 layer_factory.hpp:77] Creating layer Eltwise12
I1006 11:25:17.477324  2824 net.cpp:84] Creating Layer Eltwise12
I1006 11:25:17.477326  2824 net.cpp:406] Eltwise12 <- Eltwise11_penlu23_0_split_1
I1006 11:25:17.477329  2824 net.cpp:406] Eltwise12 <- Convolution27
I1006 11:25:17.477332  2824 net.cpp:380] Eltwise12 -> Eltwise12
I1006 11:25:17.477350  2824 net.cpp:122] Setting up Eltwise12
I1006 11:25:17.477355  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.477356  2824 net.cpp:137] Memory required for data: 627098800
I1006 11:25:17.477358  2824 layer_factory.hpp:77] Creating layer penlu25
I1006 11:25:17.477363  2824 net.cpp:84] Creating Layer penlu25
I1006 11:25:17.477366  2824 net.cpp:406] penlu25 <- Eltwise12
I1006 11:25:17.477371  2824 net.cpp:367] penlu25 -> Eltwise12 (in-place)
I1006 11:25:17.477491  2824 net.cpp:122] Setting up penlu25
I1006 11:25:17.477495  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.477497  2824 net.cpp:137] Memory required for data: 628737200
I1006 11:25:17.477502  2824 layer_factory.hpp:77] Creating layer Eltwise12_penlu25_0_split
I1006 11:25:17.477506  2824 net.cpp:84] Creating Layer Eltwise12_penlu25_0_split
I1006 11:25:17.477509  2824 net.cpp:406] Eltwise12_penlu25_0_split <- Eltwise12
I1006 11:25:17.477512  2824 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_0
I1006 11:25:17.477516  2824 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_1
I1006 11:25:17.477542  2824 net.cpp:122] Setting up Eltwise12_penlu25_0_split
I1006 11:25:17.477546  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.477548  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.477550  2824 net.cpp:137] Memory required for data: 632014000
I1006 11:25:17.477553  2824 layer_factory.hpp:77] Creating layer Convolution28
I1006 11:25:17.477558  2824 net.cpp:84] Creating Layer Convolution28
I1006 11:25:17.477561  2824 net.cpp:406] Convolution28 <- Eltwise12_penlu25_0_split_0
I1006 11:25:17.477566  2824 net.cpp:380] Convolution28 -> Convolution28
I1006 11:25:17.479602  2824 net.cpp:122] Setting up Convolution28
I1006 11:25:17.479611  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.479620  2824 net.cpp:137] Memory required for data: 633652400
I1006 11:25:17.479625  2824 layer_factory.hpp:77] Creating layer BatchNorm28
I1006 11:25:17.479630  2824 net.cpp:84] Creating Layer BatchNorm28
I1006 11:25:17.479634  2824 net.cpp:406] BatchNorm28 <- Convolution28
I1006 11:25:17.479637  2824 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I1006 11:25:17.479791  2824 net.cpp:122] Setting up BatchNorm28
I1006 11:25:17.479796  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.479799  2824 net.cpp:137] Memory required for data: 635290800
I1006 11:25:17.479804  2824 layer_factory.hpp:77] Creating layer Scale28
I1006 11:25:17.479807  2824 net.cpp:84] Creating Layer Scale28
I1006 11:25:17.479810  2824 net.cpp:406] Scale28 <- Convolution28
I1006 11:25:17.479813  2824 net.cpp:367] Scale28 -> Convolution28 (in-place)
I1006 11:25:17.479843  2824 layer_factory.hpp:77] Creating layer Scale28
I1006 11:25:17.479929  2824 net.cpp:122] Setting up Scale28
I1006 11:25:17.479933  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.479935  2824 net.cpp:137] Memory required for data: 636929200
I1006 11:25:17.479939  2824 layer_factory.hpp:77] Creating layer penlu26
I1006 11:25:17.479945  2824 net.cpp:84] Creating Layer penlu26
I1006 11:25:17.479948  2824 net.cpp:406] penlu26 <- Convolution28
I1006 11:25:17.479951  2824 net.cpp:367] penlu26 -> Convolution28 (in-place)
I1006 11:25:17.480070  2824 net.cpp:122] Setting up penlu26
I1006 11:25:17.480075  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.480077  2824 net.cpp:137] Memory required for data: 638567600
I1006 11:25:17.480082  2824 layer_factory.hpp:77] Creating layer Convolution29
I1006 11:25:17.480087  2824 net.cpp:84] Creating Layer Convolution29
I1006 11:25:17.480090  2824 net.cpp:406] Convolution29 <- Convolution28
I1006 11:25:17.480094  2824 net.cpp:380] Convolution29 -> Convolution29
I1006 11:25:17.482324  2824 net.cpp:122] Setting up Convolution29
I1006 11:25:17.482333  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.482336  2824 net.cpp:137] Memory required for data: 640206000
I1006 11:25:17.482340  2824 layer_factory.hpp:77] Creating layer BatchNorm29
I1006 11:25:17.482347  2824 net.cpp:84] Creating Layer BatchNorm29
I1006 11:25:17.482349  2824 net.cpp:406] BatchNorm29 <- Convolution29
I1006 11:25:17.482352  2824 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I1006 11:25:17.482504  2824 net.cpp:122] Setting up BatchNorm29
I1006 11:25:17.482508  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.482511  2824 net.cpp:137] Memory required for data: 641844400
I1006 11:25:17.482516  2824 layer_factory.hpp:77] Creating layer Scale29
I1006 11:25:17.482519  2824 net.cpp:84] Creating Layer Scale29
I1006 11:25:17.482522  2824 net.cpp:406] Scale29 <- Convolution29
I1006 11:25:17.482525  2824 net.cpp:367] Scale29 -> Convolution29 (in-place)
I1006 11:25:17.482555  2824 layer_factory.hpp:77] Creating layer Scale29
I1006 11:25:17.482641  2824 net.cpp:122] Setting up Scale29
I1006 11:25:17.482645  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.482648  2824 net.cpp:137] Memory required for data: 643482800
I1006 11:25:17.482651  2824 layer_factory.hpp:77] Creating layer Eltwise13
I1006 11:25:17.482656  2824 net.cpp:84] Creating Layer Eltwise13
I1006 11:25:17.482658  2824 net.cpp:406] Eltwise13 <- Eltwise12_penlu25_0_split_1
I1006 11:25:17.482661  2824 net.cpp:406] Eltwise13 <- Convolution29
I1006 11:25:17.482664  2824 net.cpp:380] Eltwise13 -> Eltwise13
I1006 11:25:17.482682  2824 net.cpp:122] Setting up Eltwise13
I1006 11:25:17.482686  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.482688  2824 net.cpp:137] Memory required for data: 645121200
I1006 11:25:17.482691  2824 layer_factory.hpp:77] Creating layer penlu27
I1006 11:25:17.482697  2824 net.cpp:84] Creating Layer penlu27
I1006 11:25:17.482698  2824 net.cpp:406] penlu27 <- Eltwise13
I1006 11:25:17.482702  2824 net.cpp:367] penlu27 -> Eltwise13 (in-place)
I1006 11:25:17.482831  2824 net.cpp:122] Setting up penlu27
I1006 11:25:17.482836  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.482838  2824 net.cpp:137] Memory required for data: 646759600
I1006 11:25:17.482842  2824 layer_factory.hpp:77] Creating layer Eltwise13_penlu27_0_split
I1006 11:25:17.482847  2824 net.cpp:84] Creating Layer Eltwise13_penlu27_0_split
I1006 11:25:17.482849  2824 net.cpp:406] Eltwise13_penlu27_0_split <- Eltwise13
I1006 11:25:17.482853  2824 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_0
I1006 11:25:17.482857  2824 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_1
I1006 11:25:17.482883  2824 net.cpp:122] Setting up Eltwise13_penlu27_0_split
I1006 11:25:17.482887  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.482889  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.482892  2824 net.cpp:137] Memory required for data: 650036400
I1006 11:25:17.482893  2824 layer_factory.hpp:77] Creating layer Convolution30
I1006 11:25:17.482899  2824 net.cpp:84] Creating Layer Convolution30
I1006 11:25:17.482902  2824 net.cpp:406] Convolution30 <- Eltwise13_penlu27_0_split_0
I1006 11:25:17.482908  2824 net.cpp:380] Convolution30 -> Convolution30
I1006 11:25:17.484901  2824 net.cpp:122] Setting up Convolution30
I1006 11:25:17.484910  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.484912  2824 net.cpp:137] Memory required for data: 651674800
I1006 11:25:17.484917  2824 layer_factory.hpp:77] Creating layer BatchNorm30
I1006 11:25:17.484922  2824 net.cpp:84] Creating Layer BatchNorm30
I1006 11:25:17.484925  2824 net.cpp:406] BatchNorm30 <- Convolution30
I1006 11:25:17.484930  2824 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I1006 11:25:17.485081  2824 net.cpp:122] Setting up BatchNorm30
I1006 11:25:17.485086  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.485088  2824 net.cpp:137] Memory required for data: 653313200
I1006 11:25:17.485092  2824 layer_factory.hpp:77] Creating layer Scale30
I1006 11:25:17.485097  2824 net.cpp:84] Creating Layer Scale30
I1006 11:25:17.485100  2824 net.cpp:406] Scale30 <- Convolution30
I1006 11:25:17.485102  2824 net.cpp:367] Scale30 -> Convolution30 (in-place)
I1006 11:25:17.485133  2824 layer_factory.hpp:77] Creating layer Scale30
I1006 11:25:17.485219  2824 net.cpp:122] Setting up Scale30
I1006 11:25:17.485224  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.485226  2824 net.cpp:137] Memory required for data: 654951600
I1006 11:25:17.485229  2824 layer_factory.hpp:77] Creating layer penlu28
I1006 11:25:17.485235  2824 net.cpp:84] Creating Layer penlu28
I1006 11:25:17.485237  2824 net.cpp:406] penlu28 <- Convolution30
I1006 11:25:17.485241  2824 net.cpp:367] penlu28 -> Convolution30 (in-place)
I1006 11:25:17.485359  2824 net.cpp:122] Setting up penlu28
I1006 11:25:17.485363  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.485365  2824 net.cpp:137] Memory required for data: 656590000
I1006 11:25:17.485369  2824 layer_factory.hpp:77] Creating layer Convolution31
I1006 11:25:17.485376  2824 net.cpp:84] Creating Layer Convolution31
I1006 11:25:17.485378  2824 net.cpp:406] Convolution31 <- Convolution30
I1006 11:25:17.485383  2824 net.cpp:380] Convolution31 -> Convolution31
I1006 11:25:17.487043  2824 net.cpp:122] Setting up Convolution31
I1006 11:25:17.487052  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.487056  2824 net.cpp:137] Memory required for data: 658228400
I1006 11:25:17.487059  2824 layer_factory.hpp:77] Creating layer BatchNorm31
I1006 11:25:17.487064  2824 net.cpp:84] Creating Layer BatchNorm31
I1006 11:25:17.487067  2824 net.cpp:406] BatchNorm31 <- Convolution31
I1006 11:25:17.487071  2824 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I1006 11:25:17.487246  2824 net.cpp:122] Setting up BatchNorm31
I1006 11:25:17.487252  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.487254  2824 net.cpp:137] Memory required for data: 659866800
I1006 11:25:17.487258  2824 layer_factory.hpp:77] Creating layer Scale31
I1006 11:25:17.487270  2824 net.cpp:84] Creating Layer Scale31
I1006 11:25:17.503358  2824 net.cpp:406] Scale31 <- Convolution31
I1006 11:25:17.503368  2824 net.cpp:367] Scale31 -> Convolution31 (in-place)
I1006 11:25:17.503415  2824 layer_factory.hpp:77] Creating layer Scale31
I1006 11:25:17.503516  2824 net.cpp:122] Setting up Scale31
I1006 11:25:17.503522  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.503525  2824 net.cpp:137] Memory required for data: 661505200
I1006 11:25:17.503530  2824 layer_factory.hpp:77] Creating layer Eltwise14
I1006 11:25:17.503535  2824 net.cpp:84] Creating Layer Eltwise14
I1006 11:25:17.503537  2824 net.cpp:406] Eltwise14 <- Eltwise13_penlu27_0_split_1
I1006 11:25:17.503540  2824 net.cpp:406] Eltwise14 <- Convolution31
I1006 11:25:17.503545  2824 net.cpp:380] Eltwise14 -> Eltwise14
I1006 11:25:17.503564  2824 net.cpp:122] Setting up Eltwise14
I1006 11:25:17.503569  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.503571  2824 net.cpp:137] Memory required for data: 663143600
I1006 11:25:17.503573  2824 layer_factory.hpp:77] Creating layer penlu29
I1006 11:25:17.503579  2824 net.cpp:84] Creating Layer penlu29
I1006 11:25:17.503582  2824 net.cpp:406] penlu29 <- Eltwise14
I1006 11:25:17.503585  2824 net.cpp:367] penlu29 -> Eltwise14 (in-place)
I1006 11:25:17.503717  2824 net.cpp:122] Setting up penlu29
I1006 11:25:17.503721  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.503723  2824 net.cpp:137] Memory required for data: 664782000
I1006 11:25:17.503728  2824 layer_factory.hpp:77] Creating layer Eltwise14_penlu29_0_split
I1006 11:25:17.503732  2824 net.cpp:84] Creating Layer Eltwise14_penlu29_0_split
I1006 11:25:17.503736  2824 net.cpp:406] Eltwise14_penlu29_0_split <- Eltwise14
I1006 11:25:17.503738  2824 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_0
I1006 11:25:17.503743  2824 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_1
I1006 11:25:17.503773  2824 net.cpp:122] Setting up Eltwise14_penlu29_0_split
I1006 11:25:17.503777  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.503780  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.503783  2824 net.cpp:137] Memory required for data: 668058800
I1006 11:25:17.503785  2824 layer_factory.hpp:77] Creating layer Convolution32
I1006 11:25:17.503793  2824 net.cpp:84] Creating Layer Convolution32
I1006 11:25:17.503794  2824 net.cpp:406] Convolution32 <- Eltwise14_penlu29_0_split_0
I1006 11:25:17.503799  2824 net.cpp:380] Convolution32 -> Convolution32
I1006 11:25:17.506222  2824 net.cpp:122] Setting up Convolution32
I1006 11:25:17.506232  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.506234  2824 net.cpp:137] Memory required for data: 669697200
I1006 11:25:17.506239  2824 layer_factory.hpp:77] Creating layer BatchNorm32
I1006 11:25:17.506244  2824 net.cpp:84] Creating Layer BatchNorm32
I1006 11:25:17.506247  2824 net.cpp:406] BatchNorm32 <- Convolution32
I1006 11:25:17.506252  2824 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I1006 11:25:17.506414  2824 net.cpp:122] Setting up BatchNorm32
I1006 11:25:17.506419  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.506422  2824 net.cpp:137] Memory required for data: 671335600
I1006 11:25:17.506427  2824 layer_factory.hpp:77] Creating layer Scale32
I1006 11:25:17.506430  2824 net.cpp:84] Creating Layer Scale32
I1006 11:25:17.506433  2824 net.cpp:406] Scale32 <- Convolution32
I1006 11:25:17.506436  2824 net.cpp:367] Scale32 -> Convolution32 (in-place)
I1006 11:25:17.506469  2824 layer_factory.hpp:77] Creating layer Scale32
I1006 11:25:17.506559  2824 net.cpp:122] Setting up Scale32
I1006 11:25:17.506563  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.506566  2824 net.cpp:137] Memory required for data: 672974000
I1006 11:25:17.506569  2824 layer_factory.hpp:77] Creating layer penlu30
I1006 11:25:17.506575  2824 net.cpp:84] Creating Layer penlu30
I1006 11:25:17.506577  2824 net.cpp:406] penlu30 <- Convolution32
I1006 11:25:17.506589  2824 net.cpp:367] penlu30 -> Convolution32 (in-place)
I1006 11:25:17.506736  2824 net.cpp:122] Setting up penlu30
I1006 11:25:17.506742  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.506743  2824 net.cpp:137] Memory required for data: 674612400
I1006 11:25:17.506747  2824 layer_factory.hpp:77] Creating layer Convolution33
I1006 11:25:17.506754  2824 net.cpp:84] Creating Layer Convolution33
I1006 11:25:17.506757  2824 net.cpp:406] Convolution33 <- Convolution32
I1006 11:25:17.506762  2824 net.cpp:380] Convolution33 -> Convolution33
I1006 11:25:17.508553  2824 net.cpp:122] Setting up Convolution33
I1006 11:25:17.508563  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.508565  2824 net.cpp:137] Memory required for data: 676250800
I1006 11:25:17.508569  2824 layer_factory.hpp:77] Creating layer BatchNorm33
I1006 11:25:17.508575  2824 net.cpp:84] Creating Layer BatchNorm33
I1006 11:25:17.508579  2824 net.cpp:406] BatchNorm33 <- Convolution33
I1006 11:25:17.508582  2824 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I1006 11:25:17.508733  2824 net.cpp:122] Setting up BatchNorm33
I1006 11:25:17.508738  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.508740  2824 net.cpp:137] Memory required for data: 677889200
I1006 11:25:17.508745  2824 layer_factory.hpp:77] Creating layer Scale33
I1006 11:25:17.508749  2824 net.cpp:84] Creating Layer Scale33
I1006 11:25:17.508751  2824 net.cpp:406] Scale33 <- Convolution33
I1006 11:25:17.508754  2824 net.cpp:367] Scale33 -> Convolution33 (in-place)
I1006 11:25:17.508785  2824 layer_factory.hpp:77] Creating layer Scale33
I1006 11:25:17.508873  2824 net.cpp:122] Setting up Scale33
I1006 11:25:17.508878  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.508880  2824 net.cpp:137] Memory required for data: 679527600
I1006 11:25:17.508884  2824 layer_factory.hpp:77] Creating layer Eltwise15
I1006 11:25:17.508889  2824 net.cpp:84] Creating Layer Eltwise15
I1006 11:25:17.508893  2824 net.cpp:406] Eltwise15 <- Eltwise14_penlu29_0_split_1
I1006 11:25:17.508895  2824 net.cpp:406] Eltwise15 <- Convolution33
I1006 11:25:17.508898  2824 net.cpp:380] Eltwise15 -> Eltwise15
I1006 11:25:17.508916  2824 net.cpp:122] Setting up Eltwise15
I1006 11:25:17.508920  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.508922  2824 net.cpp:137] Memory required for data: 681166000
I1006 11:25:17.508924  2824 layer_factory.hpp:77] Creating layer penlu31
I1006 11:25:17.508930  2824 net.cpp:84] Creating Layer penlu31
I1006 11:25:17.508932  2824 net.cpp:406] penlu31 <- Eltwise15
I1006 11:25:17.508936  2824 net.cpp:367] penlu31 -> Eltwise15 (in-place)
I1006 11:25:17.509057  2824 net.cpp:122] Setting up penlu31
I1006 11:25:17.509061  2824 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1006 11:25:17.509063  2824 net.cpp:137] Memory required for data: 682804400
I1006 11:25:17.509068  2824 layer_factory.hpp:77] Creating layer Pooling1
I1006 11:25:17.509073  2824 net.cpp:84] Creating Layer Pooling1
I1006 11:25:17.509074  2824 net.cpp:406] Pooling1 <- Eltwise15
I1006 11:25:17.509079  2824 net.cpp:380] Pooling1 -> Pooling1
I1006 11:25:17.509219  2824 net.cpp:122] Setting up Pooling1
I1006 11:25:17.509227  2824 net.cpp:129] Top shape: 100 64 1 1 (6400)
I1006 11:25:17.509229  2824 net.cpp:137] Memory required for data: 682830000
I1006 11:25:17.509232  2824 layer_factory.hpp:77] Creating layer InnerProduct1
I1006 11:25:17.509236  2824 net.cpp:84] Creating Layer InnerProduct1
I1006 11:25:17.509238  2824 net.cpp:406] InnerProduct1 <- Pooling1
I1006 11:25:17.509243  2824 net.cpp:380] InnerProduct1 -> InnerProduct1
I1006 11:25:17.509348  2824 net.cpp:122] Setting up InnerProduct1
I1006 11:25:17.509353  2824 net.cpp:129] Top shape: 100 10 (1000)
I1006 11:25:17.509356  2824 net.cpp:137] Memory required for data: 682834000
I1006 11:25:17.509359  2824 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1006 11:25:17.509362  2824 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1006 11:25:17.509366  2824 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1006 11:25:17.509377  2824 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1006 11:25:17.509382  2824 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1006 11:25:17.509410  2824 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1006 11:25:17.509414  2824 net.cpp:129] Top shape: 100 10 (1000)
I1006 11:25:17.509416  2824 net.cpp:129] Top shape: 100 10 (1000)
I1006 11:25:17.509418  2824 net.cpp:137] Memory required for data: 682842000
I1006 11:25:17.509421  2824 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1006 11:25:17.509425  2824 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1006 11:25:17.509428  2824 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I1006 11:25:17.509431  2824 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I1006 11:25:17.509434  2824 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1006 11:25:17.509439  2824 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1006 11:25:17.509963  2824 net.cpp:122] Setting up SoftmaxWithLoss1
I1006 11:25:17.509973  2824 net.cpp:129] Top shape: (1)
I1006 11:25:17.509975  2824 net.cpp:132]     with loss weight 1
I1006 11:25:17.509982  2824 net.cpp:137] Memory required for data: 682842004
I1006 11:25:17.509984  2824 layer_factory.hpp:77] Creating layer Accuracy1
I1006 11:25:17.509989  2824 net.cpp:84] Creating Layer Accuracy1
I1006 11:25:17.509992  2824 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I1006 11:25:17.509995  2824 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I1006 11:25:17.509999  2824 net.cpp:380] Accuracy1 -> Accuracy1
I1006 11:25:17.510004  2824 net.cpp:122] Setting up Accuracy1
I1006 11:25:17.510009  2824 net.cpp:129] Top shape: (1)
I1006 11:25:17.510010  2824 net.cpp:137] Memory required for data: 682842008
I1006 11:25:17.510012  2824 net.cpp:200] Accuracy1 does not need backward computation.
I1006 11:25:17.510015  2824 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1006 11:25:17.510017  2824 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1006 11:25:17.510020  2824 net.cpp:198] InnerProduct1 needs backward computation.
I1006 11:25:17.510021  2824 net.cpp:198] Pooling1 needs backward computation.
I1006 11:25:17.510025  2824 net.cpp:198] penlu31 needs backward computation.
I1006 11:25:17.510026  2824 net.cpp:198] Eltwise15 needs backward computation.
I1006 11:25:17.510028  2824 net.cpp:198] Scale33 needs backward computation.
I1006 11:25:17.510030  2824 net.cpp:198] BatchNorm33 needs backward computation.
I1006 11:25:17.510032  2824 net.cpp:198] Convolution33 needs backward computation.
I1006 11:25:17.510035  2824 net.cpp:198] penlu30 needs backward computation.
I1006 11:25:17.510036  2824 net.cpp:198] Scale32 needs backward computation.
I1006 11:25:17.510038  2824 net.cpp:198] BatchNorm32 needs backward computation.
I1006 11:25:17.510040  2824 net.cpp:198] Convolution32 needs backward computation.
I1006 11:25:17.510043  2824 net.cpp:198] Eltwise14_penlu29_0_split needs backward computation.
I1006 11:25:17.510046  2824 net.cpp:198] penlu29 needs backward computation.
I1006 11:25:17.510047  2824 net.cpp:198] Eltwise14 needs backward computation.
I1006 11:25:17.510049  2824 net.cpp:198] Scale31 needs backward computation.
I1006 11:25:17.510051  2824 net.cpp:198] BatchNorm31 needs backward computation.
I1006 11:25:17.510053  2824 net.cpp:198] Convolution31 needs backward computation.
I1006 11:25:17.510056  2824 net.cpp:198] penlu28 needs backward computation.
I1006 11:25:17.510058  2824 net.cpp:198] Scale30 needs backward computation.
I1006 11:25:17.510061  2824 net.cpp:198] BatchNorm30 needs backward computation.
I1006 11:25:17.510061  2824 net.cpp:198] Convolution30 needs backward computation.
I1006 11:25:17.510064  2824 net.cpp:198] Eltwise13_penlu27_0_split needs backward computation.
I1006 11:25:17.510066  2824 net.cpp:198] penlu27 needs backward computation.
I1006 11:25:17.510068  2824 net.cpp:198] Eltwise13 needs backward computation.
I1006 11:25:17.510078  2824 net.cpp:198] Scale29 needs backward computation.
I1006 11:25:17.510082  2824 net.cpp:198] BatchNorm29 needs backward computation.
I1006 11:25:17.510083  2824 net.cpp:198] Convolution29 needs backward computation.
I1006 11:25:17.510085  2824 net.cpp:198] penlu26 needs backward computation.
I1006 11:25:17.510087  2824 net.cpp:198] Scale28 needs backward computation.
I1006 11:25:17.510090  2824 net.cpp:198] BatchNorm28 needs backward computation.
I1006 11:25:17.510092  2824 net.cpp:198] Convolution28 needs backward computation.
I1006 11:25:17.510094  2824 net.cpp:198] Eltwise12_penlu25_0_split needs backward computation.
I1006 11:25:17.510097  2824 net.cpp:198] penlu25 needs backward computation.
I1006 11:25:17.510098  2824 net.cpp:198] Eltwise12 needs backward computation.
I1006 11:25:17.510102  2824 net.cpp:198] Scale27 needs backward computation.
I1006 11:25:17.510103  2824 net.cpp:198] BatchNorm27 needs backward computation.
I1006 11:25:17.510105  2824 net.cpp:198] Convolution27 needs backward computation.
I1006 11:25:17.510107  2824 net.cpp:198] penlu24 needs backward computation.
I1006 11:25:17.510110  2824 net.cpp:198] Scale26 needs backward computation.
I1006 11:25:17.510113  2824 net.cpp:198] BatchNorm26 needs backward computation.
I1006 11:25:17.510114  2824 net.cpp:198] Convolution26 needs backward computation.
I1006 11:25:17.510116  2824 net.cpp:198] Eltwise11_penlu23_0_split needs backward computation.
I1006 11:25:17.510118  2824 net.cpp:198] penlu23 needs backward computation.
I1006 11:25:17.510121  2824 net.cpp:198] Eltwise11 needs backward computation.
I1006 11:25:17.510123  2824 net.cpp:198] Scale25 needs backward computation.
I1006 11:25:17.510126  2824 net.cpp:198] BatchNorm25 needs backward computation.
I1006 11:25:17.510128  2824 net.cpp:198] Convolution25 needs backward computation.
I1006 11:25:17.510130  2824 net.cpp:198] penlu22 needs backward computation.
I1006 11:25:17.510133  2824 net.cpp:198] Scale24 needs backward computation.
I1006 11:25:17.510134  2824 net.cpp:198] BatchNorm24 needs backward computation.
I1006 11:25:17.510136  2824 net.cpp:198] Convolution24 needs backward computation.
I1006 11:25:17.510139  2824 net.cpp:198] Scale23 needs backward computation.
I1006 11:25:17.510141  2824 net.cpp:198] BatchNorm23 needs backward computation.
I1006 11:25:17.510143  2824 net.cpp:198] Convolution23 needs backward computation.
I1006 11:25:17.510145  2824 net.cpp:198] Eltwise10_penlu21_0_split needs backward computation.
I1006 11:25:17.510149  2824 net.cpp:198] penlu21 needs backward computation.
I1006 11:25:17.510150  2824 net.cpp:198] Eltwise10 needs backward computation.
I1006 11:25:17.510154  2824 net.cpp:198] Scale22 needs backward computation.
I1006 11:25:17.510155  2824 net.cpp:198] BatchNorm22 needs backward computation.
I1006 11:25:17.510157  2824 net.cpp:198] Convolution22 needs backward computation.
I1006 11:25:17.510159  2824 net.cpp:198] penlu20 needs backward computation.
I1006 11:25:17.510162  2824 net.cpp:198] Scale21 needs backward computation.
I1006 11:25:17.510164  2824 net.cpp:198] BatchNorm21 needs backward computation.
I1006 11:25:17.510166  2824 net.cpp:198] Convolution21 needs backward computation.
I1006 11:25:17.510169  2824 net.cpp:198] Eltwise9_penlu19_0_split needs backward computation.
I1006 11:25:17.510171  2824 net.cpp:198] penlu19 needs backward computation.
I1006 11:25:17.510174  2824 net.cpp:198] Eltwise9 needs backward computation.
I1006 11:25:17.510176  2824 net.cpp:198] Scale20 needs backward computation.
I1006 11:25:17.510179  2824 net.cpp:198] BatchNorm20 needs backward computation.
I1006 11:25:17.510180  2824 net.cpp:198] Convolution20 needs backward computation.
I1006 11:25:17.510184  2824 net.cpp:198] penlu18 needs backward computation.
I1006 11:25:17.510185  2824 net.cpp:198] Scale19 needs backward computation.
I1006 11:25:17.510187  2824 net.cpp:198] BatchNorm19 needs backward computation.
I1006 11:25:17.510190  2824 net.cpp:198] Convolution19 needs backward computation.
I1006 11:25:17.533836  2824 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1006 11:25:17.533844  2824 net.cpp:198] penlu17 needs backward computation.
I1006 11:25:17.533848  2824 net.cpp:198] Eltwise8 needs backward computation.
I1006 11:25:17.533850  2824 net.cpp:198] Scale18 needs backward computation.
I1006 11:25:17.533854  2824 net.cpp:198] BatchNorm18 needs backward computation.
I1006 11:25:17.533855  2824 net.cpp:198] Convolution18 needs backward computation.
I1006 11:25:17.533859  2824 net.cpp:198] penlu16 needs backward computation.
I1006 11:25:17.533860  2824 net.cpp:198] Scale17 needs backward computation.
I1006 11:25:17.533864  2824 net.cpp:198] BatchNorm17 needs backward computation.
I1006 11:25:17.533865  2824 net.cpp:198] Convolution17 needs backward computation.
I1006 11:25:17.533869  2824 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1006 11:25:17.533871  2824 net.cpp:198] penlu15 needs backward computation.
I1006 11:25:17.533874  2824 net.cpp:198] Eltwise7 needs backward computation.
I1006 11:25:17.533876  2824 net.cpp:198] Scale16 needs backward computation.
I1006 11:25:17.533879  2824 net.cpp:198] BatchNorm16 needs backward computation.
I1006 11:25:17.533881  2824 net.cpp:198] Convolution16 needs backward computation.
I1006 11:25:17.533885  2824 net.cpp:198] penlu14 needs backward computation.
I1006 11:25:17.533886  2824 net.cpp:198] Scale15 needs backward computation.
I1006 11:25:17.533890  2824 net.cpp:198] BatchNorm15 needs backward computation.
I1006 11:25:17.533891  2824 net.cpp:198] Convolution15 needs backward computation.
I1006 11:25:17.533895  2824 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1006 11:25:17.533896  2824 net.cpp:198] penlu13 needs backward computation.
I1006 11:25:17.533901  2824 net.cpp:198] Eltwise6 needs backward computation.
I1006 11:25:17.533905  2824 net.cpp:198] Scale14 needs backward computation.
I1006 11:25:17.533906  2824 net.cpp:198] BatchNorm14 needs backward computation.
I1006 11:25:17.533910  2824 net.cpp:198] Convolution14 needs backward computation.
I1006 11:25:17.533912  2824 net.cpp:198] penlu12 needs backward computation.
I1006 11:25:17.533915  2824 net.cpp:198] Scale13 needs backward computation.
I1006 11:25:17.533916  2824 net.cpp:198] BatchNorm13 needs backward computation.
I1006 11:25:17.533920  2824 net.cpp:198] Convolution13 needs backward computation.
I1006 11:25:17.533921  2824 net.cpp:198] Scale12 needs backward computation.
I1006 11:25:17.533924  2824 net.cpp:198] BatchNorm12 needs backward computation.
I1006 11:25:17.533926  2824 net.cpp:198] Convolution12 needs backward computation.
I1006 11:25:17.533929  2824 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1006 11:25:17.533932  2824 net.cpp:198] penlu11 needs backward computation.
I1006 11:25:17.533934  2824 net.cpp:198] Eltwise5 needs backward computation.
I1006 11:25:17.533937  2824 net.cpp:198] Scale11 needs backward computation.
I1006 11:25:17.533941  2824 net.cpp:198] BatchNorm11 needs backward computation.
I1006 11:25:17.533942  2824 net.cpp:198] Convolution11 needs backward computation.
I1006 11:25:17.533946  2824 net.cpp:198] penlu10 needs backward computation.
I1006 11:25:17.533947  2824 net.cpp:198] Scale10 needs backward computation.
I1006 11:25:17.533949  2824 net.cpp:198] BatchNorm10 needs backward computation.
I1006 11:25:17.533952  2824 net.cpp:198] Convolution10 needs backward computation.
I1006 11:25:17.533954  2824 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1006 11:25:17.533957  2824 net.cpp:198] penlu9 needs backward computation.
I1006 11:25:17.533959  2824 net.cpp:198] Eltwise4 needs backward computation.
I1006 11:25:17.533963  2824 net.cpp:198] Scale9 needs backward computation.
I1006 11:25:17.533965  2824 net.cpp:198] BatchNorm9 needs backward computation.
I1006 11:25:17.533968  2824 net.cpp:198] Convolution9 needs backward computation.
I1006 11:25:17.533972  2824 net.cpp:198] penlu8 needs backward computation.
I1006 11:25:17.533973  2824 net.cpp:198] Scale8 needs backward computation.
I1006 11:25:17.533983  2824 net.cpp:198] BatchNorm8 needs backward computation.
I1006 11:25:17.533987  2824 net.cpp:198] Convolution8 needs backward computation.
I1006 11:25:17.533989  2824 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1006 11:25:17.533993  2824 net.cpp:198] penlu7 needs backward computation.
I1006 11:25:17.533994  2824 net.cpp:198] Eltwise3 needs backward computation.
I1006 11:25:17.533998  2824 net.cpp:198] Scale7 needs backward computation.
I1006 11:25:17.534000  2824 net.cpp:198] BatchNorm7 needs backward computation.
I1006 11:25:17.534003  2824 net.cpp:198] Convolution7 needs backward computation.
I1006 11:25:17.534005  2824 net.cpp:198] penlu6 needs backward computation.
I1006 11:25:17.534008  2824 net.cpp:198] Scale6 needs backward computation.
I1006 11:25:17.534010  2824 net.cpp:198] BatchNorm6 needs backward computation.
I1006 11:25:17.534013  2824 net.cpp:198] Convolution6 needs backward computation.
I1006 11:25:17.534015  2824 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1006 11:25:17.534018  2824 net.cpp:198] penlu5 needs backward computation.
I1006 11:25:17.534020  2824 net.cpp:198] Eltwise2 needs backward computation.
I1006 11:25:17.534024  2824 net.cpp:198] Scale5 needs backward computation.
I1006 11:25:17.534026  2824 net.cpp:198] BatchNorm5 needs backward computation.
I1006 11:25:17.534029  2824 net.cpp:198] Convolution5 needs backward computation.
I1006 11:25:17.534031  2824 net.cpp:198] penlu4 needs backward computation.
I1006 11:25:17.534034  2824 net.cpp:198] Scale4 needs backward computation.
I1006 11:25:17.534036  2824 net.cpp:198] BatchNorm4 needs backward computation.
I1006 11:25:17.534039  2824 net.cpp:198] Convolution4 needs backward computation.
I1006 11:25:17.534041  2824 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1006 11:25:17.534044  2824 net.cpp:198] penlu3 needs backward computation.
I1006 11:25:17.534046  2824 net.cpp:198] Eltwise1 needs backward computation.
I1006 11:25:17.534049  2824 net.cpp:198] Scale3 needs backward computation.
I1006 11:25:17.534052  2824 net.cpp:198] BatchNorm3 needs backward computation.
I1006 11:25:17.534055  2824 net.cpp:198] Convolution3 needs backward computation.
I1006 11:25:17.534057  2824 net.cpp:198] penlu2 needs backward computation.
I1006 11:25:17.534060  2824 net.cpp:198] Scale2 needs backward computation.
I1006 11:25:17.534062  2824 net.cpp:198] BatchNorm2 needs backward computation.
I1006 11:25:17.534065  2824 net.cpp:198] Convolution2 needs backward computation.
I1006 11:25:17.534068  2824 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1006 11:25:17.534071  2824 net.cpp:198] penlu1 needs backward computation.
I1006 11:25:17.534073  2824 net.cpp:198] Scale1 needs backward computation.
I1006 11:25:17.534075  2824 net.cpp:198] BatchNorm1 needs backward computation.
I1006 11:25:17.534078  2824 net.cpp:198] Convolution1 needs backward computation.
I1006 11:25:17.534081  2824 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1006 11:25:17.534085  2824 net.cpp:200] Data1 does not need backward computation.
I1006 11:25:17.534087  2824 net.cpp:242] This network produces output Accuracy1
I1006 11:25:17.534090  2824 net.cpp:242] This network produces output SoftmaxWithLoss1
I1006 11:25:17.534148  2824 net.cpp:255] Network initialization done.
I1006 11:25:17.534638  2824 solver.cpp:56] Solver scaffolding done.
I1006 11:25:17.543485  2824 caffe.cpp:248] Starting Optimization
I1006 11:25:17.543499  2824 solver.cpp:272] Solving resnet_cifar10
I1006 11:25:17.543501  2824 solver.cpp:273] Learning Rate Policy: multistep
I1006 11:25:17.547116  2824 solver.cpp:330] Iteration 0, Testing net (#0)
I1006 11:25:19.524400  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:25:19.604604  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.1
I1006 11:25:19.604630  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I1006 11:25:19.719300  2824 solver.cpp:218] Iteration 0 (-1.35501e-32 iter/s, 2.17571s/100 iters), loss = 2.30256
I1006 11:25:19.719342  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.30256 (* 1 = 2.30256 loss)
I1006 11:25:19.719353  2824 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1006 11:25:28.060286  2824 solver.cpp:218] Iteration 100 (11.9892 iter/s, 8.34087s/100 iters), loss = 1.79878
I1006 11:25:28.060317  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.79878 (* 1 = 1.79878 loss)
I1006 11:25:28.060322  2824 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1006 11:25:36.413899  2824 solver.cpp:218] Iteration 200 (11.971 iter/s, 8.3535s/100 iters), loss = 1.74879
I1006 11:25:36.413928  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.74879 (* 1 = 1.74879 loss)
I1006 11:25:36.413934  2824 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1006 11:25:44.806241  2824 solver.cpp:218] Iteration 300 (11.9158 iter/s, 8.39223s/100 iters), loss = 1.29213
I1006 11:25:44.806269  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.29213 (* 1 = 1.29213 loss)
I1006 11:25:44.806275  2824 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1006 11:25:53.186789  2824 solver.cpp:218] Iteration 400 (11.9325 iter/s, 8.38044s/100 iters), loss = 1.13288
I1006 11:25:53.186859  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.13288 (* 1 = 1.13288 loss)
I1006 11:25:53.186866  2824 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1006 11:26:01.192235  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:26:01.531332  2824 solver.cpp:330] Iteration 500, Testing net (#0)
I1006 11:26:03.513707  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:26:03.594578  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.2091
I1006 11:26:03.594614  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.26368 (* 1 = 4.26368 loss)
I1006 11:26:03.677659  2824 solver.cpp:218] Iteration 500 (9.53225 iter/s, 10.4907s/100 iters), loss = 1.27328
I1006 11:26:03.677681  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.27328 (* 1 = 1.27328 loss)
I1006 11:26:03.677687  2824 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1006 11:26:12.130728  2824 solver.cpp:218] Iteration 600 (11.8302 iter/s, 8.45296s/100 iters), loss = 1.05611
I1006 11:26:12.130759  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.05611 (* 1 = 1.05611 loss)
I1006 11:26:12.130766  2824 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1006 11:26:20.587749  2824 solver.cpp:218] Iteration 700 (11.8246 iter/s, 8.45691s/100 iters), loss = 1.154
I1006 11:26:20.587780  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.154 (* 1 = 1.154 loss)
I1006 11:26:20.587787  2824 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1006 11:26:29.024219  2824 solver.cpp:218] Iteration 800 (11.8535 iter/s, 8.43636s/100 iters), loss = 1.07284
I1006 11:26:29.024336  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.07284 (* 1 = 1.07284 loss)
I1006 11:26:29.024353  2824 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1006 11:26:37.489140  2824 solver.cpp:218] Iteration 900 (11.8137 iter/s, 8.46473s/100 iters), loss = 0.933989
I1006 11:26:37.489171  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.933989 (* 1 = 0.933989 loss)
I1006 11:26:37.489179  2824 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1006 11:26:45.496146  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:26:45.843971  2824 solver.cpp:330] Iteration 1000, Testing net (#0)
I1006 11:26:47.818724  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:26:47.901993  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.1542
I1006 11:26:47.902032  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 8.17443 (* 1 = 8.17443 loss)
I1006 11:26:47.987073  2824 solver.cpp:218] Iteration 1000 (9.5258 iter/s, 10.4978s/100 iters), loss = 1.03544
I1006 11:26:47.987107  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03544 (* 1 = 1.03544 loss)
I1006 11:26:47.987114  2824 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1006 11:26:56.436081  2824 solver.cpp:218] Iteration 1100 (11.8359 iter/s, 8.4489s/100 iters), loss = 0.825777
I1006 11:26:56.436115  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.825777 (* 1 = 0.825777 loss)
I1006 11:26:56.436121  2824 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1006 11:27:04.865242  2824 solver.cpp:218] Iteration 1200 (11.8637 iter/s, 8.42905s/100 iters), loss = 0.939898
I1006 11:27:04.865380  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.939898 (* 1 = 0.939898 loss)
I1006 11:27:04.865397  2824 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1006 11:27:13.259078  2824 solver.cpp:218] Iteration 1300 (11.9138 iter/s, 8.39363s/100 iters), loss = 0.744674
I1006 11:27:13.259109  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.744674 (* 1 = 0.744674 loss)
I1006 11:27:13.259115  2824 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1006 11:27:21.676692  2824 solver.cpp:218] Iteration 1400 (11.88 iter/s, 8.41751s/100 iters), loss = 0.727438
I1006 11:27:21.676733  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.727438 (* 1 = 0.727438 loss)
I1006 11:27:21.676738  2824 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1006 11:27:29.727885  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:27:30.062325  2824 solver.cpp:330] Iteration 1500, Testing net (#0)
I1006 11:27:32.043640  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:27:32.125339  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3199
I1006 11:27:32.125376  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.95675 (* 1 = 2.95675 loss)
I1006 11:27:32.208299  2824 solver.cpp:218] Iteration 1500 (9.49534 iter/s, 10.5315s/100 iters), loss = 0.821354
I1006 11:27:32.208323  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.821354 (* 1 = 0.821354 loss)
I1006 11:27:32.208329  2824 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1006 11:27:40.658743  2824 solver.cpp:218] Iteration 1600 (11.8338 iter/s, 8.45035s/100 iters), loss = 0.578941
I1006 11:27:40.658857  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.578941 (* 1 = 0.578941 loss)
I1006 11:27:40.658864  2824 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1006 11:27:49.138875  2824 solver.cpp:218] Iteration 1700 (11.7925 iter/s, 8.47995s/100 iters), loss = 0.782646
I1006 11:27:49.138911  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.782646 (* 1 = 0.782646 loss)
I1006 11:27:49.138921  2824 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1006 11:27:57.599805  2824 solver.cpp:218] Iteration 1800 (11.8192 iter/s, 8.4608s/100 iters), loss = 0.738274
I1006 11:27:57.599838  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.738274 (* 1 = 0.738274 loss)
I1006 11:27:57.599846  2824 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1006 11:28:06.066707  2824 solver.cpp:218] Iteration 1900 (11.8108 iter/s, 8.46681s/100 iters), loss = 0.640725
I1006 11:28:06.066738  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.640725 (* 1 = 0.640725 loss)
I1006 11:28:06.066745  2824 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1006 11:28:14.101234  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:28:14.440686  2824 solver.cpp:330] Iteration 2000, Testing net (#0)
I1006 11:28:16.435786  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:28:16.517911  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5783
I1006 11:28:16.517946  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.21561 (* 1 = 1.21561 loss)
I1006 11:28:16.601547  2824 solver.cpp:218] Iteration 2000 (9.4924 iter/s, 10.5347s/100 iters), loss = 0.857843
I1006 11:28:16.601572  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.857843 (* 1 = 0.857843 loss)
I1006 11:28:16.601579  2824 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1006 11:28:25.063571  2824 solver.cpp:218] Iteration 2100 (11.8176 iter/s, 8.46194s/100 iters), loss = 0.60786
I1006 11:28:25.063601  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.60786 (* 1 = 0.60786 loss)
I1006 11:28:25.063606  2824 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1006 11:28:33.540877  2824 solver.cpp:218] Iteration 2200 (11.7963 iter/s, 8.47722s/100 iters), loss = 0.740272
I1006 11:28:33.540911  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.740272 (* 1 = 0.740272 loss)
I1006 11:28:33.540918  2824 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1006 11:28:42.027725  2824 solver.cpp:218] Iteration 2300 (11.7831 iter/s, 8.48676s/100 iters), loss = 0.631864
I1006 11:28:42.027766  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.631864 (* 1 = 0.631864 loss)
I1006 11:28:42.027771  2824 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1006 11:28:50.486176  2824 solver.cpp:218] Iteration 2400 (11.8226 iter/s, 8.45836s/100 iters), loss = 0.598835
I1006 11:28:50.486279  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.598835 (* 1 = 0.598835 loss)
I1006 11:28:50.486296  2824 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1006 11:28:58.543332  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:28:58.883731  2824 solver.cpp:330] Iteration 2500, Testing net (#0)
I1006 11:29:00.852623  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:29:00.937156  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6456
I1006 11:29:00.937193  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.06232 (* 1 = 1.06232 loss)
I1006 11:29:01.025355  2824 solver.cpp:218] Iteration 2500 (9.48854 iter/s, 10.539s/100 iters), loss = 0.613244
I1006 11:29:01.025401  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.613244 (* 1 = 0.613244 loss)
I1006 11:29:01.025408  2824 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1006 11:29:09.438793  2824 solver.cpp:218] Iteration 2600 (11.886 iter/s, 8.41323s/100 iters), loss = 0.463459
I1006 11:29:09.438823  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.463459 (* 1 = 0.463459 loss)
I1006 11:29:09.438829  2824 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1006 11:29:17.819577  2824 solver.cpp:218] Iteration 2700 (11.9322 iter/s, 8.38071s/100 iters), loss = 0.514681
I1006 11:29:17.819607  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.514681 (* 1 = 0.514681 loss)
I1006 11:29:17.819623  2824 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1006 11:29:26.220981  2824 solver.cpp:218] Iteration 2800 (11.9029 iter/s, 8.40133s/100 iters), loss = 0.576464
I1006 11:29:26.221101  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.576464 (* 1 = 0.576464 loss)
I1006 11:29:26.221119  2824 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1006 11:29:34.589985  2824 solver.cpp:218] Iteration 2900 (11.9491 iter/s, 8.36884s/100 iters), loss = 0.443568
I1006 11:29:34.590015  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.443568 (* 1 = 0.443568 loss)
I1006 11:29:34.590021  2824 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1006 11:29:42.556603  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:29:42.892179  2824 solver.cpp:330] Iteration 3000, Testing net (#0)
I1006 11:29:44.846385  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:29:44.927805  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6899
I1006 11:29:44.927840  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.941481 (* 1 = 0.941481 loss)
I1006 11:29:45.011559  2824 solver.cpp:218] Iteration 3000 (9.59555 iter/s, 10.4215s/100 iters), loss = 0.545462
I1006 11:29:45.011584  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.545462 (* 1 = 0.545462 loss)
I1006 11:29:45.011591  2824 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1006 11:29:53.374233  2824 solver.cpp:218] Iteration 3100 (11.958 iter/s, 8.36261s/100 iters), loss = 0.48288
I1006 11:29:53.374263  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.48288 (* 1 = 0.48288 loss)
I1006 11:29:53.374269  2824 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1006 11:30:01.735584  2824 solver.cpp:218] Iteration 3200 (11.9599 iter/s, 8.36128s/100 iters), loss = 0.473521
I1006 11:30:01.735718  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.473521 (* 1 = 0.473521 loss)
I1006 11:30:01.735734  2824 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1006 11:30:10.104444  2824 solver.cpp:218] Iteration 3300 (11.9493 iter/s, 8.36869s/100 iters), loss = 0.588871
I1006 11:30:10.104485  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.588871 (* 1 = 0.588871 loss)
I1006 11:30:10.104491  2824 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1006 11:30:18.468127  2824 solver.cpp:218] Iteration 3400 (11.9566 iter/s, 8.3636s/100 iters), loss = 0.464576
I1006 11:30:18.468166  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.464576 (* 1 = 0.464576 loss)
I1006 11:30:18.468173  2824 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1006 11:30:26.415007  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:30:26.750252  2824 solver.cpp:330] Iteration 3500, Testing net (#0)
I1006 11:30:28.704385  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:30:28.786041  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7131
I1006 11:30:28.786075  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.850368 (* 1 = 0.850368 loss)
I1006 11:30:28.869309  2824 solver.cpp:218] Iteration 3500 (9.61437 iter/s, 10.4011s/100 iters), loss = 0.554392
I1006 11:30:28.869335  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.554392 (* 1 = 0.554392 loss)
I1006 11:30:28.869343  2824 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1006 11:30:37.240438  2824 solver.cpp:218] Iteration 3600 (11.9459 iter/s, 8.37106s/100 iters), loss = 0.456888
I1006 11:30:37.240514  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.456888 (* 1 = 0.456888 loss)
I1006 11:30:37.240530  2824 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1006 11:30:45.610973  2824 solver.cpp:218] Iteration 3700 (11.9468 iter/s, 8.37042s/100 iters), loss = 0.502351
I1006 11:30:45.611014  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.502351 (* 1 = 0.502351 loss)
I1006 11:30:45.611021  2824 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1006 11:30:53.982604  2824 solver.cpp:218] Iteration 3800 (11.9452 iter/s, 8.37155s/100 iters), loss = 0.495759
I1006 11:30:53.982633  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.495759 (* 1 = 0.495759 loss)
I1006 11:30:53.982638  2824 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1006 11:31:02.350981  2824 solver.cpp:218] Iteration 3900 (11.9498 iter/s, 8.36831s/100 iters), loss = 0.477132
I1006 11:31:02.351022  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.477132 (* 1 = 0.477132 loss)
I1006 11:31:02.351027  2824 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1006 11:31:10.311255  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:31:10.646004  2824 solver.cpp:330] Iteration 4000, Testing net (#0)
I1006 11:31:12.598953  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:31:12.680696  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6433
I1006 11:31:12.680732  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.05919 (* 1 = 1.05919 loss)
I1006 11:31:12.766095  2824 solver.cpp:218] Iteration 4000 (9.60151 iter/s, 10.415s/100 iters), loss = 0.496759
I1006 11:31:12.766126  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.496759 (* 1 = 0.496759 loss)
I1006 11:31:12.766134  2824 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1006 11:31:21.126298  2824 solver.cpp:218] Iteration 4100 (11.9615 iter/s, 8.36013s/100 iters), loss = 0.345663
I1006 11:31:21.126329  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.345663 (* 1 = 0.345663 loss)
I1006 11:31:21.126335  2824 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1006 11:31:29.491833  2824 solver.cpp:218] Iteration 4200 (11.9539 iter/s, 8.36547s/100 iters), loss = 0.463776
I1006 11:31:29.491864  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.463776 (* 1 = 0.463776 loss)
I1006 11:31:29.491870  2824 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1006 11:31:37.852382  2824 solver.cpp:218] Iteration 4300 (11.961 iter/s, 8.36048s/100 iters), loss = 0.420838
I1006 11:31:37.852422  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.420838 (* 1 = 0.420838 loss)
I1006 11:31:37.852428  2824 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1006 11:31:46.205792  2824 solver.cpp:218] Iteration 4400 (11.9713 iter/s, 8.35333s/100 iters), loss = 0.347632
I1006 11:31:46.205945  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.347632 (* 1 = 0.347632 loss)
I1006 11:31:46.205953  2824 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1006 11:31:54.157619  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:31:54.492455  2824 solver.cpp:330] Iteration 4500, Testing net (#0)
I1006 11:31:56.445410  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:31:56.526793  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.632
I1006 11:31:56.526829  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.23334 (* 1 = 1.23334 loss)
I1006 11:31:56.610363  2824 solver.cpp:218] Iteration 4500 (9.61134 iter/s, 10.4044s/100 iters), loss = 0.49741
I1006 11:31:56.610388  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.49741 (* 1 = 0.49741 loss)
I1006 11:31:56.610394  2824 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1006 11:32:04.980360  2824 solver.cpp:218] Iteration 4600 (11.9475 iter/s, 8.36993s/100 iters), loss = 0.37597
I1006 11:32:04.980391  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.37597 (* 1 = 0.37597 loss)
I1006 11:32:04.980396  2824 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1006 11:32:13.356457  2824 solver.cpp:218] Iteration 4700 (11.9388 iter/s, 8.37603s/100 iters), loss = 0.425006
I1006 11:32:13.356498  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.425006 (* 1 = 0.425006 loss)
I1006 11:32:13.356503  2824 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1006 11:32:21.730049  2824 solver.cpp:218] Iteration 4800 (11.9424 iter/s, 8.37351s/100 iters), loss = 0.456465
I1006 11:32:21.730165  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.456465 (* 1 = 0.456465 loss)
I1006 11:32:21.730171  2824 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1006 11:32:30.102143  2824 solver.cpp:218] Iteration 4900 (11.9447 iter/s, 8.37194s/100 iters), loss = 0.417615
I1006 11:32:30.102183  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.417615 (* 1 = 0.417615 loss)
I1006 11:32:30.102190  2824 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1006 11:32:38.059020  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:32:38.395045  2824 solver.cpp:330] Iteration 5000, Testing net (#0)
I1006 11:32:40.348717  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:32:40.430677  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4808
I1006 11:32:40.430716  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.69474 (* 1 = 1.69474 loss)
I1006 11:32:40.514153  2824 solver.cpp:218] Iteration 5000 (9.60437 iter/s, 10.4119s/100 iters), loss = 0.389174
I1006 11:32:40.514178  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.389174 (* 1 = 0.389174 loss)
I1006 11:32:40.514184  2824 sgd_solver.cpp:105] Iteration 5000, lr = 0.1
I1006 11:32:48.880364  2824 solver.cpp:218] Iteration 5100 (11.9529 iter/s, 8.36614s/100 iters), loss = 0.316744
I1006 11:32:48.880405  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316744 (* 1 = 0.316744 loss)
I1006 11:32:48.880411  2824 sgd_solver.cpp:105] Iteration 5100, lr = 0.1
I1006 11:32:57.249896  2824 solver.cpp:218] Iteration 5200 (11.9482 iter/s, 8.36945s/100 iters), loss = 0.429827
I1006 11:32:57.250052  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.429827 (* 1 = 0.429827 loss)
I1006 11:32:57.250061  2824 sgd_solver.cpp:105] Iteration 5200, lr = 0.1
I1006 11:33:05.613163  2824 solver.cpp:218] Iteration 5300 (11.9573 iter/s, 8.36308s/100 iters), loss = 0.456723
I1006 11:33:05.613204  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.456723 (* 1 = 0.456723 loss)
I1006 11:33:05.613210  2824 sgd_solver.cpp:105] Iteration 5300, lr = 0.1
I1006 11:33:13.989572  2824 solver.cpp:218] Iteration 5400 (11.9384 iter/s, 8.37633s/100 iters), loss = 0.412783
I1006 11:33:13.989603  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.412783 (* 1 = 0.412783 loss)
I1006 11:33:13.989609  2824 sgd_solver.cpp:105] Iteration 5400, lr = 0.1
I1006 11:33:21.938215  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:33:22.273032  2824 solver.cpp:330] Iteration 5500, Testing net (#0)
I1006 11:33:24.228623  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:33:24.310170  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.639
I1006 11:33:24.310206  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.23415 (* 1 = 1.23415 loss)
I1006 11:33:24.393787  2824 solver.cpp:218] Iteration 5500 (9.61156 iter/s, 10.4041s/100 iters), loss = 0.415947
I1006 11:33:24.393813  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.415947 (* 1 = 0.415947 loss)
I1006 11:33:24.393821  2824 sgd_solver.cpp:105] Iteration 5500, lr = 0.1
I1006 11:33:32.764861  2824 solver.cpp:218] Iteration 5600 (11.946 iter/s, 8.37101s/100 iters), loss = 0.333394
I1006 11:33:32.764969  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.333394 (* 1 = 0.333394 loss)
I1006 11:33:32.764976  2824 sgd_solver.cpp:105] Iteration 5600, lr = 0.1
I1006 11:33:41.129374  2824 solver.cpp:218] Iteration 5700 (11.9555 iter/s, 8.36437s/100 iters), loss = 0.361791
I1006 11:33:41.129415  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.361791 (* 1 = 0.361791 loss)
I1006 11:33:41.129420  2824 sgd_solver.cpp:105] Iteration 5700, lr = 0.1
I1006 11:33:49.495623  2824 solver.cpp:218] Iteration 5800 (11.9529 iter/s, 8.36617s/100 iters), loss = 0.401397
I1006 11:33:49.495663  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.401397 (* 1 = 0.401397 loss)
I1006 11:33:49.495669  2824 sgd_solver.cpp:105] Iteration 5800, lr = 0.1
I1006 11:33:57.865229  2824 solver.cpp:218] Iteration 5900 (11.9481 iter/s, 8.36953s/100 iters), loss = 0.372499
I1006 11:33:57.865272  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372499 (* 1 = 0.372499 loss)
I1006 11:33:57.865276  2824 sgd_solver.cpp:105] Iteration 5900, lr = 0.1
I1006 11:34:05.818568  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:34:06.154413  2824 solver.cpp:330] Iteration 6000, Testing net (#0)
I1006 11:34:08.109273  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:34:08.191069  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6724
I1006 11:34:08.191104  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.993478 (* 1 = 0.993478 loss)
I1006 11:34:08.274766  2824 solver.cpp:218] Iteration 6000 (9.60665 iter/s, 10.4095s/100 iters), loss = 0.387738
I1006 11:34:08.274792  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.387738 (* 1 = 0.387738 loss)
I1006 11:34:08.274798  2824 sgd_solver.cpp:105] Iteration 6000, lr = 0.1
I1006 11:34:16.642686  2824 solver.cpp:218] Iteration 6100 (11.9505 iter/s, 8.36786s/100 iters), loss = 0.310361
I1006 11:34:16.642719  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310361 (* 1 = 0.310361 loss)
I1006 11:34:16.642724  2824 sgd_solver.cpp:105] Iteration 6100, lr = 0.1
I1006 11:34:25.006291  2824 solver.cpp:218] Iteration 6200 (11.9567 iter/s, 8.36353s/100 iters), loss = 0.351156
I1006 11:34:25.006340  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.351156 (* 1 = 0.351156 loss)
I1006 11:34:25.006346  2824 sgd_solver.cpp:105] Iteration 6200, lr = 0.1
I1006 11:34:33.373350  2824 solver.cpp:218] Iteration 6300 (11.9517 iter/s, 8.36698s/100 iters), loss = 0.45156
I1006 11:34:33.373392  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.45156 (* 1 = 0.45156 loss)
I1006 11:34:33.373399  2824 sgd_solver.cpp:105] Iteration 6300, lr = 0.1
I1006 11:34:41.737730  2824 solver.cpp:218] Iteration 6400 (11.9556 iter/s, 8.3643s/100 iters), loss = 0.324151
I1006 11:34:41.737856  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.324151 (* 1 = 0.324151 loss)
I1006 11:34:41.737864  2824 sgd_solver.cpp:105] Iteration 6400, lr = 0.1
I1006 11:34:49.683677  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:34:50.018020  2824 solver.cpp:330] Iteration 6500, Testing net (#0)
I1006 11:34:51.971842  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:34:52.053649  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4824
I1006 11:34:52.053685  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.9304 (* 1 = 1.9304 loss)
I1006 11:34:52.137081  2824 solver.cpp:218] Iteration 6500 (9.61614 iter/s, 10.3992s/100 iters), loss = 0.393606
I1006 11:34:52.137106  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.393607 (* 1 = 0.393607 loss)
I1006 11:34:52.137114  2824 sgd_solver.cpp:105] Iteration 6500, lr = 0.1
I1006 11:35:00.504659  2824 solver.cpp:218] Iteration 6600 (11.951 iter/s, 8.36752s/100 iters), loss = 0.339446
I1006 11:35:00.504700  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339446 (* 1 = 0.339446 loss)
I1006 11:35:00.504705  2824 sgd_solver.cpp:105] Iteration 6600, lr = 0.1
I1006 11:35:08.871407  2824 solver.cpp:218] Iteration 6700 (11.9522 iter/s, 8.36667s/100 iters), loss = 0.361346
I1006 11:35:08.871449  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.361346 (* 1 = 0.361346 loss)
I1006 11:35:08.871455  2824 sgd_solver.cpp:105] Iteration 6700, lr = 0.1
I1006 11:35:17.235869  2824 solver.cpp:218] Iteration 6800 (11.9555 iter/s, 8.36438s/100 iters), loss = 0.371457
I1006 11:35:17.236006  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.371457 (* 1 = 0.371457 loss)
I1006 11:35:17.236013  2824 sgd_solver.cpp:105] Iteration 6800, lr = 0.1
I1006 11:35:25.601416  2824 solver.cpp:218] Iteration 6900 (11.954 iter/s, 8.36539s/100 iters), loss = 0.394571
I1006 11:35:25.601456  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394571 (* 1 = 0.394571 loss)
I1006 11:35:25.601461  2824 sgd_solver.cpp:105] Iteration 6900, lr = 0.1
I1006 11:35:33.552347  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:35:33.887270  2824 solver.cpp:330] Iteration 7000, Testing net (#0)
I1006 11:35:35.840152  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:35:35.921867  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6407
I1006 11:35:35.921902  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.18021 (* 1 = 1.18021 loss)
I1006 11:35:36.005646  2824 solver.cpp:218] Iteration 7000 (9.61155 iter/s, 10.4041s/100 iters), loss = 0.315512
I1006 11:35:36.005669  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.315512 (* 1 = 0.315512 loss)
I1006 11:35:36.005676  2824 sgd_solver.cpp:105] Iteration 7000, lr = 0.1
I1006 11:35:44.382663  2824 solver.cpp:218] Iteration 7100 (11.9375 iter/s, 8.37695s/100 iters), loss = 0.403448
I1006 11:35:44.382704  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.403448 (* 1 = 0.403448 loss)
I1006 11:35:44.382710  2824 sgd_solver.cpp:105] Iteration 7100, lr = 0.1
I1006 11:35:52.757060  2824 solver.cpp:218] Iteration 7200 (11.9413 iter/s, 8.37432s/100 iters), loss = 0.319563
I1006 11:35:52.757196  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319563 (* 1 = 0.319563 loss)
I1006 11:35:52.757203  2824 sgd_solver.cpp:105] Iteration 7200, lr = 0.1
I1006 11:36:01.132266  2824 solver.cpp:218] Iteration 7300 (11.9402 iter/s, 8.37504s/100 iters), loss = 0.413706
I1006 11:36:01.132308  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.413706 (* 1 = 0.413706 loss)
I1006 11:36:01.132313  2824 sgd_solver.cpp:105] Iteration 7300, lr = 0.1
I1006 11:36:09.506928  2824 solver.cpp:218] Iteration 7400 (11.9409 iter/s, 8.37459s/100 iters), loss = 0.310174
I1006 11:36:09.506961  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310174 (* 1 = 0.310174 loss)
I1006 11:36:09.506968  2824 sgd_solver.cpp:105] Iteration 7400, lr = 0.1
I1006 11:36:17.463122  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:36:17.798630  2824 solver.cpp:330] Iteration 7500, Testing net (#0)
I1006 11:36:19.756018  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:36:19.837329  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5743
I1006 11:36:19.837364  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.76021 (* 1 = 1.76021 loss)
I1006 11:36:19.921382  2824 solver.cpp:218] Iteration 7500 (9.60211 iter/s, 10.4144s/100 iters), loss = 0.3486
I1006 11:36:19.921407  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.3486 (* 1 = 0.3486 loss)
I1006 11:36:19.921413  2824 sgd_solver.cpp:105] Iteration 7500, lr = 0.1
I1006 11:36:28.305908  2824 solver.cpp:218] Iteration 7600 (11.9268 iter/s, 8.38446s/100 iters), loss = 0.424172
I1006 11:36:28.306042  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.424172 (* 1 = 0.424172 loss)
I1006 11:36:28.306059  2824 sgd_solver.cpp:105] Iteration 7600, lr = 0.1
I1006 11:36:36.688408  2824 solver.cpp:218] Iteration 7700 (11.9298 iter/s, 8.38234s/100 iters), loss = 0.306204
I1006 11:36:36.688442  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306204 (* 1 = 0.306204 loss)
I1006 11:36:36.688448  2824 sgd_solver.cpp:105] Iteration 7700, lr = 0.1
I1006 11:36:45.072254  2824 solver.cpp:218] Iteration 7800 (11.9278 iter/s, 8.38378s/100 iters), loss = 0.358607
I1006 11:36:45.072285  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.358607 (* 1 = 0.358607 loss)
I1006 11:36:45.072290  2824 sgd_solver.cpp:105] Iteration 7800, lr = 0.1
I1006 11:36:53.450676  2824 solver.cpp:218] Iteration 7900 (11.9355 iter/s, 8.37836s/100 iters), loss = 0.313555
I1006 11:36:53.450717  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313555 (* 1 = 0.313555 loss)
I1006 11:36:53.450723  2824 sgd_solver.cpp:105] Iteration 7900, lr = 0.1
I1006 11:37:01.414332  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:37:01.749939  2824 solver.cpp:330] Iteration 8000, Testing net (#0)
I1006 11:37:03.709110  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:37:03.791146  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4848
I1006 11:37:03.791185  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.21582 (* 1 = 2.21582 loss)
I1006 11:37:03.875244  2824 solver.cpp:218] Iteration 8000 (9.5928 iter/s, 10.4245s/100 iters), loss = 0.288577
I1006 11:37:03.875268  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.288577 (* 1 = 0.288577 loss)
I1006 11:37:03.875274  2824 sgd_solver.cpp:105] Iteration 8000, lr = 0.1
I1006 11:37:12.256709  2824 solver.cpp:218] Iteration 8100 (11.9312 iter/s, 8.38141s/100 iters), loss = 0.365787
I1006 11:37:12.256750  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.365787 (* 1 = 0.365787 loss)
I1006 11:37:12.256757  2824 sgd_solver.cpp:105] Iteration 8100, lr = 0.1
I1006 11:37:20.638900  2824 solver.cpp:218] Iteration 8200 (11.9302 iter/s, 8.38211s/100 iters), loss = 0.278216
I1006 11:37:20.638942  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278216 (* 1 = 0.278216 loss)
I1006 11:37:20.638947  2824 sgd_solver.cpp:105] Iteration 8200, lr = 0.1
I1006 11:37:29.022454  2824 solver.cpp:218] Iteration 8300 (11.9282 iter/s, 8.38348s/100 iters), loss = 0.366882
I1006 11:37:29.022495  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.366882 (* 1 = 0.366882 loss)
I1006 11:37:29.022500  2824 sgd_solver.cpp:105] Iteration 8300, lr = 0.1
I1006 11:37:37.401919  2824 solver.cpp:218] Iteration 8400 (11.934 iter/s, 8.37939s/100 iters), loss = 0.229526
I1006 11:37:37.402041  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229526 (* 1 = 0.229526 loss)
I1006 11:37:37.402048  2824 sgd_solver.cpp:105] Iteration 8400, lr = 0.1
I1006 11:37:45.362767  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:37:45.698274  2824 solver.cpp:330] Iteration 8500, Testing net (#0)
I1006 11:37:47.654623  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:37:47.736294  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6959
I1006 11:37:47.736331  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.03508 (* 1 = 1.03508 loss)
I1006 11:37:47.819908  2824 solver.cpp:218] Iteration 8500 (9.59893 iter/s, 10.4178s/100 iters), loss = 0.28517
I1006 11:37:47.819933  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285171 (* 1 = 0.285171 loss)
I1006 11:37:47.819939  2824 sgd_solver.cpp:105] Iteration 8500, lr = 0.1
I1006 11:37:56.195888  2824 solver.cpp:218] Iteration 8600 (11.939 iter/s, 8.37592s/100 iters), loss = 0.384387
I1006 11:37:56.195927  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.384387 (* 1 = 0.384387 loss)
I1006 11:37:56.195932  2824 sgd_solver.cpp:105] Iteration 8600, lr = 0.1
I1006 11:38:04.572161  2824 solver.cpp:218] Iteration 8700 (11.9386 iter/s, 8.3762s/100 iters), loss = 0.36446
I1006 11:38:04.572192  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364461 (* 1 = 0.364461 loss)
I1006 11:38:04.572198  2824 sgd_solver.cpp:105] Iteration 8700, lr = 0.1
I1006 11:38:12.945055  2824 solver.cpp:218] Iteration 8800 (11.9434 iter/s, 8.37283s/100 iters), loss = 0.40937
I1006 11:38:12.945170  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.40937 (* 1 = 0.40937 loss)
I1006 11:38:12.945189  2824 sgd_solver.cpp:105] Iteration 8800, lr = 0.1
I1006 11:38:21.320308  2824 solver.cpp:218] Iteration 8900 (11.9401 iter/s, 8.37511s/100 iters), loss = 0.232533
I1006 11:38:21.320339  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232533 (* 1 = 0.232533 loss)
I1006 11:38:21.320345  2824 sgd_solver.cpp:105] Iteration 8900, lr = 0.1
I1006 11:38:29.281543  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:38:29.617486  2824 solver.cpp:330] Iteration 9000, Testing net (#0)
I1006 11:38:31.574558  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:38:31.656460  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6952
I1006 11:38:31.656486  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.00879 (* 1 = 1.00879 loss)
I1006 11:38:31.740358  2824 solver.cpp:218] Iteration 9000 (9.59695 iter/s, 10.42s/100 iters), loss = 0.296672
I1006 11:38:31.740383  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296672 (* 1 = 0.296672 loss)
I1006 11:38:31.740391  2824 sgd_solver.cpp:105] Iteration 9000, lr = 0.1
I1006 11:38:40.122004  2824 solver.cpp:218] Iteration 9100 (11.9309 iter/s, 8.38159s/100 iters), loss = 0.269106
I1006 11:38:40.122035  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269106 (* 1 = 0.269106 loss)
I1006 11:38:40.122041  2824 sgd_solver.cpp:105] Iteration 9100, lr = 0.1
I1006 11:38:48.492120  2824 solver.cpp:218] Iteration 9200 (11.9474 iter/s, 8.37005s/100 iters), loss = 0.350358
I1006 11:38:48.492250  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.350358 (* 1 = 0.350358 loss)
I1006 11:38:48.492269  2824 sgd_solver.cpp:105] Iteration 9200, lr = 0.1
I1006 11:38:56.876376  2824 solver.cpp:218] Iteration 9300 (11.9273 iter/s, 8.3841s/100 iters), loss = 0.293259
I1006 11:38:56.876406  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293259 (* 1 = 0.293259 loss)
I1006 11:38:56.876411  2824 sgd_solver.cpp:105] Iteration 9300, lr = 0.1
I1006 11:39:05.260676  2824 solver.cpp:218] Iteration 9400 (11.9271 iter/s, 8.38424s/100 iters), loss = 0.258209
I1006 11:39:05.260707  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258209 (* 1 = 0.258209 loss)
I1006 11:39:05.260716  2824 sgd_solver.cpp:105] Iteration 9400, lr = 0.1
I1006 11:39:13.226164  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:39:13.562340  2824 solver.cpp:330] Iteration 9500, Testing net (#0)
I1006 11:39:15.520025  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:39:15.602077  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5299
I1006 11:39:15.602113  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.27032 (* 1 = 2.27032 loss)
I1006 11:39:15.685756  2824 solver.cpp:218] Iteration 9500 (9.59232 iter/s, 10.425s/100 iters), loss = 0.308463
I1006 11:39:15.685781  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.308463 (* 1 = 0.308463 loss)
I1006 11:39:15.685788  2824 sgd_solver.cpp:105] Iteration 9500, lr = 0.1
I1006 11:39:24.068383  2824 solver.cpp:218] Iteration 9600 (11.9295 iter/s, 8.38257s/100 iters), loss = 0.25103
I1006 11:39:24.068521  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25103 (* 1 = 0.25103 loss)
I1006 11:39:24.068539  2824 sgd_solver.cpp:105] Iteration 9600, lr = 0.1
I1006 11:39:32.453266  2824 solver.cpp:218] Iteration 9700 (11.9265 iter/s, 8.38472s/100 iters), loss = 0.25488
I1006 11:39:32.453308  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25488 (* 1 = 0.25488 loss)
I1006 11:39:32.453313  2824 sgd_solver.cpp:105] Iteration 9700, lr = 0.1
I1006 11:39:40.836103  2824 solver.cpp:218] Iteration 9800 (11.9292 iter/s, 8.38276s/100 iters), loss = 0.297747
I1006 11:39:40.836134  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297747 (* 1 = 0.297747 loss)
I1006 11:39:40.836140  2824 sgd_solver.cpp:105] Iteration 9800, lr = 0.1
I1006 11:39:49.223884  2824 solver.cpp:218] Iteration 9900 (11.9222 iter/s, 8.38772s/100 iters), loss = 0.342696
I1006 11:39:49.223927  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.342696 (* 1 = 0.342696 loss)
I1006 11:39:49.223932  2824 sgd_solver.cpp:105] Iteration 9900, lr = 0.1
I1006 11:39:57.183078  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:39:57.518488  2824 solver.cpp:330] Iteration 10000, Testing net (#0)
I1006 11:39:59.479507  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:39:59.561389  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6283
I1006 11:39:59.561424  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.38297 (* 1 = 1.38297 loss)
I1006 11:39:59.644672  2824 solver.cpp:218] Iteration 10000 (9.59628 iter/s, 10.4207s/100 iters), loss = 0.265352
I1006 11:39:59.644696  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265353 (* 1 = 0.265353 loss)
I1006 11:39:59.644703  2824 sgd_solver.cpp:105] Iteration 10000, lr = 0.1
I1006 11:40:08.023097  2824 solver.cpp:218] Iteration 10100 (11.9355 iter/s, 8.37837s/100 iters), loss = 0.328419
I1006 11:40:08.023138  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.328419 (* 1 = 0.328419 loss)
I1006 11:40:08.023142  2824 sgd_solver.cpp:105] Iteration 10100, lr = 0.1
I1006 11:40:16.401135  2824 solver.cpp:218] Iteration 10200 (11.9361 iter/s, 8.37797s/100 iters), loss = 0.355167
I1006 11:40:16.401177  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.355167 (* 1 = 0.355167 loss)
I1006 11:40:16.401182  2824 sgd_solver.cpp:105] Iteration 10200, lr = 0.1
I1006 11:40:24.775610  2824 solver.cpp:218] Iteration 10300 (11.9412 iter/s, 8.3744s/100 iters), loss = 0.245018
I1006 11:40:24.775658  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245018 (* 1 = 0.245018 loss)
I1006 11:40:24.775665  2824 sgd_solver.cpp:105] Iteration 10300, lr = 0.1
I1006 11:40:33.146042  2824 solver.cpp:218] Iteration 10400 (11.9469 iter/s, 8.37036s/100 iters), loss = 0.246522
I1006 11:40:33.146152  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246522 (* 1 = 0.246522 loss)
I1006 11:40:33.146159  2824 sgd_solver.cpp:105] Iteration 10400, lr = 0.1
I1006 11:40:41.104586  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:40:41.440332  2824 solver.cpp:330] Iteration 10500, Testing net (#0)
I1006 11:40:43.400449  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:40:43.482220  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6572
I1006 11:40:43.482257  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.30242 (* 1 = 1.30242 loss)
I1006 11:40:43.566900  2824 solver.cpp:218] Iteration 10500 (9.59626 iter/s, 10.4207s/100 iters), loss = 0.24349
I1006 11:40:43.566938  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24349 (* 1 = 0.24349 loss)
I1006 11:40:43.566946  2824 sgd_solver.cpp:105] Iteration 10500, lr = 0.1
I1006 11:40:51.948141  2824 solver.cpp:218] Iteration 10600 (11.9315 iter/s, 8.38117s/100 iters), loss = 0.234761
I1006 11:40:51.948182  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234761 (* 1 = 0.234761 loss)
I1006 11:40:51.948189  2824 sgd_solver.cpp:105] Iteration 10600, lr = 0.1
I1006 11:41:00.335908  2824 solver.cpp:218] Iteration 10700 (11.9222 iter/s, 8.38769s/100 iters), loss = 0.328962
I1006 11:41:00.335949  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.328962 (* 1 = 0.328962 loss)
I1006 11:41:00.335954  2824 sgd_solver.cpp:105] Iteration 10700, lr = 0.1
I1006 11:41:08.717489  2824 solver.cpp:218] Iteration 10800 (11.931 iter/s, 8.38151s/100 iters), loss = 0.316464
I1006 11:41:08.717633  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316464 (* 1 = 0.316464 loss)
I1006 11:41:08.717665  2824 sgd_solver.cpp:105] Iteration 10800, lr = 0.1
I1006 11:41:17.103482  2824 solver.cpp:218] Iteration 10900 (11.9249 iter/s, 8.38583s/100 iters), loss = 0.281657
I1006 11:41:17.103513  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281657 (* 1 = 0.281657 loss)
I1006 11:41:17.103519  2824 sgd_solver.cpp:105] Iteration 10900, lr = 0.1
I1006 11:41:25.070513  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:41:25.406527  2824 solver.cpp:330] Iteration 11000, Testing net (#0)
I1006 11:41:27.363615  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:41:27.445178  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5746
I1006 11:41:27.445214  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.80643 (* 1 = 1.80643 loss)
I1006 11:41:27.529302  2824 solver.cpp:218] Iteration 11000 (9.59163 iter/s, 10.4258s/100 iters), loss = 0.171689
I1006 11:41:27.529327  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171689 (* 1 = 0.171689 loss)
I1006 11:41:27.529335  2824 sgd_solver.cpp:105] Iteration 11000, lr = 0.1
I1006 11:41:35.908244  2824 solver.cpp:218] Iteration 11100 (11.9348 iter/s, 8.37888s/100 iters), loss = 0.348325
I1006 11:41:35.908285  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348325 (* 1 = 0.348325 loss)
I1006 11:41:35.908291  2824 sgd_solver.cpp:105] Iteration 11100, lr = 0.1
I1006 11:41:44.283841  2824 solver.cpp:218] Iteration 11200 (11.9396 iter/s, 8.37552s/100 iters), loss = 0.269034
I1006 11:41:44.283964  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269034 (* 1 = 0.269034 loss)
I1006 11:41:44.283982  2824 sgd_solver.cpp:105] Iteration 11200, lr = 0.1
I1006 11:41:52.652626  2824 solver.cpp:218] Iteration 11300 (11.9494 iter/s, 8.36863s/100 iters), loss = 0.305879
I1006 11:41:52.652657  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305879 (* 1 = 0.305879 loss)
I1006 11:41:52.652673  2824 sgd_solver.cpp:105] Iteration 11300, lr = 0.1
I1006 11:42:01.029134  2824 solver.cpp:218] Iteration 11400 (11.9382 iter/s, 8.37645s/100 iters), loss = 0.176606
I1006 11:42:01.029165  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176606 (* 1 = 0.176606 loss)
I1006 11:42:01.029181  2824 sgd_solver.cpp:105] Iteration 11400, lr = 0.1
I1006 11:42:08.988514  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:42:09.324051  2824 solver.cpp:330] Iteration 11500, Testing net (#0)
I1006 11:42:11.279788  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:42:11.361637  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5281
I1006 11:42:11.361662  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.90586 (* 1 = 1.90586 loss)
I1006 11:42:11.445816  2824 solver.cpp:218] Iteration 11500 (9.60005 iter/s, 10.4166s/100 iters), loss = 0.322393
I1006 11:42:11.445842  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.322393 (* 1 = 0.322393 loss)
I1006 11:42:11.445848  2824 sgd_solver.cpp:105] Iteration 11500, lr = 0.1
I1006 11:42:19.817294  2824 solver.cpp:218] Iteration 11600 (11.9454 iter/s, 8.37142s/100 iters), loss = 0.213649
I1006 11:42:19.817417  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213649 (* 1 = 0.213649 loss)
I1006 11:42:19.817425  2824 sgd_solver.cpp:105] Iteration 11600, lr = 0.1
I1006 11:42:28.186065  2824 solver.cpp:218] Iteration 11700 (11.9494 iter/s, 8.36862s/100 iters), loss = 0.248136
I1006 11:42:28.186105  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248136 (* 1 = 0.248136 loss)
I1006 11:42:28.186111  2824 sgd_solver.cpp:105] Iteration 11700, lr = 0.1
I1006 11:42:36.553581  2824 solver.cpp:218] Iteration 11800 (11.9511 iter/s, 8.36744s/100 iters), loss = 0.328491
I1006 11:42:36.553622  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.328491 (* 1 = 0.328491 loss)
I1006 11:42:36.553628  2824 sgd_solver.cpp:105] Iteration 11800, lr = 0.1
I1006 11:42:44.924212  2824 solver.cpp:218] Iteration 11900 (11.9466 iter/s, 8.37056s/100 iters), loss = 0.203272
I1006 11:42:44.924259  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203272 (* 1 = 0.203272 loss)
I1006 11:42:44.924266  2824 sgd_solver.cpp:105] Iteration 11900, lr = 0.1
I1006 11:42:52.878355  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:42:53.214328  2824 solver.cpp:330] Iteration 12000, Testing net (#0)
I1006 11:42:55.173970  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:42:55.255483  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6622
I1006 11:42:55.255519  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.24467 (* 1 = 1.24467 loss)
I1006 11:42:55.339685  2824 solver.cpp:218] Iteration 12000 (9.60117 iter/s, 10.4154s/100 iters), loss = 0.197608
I1006 11:42:55.339710  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197608 (* 1 = 0.197608 loss)
I1006 11:42:55.339716  2824 sgd_solver.cpp:105] Iteration 12000, lr = 0.1
I1006 11:43:03.707700  2824 solver.cpp:218] Iteration 12100 (11.9503 iter/s, 8.36796s/100 iters), loss = 0.282524
I1006 11:43:03.707729  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.282524 (* 1 = 0.282524 loss)
I1006 11:43:03.707746  2824 sgd_solver.cpp:105] Iteration 12100, lr = 0.1
I1006 11:43:12.083482  2824 solver.cpp:218] Iteration 12200 (11.9393 iter/s, 8.37572s/100 iters), loss = 0.240498
I1006 11:43:12.083513  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240498 (* 1 = 0.240498 loss)
I1006 11:43:12.083518  2824 sgd_solver.cpp:105] Iteration 12200, lr = 0.1
I1006 11:43:20.457772  2824 solver.cpp:218] Iteration 12300 (11.9414 iter/s, 8.37423s/100 iters), loss = 0.270574
I1006 11:43:20.457801  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270574 (* 1 = 0.270574 loss)
I1006 11:43:20.457808  2824 sgd_solver.cpp:105] Iteration 12300, lr = 0.1
I1006 11:43:28.839051  2824 solver.cpp:218] Iteration 12400 (11.9314 iter/s, 8.38122s/100 iters), loss = 0.201127
I1006 11:43:28.839177  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201127 (* 1 = 0.201127 loss)
I1006 11:43:28.839186  2824 sgd_solver.cpp:105] Iteration 12400, lr = 0.1
I1006 11:43:36.792985  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:43:37.127899  2824 solver.cpp:330] Iteration 12500, Testing net (#0)
I1006 11:43:39.087360  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:43:39.169302  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6861
I1006 11:43:39.169337  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.08464 (* 1 = 1.08464 loss)
I1006 11:43:39.252724  2824 solver.cpp:218] Iteration 12500 (9.60291 iter/s, 10.4135s/100 iters), loss = 0.227747
I1006 11:43:39.252748  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227747 (* 1 = 0.227747 loss)
I1006 11:43:39.252754  2824 sgd_solver.cpp:105] Iteration 12500, lr = 0.1
I1006 11:43:47.629079  2824 solver.cpp:218] Iteration 12600 (11.9384 iter/s, 8.3763s/100 iters), loss = 0.318464
I1006 11:43:47.629120  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.318464 (* 1 = 0.318464 loss)
I1006 11:43:47.629127  2824 sgd_solver.cpp:105] Iteration 12600, lr = 0.1
I1006 11:43:56.005681  2824 solver.cpp:218] Iteration 12700 (11.9381 iter/s, 8.37653s/100 iters), loss = 0.28279
I1006 11:43:56.005712  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28279 (* 1 = 0.28279 loss)
I1006 11:43:56.005718  2824 sgd_solver.cpp:105] Iteration 12700, lr = 0.1
I1006 11:44:04.383105  2824 solver.cpp:218] Iteration 12800 (11.9369 iter/s, 8.37736s/100 iters), loss = 0.226261
I1006 11:44:04.383240  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226261 (* 1 = 0.226261 loss)
I1006 11:44:04.383258  2824 sgd_solver.cpp:105] Iteration 12800, lr = 0.1
I1006 11:44:12.764485  2824 solver.cpp:218] Iteration 12900 (11.9314 iter/s, 8.38122s/100 iters), loss = 0.261874
I1006 11:44:12.764533  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261874 (* 1 = 0.261874 loss)
I1006 11:44:12.764541  2824 sgd_solver.cpp:105] Iteration 12900, lr = 0.1
I1006 11:44:20.730412  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:44:21.065104  2824 solver.cpp:330] Iteration 13000, Testing net (#0)
I1006 11:44:23.024154  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:44:23.106765  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6884
I1006 11:44:23.106801  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.01434 (* 1 = 1.01434 loss)
I1006 11:44:23.190575  2824 solver.cpp:218] Iteration 13000 (9.5914 iter/s, 10.426s/100 iters), loss = 0.221927
I1006 11:44:23.190598  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221927 (* 1 = 0.221927 loss)
I1006 11:44:23.190604  2824 sgd_solver.cpp:105] Iteration 13000, lr = 0.1
I1006 11:44:31.568835  2824 solver.cpp:218] Iteration 13100 (11.9357 iter/s, 8.37821s/100 iters), loss = 0.319665
I1006 11:44:31.568876  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319665 (* 1 = 0.319665 loss)
I1006 11:44:31.568881  2824 sgd_solver.cpp:105] Iteration 13100, lr = 0.1
I1006 11:44:39.940793  2824 solver.cpp:218] Iteration 13200 (11.9447 iter/s, 8.37189s/100 iters), loss = 0.390472
I1006 11:44:39.940901  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390472 (* 1 = 0.390472 loss)
I1006 11:44:39.940908  2824 sgd_solver.cpp:105] Iteration 13200, lr = 0.1
I1006 11:44:48.309381  2824 solver.cpp:218] Iteration 13300 (11.9496 iter/s, 8.36846s/100 iters), loss = 0.282934
I1006 11:44:48.309422  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.282934 (* 1 = 0.282934 loss)
I1006 11:44:48.309428  2824 sgd_solver.cpp:105] Iteration 13300, lr = 0.1
I1006 11:44:56.683249  2824 solver.cpp:218] Iteration 13400 (11.942 iter/s, 8.37379s/100 iters), loss = 0.188068
I1006 11:44:56.683303  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.188068 (* 1 = 0.188068 loss)
I1006 11:44:56.683310  2824 sgd_solver.cpp:105] Iteration 13400, lr = 0.1
I1006 11:45:04.636765  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:45:04.971634  2824 solver.cpp:330] Iteration 13500, Testing net (#0)
I1006 11:45:06.928890  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:45:07.010464  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6927
I1006 11:45:07.010500  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.1319 (* 1 = 1.1319 loss)
I1006 11:45:07.094156  2824 solver.cpp:218] Iteration 13500 (9.60545 iter/s, 10.4108s/100 iters), loss = 0.342131
I1006 11:45:07.094178  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.342131 (* 1 = 0.342131 loss)
I1006 11:45:07.094185  2824 sgd_solver.cpp:105] Iteration 13500, lr = 0.1
I1006 11:45:15.474092  2824 solver.cpp:218] Iteration 13600 (11.9333 iter/s, 8.37988s/100 iters), loss = 0.222315
I1006 11:45:15.474195  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222315 (* 1 = 0.222315 loss)
I1006 11:45:15.474211  2824 sgd_solver.cpp:105] Iteration 13600, lr = 0.1
I1006 11:45:23.850324  2824 solver.cpp:218] Iteration 13700 (11.9387 iter/s, 8.3761s/100 iters), loss = 0.254979
I1006 11:45:23.850373  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254979 (* 1 = 0.254979 loss)
I1006 11:45:23.850380  2824 sgd_solver.cpp:105] Iteration 13700, lr = 0.1
I1006 11:45:32.231192  2824 solver.cpp:218] Iteration 13800 (11.932 iter/s, 8.38079s/100 iters), loss = 0.240322
I1006 11:45:32.231232  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240322 (* 1 = 0.240322 loss)
I1006 11:45:32.231238  2824 sgd_solver.cpp:105] Iteration 13800, lr = 0.1
I1006 11:45:40.605000  2824 solver.cpp:218] Iteration 13900 (11.9421 iter/s, 8.37374s/100 iters), loss = 0.264751
I1006 11:45:40.605029  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264751 (* 1 = 0.264751 loss)
I1006 11:45:40.605036  2824 sgd_solver.cpp:105] Iteration 13900, lr = 0.1
I1006 11:45:48.569737  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:45:48.904887  2824 solver.cpp:330] Iteration 14000, Testing net (#0)
I1006 11:45:50.863802  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:45:50.945405  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6675
I1006 11:45:50.945441  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.36172 (* 1 = 1.36172 loss)
I1006 11:45:51.029269  2824 solver.cpp:218] Iteration 14000 (9.59306 iter/s, 10.4242s/100 iters), loss = 0.285712
I1006 11:45:51.029294  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285712 (* 1 = 0.285712 loss)
I1006 11:45:51.029300  2824 sgd_solver.cpp:105] Iteration 14000, lr = 0.1
I1006 11:45:59.413681  2824 solver.cpp:218] Iteration 14100 (11.927 iter/s, 8.38436s/100 iters), loss = 0.250264
I1006 11:45:59.413722  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250264 (* 1 = 0.250264 loss)
I1006 11:45:59.413727  2824 sgd_solver.cpp:105] Iteration 14100, lr = 0.1
I1006 11:46:07.788049  2824 solver.cpp:218] Iteration 14200 (11.9413 iter/s, 8.3743s/100 iters), loss = 0.249319
I1006 11:46:07.788090  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249319 (* 1 = 0.249319 loss)
I1006 11:46:07.788096  2824 sgd_solver.cpp:105] Iteration 14200, lr = 0.1
I1006 11:46:16.167085  2824 solver.cpp:218] Iteration 14300 (11.9346 iter/s, 8.37897s/100 iters), loss = 0.26104
I1006 11:46:16.167126  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26104 (* 1 = 0.26104 loss)
I1006 11:46:16.167132  2824 sgd_solver.cpp:105] Iteration 14300, lr = 0.1
I1006 11:46:24.539486  2824 solver.cpp:218] Iteration 14400 (11.9441 iter/s, 8.37233s/100 iters), loss = 0.159645
I1006 11:46:24.539608  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159645 (* 1 = 0.159645 loss)
I1006 11:46:24.539625  2824 sgd_solver.cpp:105] Iteration 14400, lr = 0.1
I1006 11:46:32.501271  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:46:32.835611  2824 solver.cpp:330] Iteration 14500, Testing net (#0)
I1006 11:46:34.794464  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:46:34.876226  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5802
I1006 11:46:34.876261  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.73819 (* 1 = 1.73819 loss)
I1006 11:46:34.960201  2824 solver.cpp:218] Iteration 14500 (9.59641 iter/s, 10.4206s/100 iters), loss = 0.246113
I1006 11:46:34.960224  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246113 (* 1 = 0.246113 loss)
I1006 11:46:34.960232  2824 sgd_solver.cpp:105] Iteration 14500, lr = 0.1
I1006 11:46:43.340852  2824 solver.cpp:218] Iteration 14600 (11.9323 iter/s, 8.3806s/100 iters), loss = 0.306727
I1006 11:46:43.340893  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306727 (* 1 = 0.306727 loss)
I1006 11:46:43.340899  2824 sgd_solver.cpp:105] Iteration 14600, lr = 0.1
I1006 11:46:51.715867  2824 solver.cpp:218] Iteration 14700 (11.9404 iter/s, 8.37495s/100 iters), loss = 0.258238
I1006 11:46:51.715908  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258238 (* 1 = 0.258238 loss)
I1006 11:46:51.715914  2824 sgd_solver.cpp:105] Iteration 14700, lr = 0.1
I1006 11:47:00.091416  2824 solver.cpp:218] Iteration 14800 (11.9396 iter/s, 8.37548s/100 iters), loss = 0.239065
I1006 11:47:00.091538  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239065 (* 1 = 0.239065 loss)
I1006 11:47:00.091547  2824 sgd_solver.cpp:105] Iteration 14800, lr = 0.1
I1006 11:47:08.462677  2824 solver.cpp:218] Iteration 14900 (11.9458 iter/s, 8.37111s/100 iters), loss = 0.193435
I1006 11:47:08.462718  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193435 (* 1 = 0.193435 loss)
I1006 11:47:08.462723  2824 sgd_solver.cpp:105] Iteration 14900, lr = 0.1
I1006 11:47:16.422952  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:47:16.757946  2824 solver.cpp:330] Iteration 15000, Testing net (#0)
I1006 11:47:18.713146  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:47:18.794533  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5451
I1006 11:47:18.794569  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.13565 (* 1 = 2.13565 loss)
I1006 11:47:18.878552  2824 solver.cpp:218] Iteration 15000 (9.6008 iter/s, 10.4158s/100 iters), loss = 0.249705
I1006 11:47:18.878585  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249705 (* 1 = 0.249705 loss)
I1006 11:47:18.878592  2824 sgd_solver.cpp:105] Iteration 15000, lr = 0.1
I1006 11:47:27.255532  2824 solver.cpp:218] Iteration 15100 (11.9376 iter/s, 8.37692s/100 iters), loss = 0.279916
I1006 11:47:27.255571  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279916 (* 1 = 0.279916 loss)
I1006 11:47:27.255576  2824 sgd_solver.cpp:105] Iteration 15100, lr = 0.1
I1006 11:47:35.632453  2824 solver.cpp:218] Iteration 15200 (11.9377 iter/s, 8.37685s/100 iters), loss = 0.213516
I1006 11:47:35.632591  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213516 (* 1 = 0.213516 loss)
I1006 11:47:35.632599  2824 sgd_solver.cpp:105] Iteration 15200, lr = 0.1
I1006 11:47:44.011266  2824 solver.cpp:218] Iteration 15300 (11.9351 iter/s, 8.37866s/100 iters), loss = 0.249626
I1006 11:47:44.011304  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249626 (* 1 = 0.249626 loss)
I1006 11:47:44.011312  2824 sgd_solver.cpp:105] Iteration 15300, lr = 0.1
I1006 11:47:52.387595  2824 solver.cpp:218] Iteration 15400 (11.9385 iter/s, 8.37627s/100 iters), loss = 0.256796
I1006 11:47:52.387636  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256796 (* 1 = 0.256796 loss)
I1006 11:47:52.387642  2824 sgd_solver.cpp:105] Iteration 15400, lr = 0.1
I1006 11:48:00.356786  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:48:00.691506  2824 solver.cpp:330] Iteration 15500, Testing net (#0)
I1006 11:48:02.646394  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:48:02.728317  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5345
I1006 11:48:02.728353  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.29294 (* 1 = 2.29294 loss)
I1006 11:48:02.811530  2824 solver.cpp:218] Iteration 15500 (9.59337 iter/s, 10.4239s/100 iters), loss = 0.185342
I1006 11:48:02.811555  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185342 (* 1 = 0.185342 loss)
I1006 11:48:02.811561  2824 sgd_solver.cpp:105] Iteration 15500, lr = 0.1
I1006 11:48:11.191931  2824 solver.cpp:218] Iteration 15600 (11.9327 iter/s, 8.38035s/100 iters), loss = 0.185613
I1006 11:48:11.192106  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185613 (* 1 = 0.185613 loss)
I1006 11:48:11.192113  2824 sgd_solver.cpp:105] Iteration 15600, lr = 0.1
I1006 11:48:19.563400  2824 solver.cpp:218] Iteration 15700 (11.9456 iter/s, 8.37128s/100 iters), loss = 0.316091
I1006 11:48:19.563442  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316091 (* 1 = 0.316091 loss)
I1006 11:48:19.563448  2824 sgd_solver.cpp:105] Iteration 15700, lr = 0.1
I1006 11:48:27.928023  2824 solver.cpp:218] Iteration 15800 (11.9552 iter/s, 8.36455s/100 iters), loss = 0.213064
I1006 11:48:27.928053  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213064 (* 1 = 0.213064 loss)
I1006 11:48:27.928069  2824 sgd_solver.cpp:105] Iteration 15800, lr = 0.1
I1006 11:48:36.295701  2824 solver.cpp:218] Iteration 15900 (11.9508 iter/s, 8.36762s/100 iters), loss = 0.255153
I1006 11:48:36.295742  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255153 (* 1 = 0.255153 loss)
I1006 11:48:36.295747  2824 sgd_solver.cpp:105] Iteration 15900, lr = 0.1
I1006 11:48:44.248667  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:48:44.583060  2824 solver.cpp:330] Iteration 16000, Testing net (#0)
I1006 11:48:46.538967  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:48:46.620470  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5217
I1006 11:48:46.620504  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.93543 (* 1 = 1.93543 loss)
I1006 11:48:46.704435  2824 solver.cpp:218] Iteration 16000 (9.60738 iter/s, 10.4087s/100 iters), loss = 0.252249
I1006 11:48:46.704458  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.252249 (* 1 = 0.252249 loss)
I1006 11:48:46.704465  2824 sgd_solver.cpp:105] Iteration 16000, lr = 0.1
I1006 11:48:55.072401  2824 solver.cpp:218] Iteration 16100 (11.9504 iter/s, 8.36791s/100 iters), loss = 0.265684
I1006 11:48:55.072441  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265684 (* 1 = 0.265684 loss)
I1006 11:48:55.072448  2824 sgd_solver.cpp:105] Iteration 16100, lr = 0.1
I1006 11:49:03.441134  2824 solver.cpp:218] Iteration 16200 (11.9493 iter/s, 8.36867s/100 iters), loss = 0.24772
I1006 11:49:03.441175  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24772 (* 1 = 0.24772 loss)
I1006 11:49:03.441180  2824 sgd_solver.cpp:105] Iteration 16200, lr = 0.1
I1006 11:49:11.800426  2824 solver.cpp:218] Iteration 16300 (11.9628 iter/s, 8.35922s/100 iters), loss = 0.217235
I1006 11:49:11.800467  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217235 (* 1 = 0.217235 loss)
I1006 11:49:11.800472  2824 sgd_solver.cpp:105] Iteration 16300, lr = 0.1
I1006 11:49:20.170270  2824 solver.cpp:218] Iteration 16400 (11.9478 iter/s, 8.36978s/100 iters), loss = 0.143793
I1006 11:49:20.170392  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143793 (* 1 = 0.143793 loss)
I1006 11:49:20.170409  2824 sgd_solver.cpp:105] Iteration 16400, lr = 0.1
I1006 11:49:28.129741  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:49:28.466470  2824 solver.cpp:330] Iteration 16500, Testing net (#0)
I1006 11:49:30.425714  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:49:30.507513  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6771
I1006 11:49:30.507547  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.14881 (* 1 = 1.14881 loss)
I1006 11:49:30.591068  2824 solver.cpp:218] Iteration 16500 (9.59634 iter/s, 10.4206s/100 iters), loss = 0.181084
I1006 11:49:30.591091  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181084 (* 1 = 0.181084 loss)
I1006 11:49:30.591099  2824 sgd_solver.cpp:105] Iteration 16500, lr = 0.1
I1006 11:49:38.955446  2824 solver.cpp:218] Iteration 16600 (11.9555 iter/s, 8.36432s/100 iters), loss = 0.280694
I1006 11:49:38.955494  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.280694 (* 1 = 0.280694 loss)
I1006 11:49:38.955502  2824 sgd_solver.cpp:105] Iteration 16600, lr = 0.1
I1006 11:49:47.322464  2824 solver.cpp:218] Iteration 16700 (11.9518 iter/s, 8.36694s/100 iters), loss = 0.212016
I1006 11:49:47.322504  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212016 (* 1 = 0.212016 loss)
I1006 11:49:47.322510  2824 sgd_solver.cpp:105] Iteration 16700, lr = 0.1
I1006 11:49:55.687013  2824 solver.cpp:218] Iteration 16800 (11.9553 iter/s, 8.36448s/100 iters), loss = 0.264608
I1006 11:49:55.687109  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264608 (* 1 = 0.264608 loss)
I1006 11:49:55.687116  2824 sgd_solver.cpp:105] Iteration 16800, lr = 0.1
I1006 11:50:04.058255  2824 solver.cpp:218] Iteration 16900 (11.9458 iter/s, 8.37112s/100 iters), loss = 0.19639
I1006 11:50:04.058285  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19639 (* 1 = 0.19639 loss)
I1006 11:50:04.058291  2824 sgd_solver.cpp:105] Iteration 16900, lr = 0.1
I1006 11:50:12.005034  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:50:12.340752  2824 solver.cpp:330] Iteration 17000, Testing net (#0)
I1006 11:50:14.298303  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:50:14.381070  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6894
I1006 11:50:14.381103  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.16318 (* 1 = 1.16318 loss)
I1006 11:50:14.463991  2824 solver.cpp:218] Iteration 17000 (9.61014 iter/s, 10.4057s/100 iters), loss = 0.14891
I1006 11:50:14.464015  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14891 (* 1 = 0.14891 loss)
I1006 11:50:14.464022  2824 sgd_solver.cpp:105] Iteration 17000, lr = 0.1
I1006 11:50:22.841325  2824 solver.cpp:218] Iteration 17100 (11.937 iter/s, 8.37728s/100 iters), loss = 0.20493
I1006 11:50:22.841372  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20493 (* 1 = 0.20493 loss)
I1006 11:50:22.841379  2824 sgd_solver.cpp:105] Iteration 17100, lr = 0.1
I1006 11:50:31.220386  2824 solver.cpp:218] Iteration 17200 (11.9346 iter/s, 8.37899s/100 iters), loss = 0.385525
I1006 11:50:31.220504  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.385525 (* 1 = 0.385525 loss)
I1006 11:50:31.220521  2824 sgd_solver.cpp:105] Iteration 17200, lr = 0.1
I1006 11:50:39.597949  2824 solver.cpp:218] Iteration 17300 (11.9368 iter/s, 8.37742s/100 iters), loss = 0.289078
I1006 11:50:39.597991  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289078 (* 1 = 0.289078 loss)
I1006 11:50:39.597996  2824 sgd_solver.cpp:105] Iteration 17300, lr = 0.1
I1006 11:50:47.971406  2824 solver.cpp:218] Iteration 17400 (11.9426 iter/s, 8.37339s/100 iters), loss = 0.281647
I1006 11:50:47.971447  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281647 (* 1 = 0.281647 loss)
I1006 11:50:47.971453  2824 sgd_solver.cpp:105] Iteration 17400, lr = 0.1
I1006 11:50:55.931064  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:50:56.267169  2824 solver.cpp:330] Iteration 17500, Testing net (#0)
I1006 11:50:58.223546  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:50:58.305431  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6952
I1006 11:50:58.305472  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.998813 (* 1 = 0.998813 loss)
I1006 11:50:58.387713  2824 solver.cpp:218] Iteration 17500 (9.6004 iter/s, 10.4162s/100 iters), loss = 0.237857
I1006 11:50:58.387743  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237857 (* 1 = 0.237857 loss)
I1006 11:50:58.387753  2824 sgd_solver.cpp:105] Iteration 17500, lr = 0.1
I1006 11:51:06.757217  2824 solver.cpp:218] Iteration 17600 (11.9482 iter/s, 8.36945s/100 iters), loss = 0.21865
I1006 11:51:06.757355  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21865 (* 1 = 0.21865 loss)
I1006 11:51:06.757362  2824 sgd_solver.cpp:105] Iteration 17600, lr = 0.1
I1006 11:51:15.128800  2824 solver.cpp:218] Iteration 17700 (11.9454 iter/s, 8.37142s/100 iters), loss = 0.305708
I1006 11:51:15.128831  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305708 (* 1 = 0.305708 loss)
I1006 11:51:15.128836  2824 sgd_solver.cpp:105] Iteration 17700, lr = 0.1
I1006 11:51:23.490732  2824 solver.cpp:218] Iteration 17800 (11.959 iter/s, 8.36187s/100 iters), loss = 0.292654
I1006 11:51:23.490773  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292654 (* 1 = 0.292654 loss)
I1006 11:51:23.490779  2824 sgd_solver.cpp:105] Iteration 17800, lr = 0.1
I1006 11:51:31.850940  2824 solver.cpp:218] Iteration 17900 (11.9615 iter/s, 8.36014s/100 iters), loss = 0.136685
I1006 11:51:31.850981  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.136685 (* 1 = 0.136685 loss)
I1006 11:51:31.850987  2824 sgd_solver.cpp:105] Iteration 17900, lr = 0.1
I1006 11:51:39.796324  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:51:40.132510  2824 solver.cpp:330] Iteration 18000, Testing net (#0)
I1006 11:51:42.089715  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:51:42.171331  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7382
I1006 11:51:42.171372  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.854962 (* 1 = 0.854962 loss)
I1006 11:51:42.254410  2824 solver.cpp:218] Iteration 18000 (9.61225 iter/s, 10.4034s/100 iters), loss = 0.17995
I1006 11:51:42.254438  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17995 (* 1 = 0.17995 loss)
I1006 11:51:42.254446  2824 sgd_solver.cpp:105] Iteration 18000, lr = 0.1
I1006 11:51:50.617771  2824 solver.cpp:218] Iteration 18100 (11.957 iter/s, 8.36331s/100 iters), loss = 0.299165
I1006 11:51:50.617818  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.299165 (* 1 = 0.299165 loss)
I1006 11:51:50.617825  2824 sgd_solver.cpp:105] Iteration 18100, lr = 0.1
I1006 11:51:58.986294  2824 solver.cpp:218] Iteration 18200 (11.9496 iter/s, 8.36845s/100 iters), loss = 0.216022
I1006 11:51:58.986332  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216022 (* 1 = 0.216022 loss)
I1006 11:51:58.986340  2824 sgd_solver.cpp:105] Iteration 18200, lr = 0.1
I1006 11:52:07.354303  2824 solver.cpp:218] Iteration 18300 (11.9504 iter/s, 8.36795s/100 iters), loss = 0.246243
I1006 11:52:07.354343  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246243 (* 1 = 0.246243 loss)
I1006 11:52:07.354349  2824 sgd_solver.cpp:105] Iteration 18300, lr = 0.1
I1006 11:52:15.731104  2824 solver.cpp:218] Iteration 18400 (11.9378 iter/s, 8.37673s/100 iters), loss = 0.257529
I1006 11:52:15.731212  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257529 (* 1 = 0.257529 loss)
I1006 11:52:15.731232  2824 sgd_solver.cpp:105] Iteration 18400, lr = 0.1
I1006 11:52:23.677448  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:52:24.012578  2824 solver.cpp:330] Iteration 18500, Testing net (#0)
I1006 11:52:25.969372  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:52:26.051161  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6833
I1006 11:52:26.051203  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.09607 (* 1 = 1.09607 loss)
I1006 11:52:26.134084  2824 solver.cpp:218] Iteration 18500 (9.61274 iter/s, 10.4029s/100 iters), loss = 0.181688
I1006 11:52:26.134112  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181688 (* 1 = 0.181688 loss)
I1006 11:52:26.134120  2824 sgd_solver.cpp:105] Iteration 18500, lr = 0.1
I1006 11:52:34.495620  2824 solver.cpp:218] Iteration 18600 (11.9596 iter/s, 8.36148s/100 iters), loss = 0.181115
I1006 11:52:34.495666  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181115 (* 1 = 0.181115 loss)
I1006 11:52:34.495673  2824 sgd_solver.cpp:105] Iteration 18600, lr = 0.1
I1006 11:52:42.873428  2824 solver.cpp:218] Iteration 18700 (11.9364 iter/s, 8.37773s/100 iters), loss = 0.294954
I1006 11:52:42.873476  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.294954 (* 1 = 0.294954 loss)
I1006 11:52:42.873483  2824 sgd_solver.cpp:105] Iteration 18700, lr = 0.1
I1006 11:52:51.244060  2824 solver.cpp:218] Iteration 18800 (11.9466 iter/s, 8.37056s/100 iters), loss = 0.180592
I1006 11:52:51.244154  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180592 (* 1 = 0.180592 loss)
I1006 11:52:51.244161  2824 sgd_solver.cpp:105] Iteration 18800, lr = 0.1
I1006 11:52:59.624227  2824 solver.cpp:218] Iteration 18900 (11.9331 iter/s, 8.38005s/100 iters), loss = 0.156539
I1006 11:52:59.624267  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156539 (* 1 = 0.156539 loss)
I1006 11:52:59.624274  2824 sgd_solver.cpp:105] Iteration 18900, lr = 0.1
I1006 11:53:07.581856  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:53:07.918687  2824 solver.cpp:330] Iteration 19000, Testing net (#0)
I1006 11:53:09.878201  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:53:09.960191  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6056
I1006 11:53:09.960230  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.61291 (* 1 = 1.61291 loss)
I1006 11:53:10.043769  2824 solver.cpp:218] Iteration 19000 (9.59758 iter/s, 10.4193s/100 iters), loss = 0.168557
I1006 11:53:10.043798  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168557 (* 1 = 0.168557 loss)
I1006 11:53:10.043807  2824 sgd_solver.cpp:105] Iteration 19000, lr = 0.1
I1006 11:53:18.418720  2824 solver.cpp:218] Iteration 19100 (11.9405 iter/s, 8.37489s/100 iters), loss = 0.1766
I1006 11:53:18.418767  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1766 (* 1 = 0.1766 loss)
I1006 11:53:18.418774  2824 sgd_solver.cpp:105] Iteration 19100, lr = 0.1
I1006 11:53:26.794342  2824 solver.cpp:218] Iteration 19200 (11.9395 iter/s, 8.37555s/100 iters), loss = 0.178176
I1006 11:53:26.794410  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178176 (* 1 = 0.178176 loss)
I1006 11:53:26.794417  2824 sgd_solver.cpp:105] Iteration 19200, lr = 0.1
I1006 11:53:35.167090  2824 solver.cpp:218] Iteration 19300 (11.9436 iter/s, 8.37265s/100 iters), loss = 0.229785
I1006 11:53:35.167130  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229785 (* 1 = 0.229785 loss)
I1006 11:53:35.167136  2824 sgd_solver.cpp:105] Iteration 19300, lr = 0.1
I1006 11:53:43.544137  2824 solver.cpp:218] Iteration 19400 (11.9375 iter/s, 8.37698s/100 iters), loss = 0.204081
I1006 11:53:43.544178  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204081 (* 1 = 0.204081 loss)
I1006 11:53:43.544184  2824 sgd_solver.cpp:105] Iteration 19400, lr = 0.1
I1006 11:53:51.507797  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:53:51.842859  2824 solver.cpp:330] Iteration 19500, Testing net (#0)
I1006 11:53:53.799279  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:53:53.881136  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5992
I1006 11:53:53.881167  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.65658 (* 1 = 1.65658 loss)
I1006 11:53:53.964841  2824 solver.cpp:218] Iteration 19500 (9.59635 iter/s, 10.4206s/100 iters), loss = 0.177968
I1006 11:53:53.964870  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177968 (* 1 = 0.177968 loss)
I1006 11:53:53.964879  2824 sgd_solver.cpp:105] Iteration 19500, lr = 0.1
I1006 11:54:02.333981  2824 solver.cpp:218] Iteration 19600 (11.9487 iter/s, 8.36908s/100 iters), loss = 0.205458
I1006 11:54:02.334072  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205458 (* 1 = 0.205458 loss)
I1006 11:54:02.334087  2824 sgd_solver.cpp:105] Iteration 19600, lr = 0.1
I1006 11:54:10.707758  2824 solver.cpp:218] Iteration 19700 (11.9422 iter/s, 8.37366s/100 iters), loss = 0.277622
I1006 11:54:10.707805  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277622 (* 1 = 0.277622 loss)
I1006 11:54:10.707813  2824 sgd_solver.cpp:105] Iteration 19700, lr = 0.1
I1006 11:54:19.080082  2824 solver.cpp:218] Iteration 19800 (11.9442 iter/s, 8.37225s/100 iters), loss = 0.237855
I1006 11:54:19.080121  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237855 (* 1 = 0.237855 loss)
I1006 11:54:19.080128  2824 sgd_solver.cpp:105] Iteration 19800, lr = 0.1
I1006 11:54:27.449560  2824 solver.cpp:218] Iteration 19900 (11.9483 iter/s, 8.36941s/100 iters), loss = 0.14916
I1006 11:54:27.449601  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14916 (* 1 = 0.14916 loss)
I1006 11:54:27.449607  2824 sgd_solver.cpp:105] Iteration 19900, lr = 0.1
I1006 11:54:35.409036  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:54:35.744269  2824 solver.cpp:330] Iteration 20000, Testing net (#0)
I1006 11:54:37.701262  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:54:37.783660  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5358
I1006 11:54:37.783702  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.03865 (* 1 = 2.03865 loss)
I1006 11:54:37.866176  2824 solver.cpp:218] Iteration 20000 (9.60011 iter/s, 10.4165s/100 iters), loss = 0.216291
I1006 11:54:37.866204  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216291 (* 1 = 0.216291 loss)
I1006 11:54:37.866211  2824 sgd_solver.cpp:105] Iteration 20000, lr = 0.1
I1006 11:54:46.247202  2824 solver.cpp:218] Iteration 20100 (11.9318 iter/s, 8.38097s/100 iters), loss = 0.215807
I1006 11:54:46.247239  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215807 (* 1 = 0.215807 loss)
I1006 11:54:46.247246  2824 sgd_solver.cpp:105] Iteration 20100, lr = 0.1
I1006 11:54:54.625252  2824 solver.cpp:218] Iteration 20200 (11.936 iter/s, 8.37799s/100 iters), loss = 0.262353
I1006 11:54:54.625291  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262353 (* 1 = 0.262353 loss)
I1006 11:54:54.625298  2824 sgd_solver.cpp:105] Iteration 20200, lr = 0.1
I1006 11:55:03.005381  2824 solver.cpp:218] Iteration 20300 (11.9331 iter/s, 8.38006s/100 iters), loss = 0.156834
I1006 11:55:03.005411  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156834 (* 1 = 0.156834 loss)
I1006 11:55:03.005416  2824 sgd_solver.cpp:105] Iteration 20300, lr = 0.1
I1006 11:55:11.391140  2824 solver.cpp:218] Iteration 20400 (11.9251 iter/s, 8.3857s/100 iters), loss = 0.217777
I1006 11:55:11.391240  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217777 (* 1 = 0.217777 loss)
I1006 11:55:11.391258  2824 sgd_solver.cpp:105] Iteration 20400, lr = 0.1
I1006 11:55:19.357589  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:55:19.693425  2824 solver.cpp:330] Iteration 20500, Testing net (#0)
I1006 11:55:21.652114  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:55:21.733579  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4857
I1006 11:55:21.733614  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.28401 (* 1 = 2.28401 loss)
I1006 11:55:21.816730  2824 solver.cpp:218] Iteration 20500 (9.5919 iter/s, 10.4255s/100 iters), loss = 0.161987
I1006 11:55:21.816752  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.161987 (* 1 = 0.161987 loss)
I1006 11:55:21.816759  2824 sgd_solver.cpp:105] Iteration 20500, lr = 0.1
I1006 11:55:30.196046  2824 solver.cpp:218] Iteration 20600 (11.9342 iter/s, 8.37927s/100 iters), loss = 0.205978
I1006 11:55:30.196076  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205978 (* 1 = 0.205978 loss)
I1006 11:55:30.196082  2824 sgd_solver.cpp:105] Iteration 20600, lr = 0.1
I1006 11:55:38.574050  2824 solver.cpp:218] Iteration 20700 (11.9361 iter/s, 8.37795s/100 iters), loss = 0.217906
I1006 11:55:38.574092  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217906 (* 1 = 0.217906 loss)
I1006 11:55:38.574097  2824 sgd_solver.cpp:105] Iteration 20700, lr = 0.1
I1006 11:55:46.952042  2824 solver.cpp:218] Iteration 20800 (11.9361 iter/s, 8.37792s/100 iters), loss = 0.155766
I1006 11:55:46.952169  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.155766 (* 1 = 0.155766 loss)
I1006 11:55:46.952177  2824 sgd_solver.cpp:105] Iteration 20800, lr = 0.1
I1006 11:55:55.333878  2824 solver.cpp:218] Iteration 20900 (11.9308 iter/s, 8.38168s/100 iters), loss = 0.177955
I1006 11:55:55.333907  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177954 (* 1 = 0.177954 loss)
I1006 11:55:55.333914  2824 sgd_solver.cpp:105] Iteration 20900, lr = 0.1
I1006 11:56:03.293251  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:56:03.628513  2824 solver.cpp:330] Iteration 21000, Testing net (#0)
I1006 11:56:05.586632  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:56:05.668325  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5191
I1006 11:56:05.668360  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.33322 (* 1 = 2.33322 loss)
I1006 11:56:05.751304  2824 solver.cpp:218] Iteration 21000 (9.59935 iter/s, 10.4174s/100 iters), loss = 0.224284
I1006 11:56:05.751328  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224284 (* 1 = 0.224284 loss)
I1006 11:56:05.751334  2824 sgd_solver.cpp:105] Iteration 21000, lr = 0.1
I1006 11:56:14.131106  2824 solver.cpp:218] Iteration 21100 (11.9335 iter/s, 8.37975s/100 iters), loss = 0.319336
I1006 11:56:14.131136  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319336 (* 1 = 0.319336 loss)
I1006 11:56:14.131141  2824 sgd_solver.cpp:105] Iteration 21100, lr = 0.1
I1006 11:56:22.510524  2824 solver.cpp:218] Iteration 21200 (11.9341 iter/s, 8.37936s/100 iters), loss = 0.205234
I1006 11:56:22.510644  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205234 (* 1 = 0.205234 loss)
I1006 11:56:22.510653  2824 sgd_solver.cpp:105] Iteration 21200, lr = 0.1
I1006 11:56:30.892938  2824 solver.cpp:218] Iteration 21300 (11.9299 iter/s, 8.38227s/100 iters), loss = 0.301677
I1006 11:56:30.892967  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301677 (* 1 = 0.301677 loss)
I1006 11:56:30.892983  2824 sgd_solver.cpp:105] Iteration 21300, lr = 0.1
I1006 11:56:39.264977  2824 solver.cpp:218] Iteration 21400 (11.9446 iter/s, 8.37198s/100 iters), loss = 0.265717
I1006 11:56:39.265007  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265717 (* 1 = 0.265717 loss)
I1006 11:56:39.265013  2824 sgd_solver.cpp:105] Iteration 21400, lr = 0.1
I1006 11:56:47.229923  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:56:47.564299  2824 solver.cpp:330] Iteration 21500, Testing net (#0)
I1006 11:56:49.518339  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:56:49.599984  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.709
I1006 11:56:49.600021  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.01531 (* 1 = 1.01531 loss)
I1006 11:56:49.683779  2824 solver.cpp:218] Iteration 21500 (9.59809 iter/s, 10.4187s/100 iters), loss = 0.27639
I1006 11:56:49.683801  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27639 (* 1 = 0.27639 loss)
I1006 11:56:49.683809  2824 sgd_solver.cpp:105] Iteration 21500, lr = 0.1
I1006 11:56:58.063333  2824 solver.cpp:218] Iteration 21600 (11.9339 iter/s, 8.3795s/100 iters), loss = 0.335937
I1006 11:56:58.063443  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.335937 (* 1 = 0.335937 loss)
I1006 11:56:58.063460  2824 sgd_solver.cpp:105] Iteration 21600, lr = 0.1
I1006 11:57:06.444159  2824 solver.cpp:218] Iteration 21700 (11.9322 iter/s, 8.3807s/100 iters), loss = 0.209387
I1006 11:57:06.444198  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209386 (* 1 = 0.209386 loss)
I1006 11:57:06.444203  2824 sgd_solver.cpp:105] Iteration 21700, lr = 0.1
I1006 11:57:14.818728  2824 solver.cpp:218] Iteration 21800 (11.941 iter/s, 8.37451s/100 iters), loss = 0.162248
I1006 11:57:14.818758  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.162248 (* 1 = 0.162248 loss)
I1006 11:57:14.818763  2824 sgd_solver.cpp:105] Iteration 21800, lr = 0.1
I1006 11:57:23.193719  2824 solver.cpp:218] Iteration 21900 (11.9404 iter/s, 8.37494s/100 iters), loss = 0.178081
I1006 11:57:23.193759  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17808 (* 1 = 0.17808 loss)
I1006 11:57:23.193764  2824 sgd_solver.cpp:105] Iteration 21900, lr = 0.1
I1006 11:57:31.154505  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:57:31.489272  2824 solver.cpp:330] Iteration 22000, Testing net (#0)
I1006 11:57:33.444680  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:57:33.526118  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6926
I1006 11:57:33.526152  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.15729 (* 1 = 1.15729 loss)
I1006 11:57:33.610219  2824 solver.cpp:218] Iteration 22000 (9.60022 iter/s, 10.4164s/100 iters), loss = 0.170288
I1006 11:57:33.610241  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170288 (* 1 = 0.170288 loss)
I1006 11:57:33.610247  2824 sgd_solver.cpp:105] Iteration 22000, lr = 0.1
I1006 11:57:41.991338  2824 solver.cpp:218] Iteration 22100 (11.9317 iter/s, 8.38107s/100 iters), loss = 0.257702
I1006 11:57:41.991379  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257701 (* 1 = 0.257701 loss)
I1006 11:57:41.991384  2824 sgd_solver.cpp:105] Iteration 22100, lr = 0.1
I1006 11:57:50.368494  2824 solver.cpp:218] Iteration 22200 (11.9373 iter/s, 8.37709s/100 iters), loss = 0.199048
I1006 11:57:50.368533  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199047 (* 1 = 0.199047 loss)
I1006 11:57:50.368540  2824 sgd_solver.cpp:105] Iteration 22200, lr = 0.1
I1006 11:57:58.748790  2824 solver.cpp:218] Iteration 22300 (11.9328 iter/s, 8.38023s/100 iters), loss = 0.221503
I1006 11:57:58.748831  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221503 (* 1 = 0.221503 loss)
I1006 11:57:58.748836  2824 sgd_solver.cpp:105] Iteration 22300, lr = 0.1
I1006 11:58:07.128201  2824 solver.cpp:218] Iteration 22400 (11.9341 iter/s, 8.37935s/100 iters), loss = 0.22539
I1006 11:58:07.128353  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22539 (* 1 = 0.22539 loss)
I1006 11:58:07.128360  2824 sgd_solver.cpp:105] Iteration 22400, lr = 0.1
I1006 11:58:15.095898  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:58:15.430634  2824 solver.cpp:330] Iteration 22500, Testing net (#0)
I1006 11:58:17.387303  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:58:17.469173  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6783
I1006 11:58:17.469209  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.20014 (* 1 = 1.20014 loss)
I1006 11:58:17.552685  2824 solver.cpp:218] Iteration 22500 (9.59297 iter/s, 10.4243s/100 iters), loss = 0.19136
I1006 11:58:17.552709  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19136 (* 1 = 0.19136 loss)
I1006 11:58:17.552716  2824 sgd_solver.cpp:105] Iteration 22500, lr = 0.1
I1006 11:58:25.931010  2824 solver.cpp:218] Iteration 22600 (11.9356 iter/s, 8.37827s/100 iters), loss = 0.135574
I1006 11:58:25.931051  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135574 (* 1 = 0.135574 loss)
I1006 11:58:25.931056  2824 sgd_solver.cpp:105] Iteration 22600, lr = 0.1
I1006 11:58:34.307561  2824 solver.cpp:218] Iteration 22700 (11.9382 iter/s, 8.37648s/100 iters), loss = 0.339771
I1006 11:58:34.307602  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339771 (* 1 = 0.339771 loss)
I1006 11:58:34.307608  2824 sgd_solver.cpp:105] Iteration 22700, lr = 0.1
I1006 11:58:42.686820  2824 solver.cpp:218] Iteration 22800 (11.9343 iter/s, 8.37919s/100 iters), loss = 0.22057
I1006 11:58:42.686941  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22057 (* 1 = 0.22057 loss)
I1006 11:58:42.686949  2824 sgd_solver.cpp:105] Iteration 22800, lr = 0.1
I1006 11:58:51.063935  2824 solver.cpp:218] Iteration 22900 (11.9375 iter/s, 8.37698s/100 iters), loss = 0.14156
I1006 11:58:51.063977  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14156 (* 1 = 0.14156 loss)
I1006 11:58:51.063982  2824 sgd_solver.cpp:105] Iteration 22900, lr = 0.1
I1006 11:58:59.020895  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:58:59.356925  2824 solver.cpp:330] Iteration 23000, Testing net (#0)
I1006 11:59:01.316375  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:59:01.398269  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.618
I1006 11:59:01.398305  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.56466 (* 1 = 1.56466 loss)
I1006 11:59:01.481803  2824 solver.cpp:218] Iteration 23000 (9.59896 iter/s, 10.4178s/100 iters), loss = 0.143323
I1006 11:59:01.481827  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143323 (* 1 = 0.143323 loss)
I1006 11:59:01.481834  2824 sgd_solver.cpp:105] Iteration 23000, lr = 0.1
I1006 11:59:09.851572  2824 solver.cpp:218] Iteration 23100 (11.9478 iter/s, 8.36972s/100 iters), loss = 0.193741
I1006 11:59:09.851613  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193741 (* 1 = 0.193741 loss)
I1006 11:59:09.851619  2824 sgd_solver.cpp:105] Iteration 23100, lr = 0.1
I1006 11:59:18.231534  2824 solver.cpp:218] Iteration 23200 (11.9333 iter/s, 8.37989s/100 iters), loss = 0.267056
I1006 11:59:18.231653  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267056 (* 1 = 0.267056 loss)
I1006 11:59:18.231662  2824 sgd_solver.cpp:105] Iteration 23200, lr = 0.1
I1006 11:59:26.604739  2824 solver.cpp:218] Iteration 23300 (11.9431 iter/s, 8.37306s/100 iters), loss = 0.290381
I1006 11:59:26.604780  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.290381 (* 1 = 0.290381 loss)
I1006 11:59:26.604786  2824 sgd_solver.cpp:105] Iteration 23300, lr = 0.1
I1006 11:59:34.977680  2824 solver.cpp:218] Iteration 23400 (11.9433 iter/s, 8.37288s/100 iters), loss = 0.121903
I1006 11:59:34.977710  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121903 (* 1 = 0.121903 loss)
I1006 11:59:34.977716  2824 sgd_solver.cpp:105] Iteration 23400, lr = 0.1
I1006 11:59:42.938441  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:59:43.273849  2824 solver.cpp:330] Iteration 23500, Testing net (#0)
I1006 11:59:45.233111  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 11:59:45.314041  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.727
I1006 11:59:45.314077  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.01605 (* 1 = 1.01605 loss)
I1006 11:59:45.397415  2824 solver.cpp:218] Iteration 23500 (9.59723 iter/s, 10.4197s/100 iters), loss = 0.162884
I1006 11:59:45.397439  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.162884 (* 1 = 0.162884 loss)
I1006 11:59:45.397444  2824 sgd_solver.cpp:105] Iteration 23500, lr = 0.1
I1006 11:59:53.768640  2824 solver.cpp:218] Iteration 23600 (11.9458 iter/s, 8.37117s/100 iters), loss = 0.177865
I1006 11:59:53.768744  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177865 (* 1 = 0.177865 loss)
I1006 11:59:53.768751  2824 sgd_solver.cpp:105] Iteration 23600, lr = 0.1
I1006 12:00:02.154896  2824 solver.cpp:218] Iteration 23700 (11.9245 iter/s, 8.38613s/100 iters), loss = 0.277548
I1006 12:00:02.154937  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277548 (* 1 = 0.277548 loss)
I1006 12:00:02.154942  2824 sgd_solver.cpp:105] Iteration 23700, lr = 0.1
I1006 12:00:10.531657  2824 solver.cpp:218] Iteration 23800 (11.9379 iter/s, 8.3767s/100 iters), loss = 0.191761
I1006 12:00:10.531688  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191761 (* 1 = 0.191761 loss)
I1006 12:00:10.531693  2824 sgd_solver.cpp:105] Iteration 23800, lr = 0.1
I1006 12:00:18.912004  2824 solver.cpp:218] Iteration 23900 (11.9328 iter/s, 8.38029s/100 iters), loss = 0.220483
I1006 12:00:18.912045  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220483 (* 1 = 0.220483 loss)
I1006 12:00:18.912051  2824 sgd_solver.cpp:105] Iteration 23900, lr = 0.1
I1006 12:00:26.877285  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:00:27.212507  2824 solver.cpp:330] Iteration 24000, Testing net (#0)
I1006 12:00:29.170652  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:00:29.252631  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.684
I1006 12:00:29.252668  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.45224 (* 1 = 1.45224 loss)
I1006 12:00:29.336616  2824 solver.cpp:218] Iteration 24000 (9.59275 iter/s, 10.4245s/100 iters), loss = 0.130791
I1006 12:00:29.336642  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130791 (* 1 = 0.130791 loss)
I1006 12:00:29.336658  2824 sgd_solver.cpp:105] Iteration 24000, lr = 0.1
I1006 12:00:37.726858  2824 solver.cpp:218] Iteration 24100 (11.9187 iter/s, 8.39019s/100 iters), loss = 0.276425
I1006 12:00:37.726900  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276425 (* 1 = 0.276425 loss)
I1006 12:00:37.726907  2824 sgd_solver.cpp:105] Iteration 24100, lr = 0.1
I1006 12:00:46.113153  2824 solver.cpp:218] Iteration 24200 (11.9243 iter/s, 8.38623s/100 iters), loss = 0.260155
I1006 12:00:46.113184  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260155 (* 1 = 0.260155 loss)
I1006 12:00:46.113190  2824 sgd_solver.cpp:105] Iteration 24200, lr = 0.1
I1006 12:00:54.497325  2824 solver.cpp:218] Iteration 24300 (11.9273 iter/s, 8.38412s/100 iters), loss = 0.166051
I1006 12:00:54.497367  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166051 (* 1 = 0.166051 loss)
I1006 12:00:54.497373  2824 sgd_solver.cpp:105] Iteration 24300, lr = 0.1
I1006 12:01:02.884125  2824 solver.cpp:218] Iteration 24400 (11.9236 iter/s, 8.38673s/100 iters), loss = 0.210106
I1006 12:01:02.884230  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210105 (* 1 = 0.210105 loss)
I1006 12:01:02.884238  2824 sgd_solver.cpp:105] Iteration 24400, lr = 0.1
I1006 12:01:10.858700  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:01:11.194427  2824 solver.cpp:330] Iteration 24500, Testing net (#0)
I1006 12:01:13.154362  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:01:13.236615  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7542
I1006 12:01:13.236641  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.775856 (* 1 = 0.775856 loss)
I1006 12:01:13.320299  2824 solver.cpp:218] Iteration 24500 (9.58217 iter/s, 10.436s/100 iters), loss = 0.160147
I1006 12:01:13.320324  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160147 (* 1 = 0.160147 loss)
I1006 12:01:13.320332  2824 sgd_solver.cpp:105] Iteration 24500, lr = 0.1
I1006 12:01:21.696049  2824 solver.cpp:218] Iteration 24600 (11.9393 iter/s, 8.3757s/100 iters), loss = 0.179762
I1006 12:01:21.696091  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179762 (* 1 = 0.179762 loss)
I1006 12:01:21.696096  2824 sgd_solver.cpp:105] Iteration 24600, lr = 0.1
I1006 12:01:30.074174  2824 solver.cpp:218] Iteration 24700 (11.9359 iter/s, 8.37806s/100 iters), loss = 0.236993
I1006 12:01:30.074204  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236993 (* 1 = 0.236993 loss)
I1006 12:01:30.074209  2824 sgd_solver.cpp:105] Iteration 24700, lr = 0.1
I1006 12:01:38.446754  2824 solver.cpp:218] Iteration 24800 (11.9438 iter/s, 8.37252s/100 iters), loss = 0.181855
I1006 12:01:38.446851  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181855 (* 1 = 0.181855 loss)
I1006 12:01:38.446858  2824 sgd_solver.cpp:105] Iteration 24800, lr = 0.1
I1006 12:01:46.830871  2824 solver.cpp:218] Iteration 24900 (11.9275 iter/s, 8.384s/100 iters), loss = 0.168444
I1006 12:01:46.830914  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168444 (* 1 = 0.168444 loss)
I1006 12:01:46.830919  2824 sgd_solver.cpp:105] Iteration 24900, lr = 0.1
I1006 12:01:54.791121  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:01:55.126529  2824 solver.cpp:330] Iteration 25000, Testing net (#0)
I1006 12:01:57.087327  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:01:57.169168  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6935
I1006 12:01:57.169204  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.05942 (* 1 = 1.05942 loss)
I1006 12:01:57.252715  2824 solver.cpp:218] Iteration 25000 (9.5953 iter/s, 10.4218s/100 iters), loss = 0.175547
I1006 12:01:57.252739  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175546 (* 1 = 0.175546 loss)
I1006 12:01:57.252745  2824 sgd_solver.cpp:105] Iteration 25000, lr = 0.1
I1006 12:02:05.637276  2824 solver.cpp:218] Iteration 25100 (11.9268 iter/s, 8.38451s/100 iters), loss = 0.169524
I1006 12:02:05.637308  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169524 (* 1 = 0.169524 loss)
I1006 12:02:05.637327  2824 sgd_solver.cpp:105] Iteration 25100, lr = 0.1
I1006 12:02:14.016168  2824 solver.cpp:218] Iteration 25200 (11.9348 iter/s, 8.37884s/100 iters), loss = 0.196336
I1006 12:02:14.016273  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196336 (* 1 = 0.196336 loss)
I1006 12:02:14.016294  2824 sgd_solver.cpp:105] Iteration 25200, lr = 0.1
I1006 12:02:22.393040  2824 solver.cpp:218] Iteration 25300 (11.9378 iter/s, 8.37675s/100 iters), loss = 0.250484
I1006 12:02:22.393071  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250483 (* 1 = 0.250483 loss)
I1006 12:02:22.393079  2824 sgd_solver.cpp:105] Iteration 25300, lr = 0.1
I1006 12:02:30.775725  2824 solver.cpp:218] Iteration 25400 (11.9294 iter/s, 8.38263s/100 iters), loss = 0.130765
I1006 12:02:30.775755  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130764 (* 1 = 0.130764 loss)
I1006 12:02:30.775761  2824 sgd_solver.cpp:105] Iteration 25400, lr = 0.1
I1006 12:02:38.737048  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:02:39.072634  2824 solver.cpp:330] Iteration 25500, Testing net (#0)
I1006 12:02:41.031733  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:02:41.113453  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7279
I1006 12:02:41.113479  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.899689 (* 1 = 0.899689 loss)
I1006 12:02:41.196969  2824 solver.cpp:218] Iteration 25500 (9.59584 iter/s, 10.4212s/100 iters), loss = 0.208285
I1006 12:02:41.196993  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208285 (* 1 = 0.208285 loss)
I1006 12:02:41.197000  2824 sgd_solver.cpp:105] Iteration 25500, lr = 0.1
I1006 12:02:49.565141  2824 solver.cpp:218] Iteration 25600 (11.9501 iter/s, 8.36812s/100 iters), loss = 0.18681
I1006 12:02:49.565239  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18681 (* 1 = 0.18681 loss)
I1006 12:02:49.565261  2824 sgd_solver.cpp:105] Iteration 25600, lr = 0.1
I1006 12:02:57.942793  2824 solver.cpp:218] Iteration 25700 (11.9367 iter/s, 8.37754s/100 iters), loss = 0.1379
I1006 12:02:57.942826  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1379 (* 1 = 0.1379 loss)
I1006 12:02:57.942834  2824 sgd_solver.cpp:105] Iteration 25700, lr = 0.1
I1006 12:03:06.318572  2824 solver.cpp:218] Iteration 25800 (11.9393 iter/s, 8.37572s/100 iters), loss = 0.230695
I1006 12:03:06.318604  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.230694 (* 1 = 0.230694 loss)
I1006 12:03:06.318621  2824 sgd_solver.cpp:105] Iteration 25800, lr = 0.1
I1006 12:03:14.698415  2824 solver.cpp:218] Iteration 25900 (11.9335 iter/s, 8.37979s/100 iters), loss = 0.175355
I1006 12:03:14.698448  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175354 (* 1 = 0.175354 loss)
I1006 12:03:14.698467  2824 sgd_solver.cpp:105] Iteration 25900, lr = 0.1
I1006 12:03:22.657654  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:03:22.993861  2824 solver.cpp:330] Iteration 26000, Testing net (#0)
I1006 12:03:24.952133  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:03:25.033788  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7004
I1006 12:03:25.033816  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.04505 (* 1 = 1.04505 loss)
I1006 12:03:25.117245  2824 solver.cpp:218] Iteration 26000 (9.59806 iter/s, 10.4188s/100 iters), loss = 0.197102
I1006 12:03:25.117274  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197102 (* 1 = 0.197102 loss)
I1006 12:03:25.117282  2824 sgd_solver.cpp:105] Iteration 26000, lr = 0.1
I1006 12:03:33.498093  2824 solver.cpp:218] Iteration 26100 (11.932 iter/s, 8.38079s/100 iters), loss = 0.11874
I1006 12:03:33.498127  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118739 (* 1 = 0.118739 loss)
I1006 12:03:33.498145  2824 sgd_solver.cpp:105] Iteration 26100, lr = 0.1
I1006 12:03:41.879755  2824 solver.cpp:218] Iteration 26200 (11.9309 iter/s, 8.3816s/100 iters), loss = 0.231349
I1006 12:03:41.879791  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231348 (* 1 = 0.231348 loss)
I1006 12:03:41.879808  2824 sgd_solver.cpp:105] Iteration 26200, lr = 0.1
I1006 12:03:50.265102  2824 solver.cpp:218] Iteration 26300 (11.9256 iter/s, 8.38529s/100 iters), loss = 0.199206
I1006 12:03:50.265136  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199206 (* 1 = 0.199206 loss)
I1006 12:03:50.265153  2824 sgd_solver.cpp:105] Iteration 26300, lr = 0.1
I1006 12:03:58.656296  2824 solver.cpp:218] Iteration 26400 (11.9173 iter/s, 8.39114s/100 iters), loss = 0.136869
I1006 12:03:58.656385  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.136869 (* 1 = 0.136869 loss)
I1006 12:03:58.656394  2824 sgd_solver.cpp:105] Iteration 26400, lr = 0.1
I1006 12:04:06.627732  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:04:06.964413  2824 solver.cpp:330] Iteration 26500, Testing net (#0)
I1006 12:04:08.922494  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:04:09.004349  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5533
I1006 12:04:09.004386  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.71207 (* 1 = 1.71207 loss)
I1006 12:04:09.087998  2824 solver.cpp:218] Iteration 26500 (9.58626 iter/s, 10.4316s/100 iters), loss = 0.160424
I1006 12:04:09.088021  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160424 (* 1 = 0.160424 loss)
I1006 12:04:09.088027  2824 sgd_solver.cpp:105] Iteration 26500, lr = 0.1
I1006 12:04:17.465785  2824 solver.cpp:218] Iteration 26600 (11.9364 iter/s, 8.37774s/100 iters), loss = 0.28132
I1006 12:04:17.465826  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281319 (* 1 = 0.281319 loss)
I1006 12:04:17.465832  2824 sgd_solver.cpp:105] Iteration 26600, lr = 0.1
I1006 12:04:25.841563  2824 solver.cpp:218] Iteration 26700 (11.9393 iter/s, 8.37571s/100 iters), loss = 0.159111
I1006 12:04:25.841604  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159111 (* 1 = 0.159111 loss)
I1006 12:04:25.841609  2824 sgd_solver.cpp:105] Iteration 26700, lr = 0.1
I1006 12:04:34.217576  2824 solver.cpp:218] Iteration 26800 (11.9389 iter/s, 8.37595s/100 iters), loss = 0.246068
I1006 12:04:34.217694  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246068 (* 1 = 0.246068 loss)
I1006 12:04:34.217711  2824 sgd_solver.cpp:105] Iteration 26800, lr = 0.1
I1006 12:04:42.594970  2824 solver.cpp:218] Iteration 26900 (11.9371 iter/s, 8.37725s/100 iters), loss = 0.118476
I1006 12:04:42.595010  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118476 (* 1 = 0.118476 loss)
I1006 12:04:42.595016  2824 sgd_solver.cpp:105] Iteration 26900, lr = 0.1
I1006 12:04:50.553318  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:04:50.890676  2824 solver.cpp:330] Iteration 27000, Testing net (#0)
I1006 12:04:52.848886  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:04:52.930656  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7761
I1006 12:04:52.930692  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.691745 (* 1 = 0.691745 loss)
I1006 12:04:53.014096  2824 solver.cpp:218] Iteration 27000 (9.5978 iter/s, 10.4191s/100 iters), loss = 0.188639
I1006 12:04:53.014118  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.188638 (* 1 = 0.188638 loss)
I1006 12:04:53.014124  2824 sgd_solver.cpp:105] Iteration 27000, lr = 0.1
I1006 12:05:01.402654  2824 solver.cpp:218] Iteration 27100 (11.9211 iter/s, 8.38851s/100 iters), loss = 0.234938
I1006 12:05:01.402695  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234937 (* 1 = 0.234937 loss)
I1006 12:05:01.402701  2824 sgd_solver.cpp:105] Iteration 27100, lr = 0.1
I1006 12:05:09.787036  2824 solver.cpp:218] Iteration 27200 (11.927 iter/s, 8.38432s/100 iters), loss = 0.135454
I1006 12:05:09.787181  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135454 (* 1 = 0.135454 loss)
I1006 12:05:09.787189  2824 sgd_solver.cpp:105] Iteration 27200, lr = 0.1
I1006 12:05:18.176488  2824 solver.cpp:218] Iteration 27300 (11.9199 iter/s, 8.3893s/100 iters), loss = 0.240305
I1006 12:05:18.176529  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240305 (* 1 = 0.240305 loss)
I1006 12:05:18.176535  2824 sgd_solver.cpp:105] Iteration 27300, lr = 0.1
I1006 12:05:26.560883  2824 solver.cpp:218] Iteration 27400 (11.927 iter/s, 8.38433s/100 iters), loss = 0.27666
I1006 12:05:26.560925  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27666 (* 1 = 0.27666 loss)
I1006 12:05:26.560930  2824 sgd_solver.cpp:105] Iteration 27400, lr = 0.1
I1006 12:05:34.525252  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:05:34.860846  2824 solver.cpp:330] Iteration 27500, Testing net (#0)
I1006 12:05:36.819224  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:05:36.900781  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7404
I1006 12:05:36.900816  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.852234 (* 1 = 0.852234 loss)
I1006 12:05:36.985419  2824 solver.cpp:218] Iteration 27500 (9.59282 iter/s, 10.4245s/100 iters), loss = 0.147053
I1006 12:05:36.985440  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.147053 (* 1 = 0.147053 loss)
I1006 12:05:36.985447  2824 sgd_solver.cpp:105] Iteration 27500, lr = 0.1
I1006 12:05:45.366699  2824 solver.cpp:218] Iteration 27600 (11.9314 iter/s, 8.38123s/100 iters), loss = 0.228778
I1006 12:05:45.366811  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.228778 (* 1 = 0.228778 loss)
I1006 12:05:45.366817  2824 sgd_solver.cpp:105] Iteration 27600, lr = 0.1
I1006 12:05:53.739770  2824 solver.cpp:218] Iteration 27700 (11.9432 iter/s, 8.37294s/100 iters), loss = 0.298344
I1006 12:05:53.739811  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.298344 (* 1 = 0.298344 loss)
I1006 12:05:53.739817  2824 sgd_solver.cpp:105] Iteration 27700, lr = 0.1
I1006 12:06:02.125823  2824 solver.cpp:218] Iteration 27800 (11.9247 iter/s, 8.38599s/100 iters), loss = 0.196058
I1006 12:06:02.125864  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196058 (* 1 = 0.196058 loss)
I1006 12:06:02.125869  2824 sgd_solver.cpp:105] Iteration 27800, lr = 0.1
I1006 12:06:10.504099  2824 solver.cpp:218] Iteration 27900 (11.9357 iter/s, 8.37821s/100 iters), loss = 0.206031
I1006 12:06:10.504140  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206031 (* 1 = 0.206031 loss)
I1006 12:06:10.504146  2824 sgd_solver.cpp:105] Iteration 27900, lr = 0.1
I1006 12:06:18.470999  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:06:18.806568  2824 solver.cpp:330] Iteration 28000, Testing net (#0)
I1006 12:06:20.763721  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:06:20.846370  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.65
I1006 12:06:20.846406  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.40578 (* 1 = 1.40578 loss)
I1006 12:06:20.929163  2824 solver.cpp:218] Iteration 28000 (9.59233 iter/s, 10.425s/100 iters), loss = 0.160175
I1006 12:06:20.929186  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160175 (* 1 = 0.160175 loss)
I1006 12:06:20.929193  2824 sgd_solver.cpp:105] Iteration 28000, lr = 0.1
I1006 12:06:29.305539  2824 solver.cpp:218] Iteration 28100 (11.9384 iter/s, 8.37633s/100 iters), loss = 0.192554
I1006 12:06:29.305570  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192554 (* 1 = 0.192554 loss)
I1006 12:06:29.305577  2824 sgd_solver.cpp:105] Iteration 28100, lr = 0.1
I1006 12:06:37.681872  2824 solver.cpp:218] Iteration 28200 (11.9385 iter/s, 8.37628s/100 iters), loss = 0.238534
I1006 12:06:37.681903  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238534 (* 1 = 0.238534 loss)
I1006 12:06:37.681910  2824 sgd_solver.cpp:105] Iteration 28200, lr = 0.1
I1006 12:06:46.061790  2824 solver.cpp:218] Iteration 28300 (11.9334 iter/s, 8.37986s/100 iters), loss = 0.229234
I1006 12:06:46.061820  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229233 (* 1 = 0.229233 loss)
I1006 12:06:46.061826  2824 sgd_solver.cpp:105] Iteration 28300, lr = 0.1
I1006 12:06:54.437436  2824 solver.cpp:218] Iteration 28400 (11.9395 iter/s, 8.37555s/100 iters), loss = 0.137304
I1006 12:06:54.437548  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.137304 (* 1 = 0.137304 loss)
I1006 12:06:54.437554  2824 sgd_solver.cpp:105] Iteration 28400, lr = 0.1
I1006 12:07:02.401160  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:07:02.736287  2824 solver.cpp:330] Iteration 28500, Testing net (#0)
I1006 12:07:04.692046  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:07:04.773939  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7554
I1006 12:07:04.773975  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.747208 (* 1 = 0.747208 loss)
I1006 12:07:04.857538  2824 solver.cpp:218] Iteration 28500 (9.59696 iter/s, 10.42s/100 iters), loss = 0.158037
I1006 12:07:04.857561  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158036 (* 1 = 0.158036 loss)
I1006 12:07:04.857568  2824 sgd_solver.cpp:105] Iteration 28500, lr = 0.1
I1006 12:07:13.232609  2824 solver.cpp:218] Iteration 28600 (11.9403 iter/s, 8.37502s/100 iters), loss = 0.169876
I1006 12:07:13.232640  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169875 (* 1 = 0.169875 loss)
I1006 12:07:13.232645  2824 sgd_solver.cpp:105] Iteration 28600, lr = 0.1
I1006 12:07:21.603973  2824 solver.cpp:218] Iteration 28700 (11.9456 iter/s, 8.37131s/100 iters), loss = 0.197626
I1006 12:07:21.604020  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197626 (* 1 = 0.197626 loss)
I1006 12:07:21.604028  2824 sgd_solver.cpp:105] Iteration 28700, lr = 0.1
I1006 12:07:29.973541  2824 solver.cpp:218] Iteration 28800 (11.9481 iter/s, 8.3695s/100 iters), loss = 0.224447
I1006 12:07:29.973666  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224446 (* 1 = 0.224446 loss)
I1006 12:07:29.973673  2824 sgd_solver.cpp:105] Iteration 28800, lr = 0.1
I1006 12:07:38.343550  2824 solver.cpp:218] Iteration 28900 (11.9476 iter/s, 8.36987s/100 iters), loss = 0.115038
I1006 12:07:38.343580  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.115037 (* 1 = 0.115037 loss)
I1006 12:07:38.343586  2824 sgd_solver.cpp:105] Iteration 28900, lr = 0.1
I1006 12:07:46.296758  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:07:46.631295  2824 solver.cpp:330] Iteration 29000, Testing net (#0)
I1006 12:07:48.587734  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:07:48.669484  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6317
I1006 12:07:48.669519  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.46281 (* 1 = 1.46281 loss)
I1006 12:07:48.753053  2824 solver.cpp:218] Iteration 29000 (9.60666 iter/s, 10.4094s/100 iters), loss = 0.214227
I1006 12:07:48.753077  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214226 (* 1 = 0.214226 loss)
I1006 12:07:48.753083  2824 sgd_solver.cpp:105] Iteration 29000, lr = 0.1
I1006 12:07:57.130223  2824 solver.cpp:218] Iteration 29100 (11.9373 iter/s, 8.37712s/100 iters), loss = 0.219891
I1006 12:07:57.130254  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219891 (* 1 = 0.219891 loss)
I1006 12:07:57.130259  2824 sgd_solver.cpp:105] Iteration 29100, lr = 0.1
I1006 12:08:05.493738  2824 solver.cpp:218] Iteration 29200 (11.9568 iter/s, 8.36346s/100 iters), loss = 0.144438
I1006 12:08:05.493878  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.144438 (* 1 = 0.144438 loss)
I1006 12:08:05.493896  2824 sgd_solver.cpp:105] Iteration 29200, lr = 0.1
I1006 12:08:13.864964  2824 solver.cpp:218] Iteration 29300 (11.9459 iter/s, 8.37106s/100 iters), loss = 0.222226
I1006 12:08:13.864994  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222226 (* 1 = 0.222226 loss)
I1006 12:08:13.865000  2824 sgd_solver.cpp:105] Iteration 29300, lr = 0.1
I1006 12:08:22.238142  2824 solver.cpp:218] Iteration 29400 (11.943 iter/s, 8.37312s/100 iters), loss = 0.145299
I1006 12:08:22.238173  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145299 (* 1 = 0.145299 loss)
I1006 12:08:22.238178  2824 sgd_solver.cpp:105] Iteration 29400, lr = 0.1
I1006 12:08:30.190340  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:08:30.525099  2824 solver.cpp:330] Iteration 29500, Testing net (#0)
I1006 12:08:32.482997  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:08:32.564138  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3962
I1006 12:08:32.564163  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.60117 (* 1 = 3.60117 loss)
I1006 12:08:32.647474  2824 solver.cpp:218] Iteration 29500 (9.60682 iter/s, 10.4093s/100 iters), loss = 0.15507
I1006 12:08:32.647497  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15507 (* 1 = 0.15507 loss)
I1006 12:08:32.647513  2824 sgd_solver.cpp:105] Iteration 29500, lr = 0.1
I1006 12:08:41.020583  2824 solver.cpp:218] Iteration 29600 (11.9431 iter/s, 8.37306s/100 iters), loss = 0.148771
I1006 12:08:41.020696  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14877 (* 1 = 0.14877 loss)
I1006 12:08:41.020714  2824 sgd_solver.cpp:105] Iteration 29600, lr = 0.1
I1006 12:08:49.401720  2824 solver.cpp:218] Iteration 29700 (11.9318 iter/s, 8.381s/100 iters), loss = 0.223461
I1006 12:08:49.401751  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223461 (* 1 = 0.223461 loss)
I1006 12:08:49.401756  2824 sgd_solver.cpp:105] Iteration 29700, lr = 0.1
I1006 12:08:57.779093  2824 solver.cpp:218] Iteration 29800 (11.937 iter/s, 8.37732s/100 iters), loss = 0.179579
I1006 12:08:57.779130  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179578 (* 1 = 0.179578 loss)
I1006 12:08:57.779137  2824 sgd_solver.cpp:105] Iteration 29800, lr = 0.1
I1006 12:09:06.156136  2824 solver.cpp:218] Iteration 29900 (11.9375 iter/s, 8.37698s/100 iters), loss = 0.149217
I1006 12:09:06.156164  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149217 (* 1 = 0.149217 loss)
I1006 12:09:06.156172  2824 sgd_solver.cpp:105] Iteration 29900, lr = 0.1
I1006 12:09:14.116358  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:09:14.451668  2824 solver.cpp:330] Iteration 30000, Testing net (#0)
I1006 12:09:16.411257  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:09:16.493317  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6744
I1006 12:09:16.493353  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.33706 (* 1 = 1.33706 loss)
I1006 12:09:16.577030  2824 solver.cpp:218] Iteration 30000 (9.59616 iter/s, 10.4208s/100 iters), loss = 0.200059
I1006 12:09:16.577054  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200058 (* 1 = 0.200058 loss)
I1006 12:09:16.577059  2824 sgd_solver.cpp:105] Iteration 30000, lr = 0.1
I1006 12:09:24.955374  2824 solver.cpp:218] Iteration 30100 (11.9356 iter/s, 8.37829s/100 iters), loss = 0.194588
I1006 12:09:24.955413  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194588 (* 1 = 0.194588 loss)
I1006 12:09:24.955420  2824 sgd_solver.cpp:105] Iteration 30100, lr = 0.1
I1006 12:09:33.337384  2824 solver.cpp:218] Iteration 30200 (11.9304 iter/s, 8.38195s/100 iters), loss = 0.238605
I1006 12:09:33.337425  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238605 (* 1 = 0.238605 loss)
I1006 12:09:33.337431  2824 sgd_solver.cpp:105] Iteration 30200, lr = 0.1
I1006 12:09:41.712473  2824 solver.cpp:218] Iteration 30300 (11.9403 iter/s, 8.37502s/100 iters), loss = 0.219773
I1006 12:09:41.712513  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219773 (* 1 = 0.219773 loss)
I1006 12:09:41.712519  2824 sgd_solver.cpp:105] Iteration 30300, lr = 0.1
I1006 12:09:50.090142  2824 solver.cpp:218] Iteration 30400 (11.9366 iter/s, 8.3776s/100 iters), loss = 0.112391
I1006 12:09:50.090246  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.11239 (* 1 = 0.11239 loss)
I1006 12:09:50.090263  2824 sgd_solver.cpp:105] Iteration 30400, lr = 0.1
I1006 12:09:58.054913  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:09:58.388878  2824 solver.cpp:330] Iteration 30500, Testing net (#0)
I1006 12:10:00.348081  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:10:00.429872  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5574
I1006 12:10:00.429908  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.0998 (* 1 = 2.0998 loss)
I1006 12:10:00.513458  2824 solver.cpp:218] Iteration 30500 (9.594 iter/s, 10.4232s/100 iters), loss = 0.145359
I1006 12:10:00.513483  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145359 (* 1 = 0.145359 loss)
I1006 12:10:00.513489  2824 sgd_solver.cpp:105] Iteration 30500, lr = 0.1
I1006 12:10:08.898929  2824 solver.cpp:218] Iteration 30600 (11.9255 iter/s, 8.38542s/100 iters), loss = 0.264734
I1006 12:10:08.898969  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264734 (* 1 = 0.264734 loss)
I1006 12:10:08.898975  2824 sgd_solver.cpp:105] Iteration 30600, lr = 0.1
I1006 12:10:17.283654  2824 solver.cpp:218] Iteration 30700 (11.9265 iter/s, 8.38466s/100 iters), loss = 0.183331
I1006 12:10:17.283695  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18333 (* 1 = 0.18333 loss)
I1006 12:10:17.283700  2824 sgd_solver.cpp:105] Iteration 30700, lr = 0.1
I1006 12:10:25.663128  2824 solver.cpp:218] Iteration 30800 (11.934 iter/s, 8.37941s/100 iters), loss = 0.151415
I1006 12:10:25.663223  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151415 (* 1 = 0.151415 loss)
I1006 12:10:25.663238  2824 sgd_solver.cpp:105] Iteration 30800, lr = 0.1
I1006 12:10:34.042637  2824 solver.cpp:218] Iteration 30900 (11.934 iter/s, 8.37939s/100 iters), loss = 0.108349
I1006 12:10:34.042675  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108349 (* 1 = 0.108349 loss)
I1006 12:10:34.042682  2824 sgd_solver.cpp:105] Iteration 30900, lr = 0.1
I1006 12:10:42.006269  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:10:42.341152  2824 solver.cpp:330] Iteration 31000, Testing net (#0)
I1006 12:10:44.298279  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:10:44.379884  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7618
I1006 12:10:44.379920  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.723301 (* 1 = 0.723301 loss)
I1006 12:10:44.463210  2824 solver.cpp:218] Iteration 31000 (9.59646 iter/s, 10.4205s/100 iters), loss = 0.199447
I1006 12:10:44.463234  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199447 (* 1 = 0.199447 loss)
I1006 12:10:44.463240  2824 sgd_solver.cpp:105] Iteration 31000, lr = 0.1
I1006 12:10:52.839247  2824 solver.cpp:218] Iteration 31100 (11.9389 iter/s, 8.37599s/100 iters), loss = 0.156751
I1006 12:10:52.839288  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156751 (* 1 = 0.156751 loss)
I1006 12:10:52.839294  2824 sgd_solver.cpp:105] Iteration 31100, lr = 0.1
I1006 12:11:01.215844  2824 solver.cpp:218] Iteration 31200 (11.9381 iter/s, 8.37653s/100 iters), loss = 0.317627
I1006 12:11:01.215982  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.317626 (* 1 = 0.317626 loss)
I1006 12:11:01.216001  2824 sgd_solver.cpp:105] Iteration 31200, lr = 0.1
I1006 12:11:09.593634  2824 solver.cpp:218] Iteration 31300 (11.9366 iter/s, 8.37763s/100 iters), loss = 0.189121
I1006 12:11:09.593675  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18912 (* 1 = 0.18912 loss)
I1006 12:11:09.593682  2824 sgd_solver.cpp:105] Iteration 31300, lr = 0.1
I1006 12:11:17.976707  2824 solver.cpp:218] Iteration 31400 (11.9289 iter/s, 8.383s/100 iters), loss = 0.133037
I1006 12:11:17.976745  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133036 (* 1 = 0.133036 loss)
I1006 12:11:17.976752  2824 sgd_solver.cpp:105] Iteration 31400, lr = 0.1
I1006 12:11:25.938609  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:11:26.274197  2824 solver.cpp:330] Iteration 31500, Testing net (#0)
I1006 12:11:28.233206  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:11:28.314864  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6772
I1006 12:11:28.314899  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.22022 (* 1 = 1.22022 loss)
I1006 12:11:28.398820  2824 solver.cpp:218] Iteration 31500 (9.59505 iter/s, 10.422s/100 iters), loss = 0.159019
I1006 12:11:28.398845  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159018 (* 1 = 0.159018 loss)
I1006 12:11:28.398851  2824 sgd_solver.cpp:105] Iteration 31500, lr = 0.1
I1006 12:11:36.780505  2824 solver.cpp:218] Iteration 31600 (11.9309 iter/s, 8.38163s/100 iters), loss = 0.266838
I1006 12:11:36.780592  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266837 (* 1 = 0.266837 loss)
I1006 12:11:36.780609  2824 sgd_solver.cpp:105] Iteration 31600, lr = 0.1
I1006 12:11:45.166049  2824 solver.cpp:218] Iteration 31700 (11.9254 iter/s, 8.38544s/100 iters), loss = 0.175002
I1006 12:11:45.166079  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175001 (* 1 = 0.175001 loss)
I1006 12:11:45.166085  2824 sgd_solver.cpp:105] Iteration 31700, lr = 0.1
I1006 12:11:53.547523  2824 solver.cpp:218] Iteration 31800 (11.9312 iter/s, 8.38141s/100 iters), loss = 0.219206
I1006 12:11:53.547570  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219205 (* 1 = 0.219205 loss)
I1006 12:11:53.547579  2824 sgd_solver.cpp:105] Iteration 31800, lr = 0.1
I1006 12:12:01.930491  2824 solver.cpp:218] Iteration 31900 (11.9291 iter/s, 8.3829s/100 iters), loss = 0.241338
I1006 12:12:01.930531  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241337 (* 1 = 0.241337 loss)
I1006 12:12:01.930537  2824 sgd_solver.cpp:105] Iteration 31900, lr = 0.1
I1006 12:12:09.897658  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:12:10.233840  2824 solver.cpp:330] Iteration 32000, Testing net (#0)
I1006 12:12:12.192956  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:12:12.275003  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6618
I1006 12:12:12.275038  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.22907 (* 1 = 1.22907 loss)
I1006 12:12:12.359051  2824 solver.cpp:218] Iteration 32000 (9.58912 iter/s, 10.4285s/100 iters), loss = 0.175476
I1006 12:12:12.359076  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175476 (* 1 = 0.175476 loss)
I1006 12:12:12.359082  2824 sgd_solver.cpp:105] Iteration 32000, lr = 0.1
I1006 12:12:20.731490  2824 solver.cpp:218] Iteration 32100 (11.944 iter/s, 8.37239s/100 iters), loss = 0.215014
I1006 12:12:20.731531  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215013 (* 1 = 0.215013 loss)
I1006 12:12:20.731537  2824 sgd_solver.cpp:105] Iteration 32100, lr = 0.1
I1006 12:12:29.109622  2824 solver.cpp:218] Iteration 32200 (11.9359 iter/s, 8.37806s/100 iters), loss = 0.207508
I1006 12:12:29.109663  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207508 (* 1 = 0.207508 loss)
I1006 12:12:29.109669  2824 sgd_solver.cpp:105] Iteration 32200, lr = 0.1
I1006 12:12:37.488894  2824 solver.cpp:218] Iteration 32300 (11.9343 iter/s, 8.3792s/100 iters), loss = 0.20009
I1006 12:12:37.488942  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20009 (* 1 = 0.20009 loss)
I1006 12:12:37.488950  2824 sgd_solver.cpp:105] Iteration 32300, lr = 0.1
I1006 12:12:45.864212  2824 solver.cpp:218] Iteration 32400 (11.9399 iter/s, 8.37524s/100 iters), loss = 0.229891
I1006 12:12:45.864336  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229891 (* 1 = 0.229891 loss)
I1006 12:12:45.864354  2824 sgd_solver.cpp:105] Iteration 32400, lr = 0.1
I1006 12:12:53.818302  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:12:54.152662  2824 solver.cpp:330] Iteration 32500, Testing net (#0)
I1006 12:12:56.111155  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:12:56.193012  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6373
I1006 12:12:56.193048  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.35896 (* 1 = 1.35896 loss)
I1006 12:12:56.276916  2824 solver.cpp:218] Iteration 32500 (9.6038 iter/s, 10.4125s/100 iters), loss = 0.18554
I1006 12:12:56.276942  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18554 (* 1 = 0.18554 loss)
I1006 12:12:56.276948  2824 sgd_solver.cpp:105] Iteration 32500, lr = 0.1
I1006 12:13:04.654778  2824 solver.cpp:218] Iteration 32600 (11.9363 iter/s, 8.37781s/100 iters), loss = 0.183326
I1006 12:13:04.654819  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183325 (* 1 = 0.183325 loss)
I1006 12:13:04.654825  2824 sgd_solver.cpp:105] Iteration 32600, lr = 0.1
I1006 12:13:13.035660  2824 solver.cpp:218] Iteration 32700 (11.932 iter/s, 8.38081s/100 iters), loss = 0.22132
I1006 12:13:13.035701  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221319 (* 1 = 0.221319 loss)
I1006 12:13:13.035707  2824 sgd_solver.cpp:105] Iteration 32700, lr = 0.1
I1006 12:13:21.403671  2824 solver.cpp:218] Iteration 32800 (11.9504 iter/s, 8.36794s/100 iters), loss = 0.143179
I1006 12:13:21.403759  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143178 (* 1 = 0.143178 loss)
I1006 12:13:21.403776  2824 sgd_solver.cpp:105] Iteration 32800, lr = 0.1
I1006 12:13:29.774529  2824 solver.cpp:218] Iteration 32900 (11.9464 iter/s, 8.37074s/100 iters), loss = 0.0717109
I1006 12:13:29.774569  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0717104 (* 1 = 0.0717104 loss)
I1006 12:13:29.774575  2824 sgd_solver.cpp:105] Iteration 32900, lr = 0.1
I1006 12:13:37.732400  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:13:38.067041  2824 solver.cpp:330] Iteration 33000, Testing net (#0)
I1006 12:13:40.024675  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:13:40.106616  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7469
I1006 12:13:40.106650  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.863775 (* 1 = 0.863775 loss)
I1006 12:13:40.190070  2824 solver.cpp:218] Iteration 33000 (9.6011 iter/s, 10.4155s/100 iters), loss = 0.184881
I1006 12:13:40.190094  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184881 (* 1 = 0.184881 loss)
I1006 12:13:40.190101  2824 sgd_solver.cpp:105] Iteration 33000, lr = 0.1
I1006 12:13:48.573886  2824 solver.cpp:218] Iteration 33100 (11.9278 iter/s, 8.38376s/100 iters), loss = 0.278704
I1006 12:13:48.573927  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278704 (* 1 = 0.278704 loss)
I1006 12:13:48.573932  2824 sgd_solver.cpp:105] Iteration 33100, lr = 0.1
I1006 12:13:56.951076  2824 solver.cpp:218] Iteration 33200 (11.9373 iter/s, 8.37712s/100 iters), loss = 0.17612
I1006 12:13:56.951210  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17612 (* 1 = 0.17612 loss)
I1006 12:13:56.951226  2824 sgd_solver.cpp:105] Iteration 33200, lr = 0.1
I1006 12:14:05.326786  2824 solver.cpp:218] Iteration 33300 (11.9395 iter/s, 8.37555s/100 iters), loss = 0.203746
I1006 12:14:05.326827  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203746 (* 1 = 0.203746 loss)
I1006 12:14:05.326831  2824 sgd_solver.cpp:105] Iteration 33300, lr = 0.1
I1006 12:14:13.708539  2824 solver.cpp:218] Iteration 33400 (11.9308 iter/s, 8.38168s/100 iters), loss = 0.25399
I1006 12:14:13.708587  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25399 (* 1 = 0.25399 loss)
I1006 12:14:13.708595  2824 sgd_solver.cpp:105] Iteration 33400, lr = 0.1
I1006 12:14:21.669991  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:14:22.005503  2824 solver.cpp:330] Iteration 33500, Testing net (#0)
I1006 12:14:23.963714  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:14:24.045045  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6609
I1006 12:14:24.045070  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.41046 (* 1 = 1.41046 loss)
I1006 12:14:24.128979  2824 solver.cpp:218] Iteration 33500 (9.5966 iter/s, 10.4204s/100 iters), loss = 0.151096
I1006 12:14:24.129004  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151096 (* 1 = 0.151096 loss)
I1006 12:14:24.129010  2824 sgd_solver.cpp:105] Iteration 33500, lr = 0.1
I1006 12:14:32.515949  2824 solver.cpp:218] Iteration 33600 (11.9233 iter/s, 8.38692s/100 iters), loss = 0.212239
I1006 12:14:32.516072  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212239 (* 1 = 0.212239 loss)
I1006 12:14:32.516089  2824 sgd_solver.cpp:105] Iteration 33600, lr = 0.1
I1006 12:14:40.897526  2824 solver.cpp:218] Iteration 33700 (11.9311 iter/s, 8.38143s/100 iters), loss = 0.144746
I1006 12:14:40.897565  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.144746 (* 1 = 0.144746 loss)
I1006 12:14:40.897570  2824 sgd_solver.cpp:105] Iteration 33700, lr = 0.1
I1006 12:14:49.281399  2824 solver.cpp:218] Iteration 33800 (11.9278 iter/s, 8.38381s/100 iters), loss = 0.120183
I1006 12:14:49.281430  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120182 (* 1 = 0.120182 loss)
I1006 12:14:49.281445  2824 sgd_solver.cpp:105] Iteration 33800, lr = 0.1
I1006 12:14:57.671437  2824 solver.cpp:218] Iteration 33900 (11.919 iter/s, 8.38998s/100 iters), loss = 0.0850288
I1006 12:14:57.671468  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0850285 (* 1 = 0.0850285 loss)
I1006 12:14:57.671484  2824 sgd_solver.cpp:105] Iteration 33900, lr = 0.1
I1006 12:15:05.641685  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:15:05.977313  2824 solver.cpp:330] Iteration 34000, Testing net (#0)
I1006 12:15:07.936666  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:15:08.018504  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7151
I1006 12:15:08.018540  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.05048 (* 1 = 1.05048 loss)
I1006 12:15:08.102241  2824 solver.cpp:218] Iteration 34000 (9.58705 iter/s, 10.4307s/100 iters), loss = 0.126777
I1006 12:15:08.102267  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126777 (* 1 = 0.126777 loss)
I1006 12:15:08.102272  2824 sgd_solver.cpp:105] Iteration 34000, lr = 0.1
I1006 12:15:16.490597  2824 solver.cpp:218] Iteration 34100 (11.9214 iter/s, 8.3883s/100 iters), loss = 0.153266
I1006 12:15:16.490636  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.153265 (* 1 = 0.153265 loss)
I1006 12:15:16.490643  2824 sgd_solver.cpp:105] Iteration 34100, lr = 0.1
I1006 12:15:24.880656  2824 solver.cpp:218] Iteration 34200 (11.919 iter/s, 8.38999s/100 iters), loss = 0.22697
I1006 12:15:24.880697  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22697 (* 1 = 0.22697 loss)
I1006 12:15:24.880702  2824 sgd_solver.cpp:105] Iteration 34200, lr = 0.1
I1006 12:15:33.266520  2824 solver.cpp:218] Iteration 34300 (11.9249 iter/s, 8.3858s/100 iters), loss = 0.178213
I1006 12:15:33.266551  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178213 (* 1 = 0.178213 loss)
I1006 12:15:33.266557  2824 sgd_solver.cpp:105] Iteration 34300, lr = 0.1
I1006 12:15:41.651760  2824 solver.cpp:218] Iteration 34400 (11.9258 iter/s, 8.38518s/100 iters), loss = 0.119204
I1006 12:15:41.651917  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.119204 (* 1 = 0.119204 loss)
I1006 12:15:41.651924  2824 sgd_solver.cpp:105] Iteration 34400, lr = 0.1
I1006 12:15:49.623867  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:15:49.959378  2824 solver.cpp:330] Iteration 34500, Testing net (#0)
I1006 12:15:51.916018  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:15:51.997263  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6765
I1006 12:15:51.997298  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.11311 (* 1 = 1.11311 loss)
I1006 12:15:52.081107  2824 solver.cpp:218] Iteration 34500 (9.58849 iter/s, 10.4292s/100 iters), loss = 0.133648
I1006 12:15:52.081132  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133648 (* 1 = 0.133648 loss)
I1006 12:15:52.081140  2824 sgd_solver.cpp:105] Iteration 34500, lr = 0.1
I1006 12:16:00.459758  2824 solver.cpp:218] Iteration 34600 (11.9352 iter/s, 8.3786s/100 iters), loss = 0.322532
I1006 12:16:00.459797  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.322532 (* 1 = 0.322532 loss)
I1006 12:16:00.459803  2824 sgd_solver.cpp:105] Iteration 34600, lr = 0.1
I1006 12:16:08.836773  2824 solver.cpp:218] Iteration 34700 (11.9375 iter/s, 8.37695s/100 iters), loss = 0.154927
I1006 12:16:08.836803  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154927 (* 1 = 0.154927 loss)
I1006 12:16:08.836809  2824 sgd_solver.cpp:105] Iteration 34700, lr = 0.1
I1006 12:16:17.224151  2824 solver.cpp:218] Iteration 34800 (11.9228 iter/s, 8.38732s/100 iters), loss = 0.158395
I1006 12:16:17.224287  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158395 (* 1 = 0.158395 loss)
I1006 12:16:17.224293  2824 sgd_solver.cpp:105] Iteration 34800, lr = 0.1
I1006 12:16:25.596946  2824 solver.cpp:218] Iteration 34900 (11.9437 iter/s, 8.37264s/100 iters), loss = 0.183363
I1006 12:16:25.596984  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183363 (* 1 = 0.183363 loss)
I1006 12:16:25.596990  2824 sgd_solver.cpp:105] Iteration 34900, lr = 0.1
I1006 12:16:33.561695  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:16:33.896267  2824 solver.cpp:330] Iteration 35000, Testing net (#0)
I1006 12:16:35.852222  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:16:35.934023  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7254
I1006 12:16:35.934059  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.854509 (* 1 = 0.854509 loss)
I1006 12:16:36.017092  2824 solver.cpp:218] Iteration 35000 (9.59686 iter/s, 10.4201s/100 iters), loss = 0.265009
I1006 12:16:36.017115  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265009 (* 1 = 0.265009 loss)
I1006 12:16:36.017122  2824 sgd_solver.cpp:105] Iteration 35000, lr = 0.1
I1006 12:16:44.402946  2824 solver.cpp:218] Iteration 35100 (11.9249 iter/s, 8.3858s/100 iters), loss = 0.270382
I1006 12:16:44.402987  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270382 (* 1 = 0.270382 loss)
I1006 12:16:44.402993  2824 sgd_solver.cpp:105] Iteration 35100, lr = 0.1
I1006 12:16:52.785104  2824 solver.cpp:218] Iteration 35200 (11.9302 iter/s, 8.38209s/100 iters), loss = 0.219937
I1006 12:16:52.785181  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219937 (* 1 = 0.219937 loss)
I1006 12:16:52.785199  2824 sgd_solver.cpp:105] Iteration 35200, lr = 0.1
I1006 12:17:01.169019  2824 solver.cpp:218] Iteration 35300 (11.9278 iter/s, 8.38381s/100 iters), loss = 0.256098
I1006 12:17:01.169060  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256098 (* 1 = 0.256098 loss)
I1006 12:17:01.169066  2824 sgd_solver.cpp:105] Iteration 35300, lr = 0.1
I1006 12:17:09.554071  2824 solver.cpp:218] Iteration 35400 (11.9261 iter/s, 8.38498s/100 iters), loss = 0.16608
I1006 12:17:09.554112  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166079 (* 1 = 0.166079 loss)
I1006 12:17:09.554118  2824 sgd_solver.cpp:105] Iteration 35400, lr = 0.1
I1006 12:17:17.524368  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:17:17.860213  2824 solver.cpp:330] Iteration 35500, Testing net (#0)
I1006 12:17:19.815666  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:17:19.897454  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.738
I1006 12:17:19.897490  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.820051 (* 1 = 0.820051 loss)
I1006 12:17:19.980512  2824 solver.cpp:218] Iteration 35500 (9.59107 iter/s, 10.4264s/100 iters), loss = 0.217227
I1006 12:17:19.980536  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217227 (* 1 = 0.217227 loss)
I1006 12:17:19.980542  2824 sgd_solver.cpp:105] Iteration 35500, lr = 0.1
I1006 12:17:28.357507  2824 solver.cpp:218] Iteration 35600 (11.9375 iter/s, 8.37694s/100 iters), loss = 0.153599
I1006 12:17:28.357666  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.153598 (* 1 = 0.153598 loss)
I1006 12:17:28.357673  2824 sgd_solver.cpp:105] Iteration 35600, lr = 0.1
I1006 12:17:36.728513  2824 solver.cpp:218] Iteration 35700 (11.9463 iter/s, 8.37082s/100 iters), loss = 0.201326
I1006 12:17:36.728554  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201326 (* 1 = 0.201326 loss)
I1006 12:17:36.728559  2824 sgd_solver.cpp:105] Iteration 35700, lr = 0.1
I1006 12:17:45.104720  2824 solver.cpp:218] Iteration 35800 (11.9387 iter/s, 8.37614s/100 iters), loss = 0.160746
I1006 12:17:45.104751  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160746 (* 1 = 0.160746 loss)
I1006 12:17:45.104756  2824 sgd_solver.cpp:105] Iteration 35800, lr = 0.1
I1006 12:17:53.476565  2824 solver.cpp:218] Iteration 35900 (11.9449 iter/s, 8.37179s/100 iters), loss = 0.202376
I1006 12:17:53.476605  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202376 (* 1 = 0.202376 loss)
I1006 12:17:53.476611  2824 sgd_solver.cpp:105] Iteration 35900, lr = 0.1
I1006 12:18:01.441927  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:18:01.777783  2824 solver.cpp:330] Iteration 36000, Testing net (#0)
I1006 12:18:03.734562  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:18:03.816292  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.578
I1006 12:18:03.816328  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.95337 (* 1 = 1.95337 loss)
I1006 12:18:03.899889  2824 solver.cpp:218] Iteration 36000 (9.59393 iter/s, 10.4233s/100 iters), loss = 0.241499
I1006 12:18:03.899914  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241499 (* 1 = 0.241499 loss)
I1006 12:18:03.899920  2824 sgd_solver.cpp:105] Iteration 36000, lr = 0.1
I1006 12:18:12.282625  2824 solver.cpp:218] Iteration 36100 (11.9294 iter/s, 8.38268s/100 iters), loss = 0.14416
I1006 12:18:12.282665  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.144159 (* 1 = 0.144159 loss)
I1006 12:18:12.282670  2824 sgd_solver.cpp:105] Iteration 36100, lr = 0.1
I1006 12:18:20.669090  2824 solver.cpp:218] Iteration 36200 (11.9241 iter/s, 8.3864s/100 iters), loss = 0.242084
I1006 12:18:20.669131  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242083 (* 1 = 0.242083 loss)
I1006 12:18:20.669137  2824 sgd_solver.cpp:105] Iteration 36200, lr = 0.1
I1006 12:18:29.048538  2824 solver.cpp:218] Iteration 36300 (11.9341 iter/s, 8.37938s/100 iters), loss = 0.208376
I1006 12:18:29.048579  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208375 (* 1 = 0.208375 loss)
I1006 12:18:29.048585  2824 sgd_solver.cpp:105] Iteration 36300, lr = 0.1
I1006 12:18:37.431251  2824 solver.cpp:218] Iteration 36400 (11.9294 iter/s, 8.38264s/100 iters), loss = 0.134984
I1006 12:18:37.431403  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134984 (* 1 = 0.134984 loss)
I1006 12:18:37.431411  2824 sgd_solver.cpp:105] Iteration 36400, lr = 0.1
I1006 12:18:45.399140  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:18:45.734309  2824 solver.cpp:330] Iteration 36500, Testing net (#0)
I1006 12:18:47.693560  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:18:47.775409  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7083
I1006 12:18:47.775445  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.04645 (* 1 = 1.04645 loss)
I1006 12:18:47.859411  2824 solver.cpp:218] Iteration 36500 (9.58959 iter/s, 10.428s/100 iters), loss = 0.126808
I1006 12:18:47.859436  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126808 (* 1 = 0.126808 loss)
I1006 12:18:47.859441  2824 sgd_solver.cpp:105] Iteration 36500, lr = 0.1
I1006 12:18:56.240793  2824 solver.cpp:218] Iteration 36600 (11.9313 iter/s, 8.38133s/100 iters), loss = 0.203254
I1006 12:18:56.240823  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203254 (* 1 = 0.203254 loss)
I1006 12:18:56.240829  2824 sgd_solver.cpp:105] Iteration 36600, lr = 0.1
I1006 12:19:04.623666  2824 solver.cpp:218] Iteration 36700 (11.9292 iter/s, 8.38281s/100 iters), loss = 0.221552
I1006 12:19:04.623706  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221552 (* 1 = 0.221552 loss)
I1006 12:19:04.623713  2824 sgd_solver.cpp:105] Iteration 36700, lr = 0.1
I1006 12:19:13.011054  2824 solver.cpp:218] Iteration 36800 (11.9228 iter/s, 8.38732s/100 iters), loss = 0.13964
I1006 12:19:13.011150  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.13964 (* 1 = 0.13964 loss)
I1006 12:19:13.011157  2824 sgd_solver.cpp:105] Iteration 36800, lr = 0.1
I1006 12:19:21.392741  2824 solver.cpp:218] Iteration 36900 (11.9309 iter/s, 8.38157s/100 iters), loss = 0.113418
I1006 12:19:21.392782  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113418 (* 1 = 0.113418 loss)
I1006 12:19:21.392787  2824 sgd_solver.cpp:105] Iteration 36900, lr = 0.1
I1006 12:19:29.367748  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:19:29.702669  2824 solver.cpp:330] Iteration 37000, Testing net (#0)
I1006 12:19:31.661653  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:19:31.743322  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6188
I1006 12:19:31.743356  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.5211 (* 1 = 1.5211 loss)
I1006 12:19:31.827112  2824 solver.cpp:218] Iteration 37000 (9.58378 iter/s, 10.4343s/100 iters), loss = 0.201994
I1006 12:19:31.827136  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201994 (* 1 = 0.201994 loss)
I1006 12:19:31.827142  2824 sgd_solver.cpp:105] Iteration 37000, lr = 0.1
I1006 12:19:40.205641  2824 solver.cpp:218] Iteration 37100 (11.9353 iter/s, 8.37848s/100 iters), loss = 0.157139
I1006 12:19:40.205669  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.157138 (* 1 = 0.157138 loss)
I1006 12:19:40.205675  2824 sgd_solver.cpp:105] Iteration 37100, lr = 0.1
I1006 12:19:48.584723  2824 solver.cpp:218] Iteration 37200 (11.9346 iter/s, 8.37903s/100 iters), loss = 0.294049
I1006 12:19:48.584877  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.294049 (* 1 = 0.294049 loss)
I1006 12:19:48.584884  2824 sgd_solver.cpp:105] Iteration 37200, lr = 0.1
I1006 12:19:56.960440  2824 solver.cpp:218] Iteration 37300 (11.9395 iter/s, 8.37555s/100 iters), loss = 0.148684
I1006 12:19:56.960481  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148683 (* 1 = 0.148683 loss)
I1006 12:19:56.960487  2824 sgd_solver.cpp:105] Iteration 37300, lr = 0.1
I1006 12:20:05.345372  2824 solver.cpp:218] Iteration 37400 (11.9263 iter/s, 8.38486s/100 iters), loss = 0.102933
I1006 12:20:05.345413  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.102933 (* 1 = 0.102933 loss)
I1006 12:20:05.345419  2824 sgd_solver.cpp:105] Iteration 37400, lr = 0.1
I1006 12:20:13.314944  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:20:13.650041  2824 solver.cpp:330] Iteration 37500, Testing net (#0)
I1006 12:20:15.608585  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:20:15.690285  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6948
I1006 12:20:15.690320  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.07071 (* 1 = 1.07071 loss)
I1006 12:20:15.774060  2824 solver.cpp:218] Iteration 37500 (9.589 iter/s, 10.4286s/100 iters), loss = 0.163958
I1006 12:20:15.774083  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163957 (* 1 = 0.163957 loss)
I1006 12:20:15.774091  2824 sgd_solver.cpp:105] Iteration 37500, lr = 0.1
I1006 12:20:24.150372  2824 solver.cpp:218] Iteration 37600 (11.9385 iter/s, 8.37626s/100 iters), loss = 0.37605
I1006 12:20:24.150490  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.37605 (* 1 = 0.37605 loss)
I1006 12:20:24.150498  2824 sgd_solver.cpp:105] Iteration 37600, lr = 0.1
I1006 12:20:32.528970  2824 solver.cpp:218] Iteration 37700 (11.9354 iter/s, 8.37846s/100 iters), loss = 0.22752
I1006 12:20:32.529011  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22752 (* 1 = 0.22752 loss)
I1006 12:20:32.529016  2824 sgd_solver.cpp:105] Iteration 37700, lr = 0.1
I1006 12:20:40.900169  2824 solver.cpp:218] Iteration 37800 (11.9458 iter/s, 8.37113s/100 iters), loss = 0.172843
I1006 12:20:40.900199  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.172842 (* 1 = 0.172842 loss)
I1006 12:20:40.900204  2824 sgd_solver.cpp:105] Iteration 37800, lr = 0.1
I1006 12:20:49.279161  2824 solver.cpp:218] Iteration 37900 (11.9347 iter/s, 8.37894s/100 iters), loss = 0.0926458
I1006 12:20:49.279204  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0926454 (* 1 = 0.0926454 loss)
I1006 12:20:49.279211  2824 sgd_solver.cpp:105] Iteration 37900, lr = 0.1
I1006 12:20:57.238214  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:20:57.575009  2824 solver.cpp:330] Iteration 38000, Testing net (#0)
I1006 12:20:59.531790  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:20:59.613696  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7733
I1006 12:20:59.613730  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.774087 (* 1 = 0.774087 loss)
I1006 12:20:59.697620  2824 solver.cpp:218] Iteration 38000 (9.59842 iter/s, 10.4184s/100 iters), loss = 0.0994279
I1006 12:20:59.697644  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0994276 (* 1 = 0.0994276 loss)
I1006 12:20:59.697651  2824 sgd_solver.cpp:105] Iteration 38000, lr = 0.1
I1006 12:21:08.079638  2824 solver.cpp:218] Iteration 38100 (11.9304 iter/s, 8.38196s/100 iters), loss = 0.255452
I1006 12:21:08.079668  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255452 (* 1 = 0.255452 loss)
I1006 12:21:08.079674  2824 sgd_solver.cpp:105] Iteration 38100, lr = 0.1
I1006 12:21:16.463666  2824 solver.cpp:218] Iteration 38200 (11.9275 iter/s, 8.38397s/100 iters), loss = 0.158095
I1006 12:21:16.463706  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158094 (* 1 = 0.158094 loss)
I1006 12:21:16.463711  2824 sgd_solver.cpp:105] Iteration 38200, lr = 0.1
I1006 12:21:24.843413  2824 solver.cpp:218] Iteration 38300 (11.9336 iter/s, 8.37968s/100 iters), loss = 0.268409
I1006 12:21:24.843453  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268408 (* 1 = 0.268408 loss)
I1006 12:21:24.843459  2824 sgd_solver.cpp:105] Iteration 38300, lr = 0.1
I1006 12:21:33.219991  2824 solver.cpp:218] Iteration 38400 (11.9381 iter/s, 8.37651s/100 iters), loss = 0.118868
I1006 12:21:33.220137  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118868 (* 1 = 0.118868 loss)
I1006 12:21:33.220144  2824 sgd_solver.cpp:105] Iteration 38400, lr = 0.1
I1006 12:21:41.178833  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:21:41.514823  2824 solver.cpp:330] Iteration 38500, Testing net (#0)
I1006 12:21:43.473942  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:21:43.556145  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7329
I1006 12:21:43.556180  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.888788 (* 1 = 0.888788 loss)
I1006 12:21:43.639434  2824 solver.cpp:218] Iteration 38500 (9.59759 iter/s, 10.4193s/100 iters), loss = 0.150214
I1006 12:21:43.639457  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.150214 (* 1 = 0.150214 loss)
I1006 12:21:43.639463  2824 sgd_solver.cpp:105] Iteration 38500, lr = 0.1
I1006 12:21:52.012212  2824 solver.cpp:218] Iteration 38600 (11.9435 iter/s, 8.37273s/100 iters), loss = 0.233924
I1006 12:21:52.012243  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233924 (* 1 = 0.233924 loss)
I1006 12:21:52.012248  2824 sgd_solver.cpp:105] Iteration 38600, lr = 0.1
I1006 12:22:00.388885  2824 solver.cpp:218] Iteration 38700 (11.938 iter/s, 8.37662s/100 iters), loss = 0.192369
I1006 12:22:00.388926  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192369 (* 1 = 0.192369 loss)
I1006 12:22:00.388931  2824 sgd_solver.cpp:105] Iteration 38700, lr = 0.1
I1006 12:22:08.763716  2824 solver.cpp:218] Iteration 38800 (11.9406 iter/s, 8.37476s/100 iters), loss = 0.127406
I1006 12:22:08.763818  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127406 (* 1 = 0.127406 loss)
I1006 12:22:08.763834  2824 sgd_solver.cpp:105] Iteration 38800, lr = 0.1
I1006 12:22:17.141444  2824 solver.cpp:218] Iteration 38900 (11.9366 iter/s, 8.37761s/100 iters), loss = 0.15109
I1006 12:22:17.141485  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15109 (* 1 = 0.15109 loss)
I1006 12:22:17.141491  2824 sgd_solver.cpp:105] Iteration 38900, lr = 0.1
I1006 12:22:25.105325  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:22:25.440623  2824 solver.cpp:330] Iteration 39000, Testing net (#0)
I1006 12:22:27.398164  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:22:27.479841  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5326
I1006 12:22:27.479877  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.04522 (* 1 = 2.04522 loss)
I1006 12:22:27.563781  2824 solver.cpp:218] Iteration 39000 (9.59484 iter/s, 10.4223s/100 iters), loss = 0.177834
I1006 12:22:27.563805  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177833 (* 1 = 0.177833 loss)
I1006 12:22:27.563812  2824 sgd_solver.cpp:105] Iteration 39000, lr = 0.1
I1006 12:22:35.951226  2824 solver.cpp:218] Iteration 39100 (11.9227 iter/s, 8.38739s/100 iters), loss = 0.211911
I1006 12:22:35.951267  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.211911 (* 1 = 0.211911 loss)
I1006 12:22:35.951273  2824 sgd_solver.cpp:105] Iteration 39100, lr = 0.1
I1006 12:22:44.338696  2824 solver.cpp:218] Iteration 39200 (11.9226 iter/s, 8.3874s/100 iters), loss = 0.185818
I1006 12:22:44.338832  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185817 (* 1 = 0.185817 loss)
I1006 12:22:44.338840  2824 sgd_solver.cpp:105] Iteration 39200, lr = 0.1
I1006 12:22:52.726836  2824 solver.cpp:218] Iteration 39300 (11.9218 iter/s, 8.38798s/100 iters), loss = 0.133789
I1006 12:22:52.726877  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133789 (* 1 = 0.133789 loss)
I1006 12:22:52.726882  2824 sgd_solver.cpp:105] Iteration 39300, lr = 0.1
I1006 12:23:01.110540  2824 solver.cpp:218] Iteration 39400 (11.928 iter/s, 8.38364s/100 iters), loss = 0.130196
I1006 12:23:01.110570  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130196 (* 1 = 0.130196 loss)
I1006 12:23:01.110576  2824 sgd_solver.cpp:105] Iteration 39400, lr = 0.1
I1006 12:23:09.076141  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:23:09.412919  2824 solver.cpp:330] Iteration 39500, Testing net (#0)
I1006 12:23:11.369542  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:23:11.450711  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.72
I1006 12:23:11.450745  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.00827 (* 1 = 1.00827 loss)
I1006 12:23:11.534063  2824 solver.cpp:218] Iteration 39500 (9.59374 iter/s, 10.4235s/100 iters), loss = 0.171039
I1006 12:23:11.534087  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171039 (* 1 = 0.171039 loss)
I1006 12:23:11.534095  2824 sgd_solver.cpp:105] Iteration 39500, lr = 0.1
I1006 12:23:19.917006  2824 solver.cpp:218] Iteration 39600 (11.9291 iter/s, 8.38289s/100 iters), loss = 0.142259
I1006 12:23:19.917153  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.142259 (* 1 = 0.142259 loss)
I1006 12:23:19.917161  2824 sgd_solver.cpp:105] Iteration 39600, lr = 0.1
I1006 12:23:28.303227  2824 solver.cpp:218] Iteration 39700 (11.9246 iter/s, 8.38606s/100 iters), loss = 0.194692
I1006 12:23:28.303257  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194692 (* 1 = 0.194692 loss)
I1006 12:23:28.303263  2824 sgd_solver.cpp:105] Iteration 39700, lr = 0.1
I1006 12:23:36.682704  2824 solver.cpp:218] Iteration 39800 (11.934 iter/s, 8.37942s/100 iters), loss = 0.216956
I1006 12:23:36.682734  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216955 (* 1 = 0.216955 loss)
I1006 12:23:36.682749  2824 sgd_solver.cpp:105] Iteration 39800, lr = 0.1
I1006 12:23:45.066536  2824 solver.cpp:218] Iteration 39900 (11.9278 iter/s, 8.38377s/100 iters), loss = 0.155859
I1006 12:23:45.066567  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.155859 (* 1 = 0.155859 loss)
I1006 12:23:45.066573  2824 sgd_solver.cpp:105] Iteration 39900, lr = 0.1
I1006 12:23:53.036240  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:23:53.369966  2824 solver.cpp:330] Iteration 40000, Testing net (#0)
I1006 12:23:55.327448  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:23:55.408972  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7623
I1006 12:23:55.409008  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.745081 (* 1 = 0.745081 loss)
I1006 12:23:55.493068  2824 solver.cpp:218] Iteration 40000 (9.59097 iter/s, 10.4265s/100 iters), loss = 0.159821
I1006 12:23:55.493094  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159821 (* 1 = 0.159821 loss)
I1006 12:23:55.493100  2824 sgd_solver.cpp:46] MultiStep Status: Iteration 40000, step = 1
I1006 12:23:55.493103  2824 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I1006 12:24:03.885912  2824 solver.cpp:218] Iteration 40100 (11.915 iter/s, 8.39279s/100 iters), loss = 0.185395
I1006 12:24:03.885954  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185394 (* 1 = 0.185394 loss)
I1006 12:24:03.885959  2824 sgd_solver.cpp:105] Iteration 40100, lr = 0.01
I1006 12:24:12.267455  2824 solver.cpp:218] Iteration 40200 (11.9311 iter/s, 8.38147s/100 iters), loss = 0.0947647
I1006 12:24:12.267485  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0947643 (* 1 = 0.0947643 loss)
I1006 12:24:12.267491  2824 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I1006 12:24:20.653631  2824 solver.cpp:218] Iteration 40300 (11.9245 iter/s, 8.38612s/100 iters), loss = 0.106889
I1006 12:24:20.653672  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106888 (* 1 = 0.106888 loss)
I1006 12:24:20.653678  2824 sgd_solver.cpp:105] Iteration 40300, lr = 0.01
I1006 12:24:29.039273  2824 solver.cpp:218] Iteration 40400 (11.9252 iter/s, 8.38557s/100 iters), loss = 0.107073
I1006 12:24:29.039392  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107072 (* 1 = 0.107072 loss)
I1006 12:24:29.039399  2824 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I1006 12:24:37.006239  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:24:37.341393  2824 solver.cpp:330] Iteration 40500, Testing net (#0)
I1006 12:24:39.300892  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:24:39.382467  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.889
I1006 12:24:39.382503  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329736 (* 1 = 0.329736 loss)
I1006 12:24:39.466300  2824 solver.cpp:218] Iteration 40500 (9.5906 iter/s, 10.4269s/100 iters), loss = 0.135275
I1006 12:24:39.466323  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135274 (* 1 = 0.135274 loss)
I1006 12:24:39.466329  2824 sgd_solver.cpp:105] Iteration 40500, lr = 0.01
I1006 12:24:47.848718  2824 solver.cpp:218] Iteration 40600 (11.9298 iter/s, 8.38237s/100 iters), loss = 0.0910449
I1006 12:24:47.848759  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0910445 (* 1 = 0.0910445 loss)
I1006 12:24:47.848765  2824 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I1006 12:24:56.223345  2824 solver.cpp:218] Iteration 40700 (11.9409 iter/s, 8.37456s/100 iters), loss = 0.141234
I1006 12:24:56.223386  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141234 (* 1 = 0.141234 loss)
I1006 12:24:56.223392  2824 sgd_solver.cpp:105] Iteration 40700, lr = 0.01
I1006 12:25:04.610433  2824 solver.cpp:218] Iteration 40800 (11.9232 iter/s, 8.38702s/100 iters), loss = 0.118027
I1006 12:25:04.610555  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118027 (* 1 = 0.118027 loss)
I1006 12:25:04.610572  2824 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I1006 12:25:12.993875  2824 solver.cpp:218] Iteration 40900 (11.9285 iter/s, 8.38329s/100 iters), loss = 0.0458786
I1006 12:25:12.993904  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0458782 (* 1 = 0.0458782 loss)
I1006 12:25:12.993911  2824 sgd_solver.cpp:105] Iteration 40900, lr = 0.01
I1006 12:25:20.960949  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:25:21.296171  2824 solver.cpp:330] Iteration 41000, Testing net (#0)
I1006 12:25:23.254632  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:25:23.336136  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9016
I1006 12:25:23.336179  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.299538 (* 1 = 0.299538 loss)
I1006 12:25:23.418952  2824 solver.cpp:218] Iteration 41000 (9.59231 iter/s, 10.425s/100 iters), loss = 0.0685792
I1006 12:25:23.418977  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0685788 (* 1 = 0.0685788 loss)
I1006 12:25:23.418982  2824 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I1006 12:25:31.795648  2824 solver.cpp:218] Iteration 41100 (11.938 iter/s, 8.37664s/100 iters), loss = 0.117889
I1006 12:25:31.795678  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117889 (* 1 = 0.117889 loss)
I1006 12:25:31.795683  2824 sgd_solver.cpp:105] Iteration 41100, lr = 0.01
I1006 12:25:40.169703  2824 solver.cpp:218] Iteration 41200 (11.9417 iter/s, 8.374s/100 iters), loss = 0.10944
I1006 12:25:40.169826  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.10944 (* 1 = 0.10944 loss)
I1006 12:25:40.169842  2824 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I1006 12:25:48.552065  2824 solver.cpp:218] Iteration 41300 (11.93 iter/s, 8.38222s/100 iters), loss = 0.0403464
I1006 12:25:48.552093  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.040346 (* 1 = 0.040346 loss)
I1006 12:25:48.552099  2824 sgd_solver.cpp:105] Iteration 41300, lr = 0.01
I1006 12:25:56.923637  2824 solver.cpp:218] Iteration 41400 (11.9453 iter/s, 8.37152s/100 iters), loss = 0.0793012
I1006 12:25:56.923678  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0793008 (* 1 = 0.0793008 loss)
I1006 12:25:56.923684  2824 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I1006 12:26:04.875963  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:26:05.210997  2824 solver.cpp:330] Iteration 41500, Testing net (#0)
I1006 12:26:07.164007  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:26:07.245651  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9007
I1006 12:26:07.245692  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.296424 (* 1 = 0.296424 loss)
I1006 12:26:07.328779  2824 solver.cpp:218] Iteration 41500 (9.6107 iter/s, 10.4051s/100 iters), loss = 0.0677458
I1006 12:26:07.328802  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0677454 (* 1 = 0.0677454 loss)
I1006 12:26:07.328809  2824 sgd_solver.cpp:105] Iteration 41500, lr = 0.01
I1006 12:26:15.698470  2824 solver.cpp:218] Iteration 41600 (11.9479 iter/s, 8.36964s/100 iters), loss = 0.11684
I1006 12:26:15.698626  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.11684 (* 1 = 0.11684 loss)
I1006 12:26:15.698632  2824 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I1006 12:26:24.082790  2824 solver.cpp:218] Iteration 41700 (11.9273 iter/s, 8.38415s/100 iters), loss = 0.0868371
I1006 12:26:24.082820  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0868366 (* 1 = 0.0868366 loss)
I1006 12:26:24.082825  2824 sgd_solver.cpp:105] Iteration 41700, lr = 0.01
I1006 12:26:32.464422  2824 solver.cpp:218] Iteration 41800 (11.9309 iter/s, 8.38157s/100 iters), loss = 0.0587563
I1006 12:26:32.464463  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0587559 (* 1 = 0.0587559 loss)
I1006 12:26:32.464469  2824 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I1006 12:26:40.842233  2824 solver.cpp:218] Iteration 41900 (11.9364 iter/s, 8.37774s/100 iters), loss = 0.0333092
I1006 12:26:40.842273  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0333088 (* 1 = 0.0333088 loss)
I1006 12:26:40.842278  2824 sgd_solver.cpp:105] Iteration 41900, lr = 0.01
I1006 12:26:48.814509  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:26:49.150723  2824 solver.cpp:330] Iteration 42000, Testing net (#0)
I1006 12:26:51.108091  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:26:51.189560  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9015
I1006 12:26:51.189596  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.299169 (* 1 = 0.299169 loss)
I1006 12:26:51.273216  2824 solver.cpp:218] Iteration 42000 (9.58689 iter/s, 10.4309s/100 iters), loss = 0.0755423
I1006 12:26:51.273239  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0755419 (* 1 = 0.0755419 loss)
I1006 12:26:51.273246  2824 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I1006 12:26:59.661191  2824 solver.cpp:218] Iteration 42100 (11.9219 iter/s, 8.38793s/100 iters), loss = 0.0527114
I1006 12:26:59.661222  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0527109 (* 1 = 0.0527109 loss)
I1006 12:26:59.661238  2824 sgd_solver.cpp:105] Iteration 42100, lr = 0.01
I1006 12:27:08.043645  2824 solver.cpp:218] Iteration 42200 (11.9298 iter/s, 8.3824s/100 iters), loss = 0.102957
I1006 12:27:08.043676  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.102957 (* 1 = 0.102957 loss)
I1006 12:27:08.043682  2824 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I1006 12:27:16.424067  2824 solver.cpp:218] Iteration 42300 (11.9327 iter/s, 8.38036s/100 iters), loss = 0.0969885
I1006 12:27:16.424108  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.096988 (* 1 = 0.096988 loss)
I1006 12:27:16.424114  2824 sgd_solver.cpp:105] Iteration 42300, lr = 0.01
I1006 12:27:24.809010  2824 solver.cpp:218] Iteration 42400 (11.9262 iter/s, 8.38488s/100 iters), loss = 0.0369925
I1006 12:27:24.809140  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0369921 (* 1 = 0.0369921 loss)
I1006 12:27:24.809147  2824 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I1006 12:27:32.775532  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:27:33.111109  2824 solver.cpp:330] Iteration 42500, Testing net (#0)
I1006 12:27:35.065840  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:27:35.147660  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9021
I1006 12:27:35.147696  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.296667 (* 1 = 0.296667 loss)
I1006 12:27:35.231972  2824 solver.cpp:218] Iteration 42500 (9.59434 iter/s, 10.4228s/100 iters), loss = 0.0418741
I1006 12:27:35.232005  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0418736 (* 1 = 0.0418736 loss)
I1006 12:27:35.232012  2824 sgd_solver.cpp:105] Iteration 42500, lr = 0.01
I1006 12:27:43.614053  2824 solver.cpp:218] Iteration 42600 (11.9303 iter/s, 8.38202s/100 iters), loss = 0.0937037
I1006 12:27:43.614092  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0937033 (* 1 = 0.0937033 loss)
I1006 12:27:43.614099  2824 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I1006 12:27:51.994087  2824 solver.cpp:218] Iteration 42700 (11.9332 iter/s, 8.37997s/100 iters), loss = 0.0762992
I1006 12:27:51.994129  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0762988 (* 1 = 0.0762988 loss)
I1006 12:27:51.994135  2824 sgd_solver.cpp:105] Iteration 42700, lr = 0.01
I1006 12:28:00.377080  2824 solver.cpp:218] Iteration 42800 (11.929 iter/s, 8.38292s/100 iters), loss = 0.0392519
I1006 12:28:00.377234  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0392515 (* 1 = 0.0392515 loss)
I1006 12:28:00.377241  2824 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I1006 12:28:08.760084  2824 solver.cpp:218] Iteration 42900 (11.9292 iter/s, 8.38283s/100 iters), loss = 0.0685124
I1006 12:28:08.760113  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.068512 (* 1 = 0.068512 loss)
I1006 12:28:08.760119  2824 sgd_solver.cpp:105] Iteration 42900, lr = 0.01
I1006 12:28:16.720031  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:28:17.054903  2824 solver.cpp:330] Iteration 43000, Testing net (#0)
I1006 12:28:19.013950  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:28:19.095959  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9083
I1006 12:28:19.095994  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.296309 (* 1 = 0.296309 loss)
I1006 12:28:19.179961  2824 solver.cpp:218] Iteration 43000 (9.5971 iter/s, 10.4198s/100 iters), loss = 0.0438705
I1006 12:28:19.179985  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.04387 (* 1 = 0.04387 loss)
I1006 12:28:19.179991  2824 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I1006 12:28:27.554319  2824 solver.cpp:218] Iteration 43100 (11.9413 iter/s, 8.37431s/100 iters), loss = 0.0849167
I1006 12:28:27.554359  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0849163 (* 1 = 0.0849163 loss)
I1006 12:28:27.554365  2824 sgd_solver.cpp:105] Iteration 43100, lr = 0.01
I1006 12:28:35.934314  2824 solver.cpp:218] Iteration 43200 (11.9333 iter/s, 8.37993s/100 iters), loss = 0.0778877
I1006 12:28:35.934427  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0778872 (* 1 = 0.0778872 loss)
I1006 12:28:35.934442  2824 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I1006 12:28:44.312145  2824 solver.cpp:218] Iteration 43300 (11.9364 iter/s, 8.3777s/100 iters), loss = 0.0392479
I1006 12:28:44.312186  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0392475 (* 1 = 0.0392475 loss)
I1006 12:28:44.312192  2824 sgd_solver.cpp:105] Iteration 43300, lr = 0.01
I1006 12:28:52.688097  2824 solver.cpp:218] Iteration 43400 (11.939 iter/s, 8.37588s/100 iters), loss = 0.0306295
I1006 12:28:52.688136  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0306291 (* 1 = 0.0306291 loss)
I1006 12:28:52.688143  2824 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I1006 12:29:00.649596  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:29:00.985679  2824 solver.cpp:330] Iteration 43500, Testing net (#0)
I1006 12:29:02.945463  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:29:03.026423  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9114
I1006 12:29:03.026459  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.278803 (* 1 = 0.278803 loss)
I1006 12:29:03.110559  2824 solver.cpp:218] Iteration 43500 (9.59473 iter/s, 10.4224s/100 iters), loss = 0.0150235
I1006 12:29:03.110584  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0150231 (* 1 = 0.0150231 loss)
I1006 12:29:03.110590  2824 sgd_solver.cpp:105] Iteration 43500, lr = 0.01
I1006 12:29:11.486145  2824 solver.cpp:218] Iteration 43600 (11.9395 iter/s, 8.37553s/100 iters), loss = 0.0593703
I1006 12:29:11.486277  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0593699 (* 1 = 0.0593699 loss)
I1006 12:29:11.486294  2824 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I1006 12:29:19.875713  2824 solver.cpp:218] Iteration 43700 (11.9198 iter/s, 8.38942s/100 iters), loss = 0.103745
I1006 12:29:19.875754  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103745 (* 1 = 0.103745 loss)
I1006 12:29:19.875761  2824 sgd_solver.cpp:105] Iteration 43700, lr = 0.01
I1006 12:29:28.255650  2824 solver.cpp:218] Iteration 43800 (11.9334 iter/s, 8.37987s/100 iters), loss = 0.0689886
I1006 12:29:28.255692  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0689881 (* 1 = 0.0689881 loss)
I1006 12:29:28.255697  2824 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I1006 12:29:36.635058  2824 solver.cpp:218] Iteration 43900 (11.9341 iter/s, 8.37934s/100 iters), loss = 0.0341431
I1006 12:29:36.635099  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0341427 (* 1 = 0.0341427 loss)
I1006 12:29:36.635105  2824 sgd_solver.cpp:105] Iteration 43900, lr = 0.01
I1006 12:29:44.605473  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:29:44.940815  2824 solver.cpp:330] Iteration 44000, Testing net (#0)
I1006 12:29:46.896746  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:29:46.978927  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.908
I1006 12:29:46.978952  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.302674 (* 1 = 0.302674 loss)
I1006 12:29:47.062647  2824 solver.cpp:218] Iteration 44000 (9.59001 iter/s, 10.4275s/100 iters), loss = 0.0291951
I1006 12:29:47.062671  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0291946 (* 1 = 0.0291946 loss)
I1006 12:29:47.062678  2824 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I1006 12:29:55.440898  2824 solver.cpp:218] Iteration 44100 (11.9357 iter/s, 8.37819s/100 iters), loss = 0.0613513
I1006 12:29:55.440928  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0613509 (* 1 = 0.0613509 loss)
I1006 12:29:55.440934  2824 sgd_solver.cpp:105] Iteration 44100, lr = 0.01
I1006 12:30:03.824993  2824 solver.cpp:218] Iteration 44200 (11.9274 iter/s, 8.38404s/100 iters), loss = 0.112045
I1006 12:30:03.825034  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.112044 (* 1 = 0.112044 loss)
I1006 12:30:03.825040  2824 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I1006 12:30:12.202986  2824 solver.cpp:218] Iteration 44300 (11.9361 iter/s, 8.37792s/100 iters), loss = 0.059934
I1006 12:30:12.203016  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0599336 (* 1 = 0.0599336 loss)
I1006 12:30:12.203022  2824 sgd_solver.cpp:105] Iteration 44300, lr = 0.01
I1006 12:30:20.584868  2824 solver.cpp:218] Iteration 44400 (11.9306 iter/s, 8.38183s/100 iters), loss = 0.02896
I1006 12:30:20.585026  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0289595 (* 1 = 0.0289595 loss)
I1006 12:30:20.585033  2824 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I1006 12:30:28.546030  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:30:28.881711  2824 solver.cpp:330] Iteration 44500, Testing net (#0)
I1006 12:30:30.839293  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:30:30.921352  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9087
I1006 12:30:30.921388  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.302976 (* 1 = 0.302976 loss)
I1006 12:30:31.004616  2824 solver.cpp:218] Iteration 44500 (9.59732 iter/s, 10.4196s/100 iters), loss = 0.0314427
I1006 12:30:31.004640  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0314423 (* 1 = 0.0314423 loss)
I1006 12:30:31.004647  2824 sgd_solver.cpp:105] Iteration 44500, lr = 0.01
I1006 12:30:39.398874  2824 solver.cpp:218] Iteration 44600 (11.913 iter/s, 8.39421s/100 iters), loss = 0.0527707
I1006 12:30:39.398914  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0527703 (* 1 = 0.0527703 loss)
I1006 12:30:39.398921  2824 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I1006 12:30:47.791981  2824 solver.cpp:218] Iteration 44700 (11.9146 iter/s, 8.39304s/100 iters), loss = 0.0741023
I1006 12:30:47.792022  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0741019 (* 1 = 0.0741019 loss)
I1006 12:30:47.792027  2824 sgd_solver.cpp:105] Iteration 44700, lr = 0.01
I1006 12:30:56.178959  2824 solver.cpp:218] Iteration 44800 (11.9233 iter/s, 8.38691s/100 iters), loss = 0.032142
I1006 12:30:56.179085  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0321416 (* 1 = 0.0321416 loss)
I1006 12:30:56.179102  2824 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I1006 12:31:04.569506  2824 solver.cpp:218] Iteration 44900 (11.9184 iter/s, 8.3904s/100 iters), loss = 0.0168991
I1006 12:31:04.569547  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168987 (* 1 = 0.0168987 loss)
I1006 12:31:04.569553  2824 sgd_solver.cpp:105] Iteration 44900, lr = 0.01
I1006 12:31:12.535965  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:31:12.872196  2824 solver.cpp:330] Iteration 45000, Testing net (#0)
I1006 12:31:14.833436  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:31:14.915194  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9117
I1006 12:31:14.915228  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.295463 (* 1 = 0.295463 loss)
I1006 12:31:14.998975  2824 solver.cpp:218] Iteration 45000 (9.58828 iter/s, 10.4294s/100 iters), loss = 0.0361331
I1006 12:31:14.998998  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0361327 (* 1 = 0.0361327 loss)
I1006 12:31:14.999004  2824 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I1006 12:31:23.375555  2824 solver.cpp:218] Iteration 45100 (11.9381 iter/s, 8.37653s/100 iters), loss = 0.0267229
I1006 12:31:23.375597  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0267225 (* 1 = 0.0267225 loss)
I1006 12:31:23.375602  2824 sgd_solver.cpp:105] Iteration 45100, lr = 0.01
I1006 12:31:31.754122  2824 solver.cpp:218] Iteration 45200 (11.9353 iter/s, 8.3785s/100 iters), loss = 0.110643
I1006 12:31:31.754197  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110643 (* 1 = 0.110643 loss)
I1006 12:31:31.754204  2824 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I1006 12:31:40.130723  2824 solver.cpp:218] Iteration 45300 (11.9382 iter/s, 8.3765s/100 iters), loss = 0.0628465
I1006 12:31:40.130764  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.062846 (* 1 = 0.062846 loss)
I1006 12:31:40.130770  2824 sgd_solver.cpp:105] Iteration 45300, lr = 0.01
I1006 12:31:48.513777  2824 solver.cpp:218] Iteration 45400 (11.9289 iter/s, 8.38298s/100 iters), loss = 0.00940483
I1006 12:31:48.513806  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00940443 (* 1 = 0.00940443 loss)
I1006 12:31:48.513813  2824 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I1006 12:31:56.469213  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:31:56.805712  2824 solver.cpp:330] Iteration 45500, Testing net (#0)
I1006 12:31:58.764566  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:31:58.846527  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9149
I1006 12:31:58.846563  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.285129 (* 1 = 0.285129 loss)
I1006 12:31:58.929975  2824 solver.cpp:218] Iteration 45500 (9.60049 iter/s, 10.4161s/100 iters), loss = 0.025273
I1006 12:31:58.929999  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0252726 (* 1 = 0.0252726 loss)
I1006 12:31:58.930006  2824 sgd_solver.cpp:105] Iteration 45500, lr = 0.01
I1006 12:32:07.305240  2824 solver.cpp:218] Iteration 45600 (11.94 iter/s, 8.37521s/100 iters), loss = 0.0245053
I1006 12:32:07.305332  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0245049 (* 1 = 0.0245049 loss)
I1006 12:32:07.305351  2824 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I1006 12:32:15.683118  2824 solver.cpp:218] Iteration 45700 (11.9364 iter/s, 8.37776s/100 iters), loss = 0.0675008
I1006 12:32:15.683158  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0675004 (* 1 = 0.0675004 loss)
I1006 12:32:15.683166  2824 sgd_solver.cpp:105] Iteration 45700, lr = 0.01
I1006 12:32:24.059245  2824 solver.cpp:218] Iteration 45800 (11.9388 iter/s, 8.37606s/100 iters), loss = 0.0397206
I1006 12:32:24.059286  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0397202 (* 1 = 0.0397202 loss)
I1006 12:32:24.059293  2824 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I1006 12:32:32.433676  2824 solver.cpp:218] Iteration 45900 (11.9412 iter/s, 8.37436s/100 iters), loss = 0.0157933
I1006 12:32:32.433717  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0157929 (* 1 = 0.0157929 loss)
I1006 12:32:32.433722  2824 sgd_solver.cpp:105] Iteration 45900, lr = 0.01
I1006 12:32:40.392442  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:32:40.727933  2824 solver.cpp:330] Iteration 46000, Testing net (#0)
I1006 12:32:42.685374  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:32:42.767330  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9075
I1006 12:32:42.767366  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.308468 (* 1 = 0.308468 loss)
I1006 12:32:42.850986  2824 solver.cpp:218] Iteration 46000 (9.59947 iter/s, 10.4172s/100 iters), loss = 0.0152042
I1006 12:32:42.851011  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0152038 (* 1 = 0.0152038 loss)
I1006 12:32:42.851016  2824 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I1006 12:32:51.225646  2824 solver.cpp:218] Iteration 46100 (11.9409 iter/s, 8.37461s/100 iters), loss = 0.0127289
I1006 12:32:51.225677  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0127285 (* 1 = 0.0127285 loss)
I1006 12:32:51.225683  2824 sgd_solver.cpp:105] Iteration 46100, lr = 0.01
I1006 12:32:59.602862  2824 solver.cpp:218] Iteration 46200 (11.9372 iter/s, 8.37716s/100 iters), loss = 0.0563127
I1006 12:32:59.602905  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0563123 (* 1 = 0.0563123 loss)
I1006 12:32:59.602910  2824 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I1006 12:33:07.981753  2824 solver.cpp:218] Iteration 46300 (11.9349 iter/s, 8.37882s/100 iters), loss = 0.0712072
I1006 12:33:07.981794  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0712068 (* 1 = 0.0712068 loss)
I1006 12:33:07.981801  2824 sgd_solver.cpp:105] Iteration 46300, lr = 0.01
I1006 12:33:16.357295  2824 solver.cpp:218] Iteration 46400 (11.9396 iter/s, 8.37547s/100 iters), loss = 0.0194273
I1006 12:33:16.357419  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194269 (* 1 = 0.0194269 loss)
I1006 12:33:16.357426  2824 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I1006 12:33:24.316576  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:33:24.652079  2824 solver.cpp:330] Iteration 46500, Testing net (#0)
I1006 12:33:26.609508  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:33:26.691277  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9134
I1006 12:33:26.691313  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.303417 (* 1 = 0.303417 loss)
I1006 12:33:26.774569  2824 solver.cpp:218] Iteration 46500 (9.59958 iter/s, 10.4171s/100 iters), loss = 0.0072017
I1006 12:33:26.774595  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00720133 (* 1 = 0.00720133 loss)
I1006 12:33:26.774601  2824 sgd_solver.cpp:105] Iteration 46500, lr = 0.01
I1006 12:33:35.156813  2824 solver.cpp:218] Iteration 46600 (11.9301 iter/s, 8.38219s/100 iters), loss = 0.0191719
I1006 12:33:35.156855  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0191715 (* 1 = 0.0191715 loss)
I1006 12:33:35.156862  2824 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I1006 12:33:43.533282  2824 solver.cpp:218] Iteration 46700 (11.9383 iter/s, 8.3764s/100 iters), loss = 0.0651389
I1006 12:33:43.533313  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0651386 (* 1 = 0.0651386 loss)
I1006 12:33:43.533329  2824 sgd_solver.cpp:105] Iteration 46700, lr = 0.01
I1006 12:33:51.912081  2824 solver.cpp:218] Iteration 46800 (11.935 iter/s, 8.37874s/100 iters), loss = 0.00859114
I1006 12:33:51.912184  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00859078 (* 1 = 0.00859078 loss)
I1006 12:33:51.912191  2824 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I1006 12:34:00.295918  2824 solver.cpp:218] Iteration 46900 (11.9279 iter/s, 8.38372s/100 iters), loss = 0.0114365
I1006 12:34:00.295948  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114361 (* 1 = 0.0114361 loss)
I1006 12:34:00.295964  2824 sgd_solver.cpp:105] Iteration 46900, lr = 0.01
I1006 12:34:08.260258  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:34:08.597991  2824 solver.cpp:330] Iteration 47000, Testing net (#0)
I1006 12:34:10.555732  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:34:10.637303  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.906
I1006 12:34:10.637339  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.328018 (* 1 = 0.328018 loss)
I1006 12:34:10.720648  2824 solver.cpp:218] Iteration 47000 (9.59263 iter/s, 10.4247s/100 iters), loss = 0.0114411
I1006 12:34:10.720670  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114407 (* 1 = 0.0114407 loss)
I1006 12:34:10.720676  2824 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I1006 12:34:19.114778  2824 solver.cpp:218] Iteration 47100 (11.9132 iter/s, 8.39408s/100 iters), loss = 0.0429421
I1006 12:34:19.114820  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0429418 (* 1 = 0.0429418 loss)
I1006 12:34:19.114825  2824 sgd_solver.cpp:105] Iteration 47100, lr = 0.01
I1006 12:34:27.496049  2824 solver.cpp:218] Iteration 47200 (11.9315 iter/s, 8.3812s/100 iters), loss = 0.0527186
I1006 12:34:27.496137  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0527182 (* 1 = 0.0527182 loss)
I1006 12:34:27.496155  2824 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I1006 12:34:35.883963  2824 solver.cpp:218] Iteration 47300 (11.9221 iter/s, 8.3878s/100 iters), loss = 0.0222127
I1006 12:34:35.884004  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0222123 (* 1 = 0.0222123 loss)
I1006 12:34:35.884011  2824 sgd_solver.cpp:105] Iteration 47300, lr = 0.01
I1006 12:34:44.258424  2824 solver.cpp:218] Iteration 47400 (11.9412 iter/s, 8.37439s/100 iters), loss = 0.0153421
I1006 12:34:44.258466  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0153418 (* 1 = 0.0153418 loss)
I1006 12:34:44.258471  2824 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I1006 12:34:52.218308  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:34:52.553884  2824 solver.cpp:330] Iteration 47500, Testing net (#0)
I1006 12:34:54.511778  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:34:54.593686  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.907
I1006 12:34:54.593721  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332017 (* 1 = 0.332017 loss)
I1006 12:34:54.677508  2824 solver.cpp:218] Iteration 47500 (9.59784 iter/s, 10.419s/100 iters), loss = 0.0271271
I1006 12:34:54.677532  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0271267 (* 1 = 0.0271267 loss)
I1006 12:34:54.677539  2824 sgd_solver.cpp:105] Iteration 47500, lr = 0.01
I1006 12:35:03.065013  2824 solver.cpp:218] Iteration 47600 (11.9226 iter/s, 8.38745s/100 iters), loss = 0.0210475
I1006 12:35:03.065136  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0210471 (* 1 = 0.0210471 loss)
I1006 12:35:03.065143  2824 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I1006 12:35:11.444378  2824 solver.cpp:218] Iteration 47700 (11.9343 iter/s, 8.37922s/100 iters), loss = 0.0242762
I1006 12:35:11.444420  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0242758 (* 1 = 0.0242758 loss)
I1006 12:35:11.444425  2824 sgd_solver.cpp:105] Iteration 47700, lr = 0.01
I1006 12:35:19.830464  2824 solver.cpp:218] Iteration 47800 (11.9246 iter/s, 8.38602s/100 iters), loss = 0.0301274
I1006 12:35:19.830505  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.030127 (* 1 = 0.030127 loss)
I1006 12:35:19.830512  2824 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I1006 12:35:28.218696  2824 solver.cpp:218] Iteration 47900 (11.9216 iter/s, 8.38816s/100 iters), loss = 0.00527342
I1006 12:35:28.218739  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00527301 (* 1 = 0.00527301 loss)
I1006 12:35:28.218744  2824 sgd_solver.cpp:105] Iteration 47900, lr = 0.01
I1006 12:35:36.191620  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:35:36.527299  2824 solver.cpp:330] Iteration 48000, Testing net (#0)
I1006 12:35:38.482226  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:35:38.564836  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9089
I1006 12:35:38.564860  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.312967 (* 1 = 0.312967 loss)
I1006 12:35:38.647835  2824 solver.cpp:218] Iteration 48000 (9.58859 iter/s, 10.4291s/100 iters), loss = 0.0339164
I1006 12:35:38.647869  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0339159 (* 1 = 0.0339159 loss)
I1006 12:35:38.647876  2824 sgd_solver.cpp:105] Iteration 48000, lr = 0.01
I1006 12:35:47.023507  2824 solver.cpp:218] Iteration 48100 (11.9394 iter/s, 8.37561s/100 iters), loss = 0.0577536
I1006 12:35:47.023538  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0577532 (* 1 = 0.0577532 loss)
I1006 12:35:47.023545  2824 sgd_solver.cpp:105] Iteration 48100, lr = 0.01
I1006 12:35:55.401013  2824 solver.cpp:218] Iteration 48200 (11.9368 iter/s, 8.37745s/100 iters), loss = 0.0362732
I1006 12:35:55.401043  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0362728 (* 1 = 0.0362728 loss)
I1006 12:35:55.401049  2824 sgd_solver.cpp:105] Iteration 48200, lr = 0.01
I1006 12:36:03.778751  2824 solver.cpp:218] Iteration 48300 (11.9365 iter/s, 8.37768s/100 iters), loss = 0.0528215
I1006 12:36:03.778782  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0528211 (* 1 = 0.0528211 loss)
I1006 12:36:03.778789  2824 sgd_solver.cpp:105] Iteration 48300, lr = 0.01
I1006 12:36:12.150701  2824 solver.cpp:218] Iteration 48400 (11.9447 iter/s, 8.37189s/100 iters), loss = 0.0159579
I1006 12:36:12.150802  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159575 (* 1 = 0.0159575 loss)
I1006 12:36:12.150809  2824 sgd_solver.cpp:105] Iteration 48400, lr = 0.01
I1006 12:36:20.113453  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:36:20.449458  2824 solver.cpp:330] Iteration 48500, Testing net (#0)
I1006 12:36:22.406877  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:36:22.488579  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9136
I1006 12:36:22.488605  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.302472 (* 1 = 0.302472 loss)
I1006 12:36:22.572217  2824 solver.cpp:218] Iteration 48500 (9.59565 iter/s, 10.4214s/100 iters), loss = 0.0213106
I1006 12:36:22.572242  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0213101 (* 1 = 0.0213101 loss)
I1006 12:36:22.572248  2824 sgd_solver.cpp:105] Iteration 48500, lr = 0.01
I1006 12:36:30.958310  2824 solver.cpp:218] Iteration 48600 (11.9246 iter/s, 8.38604s/100 iters), loss = 0.0380753
I1006 12:36:30.958350  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380749 (* 1 = 0.0380749 loss)
I1006 12:36:30.958356  2824 sgd_solver.cpp:105] Iteration 48600, lr = 0.01
I1006 12:36:39.336473  2824 solver.cpp:218] Iteration 48700 (11.9359 iter/s, 8.3781s/100 iters), loss = 0.0404316
I1006 12:36:39.336514  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0404312 (* 1 = 0.0404312 loss)
I1006 12:36:39.336520  2824 sgd_solver.cpp:105] Iteration 48700, lr = 0.01
I1006 12:36:47.719619  2824 solver.cpp:218] Iteration 48800 (11.9288 iter/s, 8.38308s/100 iters), loss = 0.0286035
I1006 12:36:47.719774  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0286031 (* 1 = 0.0286031 loss)
I1006 12:36:47.719781  2824 sgd_solver.cpp:105] Iteration 48800, lr = 0.01
I1006 12:36:56.099242  2824 solver.cpp:218] Iteration 48900 (11.934 iter/s, 8.37944s/100 iters), loss = 0.0062475
I1006 12:36:56.099273  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00624708 (* 1 = 0.00624708 loss)
I1006 12:36:56.099279  2824 sgd_solver.cpp:105] Iteration 48900, lr = 0.01
I1006 12:37:04.065642  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:37:04.401062  2824 solver.cpp:330] Iteration 49000, Testing net (#0)
I1006 12:37:06.357090  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:37:06.438942  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9093
I1006 12:37:06.438978  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.318394 (* 1 = 0.318394 loss)
I1006 12:37:06.521950  2824 solver.cpp:218] Iteration 49000 (9.59449 iter/s, 10.4226s/100 iters), loss = 0.013463
I1006 12:37:06.521973  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0134626 (* 1 = 0.0134626 loss)
I1006 12:37:06.521980  2824 sgd_solver.cpp:105] Iteration 49000, lr = 0.01
I1006 12:37:14.905335  2824 solver.cpp:218] Iteration 49100 (11.9284 iter/s, 8.38333s/100 iters), loss = 0.0438448
I1006 12:37:14.905376  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0438444 (* 1 = 0.0438444 loss)
I1006 12:37:14.905381  2824 sgd_solver.cpp:105] Iteration 49100, lr = 0.01
I1006 12:37:23.278684  2824 solver.cpp:218] Iteration 49200 (11.9427 iter/s, 8.37328s/100 iters), loss = 0.0639572
I1006 12:37:23.278759  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0639568 (* 1 = 0.0639568 loss)
I1006 12:37:23.278775  2824 sgd_solver.cpp:105] Iteration 49200, lr = 0.01
I1006 12:37:31.654742  2824 solver.cpp:218] Iteration 49300 (11.9389 iter/s, 8.37596s/100 iters), loss = 0.0129342
I1006 12:37:31.654783  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129338 (* 1 = 0.0129338 loss)
I1006 12:37:31.654789  2824 sgd_solver.cpp:105] Iteration 49300, lr = 0.01
I1006 12:37:40.029351  2824 solver.cpp:218] Iteration 49400 (11.941 iter/s, 8.37454s/100 iters), loss = 0.00466829
I1006 12:37:40.029381  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0046679 (* 1 = 0.0046679 loss)
I1006 12:37:40.029387  2824 sgd_solver.cpp:105] Iteration 49400, lr = 0.01
I1006 12:37:47.986989  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:37:48.322791  2824 solver.cpp:330] Iteration 49500, Testing net (#0)
I1006 12:37:50.281580  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:37:50.363494  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9107
I1006 12:37:50.363529  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.318249 (* 1 = 0.318249 loss)
I1006 12:37:50.446732  2824 solver.cpp:218] Iteration 49500 (9.5994 iter/s, 10.4173s/100 iters), loss = 0.00961944
I1006 12:37:50.446756  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00961904 (* 1 = 0.00961904 loss)
I1006 12:37:50.446763  2824 sgd_solver.cpp:105] Iteration 49500, lr = 0.01
I1006 12:37:58.831321  2824 solver.cpp:218] Iteration 49600 (11.9267 iter/s, 8.38453s/100 iters), loss = 0.0178994
I1006 12:37:58.831450  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.017899 (* 1 = 0.017899 loss)
I1006 12:37:58.831468  2824 sgd_solver.cpp:105] Iteration 49600, lr = 0.01
I1006 12:38:07.207024  2824 solver.cpp:218] Iteration 49700 (11.9395 iter/s, 8.37556s/100 iters), loss = 0.0364418
I1006 12:38:07.207065  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0364414 (* 1 = 0.0364414 loss)
I1006 12:38:07.207072  2824 sgd_solver.cpp:105] Iteration 49700, lr = 0.01
I1006 12:38:15.591902  2824 solver.cpp:218] Iteration 49800 (11.9263 iter/s, 8.38481s/100 iters), loss = 0.0358573
I1006 12:38:15.591943  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0358569 (* 1 = 0.0358569 loss)
I1006 12:38:15.591948  2824 sgd_solver.cpp:105] Iteration 49800, lr = 0.01
I1006 12:38:23.971844  2824 solver.cpp:218] Iteration 49900 (11.9334 iter/s, 8.37987s/100 iters), loss = 0.00287945
I1006 12:38:23.971885  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00287908 (* 1 = 0.00287908 loss)
I1006 12:38:23.971890  2824 sgd_solver.cpp:105] Iteration 49900, lr = 0.01
I1006 12:38:31.934257  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:38:32.270872  2824 solver.cpp:330] Iteration 50000, Testing net (#0)
I1006 12:38:34.230300  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:38:34.312297  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9097
I1006 12:38:34.312332  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.335849 (* 1 = 0.335849 loss)
I1006 12:38:34.395727  2824 solver.cpp:218] Iteration 50000 (9.59342 iter/s, 10.4238s/100 iters), loss = 0.079672
I1006 12:38:34.395751  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0796716 (* 1 = 0.0796716 loss)
I1006 12:38:34.395757  2824 sgd_solver.cpp:105] Iteration 50000, lr = 0.01
I1006 12:38:42.767478  2824 solver.cpp:218] Iteration 50100 (11.945 iter/s, 8.3717s/100 iters), loss = 0.0419929
I1006 12:38:42.767519  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0419925 (* 1 = 0.0419925 loss)
I1006 12:38:42.767524  2824 sgd_solver.cpp:105] Iteration 50100, lr = 0.01
I1006 12:38:51.149505  2824 solver.cpp:218] Iteration 50200 (11.9304 iter/s, 8.38196s/100 iters), loss = 0.0883471
I1006 12:38:51.149536  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0883468 (* 1 = 0.0883468 loss)
I1006 12:38:51.149543  2824 sgd_solver.cpp:105] Iteration 50200, lr = 0.01
I1006 12:38:59.532256  2824 solver.cpp:218] Iteration 50300 (11.9293 iter/s, 8.38269s/100 iters), loss = 0.0216659
I1006 12:38:59.532286  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216656 (* 1 = 0.0216656 loss)
I1006 12:38:59.532302  2824 sgd_solver.cpp:105] Iteration 50300, lr = 0.01
I1006 12:39:07.906225  2824 solver.cpp:218] Iteration 50400 (11.9419 iter/s, 8.37391s/100 iters), loss = 0.0157151
I1006 12:39:07.906347  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0157147 (* 1 = 0.0157147 loss)
I1006 12:39:07.906363  2824 sgd_solver.cpp:105] Iteration 50400, lr = 0.01
I1006 12:39:15.868618  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:39:16.204830  2824 solver.cpp:330] Iteration 50500, Testing net (#0)
I1006 12:39:18.160809  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:39:18.242986  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9043
I1006 12:39:18.243022  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.371704 (* 1 = 0.371704 loss)
I1006 12:39:18.326689  2824 solver.cpp:218] Iteration 50500 (9.59664 iter/s, 10.4203s/100 iters), loss = 0.0937834
I1006 12:39:18.326723  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.093783 (* 1 = 0.093783 loss)
I1006 12:39:18.326730  2824 sgd_solver.cpp:105] Iteration 50500, lr = 0.01
I1006 12:39:26.708415  2824 solver.cpp:218] Iteration 50600 (11.9308 iter/s, 8.38166s/100 iters), loss = 0.0048557
I1006 12:39:26.708457  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00485532 (* 1 = 0.00485532 loss)
I1006 12:39:26.708462  2824 sgd_solver.cpp:105] Iteration 50600, lr = 0.01
I1006 12:39:35.090912  2824 solver.cpp:218] Iteration 50700 (11.9297 iter/s, 8.38243s/100 iters), loss = 0.00545825
I1006 12:39:35.090955  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00545788 (* 1 = 0.00545788 loss)
I1006 12:39:35.090960  2824 sgd_solver.cpp:105] Iteration 50700, lr = 0.01
I1006 12:39:43.471097  2824 solver.cpp:218] Iteration 50800 (11.933 iter/s, 8.38012s/100 iters), loss = 0.0176768
I1006 12:39:43.471235  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0176764 (* 1 = 0.0176764 loss)
I1006 12:39:43.471251  2824 sgd_solver.cpp:105] Iteration 50800, lr = 0.01
I1006 12:39:51.849687  2824 solver.cpp:218] Iteration 50900 (11.9354 iter/s, 8.37843s/100 iters), loss = 0.0170627
I1006 12:39:51.849730  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0170623 (* 1 = 0.0170623 loss)
I1006 12:39:51.849735  2824 sgd_solver.cpp:105] Iteration 50900, lr = 0.01
I1006 12:39:59.820814  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:40:00.156821  2824 solver.cpp:330] Iteration 51000, Testing net (#0)
I1006 12:40:02.114476  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:40:02.196478  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9094
I1006 12:40:02.196513  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.347906 (* 1 = 0.347906 loss)
I1006 12:40:02.280016  2824 solver.cpp:218] Iteration 51000 (9.58749 iter/s, 10.4303s/100 iters), loss = 0.021879
I1006 12:40:02.280050  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0218787 (* 1 = 0.0218787 loss)
I1006 12:40:02.280057  2824 sgd_solver.cpp:105] Iteration 51000, lr = 0.01
I1006 12:40:10.658172  2824 solver.cpp:218] Iteration 51100 (11.9359 iter/s, 8.37809s/100 iters), loss = 0.0285756
I1006 12:40:10.658202  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0285752 (* 1 = 0.0285752 loss)
I1006 12:40:10.658218  2824 sgd_solver.cpp:105] Iteration 51100, lr = 0.01
I1006 12:40:19.049168  2824 solver.cpp:218] Iteration 51200 (11.9176 iter/s, 8.39094s/100 iters), loss = 0.0133213
I1006 12:40:19.049274  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0133209 (* 1 = 0.0133209 loss)
I1006 12:40:19.049283  2824 sgd_solver.cpp:105] Iteration 51200, lr = 0.01
I1006 12:40:27.434267  2824 solver.cpp:218] Iteration 51300 (11.9261 iter/s, 8.38498s/100 iters), loss = 0.0105259
I1006 12:40:27.434298  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0105256 (* 1 = 0.0105256 loss)
I1006 12:40:27.434314  2824 sgd_solver.cpp:105] Iteration 51300, lr = 0.01
I1006 12:40:35.823401  2824 solver.cpp:218] Iteration 51400 (11.9203 iter/s, 8.38908s/100 iters), loss = 0.0117985
I1006 12:40:35.823431  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117981 (* 1 = 0.0117981 loss)
I1006 12:40:35.823436  2824 sgd_solver.cpp:105] Iteration 51400, lr = 0.01
I1006 12:40:43.788522  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:40:44.126067  2824 solver.cpp:330] Iteration 51500, Testing net (#0)
I1006 12:40:46.083546  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:40:46.165336  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9119
I1006 12:40:46.165372  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.327922 (* 1 = 0.327922 loss)
I1006 12:40:46.249716  2824 solver.cpp:218] Iteration 51500 (9.59117 iter/s, 10.4263s/100 iters), loss = 0.0212383
I1006 12:40:46.249739  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212379 (* 1 = 0.0212379 loss)
I1006 12:40:46.249745  2824 sgd_solver.cpp:105] Iteration 51500, lr = 0.01
I1006 12:40:54.629104  2824 solver.cpp:218] Iteration 51600 (11.9341 iter/s, 8.37934s/100 iters), loss = 0.00801691
I1006 12:40:54.629261  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00801652 (* 1 = 0.00801652 loss)
I1006 12:40:54.629278  2824 sgd_solver.cpp:105] Iteration 51600, lr = 0.01
I1006 12:41:03.009207  2824 solver.cpp:218] Iteration 51700 (11.9333 iter/s, 8.37992s/100 iters), loss = 0.0054649
I1006 12:41:03.009248  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00546451 (* 1 = 0.00546451 loss)
I1006 12:41:03.009254  2824 sgd_solver.cpp:105] Iteration 51700, lr = 0.01
I1006 12:41:11.387985  2824 solver.cpp:218] Iteration 51800 (11.935 iter/s, 8.37871s/100 iters), loss = 0.022181
I1006 12:41:11.388025  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0221807 (* 1 = 0.0221807 loss)
I1006 12:41:11.388031  2824 sgd_solver.cpp:105] Iteration 51800, lr = 0.01
I1006 12:41:19.871246  2824 solver.cpp:218] Iteration 51900 (11.788 iter/s, 8.48319s/100 iters), loss = 0.00468109
I1006 12:41:19.871278  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00468071 (* 1 = 0.00468071 loss)
I1006 12:41:19.871284  2824 sgd_solver.cpp:105] Iteration 51900, lr = 0.01
I1006 12:41:27.900187  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:41:28.235554  2824 solver.cpp:330] Iteration 52000, Testing net (#0)
I1006 12:41:30.194730  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:41:30.276188  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9107
I1006 12:41:30.276224  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.334542 (* 1 = 0.334542 loss)
I1006 12:41:30.360128  2824 solver.cpp:218] Iteration 52000 (9.53396 iter/s, 10.4888s/100 iters), loss = 0.00529376
I1006 12:41:30.360155  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00529337 (* 1 = 0.00529337 loss)
I1006 12:41:30.360162  2824 sgd_solver.cpp:105] Iteration 52000, lr = 0.01
I1006 12:41:38.736567  2824 solver.cpp:218] Iteration 52100 (11.9383 iter/s, 8.37638s/100 iters), loss = 0.00910523
I1006 12:41:38.736598  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00910484 (* 1 = 0.00910484 loss)
I1006 12:41:38.736603  2824 sgd_solver.cpp:105] Iteration 52100, lr = 0.01
I1006 12:41:47.116320  2824 solver.cpp:218] Iteration 52200 (11.9336 iter/s, 8.37969s/100 iters), loss = 0.07072
I1006 12:41:47.116350  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0707197 (* 1 = 0.0707197 loss)
I1006 12:41:47.116356  2824 sgd_solver.cpp:105] Iteration 52200, lr = 0.01
I1006 12:41:55.486671  2824 solver.cpp:218] Iteration 52300 (11.947 iter/s, 8.37029s/100 iters), loss = 0.0159848
I1006 12:41:55.486702  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159844 (* 1 = 0.0159844 loss)
I1006 12:41:55.486718  2824 sgd_solver.cpp:105] Iteration 52300, lr = 0.01
I1006 12:42:03.865752  2824 solver.cpp:218] Iteration 52400 (11.9346 iter/s, 8.37902s/100 iters), loss = 0.00423858
I1006 12:42:03.865854  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00423818 (* 1 = 0.00423818 loss)
I1006 12:42:03.865860  2824 sgd_solver.cpp:105] Iteration 52400, lr = 0.01
I1006 12:42:11.823889  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:42:12.159330  2824 solver.cpp:330] Iteration 52500, Testing net (#0)
I1006 12:42:14.117099  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:42:14.200776  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9093
I1006 12:42:14.200811  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.34238 (* 1 = 0.34238 loss)
I1006 12:42:14.283968  2824 solver.cpp:218] Iteration 52500 (9.59869 iter/s, 10.4181s/100 iters), loss = 0.0108548
I1006 12:42:14.283995  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0108543 (* 1 = 0.0108543 loss)
I1006 12:42:14.284001  2824 sgd_solver.cpp:105] Iteration 52500, lr = 0.01
I1006 12:42:22.652411  2824 solver.cpp:218] Iteration 52600 (11.9497 iter/s, 8.36839s/100 iters), loss = 0.0062524
I1006 12:42:22.652441  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.006252 (* 1 = 0.006252 loss)
I1006 12:42:22.652448  2824 sgd_solver.cpp:105] Iteration 52600, lr = 0.01
I1006 12:42:31.028667  2824 solver.cpp:218] Iteration 52700 (11.9386 iter/s, 8.37619s/100 iters), loss = 0.0383743
I1006 12:42:31.028698  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0383739 (* 1 = 0.0383739 loss)
I1006 12:42:31.028714  2824 sgd_solver.cpp:105] Iteration 52700, lr = 0.01
I1006 12:42:39.402653  2824 solver.cpp:218] Iteration 52800 (11.9418 iter/s, 8.37393s/100 iters), loss = 0.0194259
I1006 12:42:39.402788  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194255 (* 1 = 0.0194255 loss)
I1006 12:42:39.402796  2824 sgd_solver.cpp:105] Iteration 52800, lr = 0.01
I1006 12:42:47.779353  2824 solver.cpp:218] Iteration 52900 (11.9381 iter/s, 8.37654s/100 iters), loss = 0.0145188
I1006 12:42:47.779384  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0145184 (* 1 = 0.0145184 loss)
I1006 12:42:47.779400  2824 sgd_solver.cpp:105] Iteration 52900, lr = 0.01
I1006 12:42:55.739996  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:42:56.075696  2824 solver.cpp:330] Iteration 53000, Testing net (#0)
I1006 12:42:58.034255  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:42:58.115381  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9063
I1006 12:42:58.115417  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355045 (* 1 = 0.355045 loss)
I1006 12:42:58.198952  2824 solver.cpp:218] Iteration 53000 (9.59736 iter/s, 10.4195s/100 iters), loss = 0.013118
I1006 12:42:58.198979  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0131176 (* 1 = 0.0131176 loss)
I1006 12:42:58.198985  2824 sgd_solver.cpp:105] Iteration 53000, lr = 0.01
I1006 12:43:06.584573  2824 solver.cpp:218] Iteration 53100 (11.9253 iter/s, 8.38557s/100 iters), loss = 0.0136261
I1006 12:43:06.584605  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0136257 (* 1 = 0.0136257 loss)
I1006 12:43:06.584611  2824 sgd_solver.cpp:105] Iteration 53100, lr = 0.01
I1006 12:43:14.970619  2824 solver.cpp:218] Iteration 53200 (11.9247 iter/s, 8.38598s/100 iters), loss = 0.00523425
I1006 12:43:14.970732  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00523384 (* 1 = 0.00523384 loss)
I1006 12:43:14.970738  2824 sgd_solver.cpp:105] Iteration 53200, lr = 0.01
I1006 12:43:23.347631  2824 solver.cpp:218] Iteration 53300 (11.9376 iter/s, 8.37688s/100 iters), loss = 0.00964218
I1006 12:43:23.347673  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00964178 (* 1 = 0.00964178 loss)
I1006 12:43:23.347679  2824 sgd_solver.cpp:105] Iteration 53300, lr = 0.01
I1006 12:43:31.727757  2824 solver.cpp:218] Iteration 53400 (11.9331 iter/s, 8.38006s/100 iters), loss = 0.00283085
I1006 12:43:31.727799  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00283045 (* 1 = 0.00283045 loss)
I1006 12:43:31.727804  2824 sgd_solver.cpp:105] Iteration 53400, lr = 0.01
I1006 12:43:39.692986  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:43:40.028127  2824 solver.cpp:330] Iteration 53500, Testing net (#0)
I1006 12:43:41.986335  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:43:42.068007  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9084
I1006 12:43:42.068042  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358559 (* 1 = 0.358559 loss)
I1006 12:43:42.152356  2824 solver.cpp:218] Iteration 53500 (9.59276 iter/s, 10.4245s/100 iters), loss = 0.0120868
I1006 12:43:42.152382  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0120864 (* 1 = 0.0120864 loss)
I1006 12:43:42.152389  2824 sgd_solver.cpp:105] Iteration 53500, lr = 0.01
I1006 12:43:50.524096  2824 solver.cpp:218] Iteration 53600 (11.945 iter/s, 8.37169s/100 iters), loss = 0.0118179
I1006 12:43:50.524271  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0118175 (* 1 = 0.0118175 loss)
I1006 12:43:50.524278  2824 sgd_solver.cpp:105] Iteration 53600, lr = 0.01
I1006 12:43:58.900524  2824 solver.cpp:218] Iteration 53700 (11.9385 iter/s, 8.37624s/100 iters), loss = 0.0206337
I1006 12:43:58.900565  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0206333 (* 1 = 0.0206333 loss)
I1006 12:43:58.900571  2824 sgd_solver.cpp:105] Iteration 53700, lr = 0.01
I1006 12:44:07.278990  2824 solver.cpp:218] Iteration 53800 (11.9355 iter/s, 8.3784s/100 iters), loss = 0.00824661
I1006 12:44:07.279021  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00824622 (* 1 = 0.00824622 loss)
I1006 12:44:07.279026  2824 sgd_solver.cpp:105] Iteration 53800, lr = 0.01
I1006 12:44:15.671180  2824 solver.cpp:218] Iteration 53900 (11.9159 iter/s, 8.39213s/100 iters), loss = 0.00734501
I1006 12:44:15.671221  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00734462 (* 1 = 0.00734462 loss)
I1006 12:44:15.671227  2824 sgd_solver.cpp:105] Iteration 53900, lr = 0.01
I1006 12:44:23.632525  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:44:23.968726  2824 solver.cpp:330] Iteration 54000, Testing net (#0)
I1006 12:44:25.927662  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:44:26.009728  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9125
I1006 12:44:26.009763  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332733 (* 1 = 0.332733 loss)
I1006 12:44:26.092922  2824 solver.cpp:218] Iteration 54000 (9.59539 iter/s, 10.4217s/100 iters), loss = 0.0515018
I1006 12:44:26.092948  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0515014 (* 1 = 0.0515014 loss)
I1006 12:44:26.092954  2824 sgd_solver.cpp:105] Iteration 54000, lr = 0.01
I1006 12:44:34.465948  2824 solver.cpp:218] Iteration 54100 (11.9432 iter/s, 8.37297s/100 iters), loss = 0.00457741
I1006 12:44:34.465979  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00457701 (* 1 = 0.00457701 loss)
I1006 12:44:34.465986  2824 sgd_solver.cpp:105] Iteration 54100, lr = 0.01
I1006 12:44:42.848593  2824 solver.cpp:218] Iteration 54200 (11.9295 iter/s, 8.38259s/100 iters), loss = 0.0123802
I1006 12:44:42.848634  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123799 (* 1 = 0.0123799 loss)
I1006 12:44:42.848640  2824 sgd_solver.cpp:105] Iteration 54200, lr = 0.01
I1006 12:44:51.232772  2824 solver.cpp:218] Iteration 54300 (11.9273 iter/s, 8.38411s/100 iters), loss = 0.0151549
I1006 12:44:51.232803  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0151545 (* 1 = 0.0151545 loss)
I1006 12:44:51.232820  2824 sgd_solver.cpp:105] Iteration 54300, lr = 0.01
I1006 12:44:59.619560  2824 solver.cpp:218] Iteration 54400 (11.9236 iter/s, 8.38673s/100 iters), loss = 0.0612189
I1006 12:44:59.619693  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0612185 (* 1 = 0.0612185 loss)
I1006 12:44:59.619700  2824 sgd_solver.cpp:105] Iteration 54400, lr = 0.01
I1006 12:45:07.577714  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:45:07.913697  2824 solver.cpp:330] Iteration 54500, Testing net (#0)
I1006 12:45:09.873685  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:45:09.955261  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9134
I1006 12:45:09.955296  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.34736 (* 1 = 0.34736 loss)
I1006 12:45:10.039069  2824 solver.cpp:218] Iteration 54500 (9.59752 iter/s, 10.4194s/100 iters), loss = 0.00236216
I1006 12:45:10.039096  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00236176 (* 1 = 0.00236176 loss)
I1006 12:45:10.039103  2824 sgd_solver.cpp:105] Iteration 54500, lr = 0.01
I1006 12:45:18.420228  2824 solver.cpp:218] Iteration 54600 (11.9316 iter/s, 8.3811s/100 iters), loss = 0.00580807
I1006 12:45:18.420259  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00580768 (* 1 = 0.00580768 loss)
I1006 12:45:18.420265  2824 sgd_solver.cpp:105] Iteration 54600, lr = 0.01
I1006 12:45:26.805099  2824 solver.cpp:218] Iteration 54700 (11.9263 iter/s, 8.38481s/100 iters), loss = 0.00476394
I1006 12:45:26.805140  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00476355 (* 1 = 0.00476355 loss)
I1006 12:45:26.805146  2824 sgd_solver.cpp:105] Iteration 54700, lr = 0.01
I1006 12:45:35.184782  2824 solver.cpp:218] Iteration 54800 (11.9337 iter/s, 8.37961s/100 iters), loss = 0.00250861
I1006 12:45:35.184943  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00250822 (* 1 = 0.00250822 loss)
I1006 12:45:35.184952  2824 sgd_solver.cpp:105] Iteration 54800, lr = 0.01
I1006 12:45:43.570933  2824 solver.cpp:218] Iteration 54900 (11.9247 iter/s, 8.38598s/100 iters), loss = 0.012201
I1006 12:45:43.570976  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0122006 (* 1 = 0.0122006 loss)
I1006 12:45:43.570981  2824 sgd_solver.cpp:105] Iteration 54900, lr = 0.01
I1006 12:45:51.527600  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:45:51.862828  2824 solver.cpp:330] Iteration 55000, Testing net (#0)
I1006 12:45:53.821394  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:45:53.903192  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9072
I1006 12:45:53.903226  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3652 (* 1 = 0.3652 loss)
I1006 12:45:53.987258  2824 solver.cpp:218] Iteration 55000 (9.60037 iter/s, 10.4163s/100 iters), loss = 0.00746891
I1006 12:45:53.987282  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00746852 (* 1 = 0.00746852 loss)
I1006 12:45:53.987289  2824 sgd_solver.cpp:105] Iteration 55000, lr = 0.01
I1006 12:46:02.368510  2824 solver.cpp:218] Iteration 55100 (11.9315 iter/s, 8.3812s/100 iters), loss = 0.00500307
I1006 12:46:02.368552  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00500268 (* 1 = 0.00500268 loss)
I1006 12:46:02.368558  2824 sgd_solver.cpp:105] Iteration 55100, lr = 0.01
I1006 12:46:10.747138  2824 solver.cpp:218] Iteration 55200 (11.9352 iter/s, 8.37856s/100 iters), loss = 0.0214782
I1006 12:46:10.747278  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0214778 (* 1 = 0.0214778 loss)
I1006 12:46:10.747285  2824 sgd_solver.cpp:105] Iteration 55200, lr = 0.01
I1006 12:46:19.119824  2824 solver.cpp:218] Iteration 55300 (11.9438 iter/s, 8.37253s/100 iters), loss = 0.0289258
I1006 12:46:19.119856  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0289254 (* 1 = 0.0289254 loss)
I1006 12:46:19.119861  2824 sgd_solver.cpp:105] Iteration 55300, lr = 0.01
I1006 12:46:27.496654  2824 solver.cpp:218] Iteration 55400 (11.9378 iter/s, 8.37677s/100 iters), loss = 0.0369777
I1006 12:46:27.496685  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0369773 (* 1 = 0.0369773 loss)
I1006 12:46:27.496691  2824 sgd_solver.cpp:105] Iteration 55400, lr = 0.01
I1006 12:46:35.457062  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:46:35.791848  2824 solver.cpp:330] Iteration 55500, Testing net (#0)
I1006 12:46:37.748456  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:46:37.830708  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9086
I1006 12:46:37.830742  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.347521 (* 1 = 0.347521 loss)
I1006 12:46:37.914657  2824 solver.cpp:218] Iteration 55500 (9.59883 iter/s, 10.4179s/100 iters), loss = 0.0110872
I1006 12:46:37.914683  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110868 (* 1 = 0.0110868 loss)
I1006 12:46:37.914690  2824 sgd_solver.cpp:105] Iteration 55500, lr = 0.01
I1006 12:46:46.290375  2824 solver.cpp:218] Iteration 55600 (11.9394 iter/s, 8.37566s/100 iters), loss = 0.0323736
I1006 12:46:46.290443  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0323732 (* 1 = 0.0323732 loss)
I1006 12:46:46.290460  2824 sgd_solver.cpp:105] Iteration 55600, lr = 0.01
I1006 12:46:54.672209  2824 solver.cpp:218] Iteration 55700 (11.9307 iter/s, 8.38174s/100 iters), loss = 0.00403737
I1006 12:46:54.672251  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00403697 (* 1 = 0.00403697 loss)
I1006 12:46:54.672257  2824 sgd_solver.cpp:105] Iteration 55700, lr = 0.01
I1006 12:47:03.040431  2824 solver.cpp:218] Iteration 55800 (11.9501 iter/s, 8.36815s/100 iters), loss = 0.0052529
I1006 12:47:03.040472  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00525251 (* 1 = 0.00525251 loss)
I1006 12:47:03.040477  2824 sgd_solver.cpp:105] Iteration 55800, lr = 0.01
I1006 12:47:11.416749  2824 solver.cpp:218] Iteration 55900 (11.9385 iter/s, 8.37625s/100 iters), loss = 0.0235618
I1006 12:47:11.416790  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0235614 (* 1 = 0.0235614 loss)
I1006 12:47:11.416795  2824 sgd_solver.cpp:105] Iteration 55900, lr = 0.01
I1006 12:47:19.371289  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:47:19.707842  2824 solver.cpp:330] Iteration 56000, Testing net (#0)
I1006 12:47:21.665074  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:47:21.747236  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9127
I1006 12:47:21.747272  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.349811 (* 1 = 0.349811 loss)
I1006 12:47:21.831605  2824 solver.cpp:218] Iteration 56000 (9.60174 iter/s, 10.4148s/100 iters), loss = 0.00334609
I1006 12:47:21.831630  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0033457 (* 1 = 0.0033457 loss)
I1006 12:47:21.831637  2824 sgd_solver.cpp:105] Iteration 56000, lr = 0.01
I1006 12:47:30.215298  2824 solver.cpp:218] Iteration 56100 (11.928 iter/s, 8.38364s/100 iters), loss = 0.00694819
I1006 12:47:30.215329  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0069478 (* 1 = 0.0069478 loss)
I1006 12:47:30.215335  2824 sgd_solver.cpp:105] Iteration 56100, lr = 0.01
I1006 12:47:38.596457  2824 solver.cpp:218] Iteration 56200 (11.9316 iter/s, 8.3811s/100 iters), loss = 0.027477
I1006 12:47:38.596499  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0274766 (* 1 = 0.0274766 loss)
I1006 12:47:38.596505  2824 sgd_solver.cpp:105] Iteration 56200, lr = 0.01
I1006 12:47:46.971185  2824 solver.cpp:218] Iteration 56300 (11.9408 iter/s, 8.37466s/100 iters), loss = 0.0293835
I1006 12:47:46.971233  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0293831 (* 1 = 0.0293831 loss)
I1006 12:47:46.971240  2824 sgd_solver.cpp:105] Iteration 56300, lr = 0.01
I1006 12:47:55.349959  2824 solver.cpp:218] Iteration 56400 (11.935 iter/s, 8.3787s/100 iters), loss = 0.00583185
I1006 12:47:55.350062  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00583146 (* 1 = 0.00583146 loss)
I1006 12:47:55.350069  2824 sgd_solver.cpp:105] Iteration 56400, lr = 0.01
I1006 12:48:03.310984  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:48:03.646785  2824 solver.cpp:330] Iteration 56500, Testing net (#0)
I1006 12:48:05.606076  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:48:05.687724  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9144
I1006 12:48:05.687759  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.346491 (* 1 = 0.346491 loss)
I1006 12:48:05.771848  2824 solver.cpp:218] Iteration 56500 (9.59531 iter/s, 10.4218s/100 iters), loss = 0.00362966
I1006 12:48:05.771874  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00362926 (* 1 = 0.00362926 loss)
I1006 12:48:05.771881  2824 sgd_solver.cpp:105] Iteration 56500, lr = 0.01
I1006 12:48:14.155411  2824 solver.cpp:218] Iteration 56600 (11.9282 iter/s, 8.38351s/100 iters), loss = 0.00681417
I1006 12:48:14.155441  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00681377 (* 1 = 0.00681377 loss)
I1006 12:48:14.155447  2824 sgd_solver.cpp:105] Iteration 56600, lr = 0.01
I1006 12:48:22.540688  2824 solver.cpp:218] Iteration 56700 (11.9257 iter/s, 8.38522s/100 iters), loss = 0.0212223
I1006 12:48:22.540729  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212219 (* 1 = 0.0212219 loss)
I1006 12:48:22.540735  2824 sgd_solver.cpp:105] Iteration 56700, lr = 0.01
I1006 12:48:30.919422  2824 solver.cpp:218] Iteration 56800 (11.9351 iter/s, 8.37867s/100 iters), loss = 0.0035981
I1006 12:48:30.919543  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0035977 (* 1 = 0.0035977 loss)
I1006 12:48:30.919559  2824 sgd_solver.cpp:105] Iteration 56800, lr = 0.01
I1006 12:48:39.304431  2824 solver.cpp:218] Iteration 56900 (11.9263 iter/s, 8.38486s/100 iters), loss = 0.0109159
I1006 12:48:39.304472  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109155 (* 1 = 0.0109155 loss)
I1006 12:48:39.304477  2824 sgd_solver.cpp:105] Iteration 56900, lr = 0.01
I1006 12:48:47.267835  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:48:47.603771  2824 solver.cpp:330] Iteration 57000, Testing net (#0)
I1006 12:48:49.562341  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:48:49.644481  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9072
I1006 12:48:49.644516  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.366532 (* 1 = 0.366532 loss)
I1006 12:48:49.728622  2824 solver.cpp:218] Iteration 57000 (9.59314 iter/s, 10.4241s/100 iters), loss = 0.00325976
I1006 12:48:49.728647  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00325936 (* 1 = 0.00325936 loss)
I1006 12:48:49.728653  2824 sgd_solver.cpp:105] Iteration 57000, lr = 0.01
I1006 12:48:58.100316  2824 solver.cpp:218] Iteration 57100 (11.9451 iter/s, 8.37164s/100 iters), loss = 0.00940105
I1006 12:48:58.100347  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00940064 (* 1 = 0.00940064 loss)
I1006 12:48:58.100353  2824 sgd_solver.cpp:105] Iteration 57100, lr = 0.01
I1006 12:49:06.482295  2824 solver.cpp:218] Iteration 57200 (11.9304 iter/s, 8.38192s/100 iters), loss = 0.00359906
I1006 12:49:06.482415  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00359866 (* 1 = 0.00359866 loss)
I1006 12:49:06.482434  2824 sgd_solver.cpp:105] Iteration 57200, lr = 0.01
I1006 12:49:14.847435  2824 solver.cpp:218] Iteration 57300 (11.9546 iter/s, 8.36499s/100 iters), loss = 0.0059971
I1006 12:49:14.847465  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00599671 (* 1 = 0.00599671 loss)
I1006 12:49:14.847471  2824 sgd_solver.cpp:105] Iteration 57300, lr = 0.01
I1006 12:49:23.221851  2824 solver.cpp:218] Iteration 57400 (11.9412 iter/s, 8.37436s/100 iters), loss = 0.0027253
I1006 12:49:23.221884  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00272491 (* 1 = 0.00272491 loss)
I1006 12:49:23.221899  2824 sgd_solver.cpp:105] Iteration 57400, lr = 0.01
I1006 12:49:31.183812  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:49:31.519454  2824 solver.cpp:330] Iteration 57500, Testing net (#0)
I1006 12:49:33.477537  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:49:33.559320  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9115
I1006 12:49:33.559355  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.345889 (* 1 = 0.345889 loss)
I1006 12:49:33.643242  2824 solver.cpp:218] Iteration 57500 (9.59571 iter/s, 10.4213s/100 iters), loss = 0.00945496
I1006 12:49:33.643270  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00945457 (* 1 = 0.00945457 loss)
I1006 12:49:33.643275  2824 sgd_solver.cpp:105] Iteration 57500, lr = 0.01
I1006 12:49:42.019551  2824 solver.cpp:218] Iteration 57600 (11.9385 iter/s, 8.37626s/100 iters), loss = 0.00633454
I1006 12:49:42.019655  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00633415 (* 1 = 0.00633415 loss)
I1006 12:49:42.019672  2824 sgd_solver.cpp:105] Iteration 57600, lr = 0.01
I1006 12:49:50.399945  2824 solver.cpp:218] Iteration 57700 (11.9328 iter/s, 8.38026s/100 iters), loss = 0.0183733
I1006 12:49:50.399984  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.018373 (* 1 = 0.018373 loss)
I1006 12:49:50.399991  2824 sgd_solver.cpp:105] Iteration 57700, lr = 0.01
I1006 12:49:58.780503  2824 solver.cpp:218] Iteration 57800 (11.9325 iter/s, 8.38049s/100 iters), loss = 0.0138482
I1006 12:49:58.780544  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0138479 (* 1 = 0.0138479 loss)
I1006 12:49:58.780550  2824 sgd_solver.cpp:105] Iteration 57800, lr = 0.01
I1006 12:50:07.162443  2824 solver.cpp:218] Iteration 57900 (11.9305 iter/s, 8.38187s/100 iters), loss = 0.0070219
I1006 12:50:07.162474  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00702151 (* 1 = 0.00702151 loss)
I1006 12:50:07.162480  2824 sgd_solver.cpp:105] Iteration 57900, lr = 0.01
I1006 12:50:15.120615  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:50:15.456519  2824 solver.cpp:330] Iteration 58000, Testing net (#0)
I1006 12:50:17.414284  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:50:17.496193  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9096
I1006 12:50:17.496229  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.374999 (* 1 = 0.374999 loss)
I1006 12:50:17.580039  2824 solver.cpp:218] Iteration 58000 (9.59921 iter/s, 10.4175s/100 iters), loss = 0.00652125
I1006 12:50:17.580065  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00652087 (* 1 = 0.00652087 loss)
I1006 12:50:17.580071  2824 sgd_solver.cpp:105] Iteration 58000, lr = 0.01
I1006 12:50:25.963012  2824 solver.cpp:218] Iteration 58100 (11.929 iter/s, 8.38292s/100 iters), loss = 0.0052565
I1006 12:50:25.963043  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00525613 (* 1 = 0.00525613 loss)
I1006 12:50:25.963049  2824 sgd_solver.cpp:105] Iteration 58100, lr = 0.01
I1006 12:50:34.337777  2824 solver.cpp:218] Iteration 58200 (11.9407 iter/s, 8.37471s/100 iters), loss = 0.0140966
I1006 12:50:34.337815  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140962 (* 1 = 0.0140962 loss)
I1006 12:50:34.337821  2824 sgd_solver.cpp:105] Iteration 58200, lr = 0.01
I1006 12:50:42.706239  2824 solver.cpp:218] Iteration 58300 (11.9497 iter/s, 8.3684s/100 iters), loss = 0.00663379
I1006 12:50:42.706270  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0066334 (* 1 = 0.0066334 loss)
I1006 12:50:42.706276  2824 sgd_solver.cpp:105] Iteration 58300, lr = 0.01
I1006 12:50:51.082957  2824 solver.cpp:218] Iteration 58400 (11.9379 iter/s, 8.37666s/100 iters), loss = 0.00593566
I1006 12:50:51.083093  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00593527 (* 1 = 0.00593527 loss)
I1006 12:50:51.083101  2824 sgd_solver.cpp:105] Iteration 58400, lr = 0.01
I1006 12:50:59.034087  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:50:59.368733  2824 solver.cpp:330] Iteration 58500, Testing net (#0)
I1006 12:51:01.327121  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:51:01.408897  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9115
I1006 12:51:01.408934  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.344942 (* 1 = 0.344942 loss)
I1006 12:51:01.492709  2824 solver.cpp:218] Iteration 58500 (9.60652 iter/s, 10.4096s/100 iters), loss = 0.0201614
I1006 12:51:01.492735  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.020161 (* 1 = 0.020161 loss)
I1006 12:51:01.492741  2824 sgd_solver.cpp:105] Iteration 58500, lr = 0.01
I1006 12:51:09.870836  2824 solver.cpp:218] Iteration 58600 (11.9359 iter/s, 8.37807s/100 iters), loss = 0.00458645
I1006 12:51:09.870867  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00458607 (* 1 = 0.00458607 loss)
I1006 12:51:09.870873  2824 sgd_solver.cpp:105] Iteration 58600, lr = 0.01
I1006 12:51:18.249264  2824 solver.cpp:218] Iteration 58700 (11.9355 iter/s, 8.37837s/100 iters), loss = 0.0172708
I1006 12:51:18.249310  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0172704 (* 1 = 0.0172704 loss)
I1006 12:51:18.249317  2824 sgd_solver.cpp:105] Iteration 58700, lr = 0.01
I1006 12:51:26.624944  2824 solver.cpp:218] Iteration 58800 (11.9394 iter/s, 8.37561s/100 iters), loss = 0.0134717
I1006 12:51:26.625072  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0134713 (* 1 = 0.0134713 loss)
I1006 12:51:26.625080  2824 sgd_solver.cpp:105] Iteration 58800, lr = 0.01
I1006 12:51:35.009466  2824 solver.cpp:218] Iteration 58900 (11.9269 iter/s, 8.38438s/100 iters), loss = 0.0142685
I1006 12:51:35.009507  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142682 (* 1 = 0.0142682 loss)
I1006 12:51:35.009513  2824 sgd_solver.cpp:105] Iteration 58900, lr = 0.01
I1006 12:51:42.973863  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:51:43.308941  2824 solver.cpp:330] Iteration 59000, Testing net (#0)
I1006 12:51:45.268880  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:51:45.351174  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9142
I1006 12:51:45.351209  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.347023 (* 1 = 0.347023 loss)
I1006 12:51:45.435717  2824 solver.cpp:218] Iteration 59000 (9.59124 iter/s, 10.4262s/100 iters), loss = 0.00983473
I1006 12:51:45.435742  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00983435 (* 1 = 0.00983435 loss)
I1006 12:51:45.435750  2824 sgd_solver.cpp:105] Iteration 59000, lr = 0.01
I1006 12:51:53.812521  2824 solver.cpp:218] Iteration 59100 (11.9378 iter/s, 8.37675s/100 iters), loss = 0.0432932
I1006 12:51:53.812562  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0432928 (* 1 = 0.0432928 loss)
I1006 12:51:53.812568  2824 sgd_solver.cpp:105] Iteration 59100, lr = 0.01
I1006 12:52:02.190172  2824 solver.cpp:218] Iteration 59200 (11.9366 iter/s, 8.37758s/100 iters), loss = 0.0457304
I1006 12:52:02.190322  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.04573 (* 1 = 0.04573 loss)
I1006 12:52:02.190330  2824 sgd_solver.cpp:105] Iteration 59200, lr = 0.01
I1006 12:52:10.565116  2824 solver.cpp:218] Iteration 59300 (11.9406 iter/s, 8.37477s/100 iters), loss = 0.00546139
I1006 12:52:10.565165  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00546101 (* 1 = 0.00546101 loss)
I1006 12:52:10.565171  2824 sgd_solver.cpp:105] Iteration 59300, lr = 0.01
I1006 12:52:18.943938  2824 solver.cpp:218] Iteration 59400 (11.935 iter/s, 8.37875s/100 iters), loss = 0.00998598
I1006 12:52:18.943980  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0099856 (* 1 = 0.0099856 loss)
I1006 12:52:18.943985  2824 sgd_solver.cpp:105] Iteration 59400, lr = 0.01
I1006 12:52:26.905478  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:52:27.241300  2824 solver.cpp:330] Iteration 59500, Testing net (#0)
I1006 12:52:29.198462  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:52:29.280378  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9067
I1006 12:52:29.280413  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.379153 (* 1 = 0.379153 loss)
I1006 12:52:29.364524  2824 solver.cpp:218] Iteration 59500 (9.59646 iter/s, 10.4205s/100 iters), loss = 0.00142276
I1006 12:52:29.364549  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00142238 (* 1 = 0.00142238 loss)
I1006 12:52:29.364557  2824 sgd_solver.cpp:105] Iteration 59500, lr = 0.01
I1006 12:52:37.753800  2824 solver.cpp:218] Iteration 59600 (11.9201 iter/s, 8.38922s/100 iters), loss = 0.00471158
I1006 12:52:37.753909  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0047112 (* 1 = 0.0047112 loss)
I1006 12:52:37.753917  2824 sgd_solver.cpp:105] Iteration 59600, lr = 0.01
I1006 12:52:46.137820  2824 solver.cpp:218] Iteration 59700 (11.9276 iter/s, 8.38389s/100 iters), loss = 0.00812557
I1006 12:52:46.137850  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00812519 (* 1 = 0.00812519 loss)
I1006 12:52:46.137856  2824 sgd_solver.cpp:105] Iteration 59700, lr = 0.01
I1006 12:52:54.515485  2824 solver.cpp:218] Iteration 59800 (11.9366 iter/s, 8.37761s/100 iters), loss = 0.0048368
I1006 12:52:54.515532  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00483641 (* 1 = 0.00483641 loss)
I1006 12:52:54.515539  2824 sgd_solver.cpp:105] Iteration 59800, lr = 0.01
I1006 12:53:02.905256  2824 solver.cpp:218] Iteration 59900 (11.9194 iter/s, 8.3897s/100 iters), loss = 0.00897471
I1006 12:53:02.905297  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00897433 (* 1 = 0.00897433 loss)
I1006 12:53:02.905303  2824 sgd_solver.cpp:105] Iteration 59900, lr = 0.01
I1006 12:53:10.874776  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:53:11.211443  2824 solver.cpp:330] Iteration 60000, Testing net (#0)
I1006 12:53:13.169363  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:53:13.251171  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9168
I1006 12:53:13.251206  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316125 (* 1 = 0.316125 loss)
I1006 12:53:13.335260  2824 solver.cpp:218] Iteration 60000 (9.58779 iter/s, 10.4299s/100 iters), loss = 0.0125664
I1006 12:53:13.335286  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.012566 (* 1 = 0.012566 loss)
I1006 12:53:13.335294  2824 sgd_solver.cpp:105] Iteration 60000, lr = 0.01
I1006 12:53:21.720090  2824 solver.cpp:218] Iteration 60100 (11.9264 iter/s, 8.38477s/100 iters), loss = 0.00436406
I1006 12:53:21.720121  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00436367 (* 1 = 0.00436367 loss)
I1006 12:53:21.720127  2824 sgd_solver.cpp:105] Iteration 60100, lr = 0.01
I1006 12:53:30.107949  2824 solver.cpp:218] Iteration 60200 (11.9221 iter/s, 8.3878s/100 iters), loss = 0.0025757
I1006 12:53:30.107990  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00257531 (* 1 = 0.00257531 loss)
I1006 12:53:30.107995  2824 sgd_solver.cpp:105] Iteration 60200, lr = 0.01
I1006 12:53:38.497467  2824 solver.cpp:218] Iteration 60300 (11.9197 iter/s, 8.38945s/100 iters), loss = 0.00314392
I1006 12:53:38.497508  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00314353 (* 1 = 0.00314353 loss)
I1006 12:53:38.497514  2824 sgd_solver.cpp:105] Iteration 60300, lr = 0.01
I1006 12:53:46.891281  2824 solver.cpp:218] Iteration 60400 (11.9136 iter/s, 8.39375s/100 iters), loss = 0.00520987
I1006 12:53:46.891384  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00520947 (* 1 = 0.00520947 loss)
I1006 12:53:46.891391  2824 sgd_solver.cpp:105] Iteration 60400, lr = 0.01
I1006 12:53:54.856653  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:53:55.192778  2824 solver.cpp:330] Iteration 60500, Testing net (#0)
I1006 12:53:57.151516  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:53:57.233333  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9148
I1006 12:53:57.233359  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332804 (* 1 = 0.332804 loss)
I1006 12:53:57.317925  2824 solver.cpp:218] Iteration 60500 (9.59093 iter/s, 10.4265s/100 iters), loss = 0.0017261
I1006 12:53:57.317952  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0017257 (* 1 = 0.0017257 loss)
I1006 12:53:57.317960  2824 sgd_solver.cpp:105] Iteration 60500, lr = 0.01
I1006 12:54:05.700415  2824 solver.cpp:218] Iteration 60600 (11.9297 iter/s, 8.38243s/100 iters), loss = 0.00601127
I1006 12:54:05.700458  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00601087 (* 1 = 0.00601087 loss)
I1006 12:54:05.700464  2824 sgd_solver.cpp:105] Iteration 60600, lr = 0.01
I1006 12:54:14.078548  2824 solver.cpp:218] Iteration 60700 (11.9359 iter/s, 8.37806s/100 iters), loss = 0.0109057
I1006 12:54:14.078590  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109053 (* 1 = 0.0109053 loss)
I1006 12:54:14.078595  2824 sgd_solver.cpp:105] Iteration 60700, lr = 0.01
I1006 12:54:22.456199  2824 solver.cpp:218] Iteration 60800 (11.9366 iter/s, 8.37758s/100 iters), loss = 0.00363672
I1006 12:54:22.456311  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00363632 (* 1 = 0.00363632 loss)
I1006 12:54:22.456327  2824 sgd_solver.cpp:105] Iteration 60800, lr = 0.01
I1006 12:54:30.837604  2824 solver.cpp:218] Iteration 60900 (11.9314 iter/s, 8.38128s/100 iters), loss = 0.00426727
I1006 12:54:30.837646  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00426687 (* 1 = 0.00426687 loss)
I1006 12:54:30.837651  2824 sgd_solver.cpp:105] Iteration 60900, lr = 0.01
I1006 12:54:38.798491  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:54:39.133656  2824 solver.cpp:330] Iteration 61000, Testing net (#0)
I1006 12:54:41.092603  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:54:41.174212  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9128
I1006 12:54:41.174248  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355513 (* 1 = 0.355513 loss)
I1006 12:54:41.258034  2824 solver.cpp:218] Iteration 61000 (9.5966 iter/s, 10.4204s/100 iters), loss = 0.00330708
I1006 12:54:41.258060  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00330668 (* 1 = 0.00330668 loss)
I1006 12:54:41.258066  2824 sgd_solver.cpp:105] Iteration 61000, lr = 0.01
I1006 12:54:49.642616  2824 solver.cpp:218] Iteration 61100 (11.9267 iter/s, 8.38453s/100 iters), loss = 0.00199242
I1006 12:54:49.642657  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00199201 (* 1 = 0.00199201 loss)
I1006 12:54:49.642663  2824 sgd_solver.cpp:105] Iteration 61100, lr = 0.01
I1006 12:54:58.030398  2824 solver.cpp:218] Iteration 61200 (11.9222 iter/s, 8.38771s/100 iters), loss = 0.0345844
I1006 12:54:58.030475  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.034584 (* 1 = 0.034584 loss)
I1006 12:54:58.030483  2824 sgd_solver.cpp:105] Iteration 61200, lr = 0.01
I1006 12:55:06.414974  2824 solver.cpp:218] Iteration 61300 (11.9268 iter/s, 8.38447s/100 iters), loss = 0.00691264
I1006 12:55:06.415014  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00691224 (* 1 = 0.00691224 loss)
I1006 12:55:06.415019  2824 sgd_solver.cpp:105] Iteration 61300, lr = 0.01
I1006 12:55:14.797646  2824 solver.cpp:218] Iteration 61400 (11.9295 iter/s, 8.38261s/100 iters), loss = 0.0025553
I1006 12:55:14.797688  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00255489 (* 1 = 0.00255489 loss)
I1006 12:55:14.797693  2824 sgd_solver.cpp:105] Iteration 61400, lr = 0.01
I1006 12:55:22.759917  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:55:23.095896  2824 solver.cpp:330] Iteration 61500, Testing net (#0)
I1006 12:55:25.054594  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:55:25.136358  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9125
I1006 12:55:25.136394  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.352509 (* 1 = 0.352509 loss)
I1006 12:55:25.219976  2824 solver.cpp:218] Iteration 61500 (9.59485 iter/s, 10.4223s/100 iters), loss = 0.0157213
I1006 12:55:25.220001  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0157209 (* 1 = 0.0157209 loss)
I1006 12:55:25.220008  2824 sgd_solver.cpp:105] Iteration 61500, lr = 0.01
I1006 12:55:33.610219  2824 solver.cpp:218] Iteration 61600 (11.9187 iter/s, 8.39019s/100 iters), loss = 0.00591312
I1006 12:55:33.610355  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00591272 (* 1 = 0.00591272 loss)
I1006 12:55:33.610363  2824 sgd_solver.cpp:105] Iteration 61600, lr = 0.01
I1006 12:55:41.993551  2824 solver.cpp:218] Iteration 61700 (11.9287 iter/s, 8.38317s/100 iters), loss = 0.00430228
I1006 12:55:41.993592  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00430188 (* 1 = 0.00430188 loss)
I1006 12:55:41.993598  2824 sgd_solver.cpp:105] Iteration 61700, lr = 0.01
I1006 12:55:50.376281  2824 solver.cpp:218] Iteration 61800 (11.9294 iter/s, 8.38266s/100 iters), loss = 0.0253839
I1006 12:55:50.376322  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0253835 (* 1 = 0.0253835 loss)
I1006 12:55:50.376327  2824 sgd_solver.cpp:105] Iteration 61800, lr = 0.01
I1006 12:55:58.761741  2824 solver.cpp:218] Iteration 61900 (11.9255 iter/s, 8.38539s/100 iters), loss = 0.00918162
I1006 12:55:58.761781  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00918122 (* 1 = 0.00918122 loss)
I1006 12:55:58.761787  2824 sgd_solver.cpp:105] Iteration 61900, lr = 0.01
I1006 12:56:06.728741  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:56:07.064328  2824 solver.cpp:330] Iteration 62000, Testing net (#0)
I1006 12:56:09.022562  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:56:09.104307  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9097
I1006 12:56:09.104344  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.367686 (* 1 = 0.367686 loss)
I1006 12:56:09.188318  2824 solver.cpp:218] Iteration 62000 (9.59094 iter/s, 10.4265s/100 iters), loss = 0.00512607
I1006 12:56:09.188344  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00512567 (* 1 = 0.00512567 loss)
I1006 12:56:09.188351  2824 sgd_solver.cpp:105] Iteration 62000, lr = 0.01
I1006 12:56:17.575103  2824 solver.cpp:218] Iteration 62100 (11.9236 iter/s, 8.38673s/100 iters), loss = 0.00897148
I1006 12:56:17.575135  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00897108 (* 1 = 0.00897108 loss)
I1006 12:56:17.575151  2824 sgd_solver.cpp:105] Iteration 62100, lr = 0.01
I1006 12:56:25.961869  2824 solver.cpp:218] Iteration 62200 (11.9236 iter/s, 8.38671s/100 iters), loss = 0.00387975
I1006 12:56:25.961901  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00387935 (* 1 = 0.00387935 loss)
I1006 12:56:25.961917  2824 sgd_solver.cpp:105] Iteration 62200, lr = 0.01
I1006 12:56:34.344431  2824 solver.cpp:218] Iteration 62300 (11.9296 iter/s, 8.3825s/100 iters), loss = 0.00700271
I1006 12:56:34.344462  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00700231 (* 1 = 0.00700231 loss)
I1006 12:56:34.344477  2824 sgd_solver.cpp:105] Iteration 62300, lr = 0.01
I1006 12:56:42.728412  2824 solver.cpp:218] Iteration 62400 (11.9276 iter/s, 8.38392s/100 iters), loss = 0.00455431
I1006 12:56:42.728504  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00455391 (* 1 = 0.00455391 loss)
I1006 12:56:42.728519  2824 sgd_solver.cpp:105] Iteration 62400, lr = 0.01
I1006 12:56:50.694106  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:56:51.030120  2824 solver.cpp:330] Iteration 62500, Testing net (#0)
I1006 12:56:52.986471  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:56:53.068562  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9072
I1006 12:56:53.068599  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.390215 (* 1 = 0.390215 loss)
I1006 12:56:53.152510  2824 solver.cpp:218] Iteration 62500 (9.59326 iter/s, 10.424s/100 iters), loss = 0.00475451
I1006 12:56:53.152537  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0047541 (* 1 = 0.0047541 loss)
I1006 12:56:53.152544  2824 sgd_solver.cpp:105] Iteration 62500, lr = 0.01
I1006 12:57:01.542426  2824 solver.cpp:218] Iteration 62600 (11.9191 iter/s, 8.38986s/100 iters), loss = 0.00110647
I1006 12:57:01.542459  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00110606 (* 1 = 0.00110606 loss)
I1006 12:57:01.542474  2824 sgd_solver.cpp:105] Iteration 62600, lr = 0.01
I1006 12:57:09.926337  2824 solver.cpp:218] Iteration 62700 (11.9277 iter/s, 8.38385s/100 iters), loss = 0.00681214
I1006 12:57:09.926368  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00681174 (* 1 = 0.00681174 loss)
I1006 12:57:09.926384  2824 sgd_solver.cpp:105] Iteration 62700, lr = 0.01
I1006 12:57:18.310585  2824 solver.cpp:218] Iteration 62800 (11.9272 iter/s, 8.38419s/100 iters), loss = 0.0101991
I1006 12:57:18.310689  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0101987 (* 1 = 0.0101987 loss)
I1006 12:57:18.310706  2824 sgd_solver.cpp:105] Iteration 62800, lr = 0.01
I1006 12:57:26.701141  2824 solver.cpp:218] Iteration 62900 (11.9183 iter/s, 8.39044s/100 iters), loss = 0.00124347
I1006 12:57:26.701172  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00124307 (* 1 = 0.00124307 loss)
I1006 12:57:26.701187  2824 sgd_solver.cpp:105] Iteration 62900, lr = 0.01
I1006 12:57:34.666724  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:57:35.002434  2824 solver.cpp:330] Iteration 63000, Testing net (#0)
I1006 12:57:36.961186  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:57:37.043033  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9114
I1006 12:57:37.043069  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358697 (* 1 = 0.358697 loss)
I1006 12:57:37.126994  2824 solver.cpp:218] Iteration 63000 (9.5916 iter/s, 10.4258s/100 iters), loss = 0.00448247
I1006 12:57:37.127022  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00448207 (* 1 = 0.00448207 loss)
I1006 12:57:37.127027  2824 sgd_solver.cpp:105] Iteration 63000, lr = 0.01
I1006 12:57:45.515738  2824 solver.cpp:218] Iteration 63100 (11.9208 iter/s, 8.38869s/100 iters), loss = 0.0028661
I1006 12:57:45.515769  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00286569 (* 1 = 0.00286569 loss)
I1006 12:57:45.515785  2824 sgd_solver.cpp:105] Iteration 63100, lr = 0.01
I1006 12:57:53.909166  2824 solver.cpp:218] Iteration 63200 (11.9142 iter/s, 8.39337s/100 iters), loss = 0.0139982
I1006 12:57:53.909252  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0139978 (* 1 = 0.0139978 loss)
I1006 12:57:53.909268  2824 sgd_solver.cpp:105] Iteration 63200, lr = 0.01
I1006 12:58:02.295781  2824 solver.cpp:218] Iteration 63300 (11.9239 iter/s, 8.38651s/100 iters), loss = 0.00351302
I1006 12:58:02.295812  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00351262 (* 1 = 0.00351262 loss)
I1006 12:58:02.295827  2824 sgd_solver.cpp:105] Iteration 63300, lr = 0.01
I1006 12:58:10.681669  2824 solver.cpp:218] Iteration 63400 (11.9249 iter/s, 8.38583s/100 iters), loss = 0.00345699
I1006 12:58:10.681699  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00345658 (* 1 = 0.00345658 loss)
I1006 12:58:10.681706  2824 sgd_solver.cpp:105] Iteration 63400, lr = 0.01
I1006 12:58:18.648484  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:58:18.984000  2824 solver.cpp:330] Iteration 63500, Testing net (#0)
I1006 12:58:20.944262  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:58:21.025986  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9102
I1006 12:58:21.026022  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.370424 (* 1 = 0.370424 loss)
I1006 12:58:21.109529  2824 solver.cpp:218] Iteration 63500 (9.58975 iter/s, 10.4278s/100 iters), loss = 0.00345683
I1006 12:58:21.109555  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00345643 (* 1 = 0.00345643 loss)
I1006 12:58:21.109562  2824 sgd_solver.cpp:105] Iteration 63500, lr = 0.01
I1006 12:58:29.497347  2824 solver.cpp:218] Iteration 63600 (11.9221 iter/s, 8.38776s/100 iters), loss = 0.0175401
I1006 12:58:29.497500  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175397 (* 1 = 0.0175397 loss)
I1006 12:58:29.497509  2824 sgd_solver.cpp:105] Iteration 63600, lr = 0.01
I1006 12:58:37.878645  2824 solver.cpp:218] Iteration 63700 (11.9316 iter/s, 8.38112s/100 iters), loss = 0.00388313
I1006 12:58:37.878679  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00388273 (* 1 = 0.00388273 loss)
I1006 12:58:37.878693  2824 sgd_solver.cpp:105] Iteration 63700, lr = 0.01
I1006 12:58:46.258592  2824 solver.cpp:218] Iteration 63800 (11.9333 iter/s, 8.37989s/100 iters), loss = 0.00243482
I1006 12:58:46.258623  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00243441 (* 1 = 0.00243441 loss)
I1006 12:58:46.258640  2824 sgd_solver.cpp:105] Iteration 63800, lr = 0.01
I1006 12:58:54.638185  2824 solver.cpp:218] Iteration 63900 (11.9338 iter/s, 8.37953s/100 iters), loss = 0.00543182
I1006 12:58:54.638214  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00543142 (* 1 = 0.00543142 loss)
I1006 12:58:54.638231  2824 sgd_solver.cpp:105] Iteration 63900, lr = 0.01
I1006 12:59:02.603574  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:59:02.939360  2824 solver.cpp:330] Iteration 64000, Testing net (#0)
I1006 12:59:04.896932  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:59:04.978602  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9101
I1006 12:59:04.978638  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.379727 (* 1 = 0.379727 loss)
I1006 12:59:05.063159  2824 solver.cpp:218] Iteration 64000 (9.59241 iter/s, 10.4249s/100 iters), loss = 0.00497022
I1006 12:59:05.063189  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00496982 (* 1 = 0.00496982 loss)
I1006 12:59:05.063195  2824 sgd_solver.cpp:105] Iteration 64000, lr = 0.01
I1006 12:59:13.450213  2824 solver.cpp:218] Iteration 64100 (11.9232 iter/s, 8.387s/100 iters), loss = 0.00293369
I1006 12:59:13.450244  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00293328 (* 1 = 0.00293328 loss)
I1006 12:59:13.450250  2824 sgd_solver.cpp:105] Iteration 64100, lr = 0.01
I1006 12:59:21.833808  2824 solver.cpp:218] Iteration 64200 (11.9281 iter/s, 8.38354s/100 iters), loss = 0.0054946
I1006 12:59:21.833839  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00549419 (* 1 = 0.00549419 loss)
I1006 12:59:21.833854  2824 sgd_solver.cpp:105] Iteration 64200, lr = 0.01
I1006 12:59:30.217809  2824 solver.cpp:218] Iteration 64300 (11.9276 iter/s, 8.38394s/100 iters), loss = 0.00823922
I1006 12:59:30.217841  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00823881 (* 1 = 0.00823881 loss)
I1006 12:59:30.217847  2824 sgd_solver.cpp:105] Iteration 64300, lr = 0.01
I1006 12:59:38.602607  2824 solver.cpp:218] Iteration 64400 (11.9264 iter/s, 8.38474s/100 iters), loss = 0.00157513
I1006 12:59:38.602715  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00157471 (* 1 = 0.00157471 loss)
I1006 12:59:38.602722  2824 sgd_solver.cpp:105] Iteration 64400, lr = 0.01
I1006 12:59:46.565821  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:59:46.902302  2824 solver.cpp:330] Iteration 64500, Testing net (#0)
I1006 12:59:48.859413  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 12:59:48.941243  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9077
I1006 12:59:48.941279  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.367247 (* 1 = 0.367247 loss)
I1006 12:59:49.025358  2824 solver.cpp:218] Iteration 64500 (9.59452 iter/s, 10.4226s/100 iters), loss = 0.0110735
I1006 12:59:49.025384  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110731 (* 1 = 0.0110731 loss)
I1006 12:59:49.025391  2824 sgd_solver.cpp:105] Iteration 64500, lr = 0.01
I1006 12:59:57.412302  2824 solver.cpp:218] Iteration 64600 (11.9234 iter/s, 8.38689s/100 iters), loss = 0.0429319
I1006 12:59:57.412333  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0429315 (* 1 = 0.0429315 loss)
I1006 12:59:57.412338  2824 sgd_solver.cpp:105] Iteration 64600, lr = 0.01
I1006 13:00:05.804435  2824 solver.cpp:218] Iteration 64700 (11.916 iter/s, 8.39208s/100 iters), loss = 0.0053474
I1006 13:00:05.804466  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00534698 (* 1 = 0.00534698 loss)
I1006 13:00:05.804481  2824 sgd_solver.cpp:105] Iteration 64700, lr = 0.01
I1006 13:00:14.187774  2824 solver.cpp:218] Iteration 64800 (11.9285 iter/s, 8.38328s/100 iters), loss = 0.0167964
I1006 13:00:14.187937  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.016796 (* 1 = 0.016796 loss)
I1006 13:00:14.187945  2824 sgd_solver.cpp:105] Iteration 64800, lr = 0.01
I1006 13:00:22.578855  2824 solver.cpp:218] Iteration 64900 (11.9177 iter/s, 8.39089s/100 iters), loss = 0.00292372
I1006 13:00:22.578887  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0029233 (* 1 = 0.0029233 loss)
I1006 13:00:22.578902  2824 sgd_solver.cpp:105] Iteration 64900, lr = 0.01
I1006 13:00:30.541111  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:00:30.878458  2824 solver.cpp:330] Iteration 65000, Testing net (#0)
I1006 13:00:32.836752  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:00:32.918438  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9099
I1006 13:00:32.918474  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.378098 (* 1 = 0.378098 loss)
I1006 13:00:33.003005  2824 solver.cpp:218] Iteration 65000 (9.59317 iter/s, 10.4241s/100 iters), loss = 0.00304627
I1006 13:00:33.003031  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00304585 (* 1 = 0.00304585 loss)
I1006 13:00:33.003038  2824 sgd_solver.cpp:105] Iteration 65000, lr = 0.01
I1006 13:00:41.392668  2824 solver.cpp:218] Iteration 65100 (11.9195 iter/s, 8.38961s/100 iters), loss = 0.00729449
I1006 13:00:41.392699  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00729407 (* 1 = 0.00729407 loss)
I1006 13:00:41.392715  2824 sgd_solver.cpp:105] Iteration 65100, lr = 0.01
I1006 13:00:49.775560  2824 solver.cpp:218] Iteration 65200 (11.9291 iter/s, 8.38284s/100 iters), loss = 0.0172957
I1006 13:00:49.775674  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0172952 (* 1 = 0.0172952 loss)
I1006 13:00:49.775681  2824 sgd_solver.cpp:105] Iteration 65200, lr = 0.01
I1006 13:00:58.154846  2824 solver.cpp:218] Iteration 65300 (11.9344 iter/s, 8.37915s/100 iters), loss = 0.00543432
I1006 13:00:58.154878  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00543391 (* 1 = 0.00543391 loss)
I1006 13:00:58.154883  2824 sgd_solver.cpp:105] Iteration 65300, lr = 0.01
I1006 13:01:06.542048  2824 solver.cpp:218] Iteration 65400 (11.923 iter/s, 8.38714s/100 iters), loss = 0.0144097
I1006 13:01:06.542080  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144093 (* 1 = 0.0144093 loss)
I1006 13:01:06.542096  2824 sgd_solver.cpp:105] Iteration 65400, lr = 0.01
I1006 13:01:14.504568  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:01:14.840672  2824 solver.cpp:330] Iteration 65500, Testing net (#0)
I1006 13:01:16.800108  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:01:16.881731  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9144
I1006 13:01:16.881765  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.344492 (* 1 = 0.344492 loss)
I1006 13:01:16.965898  2824 solver.cpp:218] Iteration 65500 (9.59344 iter/s, 10.4238s/100 iters), loss = 0.00308995
I1006 13:01:16.965924  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00308953 (* 1 = 0.00308953 loss)
I1006 13:01:16.965931  2824 sgd_solver.cpp:105] Iteration 65500, lr = 0.01
I1006 13:01:25.352924  2824 solver.cpp:218] Iteration 65600 (11.9233 iter/s, 8.38697s/100 iters), loss = 0.0026783
I1006 13:01:25.353066  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00267789 (* 1 = 0.00267789 loss)
I1006 13:01:25.353075  2824 sgd_solver.cpp:105] Iteration 65600, lr = 0.01
I1006 13:01:33.746701  2824 solver.cpp:218] Iteration 65700 (11.9138 iter/s, 8.39361s/100 iters), loss = 0.00439466
I1006 13:01:33.746733  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00439424 (* 1 = 0.00439424 loss)
I1006 13:01:33.746749  2824 sgd_solver.cpp:105] Iteration 65700, lr = 0.01
I1006 13:01:42.132233  2824 solver.cpp:218] Iteration 65800 (11.9254 iter/s, 8.38547s/100 iters), loss = 0.0251041
I1006 13:01:42.132264  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0251036 (* 1 = 0.0251036 loss)
I1006 13:01:42.132280  2824 sgd_solver.cpp:105] Iteration 65800, lr = 0.01
I1006 13:01:50.522979  2824 solver.cpp:218] Iteration 65900 (11.918 iter/s, 8.39069s/100 iters), loss = 0.0143882
I1006 13:01:50.523010  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0143878 (* 1 = 0.0143878 loss)
I1006 13:01:50.523025  2824 sgd_solver.cpp:105] Iteration 65900, lr = 0.01
I1006 13:01:58.492157  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:01:58.828286  2824 solver.cpp:330] Iteration 66000, Testing net (#0)
I1006 13:02:00.787297  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:02:00.869119  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9144
I1006 13:02:00.869156  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.368709 (* 1 = 0.368709 loss)
I1006 13:02:00.953320  2824 solver.cpp:218] Iteration 66000 (9.58747 iter/s, 10.4303s/100 iters), loss = 0.012731
I1006 13:02:00.953346  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0127305 (* 1 = 0.0127305 loss)
I1006 13:02:00.953353  2824 sgd_solver.cpp:105] Iteration 66000, lr = 0.01
I1006 13:02:09.346187  2824 solver.cpp:218] Iteration 66100 (11.915 iter/s, 8.39281s/100 iters), loss = 0.00420791
I1006 13:02:09.346220  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00420748 (* 1 = 0.00420748 loss)
I1006 13:02:09.346226  2824 sgd_solver.cpp:105] Iteration 66100, lr = 0.01
I1006 13:02:17.734349  2824 solver.cpp:218] Iteration 66200 (11.9216 iter/s, 8.3881s/100 iters), loss = 0.0323749
I1006 13:02:17.734380  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0323745 (* 1 = 0.0323745 loss)
I1006 13:02:17.734386  2824 sgd_solver.cpp:105] Iteration 66200, lr = 0.01
I1006 13:02:26.124059  2824 solver.cpp:218] Iteration 66300 (11.9194 iter/s, 8.38965s/100 iters), loss = 0.00544005
I1006 13:02:26.124091  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00543963 (* 1 = 0.00543963 loss)
I1006 13:02:26.124097  2824 sgd_solver.cpp:105] Iteration 66300, lr = 0.01
I1006 13:02:34.511802  2824 solver.cpp:218] Iteration 66400 (11.9222 iter/s, 8.38768s/100 iters), loss = 0.003524
I1006 13:02:34.511907  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00352358 (* 1 = 0.00352358 loss)
I1006 13:02:34.511924  2824 sgd_solver.cpp:105] Iteration 66400, lr = 0.01
I1006 13:02:42.484427  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:02:42.819813  2824 solver.cpp:330] Iteration 66500, Testing net (#0)
I1006 13:02:44.777570  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:02:44.859302  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9099
I1006 13:02:44.859338  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385391 (* 1 = 0.385391 loss)
I1006 13:02:44.943435  2824 solver.cpp:218] Iteration 66500 (9.58635 iter/s, 10.4315s/100 iters), loss = 0.00673888
I1006 13:02:44.943462  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00673846 (* 1 = 0.00673846 loss)
I1006 13:02:44.943469  2824 sgd_solver.cpp:105] Iteration 66500, lr = 0.01
I1006 13:02:53.323060  2824 solver.cpp:218] Iteration 66600 (11.9338 iter/s, 8.37957s/100 iters), loss = 0.00469388
I1006 13:02:53.323092  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00469347 (* 1 = 0.00469347 loss)
I1006 13:02:53.323108  2824 sgd_solver.cpp:105] Iteration 66600, lr = 0.01
I1006 13:03:01.711673  2824 solver.cpp:218] Iteration 66700 (11.921 iter/s, 8.38855s/100 iters), loss = 0.00305889
I1006 13:03:01.711704  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00305848 (* 1 = 0.00305848 loss)
I1006 13:03:01.711720  2824 sgd_solver.cpp:105] Iteration 66700, lr = 0.01
I1006 13:03:10.092329  2824 solver.cpp:218] Iteration 66800 (11.9323 iter/s, 8.3806s/100 iters), loss = 0.00403884
I1006 13:03:10.092442  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00403843 (* 1 = 0.00403843 loss)
I1006 13:03:10.092449  2824 sgd_solver.cpp:105] Iteration 66800, lr = 0.01
I1006 13:03:18.474277  2824 solver.cpp:218] Iteration 66900 (11.9306 iter/s, 8.38182s/100 iters), loss = 0.00494306
I1006 13:03:18.474308  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00494265 (* 1 = 0.00494265 loss)
I1006 13:03:18.474324  2824 sgd_solver.cpp:105] Iteration 66900, lr = 0.01
I1006 13:03:26.439803  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:03:26.775312  2824 solver.cpp:330] Iteration 67000, Testing net (#0)
I1006 13:03:28.731654  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:03:28.813590  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9163
I1006 13:03:28.813616  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.33803 (* 1 = 0.33803 loss)
I1006 13:03:28.897219  2824 solver.cpp:218] Iteration 67000 (9.59428 iter/s, 10.4229s/100 iters), loss = 0.00626614
I1006 13:03:28.897245  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00626573 (* 1 = 0.00626573 loss)
I1006 13:03:28.897253  2824 sgd_solver.cpp:105] Iteration 67000, lr = 0.01
I1006 13:03:37.282968  2824 solver.cpp:218] Iteration 67100 (11.9251 iter/s, 8.38569s/100 iters), loss = 0.00656088
I1006 13:03:37.282999  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00656047 (* 1 = 0.00656047 loss)
I1006 13:03:37.283005  2824 sgd_solver.cpp:105] Iteration 67100, lr = 0.01
I1006 13:03:45.664719  2824 solver.cpp:218] Iteration 67200 (11.9308 iter/s, 8.38169s/100 iters), loss = 0.00418072
I1006 13:03:45.664788  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00418031 (* 1 = 0.00418031 loss)
I1006 13:03:45.664804  2824 sgd_solver.cpp:105] Iteration 67200, lr = 0.01
I1006 13:03:54.042343  2824 solver.cpp:218] Iteration 67300 (11.9367 iter/s, 8.37754s/100 iters), loss = 0.00633335
I1006 13:03:54.042376  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00633294 (* 1 = 0.00633294 loss)
I1006 13:03:54.042392  2824 sgd_solver.cpp:105] Iteration 67300, lr = 0.01
I1006 13:04:02.430495  2824 solver.cpp:218] Iteration 67400 (11.9217 iter/s, 8.38809s/100 iters), loss = 0.00153409
I1006 13:04:02.430526  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00153368 (* 1 = 0.00153368 loss)
I1006 13:04:02.430531  2824 sgd_solver.cpp:105] Iteration 67400, lr = 0.01
I1006 13:04:10.397560  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:04:10.734035  2824 solver.cpp:330] Iteration 67500, Testing net (#0)
I1006 13:04:12.692425  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:04:12.774171  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9129
I1006 13:04:12.774207  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35523 (* 1 = 0.35523 loss)
I1006 13:04:12.858062  2824 solver.cpp:218] Iteration 67500 (9.59002 iter/s, 10.4275s/100 iters), loss = 0.00279151
I1006 13:04:12.858086  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0027911 (* 1 = 0.0027911 loss)
I1006 13:04:12.858093  2824 sgd_solver.cpp:105] Iteration 67500, lr = 0.01
I1006 13:04:21.246163  2824 solver.cpp:218] Iteration 67600 (11.9217 iter/s, 8.38805s/100 iters), loss = 0.00786329
I1006 13:04:21.246268  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00786288 (* 1 = 0.00786288 loss)
I1006 13:04:21.246275  2824 sgd_solver.cpp:105] Iteration 67600, lr = 0.01
I1006 13:04:29.629184  2824 solver.cpp:218] Iteration 67700 (11.929 iter/s, 8.3829s/100 iters), loss = 0.00368439
I1006 13:04:29.629226  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00368398 (* 1 = 0.00368398 loss)
I1006 13:04:29.629232  2824 sgd_solver.cpp:105] Iteration 67700, lr = 0.01
I1006 13:04:38.009574  2824 solver.cpp:218] Iteration 67800 (11.9327 iter/s, 8.38032s/100 iters), loss = 0.00993077
I1006 13:04:38.009616  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00993035 (* 1 = 0.00993035 loss)
I1006 13:04:38.009621  2824 sgd_solver.cpp:105] Iteration 67800, lr = 0.01
I1006 13:04:46.391772  2824 solver.cpp:218] Iteration 67900 (11.9301 iter/s, 8.38213s/100 iters), loss = 0.00220231
I1006 13:04:46.391814  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0022019 (* 1 = 0.0022019 loss)
I1006 13:04:46.391820  2824 sgd_solver.cpp:105] Iteration 67900, lr = 0.01
I1006 13:04:54.352358  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:04:54.687681  2824 solver.cpp:330] Iteration 68000, Testing net (#0)
I1006 13:04:56.650236  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:04:56.732174  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9132
I1006 13:04:56.732210  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.364201 (* 1 = 0.364201 loss)
I1006 13:04:56.815969  2824 solver.cpp:218] Iteration 68000 (9.59313 iter/s, 10.4241s/100 iters), loss = 0.0247014
I1006 13:04:56.815995  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0247009 (* 1 = 0.0247009 loss)
I1006 13:04:56.816002  2824 sgd_solver.cpp:105] Iteration 68000, lr = 0.01
I1006 13:05:05.195891  2824 solver.cpp:218] Iteration 68100 (11.9334 iter/s, 8.37987s/100 iters), loss = 0.00940416
I1006 13:05:05.195922  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00940374 (* 1 = 0.00940374 loss)
I1006 13:05:05.195930  2824 sgd_solver.cpp:105] Iteration 68100, lr = 0.01
I1006 13:05:13.571502  2824 solver.cpp:218] Iteration 68200 (11.9395 iter/s, 8.37555s/100 iters), loss = 0.00673489
I1006 13:05:13.571543  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00673447 (* 1 = 0.00673447 loss)
I1006 13:05:13.571549  2824 sgd_solver.cpp:105] Iteration 68200, lr = 0.01
I1006 13:05:21.950079  2824 solver.cpp:218] Iteration 68300 (11.9353 iter/s, 8.37851s/100 iters), loss = 0.00291488
I1006 13:05:21.950120  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00291445 (* 1 = 0.00291445 loss)
I1006 13:05:21.950126  2824 sgd_solver.cpp:105] Iteration 68300, lr = 0.01
I1006 13:05:30.327742  2824 solver.cpp:218] Iteration 68400 (11.9366 iter/s, 8.37759s/100 iters), loss = 0.00162929
I1006 13:05:30.327848  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00162887 (* 1 = 0.00162887 loss)
I1006 13:05:30.327857  2824 sgd_solver.cpp:105] Iteration 68400, lr = 0.01
I1006 13:05:38.291864  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:05:38.626832  2824 solver.cpp:330] Iteration 68500, Testing net (#0)
I1006 13:05:40.585878  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:05:40.667826  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9058
I1006 13:05:40.667862  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.406287 (* 1 = 0.406287 loss)
I1006 13:05:40.751982  2824 solver.cpp:218] Iteration 68500 (9.59315 iter/s, 10.4241s/100 iters), loss = 0.000874628
I1006 13:05:40.752010  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000874205 (* 1 = 0.000874205 loss)
I1006 13:05:40.752017  2824 sgd_solver.cpp:105] Iteration 68500, lr = 0.01
I1006 13:05:49.143802  2824 solver.cpp:218] Iteration 68600 (11.9164 iter/s, 8.39176s/100 iters), loss = 0.00283287
I1006 13:05:49.143834  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00283245 (* 1 = 0.00283245 loss)
I1006 13:05:49.143841  2824 sgd_solver.cpp:105] Iteration 68600, lr = 0.01
I1006 13:05:57.538014  2824 solver.cpp:218] Iteration 68700 (11.9131 iter/s, 8.39415s/100 iters), loss = 0.00953477
I1006 13:05:57.538045  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00953434 (* 1 = 0.00953434 loss)
I1006 13:05:57.538053  2824 sgd_solver.cpp:105] Iteration 68700, lr = 0.01
I1006 13:06:05.925005  2824 solver.cpp:218] Iteration 68800 (11.9233 iter/s, 8.38693s/100 iters), loss = 0.0084775
I1006 13:06:05.925140  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00847707 (* 1 = 0.00847707 loss)
I1006 13:06:05.925158  2824 sgd_solver.cpp:105] Iteration 68800, lr = 0.01
I1006 13:06:14.317929  2824 solver.cpp:218] Iteration 68900 (11.915 iter/s, 8.39277s/100 iters), loss = 0.00764971
I1006 13:06:14.317970  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00764928 (* 1 = 0.00764928 loss)
I1006 13:06:14.317976  2824 sgd_solver.cpp:105] Iteration 68900, lr = 0.01
I1006 13:06:22.287313  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:06:22.622648  2824 solver.cpp:330] Iteration 69000, Testing net (#0)
I1006 13:06:24.581202  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:06:24.663373  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9101
I1006 13:06:24.663408  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.364493 (* 1 = 0.364493 loss)
I1006 13:06:24.747619  2824 solver.cpp:218] Iteration 69000 (9.58808 iter/s, 10.4296s/100 iters), loss = 0.00268758
I1006 13:06:24.747645  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00268715 (* 1 = 0.00268715 loss)
I1006 13:06:24.747653  2824 sgd_solver.cpp:105] Iteration 69000, lr = 0.01
I1006 13:06:33.123453  2824 solver.cpp:218] Iteration 69100 (11.9392 iter/s, 8.37578s/100 iters), loss = 0.0150281
I1006 13:06:33.123484  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0150277 (* 1 = 0.0150277 loss)
I1006 13:06:33.123491  2824 sgd_solver.cpp:105] Iteration 69100, lr = 0.01
I1006 13:06:41.498548  2824 solver.cpp:218] Iteration 69200 (11.9402 iter/s, 8.37504s/100 iters), loss = 0.0131646
I1006 13:06:41.498661  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0131642 (* 1 = 0.0131642 loss)
I1006 13:06:41.498679  2824 sgd_solver.cpp:105] Iteration 69200, lr = 0.01
I1006 13:06:49.866950  2824 solver.cpp:218] Iteration 69300 (11.9499 iter/s, 8.36826s/100 iters), loss = 0.00369359
I1006 13:06:49.866991  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00369315 (* 1 = 0.00369315 loss)
I1006 13:06:49.866997  2824 sgd_solver.cpp:105] Iteration 69300, lr = 0.01
I1006 13:06:58.243408  2824 solver.cpp:218] Iteration 69400 (11.9383 iter/s, 8.37639s/100 iters), loss = 0.00244779
I1006 13:06:58.243449  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00244735 (* 1 = 0.00244735 loss)
I1006 13:06:58.243455  2824 sgd_solver.cpp:105] Iteration 69400, lr = 0.01
I1006 13:07:06.197435  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:07:06.533849  2824 solver.cpp:330] Iteration 69500, Testing net (#0)
I1006 13:07:08.490128  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:07:08.571899  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.91
I1006 13:07:08.571935  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38032 (* 1 = 0.38032 loss)
I1006 13:07:08.655751  2824 solver.cpp:218] Iteration 69500 (9.60405 iter/s, 10.4123s/100 iters), loss = 0.00999902
I1006 13:07:08.655777  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00999859 (* 1 = 0.00999859 loss)
I1006 13:07:08.655784  2824 sgd_solver.cpp:105] Iteration 69500, lr = 0.01
I1006 13:07:17.040616  2824 solver.cpp:218] Iteration 69600 (11.9263 iter/s, 8.38481s/100 iters), loss = 0.00799679
I1006 13:07:17.040727  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00799634 (* 1 = 0.00799634 loss)
I1006 13:07:17.040735  2824 sgd_solver.cpp:105] Iteration 69600, lr = 0.01
I1006 13:07:25.415935  2824 solver.cpp:218] Iteration 69700 (11.94 iter/s, 8.37518s/100 iters), loss = 0.00657956
I1006 13:07:25.415964  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00657912 (* 1 = 0.00657912 loss)
I1006 13:07:25.415971  2824 sgd_solver.cpp:105] Iteration 69700, lr = 0.01
I1006 13:07:33.787382  2824 solver.cpp:218] Iteration 69800 (11.9454 iter/s, 8.37139s/100 iters), loss = 0.00139575
I1006 13:07:33.787413  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00139531 (* 1 = 0.00139531 loss)
I1006 13:07:33.787420  2824 sgd_solver.cpp:105] Iteration 69800, lr = 0.01
I1006 13:07:42.165170  2824 solver.cpp:218] Iteration 69900 (11.9364 iter/s, 8.37773s/100 iters), loss = 0.00204451
I1006 13:07:42.165201  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00204407 (* 1 = 0.00204407 loss)
I1006 13:07:42.165206  2824 sgd_solver.cpp:105] Iteration 69900, lr = 0.01
I1006 13:07:50.120097  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:07:50.454555  2824 solver.cpp:330] Iteration 70000, Testing net (#0)
I1006 13:07:52.412991  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:07:52.494886  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9133
I1006 13:07:52.494921  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.348587 (* 1 = 0.348587 loss)
I1006 13:07:52.578625  2824 solver.cpp:218] Iteration 70000 (9.60302 iter/s, 10.4134s/100 iters), loss = 0.00232101
I1006 13:07:52.578652  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00232057 (* 1 = 0.00232057 loss)
I1006 13:07:52.578658  2824 sgd_solver.cpp:105] Iteration 70000, lr = 0.01
I1006 13:08:00.962568  2824 solver.cpp:218] Iteration 70100 (11.9276 iter/s, 8.38389s/100 iters), loss = 0.00255886
I1006 13:08:00.962599  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00255842 (* 1 = 0.00255842 loss)
I1006 13:08:00.962604  2824 sgd_solver.cpp:105] Iteration 70100, lr = 0.01
I1006 13:08:09.346201  2824 solver.cpp:218] Iteration 70200 (11.9281 iter/s, 8.38358s/100 iters), loss = 0.00660247
I1006 13:08:09.346243  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00660203 (* 1 = 0.00660203 loss)
I1006 13:08:09.346248  2824 sgd_solver.cpp:105] Iteration 70200, lr = 0.01
I1006 13:08:17.735263  2824 solver.cpp:218] Iteration 70300 (11.9204 iter/s, 8.38899s/100 iters), loss = 0.00118572
I1006 13:08:17.735306  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00118528 (* 1 = 0.00118528 loss)
I1006 13:08:17.735311  2824 sgd_solver.cpp:105] Iteration 70300, lr = 0.01
I1006 13:08:26.117285  2824 solver.cpp:218] Iteration 70400 (11.9304 iter/s, 8.38195s/100 iters), loss = 0.00083007
I1006 13:08:26.117375  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000829635 (* 1 = 0.000829635 loss)
I1006 13:08:26.117393  2824 sgd_solver.cpp:105] Iteration 70400, lr = 0.01
I1006 13:08:34.084064  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:08:34.419241  2824 solver.cpp:330] Iteration 70500, Testing net (#0)
I1006 13:08:36.377352  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:08:36.459951  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9184
I1006 13:08:36.459976  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355541 (* 1 = 0.355541 loss)
I1006 13:08:36.543409  2824 solver.cpp:218] Iteration 70500 (9.5914 iter/s, 10.426s/100 iters), loss = 0.00472302
I1006 13:08:36.543436  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00472258 (* 1 = 0.00472258 loss)
I1006 13:08:36.543442  2824 sgd_solver.cpp:105] Iteration 70500, lr = 0.01
I1006 13:08:44.923704  2824 solver.cpp:218] Iteration 70600 (11.9328 iter/s, 8.38024s/100 iters), loss = 0.00296764
I1006 13:08:44.923746  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0029672 (* 1 = 0.0029672 loss)
I1006 13:08:44.923751  2824 sgd_solver.cpp:105] Iteration 70600, lr = 0.01
I1006 13:08:53.299907  2824 solver.cpp:218] Iteration 70700 (11.9387 iter/s, 8.37614s/100 iters), loss = 0.00326661
I1006 13:08:53.299949  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00326617 (* 1 = 0.00326617 loss)
I1006 13:08:53.299955  2824 sgd_solver.cpp:105] Iteration 70700, lr = 0.01
I1006 13:09:01.672579  2824 solver.cpp:218] Iteration 70800 (11.9437 iter/s, 8.3726s/100 iters), loss = 0.00558855
I1006 13:09:01.672724  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00558811 (* 1 = 0.00558811 loss)
I1006 13:09:01.672731  2824 sgd_solver.cpp:105] Iteration 70800, lr = 0.01
I1006 13:09:10.046452  2824 solver.cpp:218] Iteration 70900 (11.9421 iter/s, 8.37371s/100 iters), loss = 0.00361666
I1006 13:09:10.046494  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00361622 (* 1 = 0.00361622 loss)
I1006 13:09:10.046499  2824 sgd_solver.cpp:105] Iteration 70900, lr = 0.01
I1006 13:09:18.008049  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:09:18.343384  2824 solver.cpp:330] Iteration 71000, Testing net (#0)
I1006 13:09:20.300784  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:09:20.383100  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9172
I1006 13:09:20.383136  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35162 (* 1 = 0.35162 loss)
I1006 13:09:20.467089  2824 solver.cpp:218] Iteration 71000 (9.59641 iter/s, 10.4206s/100 iters), loss = 0.000955425
I1006 13:09:20.467116  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000954987 (* 1 = 0.000954987 loss)
I1006 13:09:20.467123  2824 sgd_solver.cpp:105] Iteration 71000, lr = 0.01
I1006 13:09:28.851311  2824 solver.cpp:218] Iteration 71100 (11.9272 iter/s, 8.38417s/100 iters), loss = 0.00160124
I1006 13:09:28.851354  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0016008 (* 1 = 0.0016008 loss)
I1006 13:09:28.851359  2824 sgd_solver.cpp:105] Iteration 71100, lr = 0.01
I1006 13:09:37.231848  2824 solver.cpp:218] Iteration 71200 (11.9325 iter/s, 8.38047s/100 iters), loss = 0.00553434
I1006 13:09:37.231988  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00553389 (* 1 = 0.00553389 loss)
I1006 13:09:37.231997  2824 sgd_solver.cpp:105] Iteration 71200, lr = 0.01
I1006 13:09:45.612504  2824 solver.cpp:218] Iteration 71300 (11.9325 iter/s, 8.38049s/100 iters), loss = 0.00721489
I1006 13:09:45.612545  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00721445 (* 1 = 0.00721445 loss)
I1006 13:09:45.612551  2824 sgd_solver.cpp:105] Iteration 71300, lr = 0.01
I1006 13:09:53.994562  2824 solver.cpp:218] Iteration 71400 (11.9303 iter/s, 8.38199s/100 iters), loss = 0.0209554
I1006 13:09:53.994603  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.020955 (* 1 = 0.020955 loss)
I1006 13:09:53.994608  2824 sgd_solver.cpp:105] Iteration 71400, lr = 0.01
I1006 13:10:01.958685  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:10:02.294467  2824 solver.cpp:330] Iteration 71500, Testing net (#0)
I1006 13:10:04.252912  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:10:04.335117  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9103
I1006 13:10:04.335152  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.387242 (* 1 = 0.387242 loss)
I1006 13:10:04.419013  2824 solver.cpp:218] Iteration 71500 (9.5929 iter/s, 10.4244s/100 iters), loss = 0.00169665
I1006 13:10:04.419037  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0016962 (* 1 = 0.0016962 loss)
I1006 13:10:04.419044  2824 sgd_solver.cpp:105] Iteration 71500, lr = 0.01
I1006 13:10:12.805550  2824 solver.cpp:218] Iteration 71600 (11.9239 iter/s, 8.38648s/100 iters), loss = 0.0168216
I1006 13:10:12.805619  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168212 (* 1 = 0.0168212 loss)
I1006 13:10:12.805624  2824 sgd_solver.cpp:105] Iteration 71600, lr = 0.01
I1006 13:10:21.183972  2824 solver.cpp:218] Iteration 71700 (11.9356 iter/s, 8.37833s/100 iters), loss = 0.00510186
I1006 13:10:21.184013  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00510142 (* 1 = 0.00510142 loss)
I1006 13:10:21.184018  2824 sgd_solver.cpp:105] Iteration 71700, lr = 0.01
I1006 13:10:29.568919  2824 solver.cpp:218] Iteration 71800 (11.9262 iter/s, 8.38488s/100 iters), loss = 0.00264179
I1006 13:10:29.568960  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00264136 (* 1 = 0.00264136 loss)
I1006 13:10:29.568966  2824 sgd_solver.cpp:105] Iteration 71800, lr = 0.01
I1006 13:10:37.951848  2824 solver.cpp:218] Iteration 71900 (11.9291 iter/s, 8.38285s/100 iters), loss = 0.00972412
I1006 13:10:37.951889  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00972369 (* 1 = 0.00972369 loss)
I1006 13:10:37.951895  2824 sgd_solver.cpp:105] Iteration 71900, lr = 0.01
I1006 13:10:45.911183  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:10:46.245681  2824 solver.cpp:330] Iteration 72000, Testing net (#0)
I1006 13:10:48.204860  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:10:48.286761  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9164
I1006 13:10:48.286795  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361437 (* 1 = 0.361437 loss)
I1006 13:10:48.371196  2824 solver.cpp:218] Iteration 72000 (9.5976 iter/s, 10.4193s/100 iters), loss = 0.00180239
I1006 13:10:48.371222  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00180195 (* 1 = 0.00180195 loss)
I1006 13:10:48.371228  2824 sgd_solver.cpp:105] Iteration 72000, lr = 0.01
I1006 13:10:56.751366  2824 solver.cpp:218] Iteration 72100 (11.933 iter/s, 8.38012s/100 iters), loss = 0.00462949
I1006 13:10:56.751407  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00462905 (* 1 = 0.00462905 loss)
I1006 13:10:56.751415  2824 sgd_solver.cpp:105] Iteration 72100, lr = 0.01
I1006 13:11:05.135161  2824 solver.cpp:218] Iteration 72200 (11.9279 iter/s, 8.38373s/100 iters), loss = 0.0355166
I1006 13:11:05.135203  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0355162 (* 1 = 0.0355162 loss)
I1006 13:11:05.135210  2824 sgd_solver.cpp:105] Iteration 72200, lr = 0.01
I1006 13:11:13.506830  2824 solver.cpp:218] Iteration 72300 (11.9451 iter/s, 8.3716s/100 iters), loss = 0.00127721
I1006 13:11:13.506872  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00127676 (* 1 = 0.00127676 loss)
I1006 13:11:13.506878  2824 sgd_solver.cpp:105] Iteration 72300, lr = 0.01
I1006 13:11:21.885886  2824 solver.cpp:218] Iteration 72400 (11.9346 iter/s, 8.37898s/100 iters), loss = 0.00183982
I1006 13:11:21.885975  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00183937 (* 1 = 0.00183937 loss)
I1006 13:11:21.885983  2824 sgd_solver.cpp:105] Iteration 72400, lr = 0.01
I1006 13:11:29.846061  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:11:30.181016  2824 solver.cpp:330] Iteration 72500, Testing net (#0)
I1006 13:11:32.141438  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:11:32.223443  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9043
I1006 13:11:32.223467  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.414355 (* 1 = 0.414355 loss)
I1006 13:11:32.307421  2824 solver.cpp:218] Iteration 72500 (9.59563 iter/s, 10.4214s/100 iters), loss = 0.00163623
I1006 13:11:32.307447  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00163579 (* 1 = 0.00163579 loss)
I1006 13:11:32.307454  2824 sgd_solver.cpp:105] Iteration 72500, lr = 0.01
I1006 13:11:40.689129  2824 solver.cpp:218] Iteration 72600 (11.9308 iter/s, 8.38165s/100 iters), loss = 0.00537384
I1006 13:11:40.689170  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00537339 (* 1 = 0.00537339 loss)
I1006 13:11:40.689177  2824 sgd_solver.cpp:105] Iteration 72600, lr = 0.01
I1006 13:11:49.069172  2824 solver.cpp:218] Iteration 72700 (11.9332 iter/s, 8.37997s/100 iters), loss = 0.00097959
I1006 13:11:49.069216  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000979146 (* 1 = 0.000979146 loss)
I1006 13:11:49.069221  2824 sgd_solver.cpp:105] Iteration 72700, lr = 0.01
I1006 13:11:57.447389  2824 solver.cpp:218] Iteration 72800 (11.9358 iter/s, 8.37815s/100 iters), loss = 0.00481159
I1006 13:11:57.447509  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00481115 (* 1 = 0.00481115 loss)
I1006 13:11:57.447515  2824 sgd_solver.cpp:105] Iteration 72800, lr = 0.01
I1006 13:12:05.830369  2824 solver.cpp:218] Iteration 72900 (11.9291 iter/s, 8.38284s/100 iters), loss = 0.00482987
I1006 13:12:05.830411  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00482942 (* 1 = 0.00482942 loss)
I1006 13:12:05.830417  2824 sgd_solver.cpp:105] Iteration 72900, lr = 0.01
I1006 13:12:13.794729  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:12:14.130180  2824 solver.cpp:330] Iteration 73000, Testing net (#0)
I1006 13:12:16.088394  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:12:16.170419  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9093
I1006 13:12:16.170455  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.40095 (* 1 = 0.40095 loss)
I1006 13:12:16.254968  2824 solver.cpp:218] Iteration 73000 (9.59276 iter/s, 10.4245s/100 iters), loss = 0.00773306
I1006 13:12:16.254994  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0077326 (* 1 = 0.0077326 loss)
I1006 13:12:16.255000  2824 sgd_solver.cpp:105] Iteration 73000, lr = 0.01
I1006 13:12:24.650569  2824 solver.cpp:218] Iteration 73100 (11.9111 iter/s, 8.39555s/100 iters), loss = 0.00794077
I1006 13:12:24.650600  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00794031 (* 1 = 0.00794031 loss)
I1006 13:12:24.650606  2824 sgd_solver.cpp:105] Iteration 73100, lr = 0.01
I1006 13:12:33.040761  2824 solver.cpp:218] Iteration 73200 (11.9188 iter/s, 8.39013s/100 iters), loss = 0.00134551
I1006 13:12:33.040880  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00134505 (* 1 = 0.00134505 loss)
I1006 13:12:33.040887  2824 sgd_solver.cpp:105] Iteration 73200, lr = 0.01
I1006 13:12:41.429828  2824 solver.cpp:218] Iteration 73300 (11.9205 iter/s, 8.38892s/100 iters), loss = 0.00289478
I1006 13:12:41.429869  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00289433 (* 1 = 0.00289433 loss)
I1006 13:12:41.429874  2824 sgd_solver.cpp:105] Iteration 73300, lr = 0.01
I1006 13:12:49.853410  2824 solver.cpp:218] Iteration 73400 (11.8715 iter/s, 8.42351s/100 iters), loss = 0.00772486
I1006 13:12:49.853442  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00772441 (* 1 = 0.00772441 loss)
I1006 13:12:49.853447  2824 sgd_solver.cpp:105] Iteration 73400, lr = 0.01
I1006 13:12:57.864653  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:12:58.201287  2824 solver.cpp:330] Iteration 73500, Testing net (#0)
I1006 13:13:00.158377  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:13:00.240710  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9056
I1006 13:13:00.240746  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.398922 (* 1 = 0.398922 loss)
I1006 13:13:00.324590  2824 solver.cpp:218] Iteration 73500 (9.55008 iter/s, 10.4711s/100 iters), loss = 0.0158098
I1006 13:13:00.324616  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158094 (* 1 = 0.0158094 loss)
I1006 13:13:00.324622  2824 sgd_solver.cpp:105] Iteration 73500, lr = 0.01
I1006 13:13:08.706902  2824 solver.cpp:218] Iteration 73600 (11.93 iter/s, 8.38226s/100 iters), loss = 0.0125302
I1006 13:13:08.707032  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125298 (* 1 = 0.0125298 loss)
I1006 13:13:08.707039  2824 sgd_solver.cpp:105] Iteration 73600, lr = 0.01
I1006 13:13:17.093814  2824 solver.cpp:218] Iteration 73700 (11.9235 iter/s, 8.38677s/100 iters), loss = 0.0292627
I1006 13:13:17.093845  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0292622 (* 1 = 0.0292622 loss)
I1006 13:13:17.093852  2824 sgd_solver.cpp:105] Iteration 73700, lr = 0.01
I1006 13:13:25.472580  2824 solver.cpp:218] Iteration 73800 (11.935 iter/s, 8.37871s/100 iters), loss = 0.00280185
I1006 13:13:25.472621  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0028014 (* 1 = 0.0028014 loss)
I1006 13:13:25.472627  2824 sgd_solver.cpp:105] Iteration 73800, lr = 0.01
I1006 13:13:33.853385  2824 solver.cpp:218] Iteration 73900 (11.9321 iter/s, 8.38074s/100 iters), loss = 0.00364506
I1006 13:13:33.853426  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00364461 (* 1 = 0.00364461 loss)
I1006 13:13:33.853432  2824 sgd_solver.cpp:105] Iteration 73900, lr = 0.01
I1006 13:13:41.810195  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:13:42.145622  2824 solver.cpp:330] Iteration 74000, Testing net (#0)
I1006 13:13:44.103310  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:13:44.185319  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9105
I1006 13:13:44.185360  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.381943 (* 1 = 0.381943 loss)
I1006 13:13:44.269583  2824 solver.cpp:218] Iteration 74000 (9.6005 iter/s, 10.4161s/100 iters), loss = 0.00148329
I1006 13:13:44.269608  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00148284 (* 1 = 0.00148284 loss)
I1006 13:13:44.269614  2824 sgd_solver.cpp:105] Iteration 74000, lr = 0.01
I1006 13:13:52.651968  2824 solver.cpp:218] Iteration 74100 (11.9299 iter/s, 8.38233s/100 iters), loss = 0.00876601
I1006 13:13:52.651999  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00876555 (* 1 = 0.00876555 loss)
I1006 13:13:52.652005  2824 sgd_solver.cpp:105] Iteration 74100, lr = 0.01
I1006 13:14:01.034282  2824 solver.cpp:218] Iteration 74200 (11.93 iter/s, 8.38226s/100 iters), loss = 0.00355255
I1006 13:14:01.034322  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0035521 (* 1 = 0.0035521 loss)
I1006 13:14:01.034328  2824 sgd_solver.cpp:105] Iteration 74200, lr = 0.01
I1006 13:14:09.411047  2824 solver.cpp:218] Iteration 74300 (11.9379 iter/s, 8.3767s/100 iters), loss = 0.0132763
I1006 13:14:09.411087  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0132759 (* 1 = 0.0132759 loss)
I1006 13:14:09.411093  2824 sgd_solver.cpp:105] Iteration 74300, lr = 0.01
I1006 13:14:17.793560  2824 solver.cpp:218] Iteration 74400 (11.9297 iter/s, 8.38244s/100 iters), loss = 0.000404744
I1006 13:14:17.793643  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000404287 (* 1 = 0.000404287 loss)
I1006 13:14:17.793659  2824 sgd_solver.cpp:105] Iteration 74400, lr = 0.01
I1006 13:14:25.754046  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:14:26.089074  2824 solver.cpp:330] Iteration 74500, Testing net (#0)
I1006 13:14:28.048238  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:14:28.130033  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9074
I1006 13:14:28.130074  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.407102 (* 1 = 0.407102 loss)
I1006 13:14:28.214361  2824 solver.cpp:218] Iteration 74500 (9.59629 iter/s, 10.4207s/100 iters), loss = 0.00127026
I1006 13:14:28.214386  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0012698 (* 1 = 0.0012698 loss)
I1006 13:14:28.214393  2824 sgd_solver.cpp:105] Iteration 74500, lr = 0.01
I1006 13:14:36.594770  2824 solver.cpp:218] Iteration 74600 (11.9327 iter/s, 8.38036s/100 iters), loss = 0.00177541
I1006 13:14:36.594801  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00177495 (* 1 = 0.00177495 loss)
I1006 13:14:36.594807  2824 sgd_solver.cpp:105] Iteration 74600, lr = 0.01
I1006 13:14:44.969744  2824 solver.cpp:218] Iteration 74700 (11.9404 iter/s, 8.37492s/100 iters), loss = 0.0048411
I1006 13:14:44.969786  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00484064 (* 1 = 0.00484064 loss)
I1006 13:14:44.969792  2824 sgd_solver.cpp:105] Iteration 74700, lr = 0.01
I1006 13:14:53.339172  2824 solver.cpp:218] Iteration 74800 (11.9483 iter/s, 8.36936s/100 iters), loss = 0.00619005
I1006 13:14:53.339262  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00618959 (* 1 = 0.00618959 loss)
I1006 13:14:53.339268  2824 sgd_solver.cpp:105] Iteration 74800, lr = 0.01
I1006 13:15:01.717767  2824 solver.cpp:218] Iteration 74900 (11.9353 iter/s, 8.37848s/100 iters), loss = 0.00474395
I1006 13:15:01.717808  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00474349 (* 1 = 0.00474349 loss)
I1006 13:15:01.717813  2824 sgd_solver.cpp:105] Iteration 74900, lr = 0.01
I1006 13:15:09.670719  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:15:10.005864  2824 solver.cpp:330] Iteration 75000, Testing net (#0)
I1006 13:15:11.964726  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:15:12.046308  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9077
I1006 13:15:12.046334  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.392186 (* 1 = 0.392186 loss)
I1006 13:15:12.130128  2824 solver.cpp:218] Iteration 75000 (9.60404 iter/s, 10.4123s/100 iters), loss = 0.00171209
I1006 13:15:12.130156  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00171163 (* 1 = 0.00171163 loss)
I1006 13:15:12.130162  2824 sgd_solver.cpp:105] Iteration 75000, lr = 0.01
I1006 13:15:20.520706  2824 solver.cpp:218] Iteration 75100 (11.9182 iter/s, 8.39052s/100 iters), loss = 0.00149129
I1006 13:15:20.520748  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00149082 (* 1 = 0.00149082 loss)
I1006 13:15:20.520754  2824 sgd_solver.cpp:105] Iteration 75100, lr = 0.01
I1006 13:15:28.908026  2824 solver.cpp:218] Iteration 75200 (11.9229 iter/s, 8.38725s/100 iters), loss = 0.00748322
I1006 13:15:28.908102  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00748275 (* 1 = 0.00748275 loss)
I1006 13:15:28.908108  2824 sgd_solver.cpp:105] Iteration 75200, lr = 0.01
I1006 13:15:37.287320  2824 solver.cpp:218] Iteration 75300 (11.9343 iter/s, 8.37919s/100 iters), loss = 0.00496623
I1006 13:15:37.287361  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00496576 (* 1 = 0.00496576 loss)
I1006 13:15:37.287367  2824 sgd_solver.cpp:105] Iteration 75300, lr = 0.01
I1006 13:15:45.675176  2824 solver.cpp:218] Iteration 75400 (11.9221 iter/s, 8.38778s/100 iters), loss = 0.00630987
I1006 13:15:45.675217  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0063094 (* 1 = 0.0063094 loss)
I1006 13:15:45.675223  2824 sgd_solver.cpp:105] Iteration 75400, lr = 0.01
I1006 13:15:53.642484  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:15:53.978487  2824 solver.cpp:330] Iteration 75500, Testing net (#0)
I1006 13:15:55.936645  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:15:56.018887  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9127
I1006 13:15:56.018929  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.388764 (* 1 = 0.388764 loss)
I1006 13:15:56.103080  2824 solver.cpp:218] Iteration 75500 (9.58972 iter/s, 10.4278s/100 iters), loss = 0.00162776
I1006 13:15:56.103106  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00162728 (* 1 = 0.00162728 loss)
I1006 13:15:56.103113  2824 sgd_solver.cpp:105] Iteration 75500, lr = 0.01
I1006 13:16:04.484047  2824 solver.cpp:218] Iteration 75600 (11.9319 iter/s, 8.38091s/100 iters), loss = 0.0123441
I1006 13:16:04.484131  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123436 (* 1 = 0.0123436 loss)
I1006 13:16:04.484148  2824 sgd_solver.cpp:105] Iteration 75600, lr = 0.01
I1006 13:16:12.861795  2824 solver.cpp:218] Iteration 75700 (11.9365 iter/s, 8.37764s/100 iters), loss = 0.00217058
I1006 13:16:12.861837  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00217012 (* 1 = 0.00217012 loss)
I1006 13:16:12.861843  2824 sgd_solver.cpp:105] Iteration 75700, lr = 0.01
I1006 13:16:21.235469  2824 solver.cpp:218] Iteration 75800 (11.9423 iter/s, 8.3736s/100 iters), loss = 0.0146105
I1006 13:16:21.235512  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.01461 (* 1 = 0.01461 loss)
I1006 13:16:21.235517  2824 sgd_solver.cpp:105] Iteration 75800, lr = 0.01
I1006 13:16:29.612670  2824 solver.cpp:218] Iteration 75900 (11.9373 iter/s, 8.37713s/100 iters), loss = 0.0171118
I1006 13:16:29.612711  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171113 (* 1 = 0.0171113 loss)
I1006 13:16:29.612716  2824 sgd_solver.cpp:105] Iteration 75900, lr = 0.01
I1006 13:16:37.573217  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:16:37.908710  2824 solver.cpp:330] Iteration 76000, Testing net (#0)
I1006 13:16:39.866542  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:16:39.948601  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9147
I1006 13:16:39.948637  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.374067 (* 1 = 0.374067 loss)
I1006 13:16:40.032588  2824 solver.cpp:218] Iteration 76000 (9.59707 iter/s, 10.4198s/100 iters), loss = 0.00255387
I1006 13:16:40.032614  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0025534 (* 1 = 0.0025534 loss)
I1006 13:16:40.032621  2824 sgd_solver.cpp:105] Iteration 76000, lr = 0.01
I1006 13:16:48.421612  2824 solver.cpp:218] Iteration 76100 (11.9204 iter/s, 8.38897s/100 iters), loss = 0.00509543
I1006 13:16:48.421653  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00509495 (* 1 = 0.00509495 loss)
I1006 13:16:48.421659  2824 sgd_solver.cpp:105] Iteration 76100, lr = 0.01
I1006 13:16:56.804178  2824 solver.cpp:218] Iteration 76200 (11.9296 iter/s, 8.3825s/100 iters), loss = 0.017423
I1006 13:16:56.804209  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0174225 (* 1 = 0.0174225 loss)
I1006 13:16:56.804215  2824 sgd_solver.cpp:105] Iteration 76200, lr = 0.01
I1006 13:17:05.185009  2824 solver.cpp:218] Iteration 76300 (11.9321 iter/s, 8.38077s/100 iters), loss = 0.00396462
I1006 13:17:05.185050  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00396414 (* 1 = 0.00396414 loss)
I1006 13:17:05.185056  2824 sgd_solver.cpp:105] Iteration 76300, lr = 0.01
I1006 13:17:13.577003  2824 solver.cpp:218] Iteration 76400 (11.9162 iter/s, 8.39192s/100 iters), loss = 0.00350072
I1006 13:17:13.577107  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00350024 (* 1 = 0.00350024 loss)
I1006 13:17:13.577116  2824 sgd_solver.cpp:105] Iteration 76400, lr = 0.01
I1006 13:17:21.538897  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:17:21.874253  2824 solver.cpp:330] Iteration 76500, Testing net (#0)
I1006 13:17:23.831317  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:17:23.913092  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9182
I1006 13:17:23.913128  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361673 (* 1 = 0.361673 loss)
I1006 13:17:23.996848  2824 solver.cpp:218] Iteration 76500 (9.59719 iter/s, 10.4197s/100 iters), loss = 0.00369547
I1006 13:17:23.996873  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00369499 (* 1 = 0.00369499 loss)
I1006 13:17:23.996881  2824 sgd_solver.cpp:105] Iteration 76500, lr = 0.01
I1006 13:17:32.378870  2824 solver.cpp:218] Iteration 76600 (11.9304 iter/s, 8.38197s/100 iters), loss = 0.0096225
I1006 13:17:32.378909  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00962202 (* 1 = 0.00962202 loss)
I1006 13:17:32.378916  2824 sgd_solver.cpp:105] Iteration 76600, lr = 0.01
I1006 13:17:40.763578  2824 solver.cpp:218] Iteration 76700 (11.9266 iter/s, 8.38464s/100 iters), loss = 0.00923202
I1006 13:17:40.763620  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00923154 (* 1 = 0.00923154 loss)
I1006 13:17:40.763626  2824 sgd_solver.cpp:105] Iteration 76700, lr = 0.01
I1006 13:17:49.144011  2824 solver.cpp:218] Iteration 76800 (11.9327 iter/s, 8.38036s/100 iters), loss = 0.00511432
I1006 13:17:49.144078  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00511384 (* 1 = 0.00511384 loss)
I1006 13:17:49.144085  2824 sgd_solver.cpp:105] Iteration 76800, lr = 0.01
I1006 13:17:57.531226  2824 solver.cpp:218] Iteration 76900 (11.923 iter/s, 8.38712s/100 iters), loss = 0.00178961
I1006 13:17:57.531267  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00178913 (* 1 = 0.00178913 loss)
I1006 13:17:57.531273  2824 sgd_solver.cpp:105] Iteration 76900, lr = 0.01
I1006 13:18:05.501329  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:18:05.836905  2824 solver.cpp:330] Iteration 77000, Testing net (#0)
I1006 13:18:07.796823  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:18:07.878808  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9071
I1006 13:18:07.878844  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3907 (* 1 = 0.3907 loss)
I1006 13:18:07.962610  2824 solver.cpp:218] Iteration 77000 (9.58652 iter/s, 10.4313s/100 iters), loss = 0.010166
I1006 13:18:07.962636  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0101655 (* 1 = 0.0101655 loss)
I1006 13:18:07.962643  2824 sgd_solver.cpp:105] Iteration 77000, lr = 0.01
I1006 13:18:16.342759  2824 solver.cpp:218] Iteration 77100 (11.933 iter/s, 8.38009s/100 iters), loss = 0.00177373
I1006 13:18:16.342803  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00177324 (* 1 = 0.00177324 loss)
I1006 13:18:16.342808  2824 sgd_solver.cpp:105] Iteration 77100, lr = 0.01
I1006 13:18:24.727653  2824 solver.cpp:218] Iteration 77200 (11.9263 iter/s, 8.38482s/100 iters), loss = 0.0109508
I1006 13:18:24.727793  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109503 (* 1 = 0.0109503 loss)
I1006 13:18:24.727811  2824 sgd_solver.cpp:105] Iteration 77200, lr = 0.01
I1006 13:18:33.113960  2824 solver.cpp:218] Iteration 77300 (11.9244 iter/s, 8.38615s/100 iters), loss = 0.00572568
I1006 13:18:33.113989  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00572519 (* 1 = 0.00572519 loss)
I1006 13:18:33.113996  2824 sgd_solver.cpp:105] Iteration 77300, lr = 0.01
I1006 13:18:41.504704  2824 solver.cpp:218] Iteration 77400 (11.918 iter/s, 8.39069s/100 iters), loss = 0.0151233
I1006 13:18:41.504745  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0151228 (* 1 = 0.0151228 loss)
I1006 13:18:41.504751  2824 sgd_solver.cpp:105] Iteration 77400, lr = 0.01
I1006 13:18:49.480113  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:18:49.816720  2824 solver.cpp:330] Iteration 77500, Testing net (#0)
I1006 13:18:51.774950  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:18:51.856464  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9157
I1006 13:18:51.856500  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.374498 (* 1 = 0.374498 loss)
I1006 13:18:51.940521  2824 solver.cpp:218] Iteration 77500 (9.58245 iter/s, 10.4357s/100 iters), loss = 0.00492558
I1006 13:18:51.940549  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00492509 (* 1 = 0.00492509 loss)
I1006 13:18:51.940557  2824 sgd_solver.cpp:105] Iteration 77500, lr = 0.01
I1006 13:19:00.330463  2824 solver.cpp:218] Iteration 77600 (11.9191 iter/s, 8.38989s/100 iters), loss = 0.0158354
I1006 13:19:00.330575  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158349 (* 1 = 0.0158349 loss)
I1006 13:19:00.330592  2824 sgd_solver.cpp:105] Iteration 77600, lr = 0.01
I1006 13:19:08.714512  2824 solver.cpp:218] Iteration 77700 (11.9276 iter/s, 8.38392s/100 iters), loss = 0.00148866
I1006 13:19:08.714553  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00148817 (* 1 = 0.00148817 loss)
I1006 13:19:08.714560  2824 sgd_solver.cpp:105] Iteration 77700, lr = 0.01
I1006 13:19:17.096411  2824 solver.cpp:218] Iteration 77800 (11.9306 iter/s, 8.38183s/100 iters), loss = 0.00111779
I1006 13:19:17.096452  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0011173 (* 1 = 0.0011173 loss)
I1006 13:19:17.096458  2824 sgd_solver.cpp:105] Iteration 77800, lr = 0.01
I1006 13:19:25.475862  2824 solver.cpp:218] Iteration 77900 (11.9341 iter/s, 8.37938s/100 iters), loss = 0.00178387
I1006 13:19:25.475904  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00178337 (* 1 = 0.00178337 loss)
I1006 13:19:25.475910  2824 sgd_solver.cpp:105] Iteration 77900, lr = 0.01
I1006 13:19:33.435485  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:19:33.770877  2824 solver.cpp:330] Iteration 78000, Testing net (#0)
I1006 13:19:35.727500  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:19:35.809152  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9098
I1006 13:19:35.809176  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.394513 (* 1 = 0.394513 loss)
I1006 13:19:35.893424  2824 solver.cpp:218] Iteration 78000 (9.59924 iter/s, 10.4175s/100 iters), loss = 0.00200314
I1006 13:19:35.893450  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00200264 (* 1 = 0.00200264 loss)
I1006 13:19:35.893457  2824 sgd_solver.cpp:105] Iteration 78000, lr = 0.01
I1006 13:19:44.273519  2824 solver.cpp:218] Iteration 78100 (11.9331 iter/s, 8.38004s/100 iters), loss = 0.00231577
I1006 13:19:44.273550  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00231527 (* 1 = 0.00231527 loss)
I1006 13:19:44.273556  2824 sgd_solver.cpp:105] Iteration 78100, lr = 0.01
I1006 13:19:52.659587  2824 solver.cpp:218] Iteration 78200 (11.9246 iter/s, 8.38601s/100 iters), loss = 0.00356907
I1006 13:19:52.659621  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00356857 (* 1 = 0.00356857 loss)
I1006 13:19:52.659626  2824 sgd_solver.cpp:105] Iteration 78200, lr = 0.01
I1006 13:20:01.041590  2824 solver.cpp:218] Iteration 78300 (11.9304 iter/s, 8.38194s/100 iters), loss = 0.00106589
I1006 13:20:01.041632  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00106539 (* 1 = 0.00106539 loss)
I1006 13:20:01.041638  2824 sgd_solver.cpp:105] Iteration 78300, lr = 0.01
I1006 13:20:09.425830  2824 solver.cpp:218] Iteration 78400 (11.9272 iter/s, 8.38417s/100 iters), loss = 0.00354749
I1006 13:20:09.425972  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.003547 (* 1 = 0.003547 loss)
I1006 13:20:09.425981  2824 sgd_solver.cpp:105] Iteration 78400, lr = 0.01
I1006 13:20:17.390503  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:20:17.726088  2824 solver.cpp:330] Iteration 78500, Testing net (#0)
I1006 13:20:19.684054  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:20:19.765941  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9031
I1006 13:20:19.765977  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.432815 (* 1 = 0.432815 loss)
I1006 13:20:19.850401  2824 solver.cpp:218] Iteration 78500 (9.59288 iter/s, 10.4244s/100 iters), loss = 0.00269811
I1006 13:20:19.850425  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00269762 (* 1 = 0.00269762 loss)
I1006 13:20:19.850432  2824 sgd_solver.cpp:105] Iteration 78500, lr = 0.01
I1006 13:20:28.240819  2824 solver.cpp:218] Iteration 78600 (11.9184 iter/s, 8.39037s/100 iters), loss = 0.0017572
I1006 13:20:28.240860  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0017567 (* 1 = 0.0017567 loss)
I1006 13:20:28.240875  2824 sgd_solver.cpp:105] Iteration 78600, lr = 0.01
I1006 13:20:36.623841  2824 solver.cpp:218] Iteration 78700 (11.929 iter/s, 8.38295s/100 iters), loss = 0.00857991
I1006 13:20:36.623872  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00857941 (* 1 = 0.00857941 loss)
I1006 13:20:36.623888  2824 sgd_solver.cpp:105] Iteration 78700, lr = 0.01
I1006 13:20:45.003449  2824 solver.cpp:218] Iteration 78800 (11.9338 iter/s, 8.37955s/100 iters), loss = 0.00994864
I1006 13:20:45.003562  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00994815 (* 1 = 0.00994815 loss)
I1006 13:20:45.003569  2824 sgd_solver.cpp:105] Iteration 78800, lr = 0.01
I1006 13:20:53.390534  2824 solver.cpp:218] Iteration 78900 (11.9233 iter/s, 8.38696s/100 iters), loss = 0.00159414
I1006 13:20:53.390564  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00159365 (* 1 = 0.00159365 loss)
I1006 13:20:53.390570  2824 sgd_solver.cpp:105] Iteration 78900, lr = 0.01
I1006 13:21:01.358625  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:21:01.693331  2824 solver.cpp:330] Iteration 79000, Testing net (#0)
I1006 13:21:03.652204  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:21:03.733975  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9071
I1006 13:21:03.734011  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.394929 (* 1 = 0.394929 loss)
I1006 13:21:03.817978  2824 solver.cpp:218] Iteration 79000 (9.59014 iter/s, 10.4274s/100 iters), loss = 0.0112797
I1006 13:21:03.818004  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112792 (* 1 = 0.0112792 loss)
I1006 13:21:03.818011  2824 sgd_solver.cpp:105] Iteration 79000, lr = 0.01
I1006 13:21:12.207758  2824 solver.cpp:218] Iteration 79100 (11.9193 iter/s, 8.38973s/100 iters), loss = 0.00108587
I1006 13:21:12.207789  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00108538 (* 1 = 0.00108538 loss)
I1006 13:21:12.207795  2824 sgd_solver.cpp:105] Iteration 79100, lr = 0.01
I1006 13:21:20.597149  2824 solver.cpp:218] Iteration 79200 (11.9199 iter/s, 8.38933s/100 iters), loss = 0.00476955
I1006 13:21:20.597317  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00476906 (* 1 = 0.00476906 loss)
I1006 13:21:20.597327  2824 sgd_solver.cpp:105] Iteration 79200, lr = 0.01
I1006 13:21:28.982023  2824 solver.cpp:218] Iteration 79300 (11.9265 iter/s, 8.38469s/100 iters), loss = 0.0057807
I1006 13:21:28.982066  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00578021 (* 1 = 0.00578021 loss)
I1006 13:21:28.982071  2824 sgd_solver.cpp:105] Iteration 79300, lr = 0.01
I1006 13:21:37.369174  2824 solver.cpp:218] Iteration 79400 (11.9231 iter/s, 8.38708s/100 iters), loss = 0.00181659
I1006 13:21:37.369215  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0018161 (* 1 = 0.0018161 loss)
I1006 13:21:37.369220  2824 sgd_solver.cpp:105] Iteration 79400, lr = 0.01
I1006 13:21:45.342214  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:21:45.677145  2824 solver.cpp:330] Iteration 79500, Testing net (#0)
I1006 13:21:47.636466  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:21:47.718536  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8997
I1006 13:21:47.718572  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.453409 (* 1 = 0.453409 loss)
I1006 13:21:47.802266  2824 solver.cpp:218] Iteration 79500 (9.58495 iter/s, 10.433s/100 iters), loss = 0.00150869
I1006 13:21:47.802292  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0015082 (* 1 = 0.0015082 loss)
I1006 13:21:47.802299  2824 sgd_solver.cpp:105] Iteration 79500, lr = 0.01
I1006 13:21:56.175045  2824 solver.cpp:218] Iteration 79600 (11.9435 iter/s, 8.37273s/100 iters), loss = 0.0056699
I1006 13:21:56.175115  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00566941 (* 1 = 0.00566941 loss)
I1006 13:21:56.175124  2824 sgd_solver.cpp:105] Iteration 79600, lr = 0.01
I1006 13:22:04.550971  2824 solver.cpp:218] Iteration 79700 (11.9391 iter/s, 8.37583s/100 iters), loss = 0.0319226
I1006 13:22:04.551012  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0319221 (* 1 = 0.0319221 loss)
I1006 13:22:04.551017  2824 sgd_solver.cpp:105] Iteration 79700, lr = 0.01
I1006 13:22:12.925457  2824 solver.cpp:218] Iteration 79800 (11.9411 iter/s, 8.37442s/100 iters), loss = 0.00505017
I1006 13:22:12.925499  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00504968 (* 1 = 0.00504968 loss)
I1006 13:22:12.925505  2824 sgd_solver.cpp:105] Iteration 79800, lr = 0.01
I1006 13:22:21.299487  2824 solver.cpp:218] Iteration 79900 (11.9418 iter/s, 8.37396s/100 iters), loss = 0.0163565
I1006 13:22:21.299518  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.016356 (* 1 = 0.016356 loss)
I1006 13:22:21.299525  2824 sgd_solver.cpp:105] Iteration 79900, lr = 0.01
I1006 13:22:29.258949  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:22:29.594177  2824 solver.cpp:330] Iteration 80000, Testing net (#0)
I1006 13:22:31.552217  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:22:31.634317  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.91
I1006 13:22:31.634354  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.371521 (* 1 = 0.371521 loss)
I1006 13:22:31.718288  2824 solver.cpp:218] Iteration 80000 (9.59809 iter/s, 10.4187s/100 iters), loss = 0.00340644
I1006 13:22:31.718313  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00340595 (* 1 = 0.00340595 loss)
I1006 13:22:31.718319  2824 sgd_solver.cpp:46] MultiStep Status: Iteration 80000, step = 2
I1006 13:22:31.718322  2824 sgd_solver.cpp:105] Iteration 80000, lr = 0.001
I1006 13:22:40.103693  2824 solver.cpp:218] Iteration 80100 (11.9256 iter/s, 8.38535s/100 iters), loss = 0.00140604
I1006 13:22:40.103724  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00140555 (* 1 = 0.00140555 loss)
I1006 13:22:40.103729  2824 sgd_solver.cpp:105] Iteration 80100, lr = 0.001
I1006 13:22:48.491413  2824 solver.cpp:218] Iteration 80200 (11.9223 iter/s, 8.38766s/100 iters), loss = 0.00309033
I1006 13:22:48.491456  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00308984 (* 1 = 0.00308984 loss)
I1006 13:22:48.491462  2824 sgd_solver.cpp:105] Iteration 80200, lr = 0.001
I1006 13:22:56.871682  2824 solver.cpp:218] Iteration 80300 (11.9329 iter/s, 8.3802s/100 iters), loss = 0.0027235
I1006 13:22:56.871713  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00272301 (* 1 = 0.00272301 loss)
I1006 13:22:56.871719  2824 sgd_solver.cpp:105] Iteration 80300, lr = 0.001
I1006 13:23:05.259512  2824 solver.cpp:218] Iteration 80400 (11.9221 iter/s, 8.38777s/100 iters), loss = 0.00105679
I1006 13:23:05.259611  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0010563 (* 1 = 0.0010563 loss)
I1006 13:23:05.259621  2824 sgd_solver.cpp:105] Iteration 80400, lr = 0.001
I1006 13:23:13.227823  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:23:13.562497  2824 solver.cpp:330] Iteration 80500, Testing net (#0)
I1006 13:23:15.517014  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:23:15.599241  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9238
I1006 13:23:15.599275  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.322661 (* 1 = 0.322661 loss)
I1006 13:23:15.683291  2824 solver.cpp:218] Iteration 80500 (9.59356 iter/s, 10.4237s/100 iters), loss = 0.00297927
I1006 13:23:15.683316  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00297879 (* 1 = 0.00297879 loss)
I1006 13:23:15.683322  2824 sgd_solver.cpp:105] Iteration 80500, lr = 0.001
I1006 13:23:24.068614  2824 solver.cpp:218] Iteration 80600 (11.9257 iter/s, 8.38527s/100 iters), loss = 0.00911968
I1006 13:23:24.068644  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0091192 (* 1 = 0.0091192 loss)
I1006 13:23:24.068648  2824 sgd_solver.cpp:105] Iteration 80600, lr = 0.001
I1006 13:23:32.446614  2824 solver.cpp:218] Iteration 80700 (11.9361 iter/s, 8.37794s/100 iters), loss = 0.000776153
I1006 13:23:32.446645  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000775664 (* 1 = 0.000775664 loss)
I1006 13:23:32.446660  2824 sgd_solver.cpp:105] Iteration 80700, lr = 0.001
I1006 13:23:40.828425  2824 solver.cpp:218] Iteration 80800 (11.9307 iter/s, 8.38175s/100 iters), loss = 0.00119711
I1006 13:23:40.828497  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00119662 (* 1 = 0.00119662 loss)
I1006 13:23:40.828516  2824 sgd_solver.cpp:105] Iteration 80800, lr = 0.001
I1006 13:23:49.214712  2824 solver.cpp:218] Iteration 80900 (11.9244 iter/s, 8.38619s/100 iters), loss = 0.00242437
I1006 13:23:49.214742  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00242388 (* 1 = 0.00242388 loss)
I1006 13:23:49.214748  2824 sgd_solver.cpp:105] Iteration 80900, lr = 0.001
I1006 13:23:57.177729  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:23:57.513671  2824 solver.cpp:330] Iteration 81000, Testing net (#0)
I1006 13:23:59.473871  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:23:59.556035  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9221
I1006 13:23:59.556069  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.319085 (* 1 = 0.319085 loss)
I1006 13:23:59.639896  2824 solver.cpp:218] Iteration 81000 (9.59221 iter/s, 10.4251s/100 iters), loss = 0.00741637
I1006 13:23:59.639920  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00741588 (* 1 = 0.00741588 loss)
I1006 13:23:59.639927  2824 sgd_solver.cpp:105] Iteration 81000, lr = 0.001
I1006 13:24:08.028445  2824 solver.cpp:218] Iteration 81100 (11.9211 iter/s, 8.3885s/100 iters), loss = 0.00092309
I1006 13:24:08.028486  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000922602 (* 1 = 0.000922602 loss)
I1006 13:24:08.028491  2824 sgd_solver.cpp:105] Iteration 81100, lr = 0.001
I1006 13:24:16.419253  2824 solver.cpp:218] Iteration 81200 (11.9179 iter/s, 8.39074s/100 iters), loss = 0.002022
I1006 13:24:16.419394  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00202151 (* 1 = 0.00202151 loss)
I1006 13:24:16.419411  2824 sgd_solver.cpp:105] Iteration 81200, lr = 0.001
I1006 13:24:24.800820  2824 solver.cpp:218] Iteration 81300 (11.9312 iter/s, 8.3814s/100 iters), loss = 0.00224041
I1006 13:24:24.800861  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00223992 (* 1 = 0.00223992 loss)
I1006 13:24:24.800868  2824 sgd_solver.cpp:105] Iteration 81300, lr = 0.001
I1006 13:24:33.188593  2824 solver.cpp:218] Iteration 81400 (11.9222 iter/s, 8.3877s/100 iters), loss = 0.00324864
I1006 13:24:33.188634  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00324815 (* 1 = 0.00324815 loss)
I1006 13:24:33.188640  2824 sgd_solver.cpp:105] Iteration 81400, lr = 0.001
I1006 13:24:41.162853  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:24:41.498368  2824 solver.cpp:330] Iteration 81500, Testing net (#0)
I1006 13:24:43.457741  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:24:43.539847  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9246
I1006 13:24:43.539883  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.318066 (* 1 = 0.318066 loss)
I1006 13:24:43.623461  2824 solver.cpp:218] Iteration 81500 (9.58332 iter/s, 10.4348s/100 iters), loss = 0.00636697
I1006 13:24:43.623486  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00636648 (* 1 = 0.00636648 loss)
I1006 13:24:43.623492  2824 sgd_solver.cpp:105] Iteration 81500, lr = 0.001
I1006 13:24:52.001209  2824 solver.cpp:218] Iteration 81600 (11.9365 iter/s, 8.37769s/100 iters), loss = 0.000529186
I1006 13:24:52.001322  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000528692 (* 1 = 0.000528692 loss)
I1006 13:24:52.001330  2824 sgd_solver.cpp:105] Iteration 81600, lr = 0.001
I1006 13:25:00.379320  2824 solver.cpp:218] Iteration 81700 (11.936 iter/s, 8.37798s/100 iters), loss = 0.00259173
I1006 13:25:00.379350  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00259124 (* 1 = 0.00259124 loss)
I1006 13:25:00.379366  2824 sgd_solver.cpp:105] Iteration 81700, lr = 0.001
I1006 13:25:08.756794  2824 solver.cpp:218] Iteration 81800 (11.9369 iter/s, 8.37742s/100 iters), loss = 0.00243678
I1006 13:25:08.756825  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00243629 (* 1 = 0.00243629 loss)
I1006 13:25:08.756841  2824 sgd_solver.cpp:105] Iteration 81800, lr = 0.001
I1006 13:25:17.134949  2824 solver.cpp:218] Iteration 81900 (11.9359 iter/s, 8.3781s/100 iters), loss = 0.00243167
I1006 13:25:17.134990  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00243119 (* 1 = 0.00243119 loss)
I1006 13:25:17.134995  2824 sgd_solver.cpp:105] Iteration 81900, lr = 0.001
I1006 13:25:25.093910  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:25:25.428611  2824 solver.cpp:330] Iteration 82000, Testing net (#0)
I1006 13:25:27.385133  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:25:27.467224  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9234
I1006 13:25:27.467259  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.320133 (* 1 = 0.320133 loss)
I1006 13:25:27.551137  2824 solver.cpp:218] Iteration 82000 (9.60051 iter/s, 10.4161s/100 iters), loss = 0.000943425
I1006 13:25:27.551162  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000942937 (* 1 = 0.000942937 loss)
I1006 13:25:27.551170  2824 sgd_solver.cpp:105] Iteration 82000, lr = 0.001
I1006 13:25:35.932518  2824 solver.cpp:218] Iteration 82100 (11.9313 iter/s, 8.38133s/100 iters), loss = 0.00354291
I1006 13:25:35.932548  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00354243 (* 1 = 0.00354243 loss)
I1006 13:25:35.932564  2824 sgd_solver.cpp:105] Iteration 82100, lr = 0.001
I1006 13:25:44.313638  2824 solver.cpp:218] Iteration 82200 (11.9317 iter/s, 8.38107s/100 iters), loss = 0.000523218
I1006 13:25:44.313666  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000522736 (* 1 = 0.000522736 loss)
I1006 13:25:44.313673  2824 sgd_solver.cpp:105] Iteration 82200, lr = 0.001
I1006 13:25:52.686981  2824 solver.cpp:218] Iteration 82300 (11.9427 iter/s, 8.37329s/100 iters), loss = 0.00262501
I1006 13:25:52.687012  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00262453 (* 1 = 0.00262453 loss)
I1006 13:25:52.687018  2824 sgd_solver.cpp:105] Iteration 82300, lr = 0.001
I1006 13:26:01.063576  2824 solver.cpp:218] Iteration 82400 (11.9381 iter/s, 8.37654s/100 iters), loss = 0.00680182
I1006 13:26:01.063696  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00680134 (* 1 = 0.00680134 loss)
I1006 13:26:01.063704  2824 sgd_solver.cpp:105] Iteration 82400, lr = 0.001
I1006 13:26:09.019855  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:26:09.354653  2824 solver.cpp:330] Iteration 82500, Testing net (#0)
I1006 13:26:11.312352  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:26:11.394032  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9231
I1006 13:26:11.394068  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315276 (* 1 = 0.315276 loss)
I1006 13:26:11.477844  2824 solver.cpp:218] Iteration 82500 (9.60234 iter/s, 10.4141s/100 iters), loss = 0.000586192
I1006 13:26:11.477869  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000585714 (* 1 = 0.000585714 loss)
I1006 13:26:11.477876  2824 sgd_solver.cpp:105] Iteration 82500, lr = 0.001
I1006 13:26:19.854307  2824 solver.cpp:218] Iteration 82600 (11.9383 iter/s, 8.37641s/100 iters), loss = 0.00524772
I1006 13:26:19.854349  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00524724 (* 1 = 0.00524724 loss)
I1006 13:26:19.854356  2824 sgd_solver.cpp:105] Iteration 82600, lr = 0.001
I1006 13:26:28.232130  2824 solver.cpp:218] Iteration 82700 (11.9364 iter/s, 8.37775s/100 iters), loss = 0.00168489
I1006 13:26:28.232157  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00168441 (* 1 = 0.00168441 loss)
I1006 13:26:28.232162  2824 sgd_solver.cpp:105] Iteration 82700, lr = 0.001
I1006 13:26:36.602669  2824 solver.cpp:218] Iteration 82800 (11.9467 iter/s, 8.37048s/100 iters), loss = 0.0138672
I1006 13:26:36.602782  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0138667 (* 1 = 0.0138667 loss)
I1006 13:26:36.602788  2824 sgd_solver.cpp:105] Iteration 82800, lr = 0.001
I1006 13:26:44.983877  2824 solver.cpp:218] Iteration 82900 (11.9316 iter/s, 8.38107s/100 iters), loss = 0.0012779
I1006 13:26:44.983907  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00127743 (* 1 = 0.00127743 loss)
I1006 13:26:44.983913  2824 sgd_solver.cpp:105] Iteration 82900, lr = 0.001
I1006 13:26:52.944159  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:26:53.279791  2824 solver.cpp:330] Iteration 83000, Testing net (#0)
I1006 13:26:55.238661  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:26:55.320858  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9237
I1006 13:26:55.320893  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315625 (* 1 = 0.315625 loss)
I1006 13:26:55.404328  2824 solver.cpp:218] Iteration 83000 (9.59657 iter/s, 10.4204s/100 iters), loss = 0.00120791
I1006 13:26:55.404353  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00120743 (* 1 = 0.00120743 loss)
I1006 13:26:55.404361  2824 sgd_solver.cpp:105] Iteration 83000, lr = 0.001
I1006 13:27:03.794404  2824 solver.cpp:218] Iteration 83100 (11.9189 iter/s, 8.39002s/100 iters), loss = 0.00185259
I1006 13:27:03.794435  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00185211 (* 1 = 0.00185211 loss)
I1006 13:27:03.794441  2824 sgd_solver.cpp:105] Iteration 83100, lr = 0.001
I1006 13:27:12.175642  2824 solver.cpp:218] Iteration 83200 (11.9315 iter/s, 8.38118s/100 iters), loss = 0.00429415
I1006 13:27:12.175732  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00429367 (* 1 = 0.00429367 loss)
I1006 13:27:12.175748  2824 sgd_solver.cpp:105] Iteration 83200, lr = 0.001
I1006 13:27:20.556272  2824 solver.cpp:218] Iteration 83300 (11.9324 iter/s, 8.38052s/100 iters), loss = 0.00164779
I1006 13:27:20.556310  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00164731 (* 1 = 0.00164731 loss)
I1006 13:27:20.556316  2824 sgd_solver.cpp:105] Iteration 83300, lr = 0.001
I1006 13:27:28.937045  2824 solver.cpp:218] Iteration 83400 (11.9322 iter/s, 8.38071s/100 iters), loss = 0.000552059
I1006 13:27:28.937088  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000551579 (* 1 = 0.000551579 loss)
I1006 13:27:28.937093  2824 sgd_solver.cpp:105] Iteration 83400, lr = 0.001
I1006 13:27:36.902050  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:27:37.236816  2824 solver.cpp:330] Iteration 83500, Testing net (#0)
I1006 13:27:39.198060  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:27:39.280396  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9246
I1006 13:27:39.280438  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314681 (* 1 = 0.314681 loss)
I1006 13:27:39.364567  2824 solver.cpp:218] Iteration 83500 (9.59007 iter/s, 10.4274s/100 iters), loss = 0.00180493
I1006 13:27:39.364593  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00180445 (* 1 = 0.00180445 loss)
I1006 13:27:39.364600  2824 sgd_solver.cpp:105] Iteration 83500, lr = 0.001
I1006 13:27:47.757099  2824 solver.cpp:218] Iteration 83600 (11.9154 iter/s, 8.39248s/100 iters), loss = 0.00225414
I1006 13:27:47.757212  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00225366 (* 1 = 0.00225366 loss)
I1006 13:27:47.757220  2824 sgd_solver.cpp:105] Iteration 83600, lr = 0.001
I1006 13:27:56.154763  2824 solver.cpp:218] Iteration 83700 (11.9083 iter/s, 8.39753s/100 iters), loss = 0.010002
I1006 13:27:56.154803  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0100015 (* 1 = 0.0100015 loss)
I1006 13:27:56.154809  2824 sgd_solver.cpp:105] Iteration 83700, lr = 0.001
I1006 13:28:04.549311  2824 solver.cpp:218] Iteration 83800 (11.9126 iter/s, 8.39448s/100 iters), loss = 0.00320973
I1006 13:28:04.549350  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00320925 (* 1 = 0.00320925 loss)
I1006 13:28:04.549356  2824 sgd_solver.cpp:105] Iteration 83800, lr = 0.001
I1006 13:28:12.939339  2824 solver.cpp:218] Iteration 83900 (11.919 iter/s, 8.38996s/100 iters), loss = 0.00192352
I1006 13:28:12.939368  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00192304 (* 1 = 0.00192304 loss)
I1006 13:28:12.939374  2824 sgd_solver.cpp:105] Iteration 83900, lr = 0.001
I1006 13:28:20.907050  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:28:21.242596  2824 solver.cpp:330] Iteration 84000, Testing net (#0)
I1006 13:28:23.202375  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:28:23.284250  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.924
I1006 13:28:23.284286  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314088 (* 1 = 0.314088 loss)
I1006 13:28:23.368240  2824 solver.cpp:218] Iteration 84000 (9.58879 iter/s, 10.4288s/100 iters), loss = 0.00069906
I1006 13:28:23.368265  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000698575 (* 1 = 0.000698575 loss)
I1006 13:28:23.368273  2824 sgd_solver.cpp:105] Iteration 84000, lr = 0.001
I1006 13:28:31.757585  2824 solver.cpp:218] Iteration 84100 (11.92 iter/s, 8.38929s/100 iters), loss = 0.0051658
I1006 13:28:31.757624  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00516532 (* 1 = 0.00516532 loss)
I1006 13:28:31.757630  2824 sgd_solver.cpp:105] Iteration 84100, lr = 0.001
I1006 13:28:40.144718  2824 solver.cpp:218] Iteration 84200 (11.9231 iter/s, 8.38707s/100 iters), loss = 0.00265224
I1006 13:28:40.144760  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00265175 (* 1 = 0.00265175 loss)
I1006 13:28:40.144767  2824 sgd_solver.cpp:105] Iteration 84200, lr = 0.001
I1006 13:28:48.524011  2824 solver.cpp:218] Iteration 84300 (11.9343 iter/s, 8.37923s/100 iters), loss = 0.000936433
I1006 13:28:48.524052  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000935946 (* 1 = 0.000935946 loss)
I1006 13:28:48.524058  2824 sgd_solver.cpp:105] Iteration 84300, lr = 0.001
I1006 13:28:56.905555  2824 solver.cpp:218] Iteration 84400 (11.9311 iter/s, 8.38148s/100 iters), loss = 0.00064912
I1006 13:28:56.905661  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000648633 (* 1 = 0.000648633 loss)
I1006 13:28:56.905669  2824 sgd_solver.cpp:105] Iteration 84400, lr = 0.001
I1006 13:29:04.865198  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:29:05.200542  2824 solver.cpp:330] Iteration 84500, Testing net (#0)
I1006 13:29:07.159209  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:29:07.241020  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9244
I1006 13:29:07.241055  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315294 (* 1 = 0.315294 loss)
I1006 13:29:07.325047  2824 solver.cpp:218] Iteration 84500 (9.59752 iter/s, 10.4194s/100 iters), loss = 0.00131676
I1006 13:29:07.325073  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00131627 (* 1 = 0.00131627 loss)
I1006 13:29:07.325078  2824 sgd_solver.cpp:105] Iteration 84500, lr = 0.001
I1006 13:29:15.718128  2824 solver.cpp:218] Iteration 84600 (11.9147 iter/s, 8.39303s/100 iters), loss = 0.0134053
I1006 13:29:15.718160  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0134048 (* 1 = 0.0134048 loss)
I1006 13:29:15.718166  2824 sgd_solver.cpp:105] Iteration 84600, lr = 0.001
I1006 13:29:24.110862  2824 solver.cpp:218] Iteration 84700 (11.9152 iter/s, 8.39268s/100 iters), loss = 0.00369113
I1006 13:29:24.110894  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00369064 (* 1 = 0.00369064 loss)
I1006 13:29:24.110903  2824 sgd_solver.cpp:105] Iteration 84700, lr = 0.001
I1006 13:29:32.497555  2824 solver.cpp:218] Iteration 84800 (11.9237 iter/s, 8.38664s/100 iters), loss = 0.00279089
I1006 13:29:32.497675  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0027904 (* 1 = 0.0027904 loss)
I1006 13:29:32.497694  2824 sgd_solver.cpp:105] Iteration 84800, lr = 0.001
I1006 13:29:40.882920  2824 solver.cpp:218] Iteration 84900 (11.9257 iter/s, 8.38523s/100 iters), loss = 0.00107563
I1006 13:29:40.882951  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00107514 (* 1 = 0.00107514 loss)
I1006 13:29:40.882959  2824 sgd_solver.cpp:105] Iteration 84900, lr = 0.001
I1006 13:29:48.855605  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:29:49.191864  2824 solver.cpp:330] Iteration 85000, Testing net (#0)
I1006 13:29:51.149066  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:29:51.231178  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9241
I1006 13:29:51.231214  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.320096 (* 1 = 0.320096 loss)
I1006 13:29:51.315229  2824 solver.cpp:218] Iteration 85000 (9.58566 iter/s, 10.4322s/100 iters), loss = 0.00291148
I1006 13:29:51.315254  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00291098 (* 1 = 0.00291098 loss)
I1006 13:29:51.315260  2824 sgd_solver.cpp:105] Iteration 85000, lr = 0.001
I1006 13:29:59.698705  2824 solver.cpp:218] Iteration 85100 (11.9283 iter/s, 8.38342s/100 iters), loss = 0.00320442
I1006 13:29:59.698736  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00320392 (* 1 = 0.00320392 loss)
I1006 13:29:59.698742  2824 sgd_solver.cpp:105] Iteration 85100, lr = 0.001
I1006 13:30:08.078819  2824 solver.cpp:218] Iteration 85200 (11.9331 iter/s, 8.38005s/100 iters), loss = 0.00137137
I1006 13:30:08.078935  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00137088 (* 1 = 0.00137088 loss)
I1006 13:30:08.078943  2824 sgd_solver.cpp:105] Iteration 85200, lr = 0.001
I1006 13:30:16.454126  2824 solver.cpp:218] Iteration 85300 (11.94 iter/s, 8.37518s/100 iters), loss = 0.00254922
I1006 13:30:16.454166  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00254872 (* 1 = 0.00254872 loss)
I1006 13:30:16.454172  2824 sgd_solver.cpp:105] Iteration 85300, lr = 0.001
I1006 13:30:24.832923  2824 solver.cpp:218] Iteration 85400 (11.935 iter/s, 8.37873s/100 iters), loss = 0.00213198
I1006 13:30:24.832954  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00213149 (* 1 = 0.00213149 loss)
I1006 13:30:24.832962  2824 sgd_solver.cpp:105] Iteration 85400, lr = 0.001
I1006 13:30:32.790426  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:30:33.126251  2824 solver.cpp:330] Iteration 85500, Testing net (#0)
I1006 13:30:35.084458  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:30:35.166867  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9228
I1006 13:30:35.166903  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317398 (* 1 = 0.317398 loss)
I1006 13:30:35.250485  2824 solver.cpp:218] Iteration 85500 (9.59923 iter/s, 10.4175s/100 iters), loss = 0.000740012
I1006 13:30:35.250510  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000739519 (* 1 = 0.000739519 loss)
I1006 13:30:35.250517  2824 sgd_solver.cpp:105] Iteration 85500, lr = 0.001
I1006 13:30:43.633143  2824 solver.cpp:218] Iteration 85600 (11.9295 iter/s, 8.3826s/100 iters), loss = 0.00594176
I1006 13:30:43.633278  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00594126 (* 1 = 0.00594126 loss)
I1006 13:30:43.633286  2824 sgd_solver.cpp:105] Iteration 85600, lr = 0.001
I1006 13:30:52.009177  2824 solver.cpp:218] Iteration 85700 (11.939 iter/s, 8.37588s/100 iters), loss = 0.000653009
I1006 13:30:52.009218  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000652514 (* 1 = 0.000652514 loss)
I1006 13:30:52.009224  2824 sgd_solver.cpp:105] Iteration 85700, lr = 0.001
I1006 13:31:00.382874  2824 solver.cpp:218] Iteration 85800 (11.9423 iter/s, 8.37363s/100 iters), loss = 0.000764345
I1006 13:31:00.382915  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00076385 (* 1 = 0.00076385 loss)
I1006 13:31:00.382921  2824 sgd_solver.cpp:105] Iteration 85800, lr = 0.001
I1006 13:31:08.755875  2824 solver.cpp:218] Iteration 85900 (11.9432 iter/s, 8.37293s/100 iters), loss = 0.00511263
I1006 13:31:08.755906  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00511214 (* 1 = 0.00511214 loss)
I1006 13:31:08.755913  2824 sgd_solver.cpp:105] Iteration 85900, lr = 0.001
I1006 13:31:16.715699  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:31:17.050644  2824 solver.cpp:330] Iteration 86000, Testing net (#0)
I1006 13:31:19.010592  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:31:19.092411  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9225
I1006 13:31:19.092447  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316318 (* 1 = 0.316318 loss)
I1006 13:31:19.176775  2824 solver.cpp:218] Iteration 86000 (9.59616 iter/s, 10.4208s/100 iters), loss = 0.00122071
I1006 13:31:19.176802  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00122022 (* 1 = 0.00122022 loss)
I1006 13:31:19.176810  2824 sgd_solver.cpp:105] Iteration 86000, lr = 0.001
I1006 13:31:27.560573  2824 solver.cpp:218] Iteration 86100 (11.9278 iter/s, 8.38374s/100 iters), loss = 0.001284
I1006 13:31:27.560616  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00128351 (* 1 = 0.00128351 loss)
I1006 13:31:27.560622  2824 sgd_solver.cpp:105] Iteration 86100, lr = 0.001
I1006 13:31:35.950764  2824 solver.cpp:218] Iteration 86200 (11.9188 iter/s, 8.39012s/100 iters), loss = 0.000514226
I1006 13:31:35.950806  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000513737 (* 1 = 0.000513737 loss)
I1006 13:31:35.950811  2824 sgd_solver.cpp:105] Iteration 86200, lr = 0.001
I1006 13:31:44.330365  2824 solver.cpp:218] Iteration 86300 (11.9338 iter/s, 8.37953s/100 iters), loss = 0.000987806
I1006 13:31:44.330407  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000987317 (* 1 = 0.000987317 loss)
I1006 13:31:44.330412  2824 sgd_solver.cpp:105] Iteration 86300, lr = 0.001
I1006 13:31:52.715927  2824 solver.cpp:218] Iteration 86400 (11.9254 iter/s, 8.38549s/100 iters), loss = 0.00189643
I1006 13:31:52.716038  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00189594 (* 1 = 0.00189594 loss)
I1006 13:31:52.716054  2824 sgd_solver.cpp:105] Iteration 86400, lr = 0.001
I1006 13:32:00.682639  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:32:01.018674  2824 solver.cpp:330] Iteration 86500, Testing net (#0)
I1006 13:32:02.976274  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:32:03.058014  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9245
I1006 13:32:03.058050  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314319 (* 1 = 0.314319 loss)
I1006 13:32:03.142088  2824 solver.cpp:218] Iteration 86500 (9.59138 iter/s, 10.426s/100 iters), loss = 0.0015043
I1006 13:32:03.142112  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00150382 (* 1 = 0.00150382 loss)
I1006 13:32:03.142119  2824 sgd_solver.cpp:105] Iteration 86500, lr = 0.001
I1006 13:32:11.526623  2824 solver.cpp:218] Iteration 86600 (11.9268 iter/s, 8.38448s/100 iters), loss = 0.000984806
I1006 13:32:11.526654  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00098432 (* 1 = 0.00098432 loss)
I1006 13:32:11.526661  2824 sgd_solver.cpp:105] Iteration 86600, lr = 0.001
I1006 13:32:19.906056  2824 solver.cpp:218] Iteration 86700 (11.9341 iter/s, 8.37938s/100 iters), loss = 0.00218492
I1006 13:32:19.906097  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00218443 (* 1 = 0.00218443 loss)
I1006 13:32:19.906103  2824 sgd_solver.cpp:105] Iteration 86700, lr = 0.001
I1006 13:32:28.278365  2824 solver.cpp:218] Iteration 86800 (11.9442 iter/s, 8.37224s/100 iters), loss = 0.0038527
I1006 13:32:28.278467  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00385222 (* 1 = 0.00385222 loss)
I1006 13:32:28.278486  2824 sgd_solver.cpp:105] Iteration 86800, lr = 0.001
I1006 13:32:36.662259  2824 solver.cpp:218] Iteration 86900 (11.9278 iter/s, 8.38378s/100 iters), loss = 0.0034383
I1006 13:32:36.662300  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00343782 (* 1 = 0.00343782 loss)
I1006 13:32:36.662307  2824 sgd_solver.cpp:105] Iteration 86900, lr = 0.001
I1006 13:32:44.626355  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:32:44.961841  2824 solver.cpp:330] Iteration 87000, Testing net (#0)
I1006 13:32:46.919935  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:32:47.001868  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9247
I1006 13:32:47.001904  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.313819 (* 1 = 0.313819 loss)
I1006 13:32:47.085867  2824 solver.cpp:218] Iteration 87000 (9.59367 iter/s, 10.4235s/100 iters), loss = 0.00107146
I1006 13:32:47.085893  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00107097 (* 1 = 0.00107097 loss)
I1006 13:32:47.085901  2824 sgd_solver.cpp:105] Iteration 87000, lr = 0.001
I1006 13:32:55.470010  2824 solver.cpp:218] Iteration 87100 (11.9274 iter/s, 8.38409s/100 iters), loss = 0.00188775
I1006 13:32:55.470041  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00188726 (* 1 = 0.00188726 loss)
I1006 13:32:55.470047  2824 sgd_solver.cpp:105] Iteration 87100, lr = 0.001
I1006 13:33:03.854365  2824 solver.cpp:218] Iteration 87200 (11.9271 iter/s, 8.3843s/100 iters), loss = 0.00093166
I1006 13:33:03.854485  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000931175 (* 1 = 0.000931175 loss)
I1006 13:33:03.854493  2824 sgd_solver.cpp:105] Iteration 87200, lr = 0.001
I1006 13:33:12.236135  2824 solver.cpp:218] Iteration 87300 (11.9309 iter/s, 8.38162s/100 iters), loss = 0.00186043
I1006 13:33:12.236176  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00185995 (* 1 = 0.00185995 loss)
I1006 13:33:12.236181  2824 sgd_solver.cpp:105] Iteration 87300, lr = 0.001
I1006 13:33:20.616101  2824 solver.cpp:218] Iteration 87400 (11.9333 iter/s, 8.3799s/100 iters), loss = 0.00186581
I1006 13:33:20.616142  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00186533 (* 1 = 0.00186533 loss)
I1006 13:33:20.616148  2824 sgd_solver.cpp:105] Iteration 87400, lr = 0.001
I1006 13:33:28.578951  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:33:28.914201  2824 solver.cpp:330] Iteration 87500, Testing net (#0)
I1006 13:33:30.872992  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:33:30.954824  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9244
I1006 13:33:30.954860  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314826 (* 1 = 0.314826 loss)
I1006 13:33:31.039085  2824 solver.cpp:218] Iteration 87500 (9.59425 iter/s, 10.4229s/100 iters), loss = 0.00116443
I1006 13:33:31.039111  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00116394 (* 1 = 0.00116394 loss)
I1006 13:33:31.039119  2824 sgd_solver.cpp:105] Iteration 87500, lr = 0.001
I1006 13:33:39.426798  2824 solver.cpp:218] Iteration 87600 (11.9223 iter/s, 8.38766s/100 iters), loss = 0.00390246
I1006 13:33:39.426942  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00390197 (* 1 = 0.00390197 loss)
I1006 13:33:39.426951  2824 sgd_solver.cpp:105] Iteration 87600, lr = 0.001
I1006 13:33:47.815062  2824 solver.cpp:218] Iteration 87700 (11.9217 iter/s, 8.38809s/100 iters), loss = 0.00274173
I1006 13:33:47.815102  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00274125 (* 1 = 0.00274125 loss)
I1006 13:33:47.815109  2824 sgd_solver.cpp:105] Iteration 87700, lr = 0.001
I1006 13:33:56.198297  2824 solver.cpp:218] Iteration 87800 (11.9287 iter/s, 8.38317s/100 iters), loss = 0.00219281
I1006 13:33:56.198338  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00219232 (* 1 = 0.00219232 loss)
I1006 13:33:56.198344  2824 sgd_solver.cpp:105] Iteration 87800, lr = 0.001
I1006 13:34:04.587875  2824 solver.cpp:218] Iteration 87900 (11.9196 iter/s, 8.38951s/100 iters), loss = 0.00194452
I1006 13:34:04.587918  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00194404 (* 1 = 0.00194404 loss)
I1006 13:34:04.587924  2824 sgd_solver.cpp:105] Iteration 87900, lr = 0.001
I1006 13:34:12.560346  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:34:12.896365  2824 solver.cpp:330] Iteration 88000, Testing net (#0)
I1006 13:34:14.854804  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:34:14.937026  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9236
I1006 13:34:14.937062  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315512 (* 1 = 0.315512 loss)
I1006 13:34:15.021235  2824 solver.cpp:218] Iteration 88000 (9.58471 iter/s, 10.4333s/100 iters), loss = 0.000941428
I1006 13:34:15.021261  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000940947 (* 1 = 0.000940947 loss)
I1006 13:34:15.021268  2824 sgd_solver.cpp:105] Iteration 88000, lr = 0.001
I1006 13:34:23.400722  2824 solver.cpp:218] Iteration 88100 (11.934 iter/s, 8.37943s/100 iters), loss = 0.0021793
I1006 13:34:23.400751  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00217882 (* 1 = 0.00217882 loss)
I1006 13:34:23.400758  2824 sgd_solver.cpp:105] Iteration 88100, lr = 0.001
I1006 13:34:31.771462  2824 solver.cpp:218] Iteration 88200 (11.9465 iter/s, 8.37068s/100 iters), loss = 0.00313441
I1006 13:34:31.771493  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00313393 (* 1 = 0.00313393 loss)
I1006 13:34:31.771499  2824 sgd_solver.cpp:105] Iteration 88200, lr = 0.001
I1006 13:34:40.153518  2824 solver.cpp:218] Iteration 88300 (11.9303 iter/s, 8.382s/100 iters), loss = 0.00114805
I1006 13:34:40.153559  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00114757 (* 1 = 0.00114757 loss)
I1006 13:34:40.153565  2824 sgd_solver.cpp:105] Iteration 88300, lr = 0.001
I1006 13:34:48.537189  2824 solver.cpp:218] Iteration 88400 (11.9281 iter/s, 8.3836s/100 iters), loss = 0.00504976
I1006 13:34:48.537279  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00504928 (* 1 = 0.00504928 loss)
I1006 13:34:48.537297  2824 sgd_solver.cpp:105] Iteration 88400, lr = 0.001
I1006 13:34:56.503006  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:34:56.838083  2824 solver.cpp:330] Iteration 88500, Testing net (#0)
I1006 13:34:58.796351  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:34:58.878093  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9241
I1006 13:34:58.878129  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316402 (* 1 = 0.316402 loss)
I1006 13:34:58.961786  2824 solver.cpp:218] Iteration 88500 (9.59281 iter/s, 10.4245s/100 iters), loss = 0.0029213
I1006 13:34:58.961812  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00292082 (* 1 = 0.00292082 loss)
I1006 13:34:58.961818  2824 sgd_solver.cpp:105] Iteration 88500, lr = 0.001
I1006 13:35:07.340371  2824 solver.cpp:218] Iteration 88600 (11.9353 iter/s, 8.37853s/100 iters), loss = 0.0021896
I1006 13:35:07.340404  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00218912 (* 1 = 0.00218912 loss)
I1006 13:35:07.340409  2824 sgd_solver.cpp:105] Iteration 88600, lr = 0.001
I1006 13:35:15.723325  2824 solver.cpp:218] Iteration 88700 (11.9291 iter/s, 8.38286s/100 iters), loss = 0.00122578
I1006 13:35:15.723356  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00122529 (* 1 = 0.00122529 loss)
I1006 13:35:15.723361  2824 sgd_solver.cpp:105] Iteration 88700, lr = 0.001
I1006 13:35:24.101140  2824 solver.cpp:218] Iteration 88800 (11.9364 iter/s, 8.37776s/100 iters), loss = 0.0012273
I1006 13:35:24.101235  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00122682 (* 1 = 0.00122682 loss)
I1006 13:35:24.101243  2824 sgd_solver.cpp:105] Iteration 88800, lr = 0.001
I1006 13:35:32.482017  2824 solver.cpp:218] Iteration 88900 (11.9321 iter/s, 8.38076s/100 iters), loss = 0.0030163
I1006 13:35:32.482059  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00301582 (* 1 = 0.00301582 loss)
I1006 13:35:32.482064  2824 sgd_solver.cpp:105] Iteration 88900, lr = 0.001
I1006 13:35:40.444319  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:35:40.780640  2824 solver.cpp:330] Iteration 89000, Testing net (#0)
I1006 13:35:42.738421  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:35:42.820250  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9243
I1006 13:35:42.820286  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315586 (* 1 = 0.315586 loss)
I1006 13:35:42.904014  2824 solver.cpp:218] Iteration 89000 (9.59516 iter/s, 10.4219s/100 iters), loss = 0.00115647
I1006 13:35:42.904040  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00115599 (* 1 = 0.00115599 loss)
I1006 13:35:42.904047  2824 sgd_solver.cpp:105] Iteration 89000, lr = 0.001
I1006 13:35:51.285269  2824 solver.cpp:218] Iteration 89100 (11.9315 iter/s, 8.3812s/100 iters), loss = 0.00295794
I1006 13:35:51.285298  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00295746 (* 1 = 0.00295746 loss)
I1006 13:35:51.285305  2824 sgd_solver.cpp:105] Iteration 89100, lr = 0.001
I1006 13:35:59.662907  2824 solver.cpp:218] Iteration 89200 (11.9366 iter/s, 8.37758s/100 iters), loss = 0.00140341
I1006 13:35:59.663043  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00140293 (* 1 = 0.00140293 loss)
I1006 13:35:59.663060  2824 sgd_solver.cpp:105] Iteration 89200, lr = 0.001
I1006 13:36:08.037292  2824 solver.cpp:218] Iteration 89300 (11.9414 iter/s, 8.37423s/100 iters), loss = 0.0046618
I1006 13:36:08.037322  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00466132 (* 1 = 0.00466132 loss)
I1006 13:36:08.037328  2824 sgd_solver.cpp:105] Iteration 89300, lr = 0.001
I1006 13:36:16.419176  2824 solver.cpp:218] Iteration 89400 (11.9306 iter/s, 8.38183s/100 iters), loss = 0.00311878
I1006 13:36:16.419208  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0031183 (* 1 = 0.0031183 loss)
I1006 13:36:16.419214  2824 sgd_solver.cpp:105] Iteration 89400, lr = 0.001
I1006 13:36:24.383817  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:36:24.718336  2824 solver.cpp:330] Iteration 89500, Testing net (#0)
I1006 13:36:26.673413  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:36:26.755141  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.924
I1006 13:36:26.755179  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314233 (* 1 = 0.314233 loss)
I1006 13:36:26.838591  2824 solver.cpp:218] Iteration 89500 (9.59753 iter/s, 10.4194s/100 iters), loss = 0.00120635
I1006 13:36:26.838616  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00120587 (* 1 = 0.00120587 loss)
I1006 13:36:26.838624  2824 sgd_solver.cpp:105] Iteration 89500, lr = 0.001
I1006 13:36:35.218588  2824 solver.cpp:218] Iteration 89600 (11.9333 iter/s, 8.37994s/100 iters), loss = 0.00180934
I1006 13:36:35.218739  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00180885 (* 1 = 0.00180885 loss)
I1006 13:36:35.218745  2824 sgd_solver.cpp:105] Iteration 89600, lr = 0.001
I1006 13:36:43.597398  2824 solver.cpp:218] Iteration 89700 (11.9351 iter/s, 8.37863s/100 iters), loss = 0.00123276
I1006 13:36:43.597429  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00123228 (* 1 = 0.00123228 loss)
I1006 13:36:43.597434  2824 sgd_solver.cpp:105] Iteration 89700, lr = 0.001
I1006 13:36:51.969116  2824 solver.cpp:218] Iteration 89800 (11.9451 iter/s, 8.37166s/100 iters), loss = 0.00167061
I1006 13:36:51.969147  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00167012 (* 1 = 0.00167012 loss)
I1006 13:36:51.969163  2824 sgd_solver.cpp:105] Iteration 89800, lr = 0.001
I1006 13:37:00.345533  2824 solver.cpp:218] Iteration 89900 (11.9384 iter/s, 8.37636s/100 iters), loss = 0.0025428
I1006 13:37:00.345564  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00254232 (* 1 = 0.00254232 loss)
I1006 13:37:00.345571  2824 sgd_solver.cpp:105] Iteration 89900, lr = 0.001
I1006 13:37:08.305122  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:37:08.639948  2824 solver.cpp:330] Iteration 90000, Testing net (#0)
I1006 13:37:10.597275  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:37:10.678635  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9239
I1006 13:37:10.678676  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.31424 (* 1 = 0.31424 loss)
I1006 13:37:10.761864  2824 solver.cpp:218] Iteration 90000 (9.60037 iter/s, 10.4163s/100 iters), loss = 0.00104081
I1006 13:37:10.761896  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00104032 (* 1 = 0.00104032 loss)
I1006 13:37:10.761904  2824 sgd_solver.cpp:105] Iteration 90000, lr = 0.001
I1006 13:37:19.150044  2824 solver.cpp:218] Iteration 90100 (11.9216 iter/s, 8.38812s/100 iters), loss = 0.00212075
I1006 13:37:19.150076  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00212026 (* 1 = 0.00212026 loss)
I1006 13:37:19.150082  2824 sgd_solver.cpp:105] Iteration 90100, lr = 0.001
I1006 13:37:27.536233  2824 solver.cpp:218] Iteration 90200 (11.9245 iter/s, 8.38613s/100 iters), loss = 0.00153038
I1006 13:37:27.536274  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00152989 (* 1 = 0.00152989 loss)
I1006 13:37:27.536280  2824 sgd_solver.cpp:105] Iteration 90200, lr = 0.001
I1006 13:37:35.914170  2824 solver.cpp:218] Iteration 90300 (11.9362 iter/s, 8.37787s/100 iters), loss = 0.00118968
I1006 13:37:35.914201  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0011892 (* 1 = 0.0011892 loss)
I1006 13:37:35.914217  2824 sgd_solver.cpp:105] Iteration 90300, lr = 0.001
I1006 13:37:44.299713  2824 solver.cpp:218] Iteration 90400 (11.9254 iter/s, 8.38548s/100 iters), loss = 0.000673705
I1006 13:37:44.299798  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000673218 (* 1 = 0.000673218 loss)
I1006 13:37:44.299814  2824 sgd_solver.cpp:105] Iteration 90400, lr = 0.001
I1006 13:37:52.262698  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:37:52.597789  2824 solver.cpp:330] Iteration 90500, Testing net (#0)
I1006 13:37:54.557082  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:37:54.639037  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9254
I1006 13:37:54.639073  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315886 (* 1 = 0.315886 loss)
I1006 13:37:54.722987  2824 solver.cpp:218] Iteration 90500 (9.59401 iter/s, 10.4232s/100 iters), loss = 0.000727464
I1006 13:37:54.723012  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000726976 (* 1 = 0.000726976 loss)
I1006 13:37:54.723019  2824 sgd_solver.cpp:105] Iteration 90500, lr = 0.001
I1006 13:38:03.103169  2824 solver.cpp:218] Iteration 90600 (11.933 iter/s, 8.38013s/100 iters), loss = 0.00229421
I1006 13:38:03.103200  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00229372 (* 1 = 0.00229372 loss)
I1006 13:38:03.103206  2824 sgd_solver.cpp:105] Iteration 90600, lr = 0.001
I1006 13:38:11.485294  2824 solver.cpp:218] Iteration 90700 (11.9302 iter/s, 8.38206s/100 iters), loss = 0.00245806
I1006 13:38:11.485327  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00245757 (* 1 = 0.00245757 loss)
I1006 13:38:11.485342  2824 sgd_solver.cpp:105] Iteration 90700, lr = 0.001
I1006 13:38:19.864811  2824 solver.cpp:218] Iteration 90800 (11.9339 iter/s, 8.37946s/100 iters), loss = 0.00281275
I1006 13:38:19.864936  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00281227 (* 1 = 0.00281227 loss)
I1006 13:38:19.864943  2824 sgd_solver.cpp:105] Iteration 90800, lr = 0.001
I1006 13:38:28.246268  2824 solver.cpp:218] Iteration 90900 (11.9313 iter/s, 8.38132s/100 iters), loss = 0.00204794
I1006 13:38:28.246310  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00204746 (* 1 = 0.00204746 loss)
I1006 13:38:28.246316  2824 sgd_solver.cpp:105] Iteration 90900, lr = 0.001
I1006 13:38:36.208060  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:38:36.543823  2824 solver.cpp:330] Iteration 91000, Testing net (#0)
I1006 13:38:38.501286  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:38:38.583855  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9238
I1006 13:38:38.583890  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316627 (* 1 = 0.316627 loss)
I1006 13:38:38.667681  2824 solver.cpp:218] Iteration 91000 (9.59569 iter/s, 10.4213s/100 iters), loss = 0.00956518
I1006 13:38:38.667706  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00956469 (* 1 = 0.00956469 loss)
I1006 13:38:38.667712  2824 sgd_solver.cpp:105] Iteration 91000, lr = 0.001
I1006 13:38:47.054318  2824 solver.cpp:218] Iteration 91100 (11.9238 iter/s, 8.38658s/100 iters), loss = 0.00155044
I1006 13:38:47.054349  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00154996 (* 1 = 0.00154996 loss)
I1006 13:38:47.054355  2824 sgd_solver.cpp:105] Iteration 91100, lr = 0.001
I1006 13:38:55.431119  2824 solver.cpp:218] Iteration 91200 (11.9378 iter/s, 8.37674s/100 iters), loss = 0.0014521
I1006 13:38:55.431216  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00145162 (* 1 = 0.00145162 loss)
I1006 13:38:55.431222  2824 sgd_solver.cpp:105] Iteration 91200, lr = 0.001
I1006 13:39:03.806998  2824 solver.cpp:218] Iteration 91300 (11.9392 iter/s, 8.37576s/100 iters), loss = 0.00156036
I1006 13:39:03.807027  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00155987 (* 1 = 0.00155987 loss)
I1006 13:39:03.807034  2824 sgd_solver.cpp:105] Iteration 91300, lr = 0.001
I1006 13:39:12.188331  2824 solver.cpp:218] Iteration 91400 (11.9314 iter/s, 8.38128s/100 iters), loss = 0.00102595
I1006 13:39:12.188372  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00102547 (* 1 = 0.00102547 loss)
I1006 13:39:12.188379  2824 sgd_solver.cpp:105] Iteration 91400, lr = 0.001
I1006 13:39:20.146368  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:39:20.481434  2824 solver.cpp:330] Iteration 91500, Testing net (#0)
I1006 13:39:22.438519  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:39:22.520927  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9253
I1006 13:39:22.520963  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316144 (* 1 = 0.316144 loss)
I1006 13:39:22.605263  2824 solver.cpp:218] Iteration 91500 (9.59982 iter/s, 10.4169s/100 iters), loss = 0.0014129
I1006 13:39:22.605288  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00141242 (* 1 = 0.00141242 loss)
I1006 13:39:22.605295  2824 sgd_solver.cpp:105] Iteration 91500, lr = 0.001
I1006 13:39:30.987285  2824 solver.cpp:218] Iteration 91600 (11.9304 iter/s, 8.38197s/100 iters), loss = 0.00217141
I1006 13:39:30.987385  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00217092 (* 1 = 0.00217092 loss)
I1006 13:39:30.987401  2824 sgd_solver.cpp:105] Iteration 91600, lr = 0.001
I1006 13:39:39.368288  2824 solver.cpp:218] Iteration 91700 (11.9319 iter/s, 8.38088s/100 iters), loss = 0.00421442
I1006 13:39:39.368319  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00421393 (* 1 = 0.00421393 loss)
I1006 13:39:39.368325  2824 sgd_solver.cpp:105] Iteration 91700, lr = 0.001
I1006 13:39:47.749653  2824 solver.cpp:218] Iteration 91800 (11.9313 iter/s, 8.38131s/100 iters), loss = 0.00315081
I1006 13:39:47.749693  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00315032 (* 1 = 0.00315032 loss)
I1006 13:39:47.749699  2824 sgd_solver.cpp:105] Iteration 91800, lr = 0.001
I1006 13:39:56.129294  2824 solver.cpp:218] Iteration 91900 (11.9338 iter/s, 8.37957s/100 iters), loss = 0.00255453
I1006 13:39:56.129336  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00255405 (* 1 = 0.00255405 loss)
I1006 13:39:56.129343  2824 sgd_solver.cpp:105] Iteration 91900, lr = 0.001
I1006 13:40:04.095041  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:40:04.430624  2824 solver.cpp:330] Iteration 92000, Testing net (#0)
I1006 13:40:06.388859  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:40:06.471010  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9248
I1006 13:40:06.471046  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316224 (* 1 = 0.316224 loss)
I1006 13:40:06.554942  2824 solver.cpp:218] Iteration 92000 (9.5918 iter/s, 10.4256s/100 iters), loss = 0.00233471
I1006 13:40:06.554966  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00233423 (* 1 = 0.00233423 loss)
I1006 13:40:06.554972  2824 sgd_solver.cpp:105] Iteration 92000, lr = 0.001
I1006 13:40:14.937047  2824 solver.cpp:218] Iteration 92100 (11.9303 iter/s, 8.38205s/100 iters), loss = 0.000668472
I1006 13:40:14.937078  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000667988 (* 1 = 0.000667988 loss)
I1006 13:40:14.937085  2824 sgd_solver.cpp:105] Iteration 92100, lr = 0.001
I1006 13:40:23.316339  2824 solver.cpp:218] Iteration 92200 (11.9343 iter/s, 8.37923s/100 iters), loss = 0.000545952
I1006 13:40:23.316380  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000545467 (* 1 = 0.000545467 loss)
I1006 13:40:23.316386  2824 sgd_solver.cpp:105] Iteration 92200, lr = 0.001
I1006 13:40:31.694146  2824 solver.cpp:218] Iteration 92300 (11.9364 iter/s, 8.37774s/100 iters), loss = 0.000866013
I1006 13:40:31.694177  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000865529 (* 1 = 0.000865529 loss)
I1006 13:40:31.694183  2824 sgd_solver.cpp:105] Iteration 92300, lr = 0.001
I1006 13:40:40.072149  2824 solver.cpp:218] Iteration 92400 (11.9361 iter/s, 8.37794s/100 iters), loss = 0.000511703
I1006 13:40:40.072239  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000511219 (* 1 = 0.000511219 loss)
I1006 13:40:40.072257  2824 sgd_solver.cpp:105] Iteration 92400, lr = 0.001
I1006 13:40:48.032693  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:40:48.367727  2824 solver.cpp:330] Iteration 92500, Testing net (#0)
I1006 13:40:50.328346  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:40:50.410133  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.925
I1006 13:40:50.410169  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316246 (* 1 = 0.316246 loss)
I1006 13:40:50.494176  2824 solver.cpp:218] Iteration 92500 (9.59517 iter/s, 10.4219s/100 iters), loss = 0.000870428
I1006 13:40:50.494201  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000869944 (* 1 = 0.000869944 loss)
I1006 13:40:50.494208  2824 sgd_solver.cpp:105] Iteration 92500, lr = 0.001
I1006 13:40:58.878051  2824 solver.cpp:218] Iteration 92600 (11.9277 iter/s, 8.38382s/100 iters), loss = 0.00140655
I1006 13:40:58.878093  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00140607 (* 1 = 0.00140607 loss)
I1006 13:40:58.878099  2824 sgd_solver.cpp:105] Iteration 92600, lr = 0.001
I1006 13:41:07.262162  2824 solver.cpp:218] Iteration 92700 (11.9274 iter/s, 8.38404s/100 iters), loss = 0.000985986
I1006 13:41:07.262203  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000985498 (* 1 = 0.000985498 loss)
I1006 13:41:07.262209  2824 sgd_solver.cpp:105] Iteration 92700, lr = 0.001
I1006 13:41:15.644198  2824 solver.cpp:218] Iteration 92800 (11.9304 iter/s, 8.38197s/100 iters), loss = 0.00141044
I1006 13:41:15.644337  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00140995 (* 1 = 0.00140995 loss)
I1006 13:41:15.644345  2824 sgd_solver.cpp:105] Iteration 92800, lr = 0.001
I1006 13:41:24.026526  2824 solver.cpp:218] Iteration 92900 (11.9301 iter/s, 8.38217s/100 iters), loss = 0.00142035
I1006 13:41:24.026558  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00141986 (* 1 = 0.00141986 loss)
I1006 13:41:24.026566  2824 sgd_solver.cpp:105] Iteration 92900, lr = 0.001
I1006 13:41:31.989904  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:41:32.325374  2824 solver.cpp:330] Iteration 93000, Testing net (#0)
I1006 13:41:34.284117  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:41:34.366135  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9246
I1006 13:41:34.366171  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3172 (* 1 = 0.3172 loss)
I1006 13:41:34.450453  2824 solver.cpp:218] Iteration 93000 (9.59337 iter/s, 10.4239s/100 iters), loss = 0.00231773
I1006 13:41:34.450476  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00231724 (* 1 = 0.00231724 loss)
I1006 13:41:34.450484  2824 sgd_solver.cpp:105] Iteration 93000, lr = 0.001
I1006 13:41:42.832674  2824 solver.cpp:218] Iteration 93100 (11.9301 iter/s, 8.38217s/100 iters), loss = 0.000863475
I1006 13:41:42.832716  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000862984 (* 1 = 0.000862984 loss)
I1006 13:41:42.832722  2824 sgd_solver.cpp:105] Iteration 93100, lr = 0.001
I1006 13:41:51.214076  2824 solver.cpp:218] Iteration 93200 (11.9313 iter/s, 8.38133s/100 iters), loss = 0.00346987
I1006 13:41:51.214231  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00346938 (* 1 = 0.00346938 loss)
I1006 13:41:51.214238  2824 sgd_solver.cpp:105] Iteration 93200, lr = 0.001
I1006 13:41:59.587893  2824 solver.cpp:218] Iteration 93300 (11.9422 iter/s, 8.37365s/100 iters), loss = 0.000227832
I1006 13:41:59.587935  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000227343 (* 1 = 0.000227343 loss)
I1006 13:41:59.587940  2824 sgd_solver.cpp:105] Iteration 93300, lr = 0.001
I1006 13:42:07.965289  2824 solver.cpp:218] Iteration 93400 (11.937 iter/s, 8.37733s/100 iters), loss = 0.000561878
I1006 13:42:07.965332  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000561387 (* 1 = 0.000561387 loss)
I1006 13:42:07.965337  2824 sgd_solver.cpp:105] Iteration 93400, lr = 0.001
I1006 13:42:15.920969  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:42:16.256315  2824 solver.cpp:330] Iteration 93500, Testing net (#0)
I1006 13:42:18.212842  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:42:18.294600  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9258
I1006 13:42:18.294637  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316903 (* 1 = 0.316903 loss)
I1006 13:42:18.378324  2824 solver.cpp:218] Iteration 93500 (9.60342 iter/s, 10.413s/100 iters), loss = 0.00215241
I1006 13:42:18.378350  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00215192 (* 1 = 0.00215192 loss)
I1006 13:42:18.378356  2824 sgd_solver.cpp:105] Iteration 93500, lr = 0.001
I1006 13:42:26.757040  2824 solver.cpp:218] Iteration 93600 (11.9351 iter/s, 8.37866s/100 iters), loss = 0.00119082
I1006 13:42:26.757179  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00119033 (* 1 = 0.00119033 loss)
I1006 13:42:26.757186  2824 sgd_solver.cpp:105] Iteration 93600, lr = 0.001
I1006 13:42:35.136518  2824 solver.cpp:218] Iteration 93700 (11.9342 iter/s, 8.37931s/100 iters), loss = 0.00325114
I1006 13:42:35.136548  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00325065 (* 1 = 0.00325065 loss)
I1006 13:42:35.136554  2824 sgd_solver.cpp:105] Iteration 93700, lr = 0.001
I1006 13:42:43.513722  2824 solver.cpp:218] Iteration 93800 (11.9372 iter/s, 8.37715s/100 iters), loss = 0.000903439
I1006 13:42:43.513763  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000902948 (* 1 = 0.000902948 loss)
I1006 13:42:43.513769  2824 sgd_solver.cpp:105] Iteration 93800, lr = 0.001
I1006 13:42:51.890799  2824 solver.cpp:218] Iteration 93900 (11.9374 iter/s, 8.37701s/100 iters), loss = 0.000699301
I1006 13:42:51.890841  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000698809 (* 1 = 0.000698809 loss)
I1006 13:42:51.890846  2824 sgd_solver.cpp:105] Iteration 93900, lr = 0.001
I1006 13:42:59.846427  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:43:00.182627  2824 solver.cpp:330] Iteration 94000, Testing net (#0)
I1006 13:43:02.141191  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:43:02.222798  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9256
I1006 13:43:02.222825  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315456 (* 1 = 0.315456 loss)
I1006 13:43:02.306982  2824 solver.cpp:218] Iteration 94000 (9.60051 iter/s, 10.4161s/100 iters), loss = 0.000539963
I1006 13:43:02.307008  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000539472 (* 1 = 0.000539472 loss)
I1006 13:43:02.307013  2824 sgd_solver.cpp:105] Iteration 94000, lr = 0.001
I1006 13:43:10.682922  2824 solver.cpp:218] Iteration 94100 (11.939 iter/s, 8.37589s/100 iters), loss = 0.00106164
I1006 13:43:10.682963  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00106115 (* 1 = 0.00106115 loss)
I1006 13:43:10.682970  2824 sgd_solver.cpp:105] Iteration 94100, lr = 0.001
I1006 13:43:19.060989  2824 solver.cpp:218] Iteration 94200 (11.936 iter/s, 8.378s/100 iters), loss = 0.00223095
I1006 13:43:19.061031  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00223046 (* 1 = 0.00223046 loss)
I1006 13:43:19.061036  2824 sgd_solver.cpp:105] Iteration 94200, lr = 0.001
I1006 13:43:27.435325  2824 solver.cpp:218] Iteration 94300 (11.9413 iter/s, 8.37427s/100 iters), loss = 0.00406717
I1006 13:43:27.435366  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00406668 (* 1 = 0.00406668 loss)
I1006 13:43:27.435372  2824 sgd_solver.cpp:105] Iteration 94300, lr = 0.001
I1006 13:43:35.814952  2824 solver.cpp:218] Iteration 94400 (11.9338 iter/s, 8.37956s/100 iters), loss = 0.000491549
I1006 13:43:35.815084  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000491057 (* 1 = 0.000491057 loss)
I1006 13:43:35.815091  2824 sgd_solver.cpp:105] Iteration 94400, lr = 0.001
I1006 13:43:43.768991  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:43:44.104683  2824 solver.cpp:330] Iteration 94500, Testing net (#0)
I1006 13:43:46.063654  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:43:46.145267  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9241
I1006 13:43:46.145301  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317162 (* 1 = 0.317162 loss)
I1006 13:43:46.229396  2824 solver.cpp:218] Iteration 94500 (9.60219 iter/s, 10.4143s/100 iters), loss = 0.000809242
I1006 13:43:46.229420  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00080875 (* 1 = 0.00080875 loss)
I1006 13:43:46.229427  2824 sgd_solver.cpp:105] Iteration 94500, lr = 0.001
I1006 13:43:54.611213  2824 solver.cpp:218] Iteration 94600 (11.9307 iter/s, 8.38176s/100 iters), loss = 0.000965293
I1006 13:43:54.611245  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0009648 (* 1 = 0.0009648 loss)
I1006 13:43:54.611261  2824 sgd_solver.cpp:105] Iteration 94600, lr = 0.001
I1006 13:44:02.999893  2824 solver.cpp:218] Iteration 94700 (11.9209 iter/s, 8.38862s/100 iters), loss = 0.000801155
I1006 13:44:02.999935  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000800658 (* 1 = 0.000800658 loss)
I1006 13:44:02.999940  2824 sgd_solver.cpp:105] Iteration 94700, lr = 0.001
I1006 13:44:11.373862  2824 solver.cpp:218] Iteration 94800 (11.9419 iter/s, 8.3739s/100 iters), loss = 0.00373949
I1006 13:44:11.374014  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00373899 (* 1 = 0.00373899 loss)
I1006 13:44:11.374022  2824 sgd_solver.cpp:105] Iteration 94800, lr = 0.001
I1006 13:44:19.760093  2824 solver.cpp:218] Iteration 94900 (11.9246 iter/s, 8.38606s/100 iters), loss = 0.000675445
I1006 13:44:19.760134  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000674948 (* 1 = 0.000674948 loss)
I1006 13:44:19.760140  2824 sgd_solver.cpp:105] Iteration 94900, lr = 0.001
I1006 13:44:27.726794  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:44:28.062067  2824 solver.cpp:330] Iteration 95000, Testing net (#0)
I1006 13:44:30.022624  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:44:30.104897  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9244
I1006 13:44:30.104933  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.319758 (* 1 = 0.319758 loss)
I1006 13:44:30.188508  2824 solver.cpp:218] Iteration 95000 (9.58925 iter/s, 10.4283s/100 iters), loss = 0.000307159
I1006 13:44:30.188534  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000306662 (* 1 = 0.000306662 loss)
I1006 13:44:30.188541  2824 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1006 13:44:38.569696  2824 solver.cpp:218] Iteration 95100 (11.9316 iter/s, 8.38113s/100 iters), loss = 0.000651859
I1006 13:44:38.569727  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000651363 (* 1 = 0.000651363 loss)
I1006 13:44:38.569733  2824 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1006 13:44:46.953722  2824 solver.cpp:218] Iteration 95200 (11.9275 iter/s, 8.38397s/100 iters), loss = 0.002804
I1006 13:44:46.953878  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00280351 (* 1 = 0.00280351 loss)
I1006 13:44:46.953886  2824 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1006 13:44:55.335206  2824 solver.cpp:218] Iteration 95300 (11.9313 iter/s, 8.38131s/100 iters), loss = 0.000951904
I1006 13:44:55.335237  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000951409 (* 1 = 0.000951409 loss)
I1006 13:44:55.335253  2824 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1006 13:45:03.710454  2824 solver.cpp:218] Iteration 95400 (11.94 iter/s, 8.37519s/100 iters), loss = 0.000472592
I1006 13:45:03.710494  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000472096 (* 1 = 0.000472096 loss)
I1006 13:45:03.710500  2824 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1006 13:45:11.672233  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:45:12.007922  2824 solver.cpp:330] Iteration 95500, Testing net (#0)
I1006 13:45:13.966565  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:45:14.048274  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9242
I1006 13:45:14.048310  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.318137 (* 1 = 0.318137 loss)
I1006 13:45:14.132045  2824 solver.cpp:218] Iteration 95500 (9.59553 iter/s, 10.4215s/100 iters), loss = 0.000693111
I1006 13:45:14.132071  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000692615 (* 1 = 0.000692615 loss)
I1006 13:45:14.132077  2824 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1006 13:45:22.508168  2824 solver.cpp:218] Iteration 95600 (11.9388 iter/s, 8.37607s/100 iters), loss = 0.0015202
I1006 13:45:22.508280  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0015197 (* 1 = 0.0015197 loss)
I1006 13:45:22.508288  2824 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1006 13:45:30.885784  2824 solver.cpp:218] Iteration 95700 (11.9368 iter/s, 8.37745s/100 iters), loss = 0.00383561
I1006 13:45:30.885826  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00383512 (* 1 = 0.00383512 loss)
I1006 13:45:30.885833  2824 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1006 13:45:39.259300  2824 solver.cpp:218] Iteration 95800 (11.9425 iter/s, 8.37344s/100 iters), loss = 0.00055073
I1006 13:45:39.259331  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000550236 (* 1 = 0.000550236 loss)
I1006 13:45:39.259347  2824 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1006 13:45:47.642745  2824 solver.cpp:218] Iteration 95900 (11.9284 iter/s, 8.38339s/100 iters), loss = 0.00111359
I1006 13:45:47.642786  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00111309 (* 1 = 0.00111309 loss)
I1006 13:45:47.642791  2824 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1006 13:45:55.610621  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:45:55.945814  2824 solver.cpp:330] Iteration 96000, Testing net (#0)
I1006 13:45:57.903971  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:45:57.986099  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.925
I1006 13:45:57.986135  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317699 (* 1 = 0.317699 loss)
I1006 13:45:58.070155  2824 solver.cpp:218] Iteration 96000 (9.59017 iter/s, 10.4273s/100 iters), loss = 0.000607679
I1006 13:45:58.070181  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000607186 (* 1 = 0.000607186 loss)
I1006 13:45:58.070188  2824 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1006 13:46:06.455198  2824 solver.cpp:218] Iteration 96100 (11.9261 iter/s, 8.38499s/100 iters), loss = 0.00213331
I1006 13:46:06.455226  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00213281 (* 1 = 0.00213281 loss)
I1006 13:46:06.455232  2824 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1006 13:46:14.840270  2824 solver.cpp:218] Iteration 96200 (11.926 iter/s, 8.38502s/100 iters), loss = 0.000846967
I1006 13:46:14.840311  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000846474 (* 1 = 0.000846474 loss)
I1006 13:46:14.840317  2824 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1006 13:46:23.216055  2824 solver.cpp:218] Iteration 96300 (11.9393 iter/s, 8.37572s/100 iters), loss = 0.00130005
I1006 13:46:23.216095  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00129956 (* 1 = 0.00129956 loss)
I1006 13:46:23.216101  2824 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1006 13:46:31.599007  2824 solver.cpp:218] Iteration 96400 (11.9291 iter/s, 8.38289s/100 iters), loss = 0.000475691
I1006 13:46:31.599119  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0004752 (* 1 = 0.0004752 loss)
I1006 13:46:31.599136  2824 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1006 13:46:39.561728  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:46:39.897837  2824 solver.cpp:330] Iteration 96500, Testing net (#0)
I1006 13:46:41.855204  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:46:41.937064  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9246
I1006 13:46:41.937100  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317312 (* 1 = 0.317312 loss)
I1006 13:46:42.021080  2824 solver.cpp:218] Iteration 96500 (9.59515 iter/s, 10.4219s/100 iters), loss = 0.00486946
I1006 13:46:42.021106  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00486897 (* 1 = 0.00486897 loss)
I1006 13:46:42.021113  2824 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1006 13:46:50.402503  2824 solver.cpp:218] Iteration 96600 (11.9312 iter/s, 8.38137s/100 iters), loss = 0.00190961
I1006 13:46:50.402544  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00190911 (* 1 = 0.00190911 loss)
I1006 13:46:50.402550  2824 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1006 13:46:58.781150  2824 solver.cpp:218] Iteration 96700 (11.9352 iter/s, 8.37858s/100 iters), loss = 0.00146143
I1006 13:46:58.781191  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00146093 (* 1 = 0.00146093 loss)
I1006 13:46:58.781198  2824 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1006 13:47:07.157249  2824 solver.cpp:218] Iteration 96800 (11.9388 iter/s, 8.37603s/100 iters), loss = 0.000627097
I1006 13:47:07.157383  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000626604 (* 1 = 0.000626604 loss)
I1006 13:47:07.157390  2824 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1006 13:47:15.542886  2824 solver.cpp:218] Iteration 96900 (11.9254 iter/s, 8.38549s/100 iters), loss = 0.00176185
I1006 13:47:15.542927  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00176136 (* 1 = 0.00176136 loss)
I1006 13:47:15.542933  2824 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1006 13:47:23.506479  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:47:23.842051  2824 solver.cpp:330] Iteration 97000, Testing net (#0)
I1006 13:47:25.800546  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:47:25.882424  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9243
I1006 13:47:25.882458  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317064 (* 1 = 0.317064 loss)
I1006 13:47:25.966490  2824 solver.cpp:218] Iteration 97000 (9.59368 iter/s, 10.4235s/100 iters), loss = 0.00324814
I1006 13:47:25.966514  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00324764 (* 1 = 0.00324764 loss)
I1006 13:47:25.966521  2824 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1006 13:47:34.348942  2824 solver.cpp:218] Iteration 97100 (11.9298 iter/s, 8.3824s/100 iters), loss = 0.000954078
I1006 13:47:34.348971  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000953584 (* 1 = 0.000953584 loss)
I1006 13:47:34.348978  2824 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1006 13:47:42.736078  2824 solver.cpp:218] Iteration 97200 (11.9231 iter/s, 8.38708s/100 iters), loss = 0.00158835
I1006 13:47:42.736204  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00158786 (* 1 = 0.00158786 loss)
I1006 13:47:42.736212  2824 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1006 13:47:51.118710  2824 solver.cpp:218] Iteration 97300 (11.9296 iter/s, 8.38248s/100 iters), loss = 0.000645284
I1006 13:47:51.118752  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000644788 (* 1 = 0.000644788 loss)
I1006 13:47:51.118757  2824 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1006 13:47:59.507350  2824 solver.cpp:218] Iteration 97400 (11.921 iter/s, 8.38857s/100 iters), loss = 0.000803285
I1006 13:47:59.507381  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000802788 (* 1 = 0.000802788 loss)
I1006 13:47:59.507388  2824 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1006 13:48:07.465898  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:48:07.801091  2824 solver.cpp:330] Iteration 97500, Testing net (#0)
I1006 13:48:09.760735  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:48:09.842739  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9246
I1006 13:48:09.842773  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316637 (* 1 = 0.316637 loss)
I1006 13:48:09.926796  2824 solver.cpp:218] Iteration 97500 (9.5975 iter/s, 10.4194s/100 iters), loss = 0.00111891
I1006 13:48:09.926821  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00111841 (* 1 = 0.00111841 loss)
I1006 13:48:09.926827  2824 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1006 13:48:18.307373  2824 solver.cpp:218] Iteration 97600 (11.9324 iter/s, 8.38052s/100 iters), loss = 0.00216869
I1006 13:48:18.307514  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00216819 (* 1 = 0.00216819 loss)
I1006 13:48:18.307523  2824 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1006 13:48:26.696272  2824 solver.cpp:218] Iteration 97700 (11.9207 iter/s, 8.38873s/100 iters), loss = 0.00108683
I1006 13:48:26.696313  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00108633 (* 1 = 0.00108633 loss)
I1006 13:48:26.696319  2824 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1006 13:48:35.075103  2824 solver.cpp:218] Iteration 97800 (11.9349 iter/s, 8.37876s/100 iters), loss = 0.00239952
I1006 13:48:35.075143  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00239902 (* 1 = 0.00239902 loss)
I1006 13:48:35.075150  2824 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1006 13:48:43.457090  2824 solver.cpp:218] Iteration 97900 (11.9304 iter/s, 8.38192s/100 iters), loss = 0.00149311
I1006 13:48:43.457121  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00149262 (* 1 = 0.00149262 loss)
I1006 13:48:43.457128  2824 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1006 13:48:51.421862  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:48:51.757385  2824 solver.cpp:330] Iteration 98000, Testing net (#0)
I1006 13:48:53.715157  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:48:53.796763  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9247
I1006 13:48:53.796799  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316563 (* 1 = 0.316563 loss)
I1006 13:48:53.880899  2824 solver.cpp:218] Iteration 98000 (9.59348 iter/s, 10.4237s/100 iters), loss = 0.000807472
I1006 13:48:53.880923  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000806973 (* 1 = 0.000806973 loss)
I1006 13:48:53.880930  2824 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1006 13:49:02.267822  2824 solver.cpp:218] Iteration 98100 (11.9234 iter/s, 8.38687s/100 iters), loss = 0.00204111
I1006 13:49:02.267853  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00204061 (* 1 = 0.00204061 loss)
I1006 13:49:02.267858  2824 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1006 13:49:10.647666  2824 solver.cpp:218] Iteration 98200 (11.9335 iter/s, 8.37979s/100 iters), loss = 0.000874957
I1006 13:49:10.647708  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000874458 (* 1 = 0.000874458 loss)
I1006 13:49:10.647713  2824 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1006 13:49:19.026724  2824 solver.cpp:218] Iteration 98300 (11.9346 iter/s, 8.37899s/100 iters), loss = 0.00288997
I1006 13:49:19.026765  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00288948 (* 1 = 0.00288948 loss)
I1006 13:49:19.026770  2824 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1006 13:49:27.407130  2824 solver.cpp:218] Iteration 98400 (11.9327 iter/s, 8.38034s/100 iters), loss = 0.00158248
I1006 13:49:27.407249  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00158198 (* 1 = 0.00158198 loss)
I1006 13:49:27.407256  2824 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1006 13:49:35.369299  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:49:35.705428  2824 solver.cpp:330] Iteration 98500, Testing net (#0)
I1006 13:49:37.663215  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:49:37.744823  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9235
I1006 13:49:37.744859  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317334 (* 1 = 0.317334 loss)
I1006 13:49:37.829421  2824 solver.cpp:218] Iteration 98500 (9.59495 iter/s, 10.4222s/100 iters), loss = 0.00110338
I1006 13:49:37.829447  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00110288 (* 1 = 0.00110288 loss)
I1006 13:49:37.829454  2824 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1006 13:49:46.208747  2824 solver.cpp:218] Iteration 98600 (11.9342 iter/s, 8.37927s/100 iters), loss = 0.00106125
I1006 13:49:46.208778  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00106075 (* 1 = 0.00106075 loss)
I1006 13:49:46.208786  2824 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1006 13:49:54.584676  2824 solver.cpp:218] Iteration 98700 (11.9391 iter/s, 8.37583s/100 iters), loss = 0.00137427
I1006 13:49:54.584705  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00137378 (* 1 = 0.00137378 loss)
I1006 13:49:54.584712  2824 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1006 13:50:02.961560  2824 solver.cpp:218] Iteration 98800 (11.9377 iter/s, 8.37683s/100 iters), loss = 0.00152865
I1006 13:50:02.961635  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00152815 (* 1 = 0.00152815 loss)
I1006 13:50:02.961642  2824 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1006 13:50:11.342185  2824 solver.cpp:218] Iteration 98900 (11.9324 iter/s, 8.38052s/100 iters), loss = 0.00046523
I1006 13:50:11.342216  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000464732 (* 1 = 0.000464732 loss)
I1006 13:50:11.342221  2824 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1006 13:50:19.300189  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:50:19.636121  2824 solver.cpp:330] Iteration 99000, Testing net (#0)
I1006 13:50:21.594310  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:50:21.675901  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9235
I1006 13:50:21.675936  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316697 (* 1 = 0.316697 loss)
I1006 13:50:21.759225  2824 solver.cpp:218] Iteration 99000 (9.59971 iter/s, 10.417s/100 iters), loss = 0.00114717
I1006 13:50:21.759251  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00114667 (* 1 = 0.00114667 loss)
I1006 13:50:21.759258  2824 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1006 13:50:30.139753  2824 solver.cpp:218] Iteration 99100 (11.9325 iter/s, 8.38047s/100 iters), loss = 0.000616437
I1006 13:50:30.139784  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000615938 (* 1 = 0.000615938 loss)
I1006 13:50:30.139791  2824 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1006 13:50:38.516371  2824 solver.cpp:218] Iteration 99200 (11.9381 iter/s, 8.37656s/100 iters), loss = 0.00107618
I1006 13:50:38.516523  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00107568 (* 1 = 0.00107568 loss)
I1006 13:50:38.516531  2824 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1006 13:50:46.891521  2824 solver.cpp:218] Iteration 99300 (11.9403 iter/s, 8.37498s/100 iters), loss = 0.0034446
I1006 13:50:46.891562  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0034441 (* 1 = 0.0034441 loss)
I1006 13:50:46.891567  2824 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1006 13:50:55.273838  2824 solver.cpp:218] Iteration 99400 (11.93 iter/s, 8.38225s/100 iters), loss = 0.00167506
I1006 13:50:55.273878  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00167456 (* 1 = 0.00167456 loss)
I1006 13:50:55.273885  2824 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1006 13:51:03.232872  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:51:03.568305  2824 solver.cpp:330] Iteration 99500, Testing net (#0)
I1006 13:51:05.527895  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:51:05.609693  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9236
I1006 13:51:05.609728  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317011 (* 1 = 0.317011 loss)
I1006 13:51:05.693420  2824 solver.cpp:218] Iteration 99500 (9.59738 iter/s, 10.4195s/100 iters), loss = 0.000404994
I1006 13:51:05.693444  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000404492 (* 1 = 0.000404492 loss)
I1006 13:51:05.693451  2824 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1006 13:51:14.076558  2824 solver.cpp:218] Iteration 99600 (11.9288 iter/s, 8.38308s/100 iters), loss = 0.00127182
I1006 13:51:14.076668  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00127132 (* 1 = 0.00127132 loss)
I1006 13:51:14.076674  2824 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1006 13:51:22.463691  2824 solver.cpp:218] Iteration 99700 (11.9232 iter/s, 8.38701s/100 iters), loss = 0.000912395
I1006 13:51:22.463732  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000911892 (* 1 = 0.000911892 loss)
I1006 13:51:22.463738  2824 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1006 13:51:30.841879  2824 solver.cpp:218] Iteration 99800 (11.9359 iter/s, 8.37812s/100 iters), loss = 0.000680601
I1006 13:51:30.841922  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000680098 (* 1 = 0.000680098 loss)
I1006 13:51:30.841928  2824 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1006 13:51:39.221904  2824 solver.cpp:218] Iteration 99900 (11.9332 iter/s, 8.37996s/100 iters), loss = 0.000792446
I1006 13:51:39.221945  2824 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00079194 (* 1 = 0.00079194 loss)
I1006 13:51:39.221951  2824 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1006 13:51:47.182639  2832 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:51:47.517884  2824 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res32/res32_penlu_alpha0.25_beta1_etanostudy_2study_nodecay_gauss_iter_100000.caffemodel
I1006 13:51:47.531230  2824 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res32/res32_penlu_alpha0.25_beta1_etanostudy_2study_nodecay_gauss_iter_100000.solverstate
I1006 13:51:47.554689  2824 solver.cpp:310] Iteration 100000, loss = 0.00209567
I1006 13:51:47.554709  2824 solver.cpp:330] Iteration 100000, Testing net (#0)
I1006 13:51:49.512316  2833 data_layer.cpp:73] Restarting data prefetching from start.
I1006 13:51:49.593891  2824 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9238
I1006 13:51:49.593927  2824 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316221 (* 1 = 0.316221 loss)
I1006 13:51:49.593932  2824 solver.cpp:315] Optimization Done.
I1006 13:51:49.593935  2824 caffe.cpp:259] Optimization Done.
