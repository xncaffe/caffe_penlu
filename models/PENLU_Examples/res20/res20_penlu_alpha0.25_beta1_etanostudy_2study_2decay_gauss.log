I1001 10:43:05.588500  5182 caffe.cpp:218] Using GPUs 0
I1001 10:43:05.668984  5182 caffe.cpp:223] GPU 0: GeForce GTX 1080
I1001 10:43:05.954833  5182 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 40000
stepvalue: 80000
I1001 10:43:05.954967  5182 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 10:43:05.956504  5182 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 10:43:05.956514  5182 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1001 10:43:05.956660  5182 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I1001 10:43:05.956735  5182 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I1001 10:43:05.957214  5182 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1001 10:43:05.957535  5182 layer_factory.hpp:77] Creating layer Data1
I1001 10:43:05.957614  5182 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb
I1001 10:43:05.957633  5182 net.cpp:84] Creating Layer Data1
I1001 10:43:05.957638  5182 net.cpp:380] Data1 -> Data1
I1001 10:43:05.957655  5182 net.cpp:380] Data1 -> Data2
I1001 10:43:05.957666  5182 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I1001 10:43:05.975893  5182 data_layer.cpp:45] output data size: 100,3,28,28
I1001 10:43:05.978376  5182 net.cpp:122] Setting up Data1
I1001 10:43:05.978404  5182 net.cpp:129] Top shape: 100 3 28 28 (235200)
I1001 10:43:05.978410  5182 net.cpp:129] Top shape: 100 (100)
I1001 10:43:05.978411  5182 net.cpp:137] Memory required for data: 941200
I1001 10:43:05.978418  5182 layer_factory.hpp:77] Creating layer Convolution1
I1001 10:43:05.978449  5182 net.cpp:84] Creating Layer Convolution1
I1001 10:43:05.978464  5182 net.cpp:406] Convolution1 <- Data1
I1001 10:43:05.978483  5182 net.cpp:380] Convolution1 -> Convolution1
I1001 10:43:06.167225  5182 net.cpp:122] Setting up Convolution1
I1001 10:43:06.167253  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.167256  5182 net.cpp:137] Memory required for data: 5958800
I1001 10:43:06.167282  5182 layer_factory.hpp:77] Creating layer BatchNorm1
I1001 10:43:06.167294  5182 net.cpp:84] Creating Layer BatchNorm1
I1001 10:43:06.167307  5182 net.cpp:406] BatchNorm1 <- Convolution1
I1001 10:43:06.167313  5182 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1001 10:43:06.167464  5182 net.cpp:122] Setting up BatchNorm1
I1001 10:43:06.167469  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.167472  5182 net.cpp:137] Memory required for data: 10976400
I1001 10:43:06.167479  5182 layer_factory.hpp:77] Creating layer Scale1
I1001 10:43:06.167500  5182 net.cpp:84] Creating Layer Scale1
I1001 10:43:06.167503  5182 net.cpp:406] Scale1 <- Convolution1
I1001 10:43:06.167507  5182 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1001 10:43:06.167569  5182 layer_factory.hpp:77] Creating layer Scale1
I1001 10:43:06.167690  5182 net.cpp:122] Setting up Scale1
I1001 10:43:06.167695  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.167697  5182 net.cpp:137] Memory required for data: 15994000
I1001 10:43:06.167701  5182 layer_factory.hpp:77] Creating layer penlu1
I1001 10:43:06.167711  5182 net.cpp:84] Creating Layer penlu1
I1001 10:43:06.167713  5182 net.cpp:406] penlu1 <- Convolution1
I1001 10:43:06.167716  5182 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1001 10:43:06.168366  5182 net.cpp:122] Setting up penlu1
I1001 10:43:06.168376  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.168380  5182 net.cpp:137] Memory required for data: 21011600
I1001 10:43:06.168387  5182 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1001 10:43:06.168396  5182 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1001 10:43:06.168408  5182 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1001 10:43:06.168412  5182 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1001 10:43:06.168418  5182 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1001 10:43:06.168442  5182 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1001 10:43:06.168455  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.168459  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.168462  5182 net.cpp:137] Memory required for data: 31046800
I1001 10:43:06.168473  5182 layer_factory.hpp:77] Creating layer Convolution2
I1001 10:43:06.168481  5182 net.cpp:84] Creating Layer Convolution2
I1001 10:43:06.168485  5182 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1001 10:43:06.168489  5182 net.cpp:380] Convolution2 -> Convolution2
I1001 10:43:06.169338  5182 net.cpp:122] Setting up Convolution2
I1001 10:43:06.169349  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.169353  5182 net.cpp:137] Memory required for data: 36064400
I1001 10:43:06.169378  5182 layer_factory.hpp:77] Creating layer BatchNorm2
I1001 10:43:06.169385  5182 net.cpp:84] Creating Layer BatchNorm2
I1001 10:43:06.169387  5182 net.cpp:406] BatchNorm2 <- Convolution2
I1001 10:43:06.169391  5182 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1001 10:43:06.169540  5182 net.cpp:122] Setting up BatchNorm2
I1001 10:43:06.169545  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.169548  5182 net.cpp:137] Memory required for data: 41082000
I1001 10:43:06.169562  5182 layer_factory.hpp:77] Creating layer Scale2
I1001 10:43:06.169569  5182 net.cpp:84] Creating Layer Scale2
I1001 10:43:06.169572  5182 net.cpp:406] Scale2 <- Convolution2
I1001 10:43:06.169575  5182 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1001 10:43:06.169600  5182 layer_factory.hpp:77] Creating layer Scale2
I1001 10:43:06.169698  5182 net.cpp:122] Setting up Scale2
I1001 10:43:06.169703  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.169704  5182 net.cpp:137] Memory required for data: 46099600
I1001 10:43:06.169721  5182 layer_factory.hpp:77] Creating layer penlu2
I1001 10:43:06.169728  5182 net.cpp:84] Creating Layer penlu2
I1001 10:43:06.169730  5182 net.cpp:406] penlu2 <- Convolution2
I1001 10:43:06.169734  5182 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1001 10:43:06.169837  5182 net.cpp:122] Setting up penlu2
I1001 10:43:06.169842  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.169845  5182 net.cpp:137] Memory required for data: 51117200
I1001 10:43:06.169849  5182 layer_factory.hpp:77] Creating layer Convolution3
I1001 10:43:06.169857  5182 net.cpp:84] Creating Layer Convolution3
I1001 10:43:06.169859  5182 net.cpp:406] Convolution3 <- Convolution2
I1001 10:43:06.169863  5182 net.cpp:380] Convolution3 -> Convolution3
I1001 10:43:06.170723  5182 net.cpp:122] Setting up Convolution3
I1001 10:43:06.170734  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.170737  5182 net.cpp:137] Memory required for data: 56134800
I1001 10:43:06.170742  5182 layer_factory.hpp:77] Creating layer BatchNorm3
I1001 10:43:06.170747  5182 net.cpp:84] Creating Layer BatchNorm3
I1001 10:43:06.170750  5182 net.cpp:406] BatchNorm3 <- Convolution3
I1001 10:43:06.170754  5182 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1001 10:43:06.170866  5182 net.cpp:122] Setting up BatchNorm3
I1001 10:43:06.170871  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.170872  5182 net.cpp:137] Memory required for data: 61152400
I1001 10:43:06.170876  5182 layer_factory.hpp:77] Creating layer Scale3
I1001 10:43:06.170882  5182 net.cpp:84] Creating Layer Scale3
I1001 10:43:06.170886  5182 net.cpp:406] Scale3 <- Convolution3
I1001 10:43:06.170888  5182 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1001 10:43:06.170912  5182 layer_factory.hpp:77] Creating layer Scale3
I1001 10:43:06.170977  5182 net.cpp:122] Setting up Scale3
I1001 10:43:06.170982  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.170984  5182 net.cpp:137] Memory required for data: 66170000
I1001 10:43:06.170989  5182 layer_factory.hpp:77] Creating layer Eltwise1
I1001 10:43:06.170992  5182 net.cpp:84] Creating Layer Eltwise1
I1001 10:43:06.170996  5182 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1001 10:43:06.171000  5182 net.cpp:406] Eltwise1 <- Convolution3
I1001 10:43:06.171002  5182 net.cpp:380] Eltwise1 -> Eltwise1
I1001 10:43:06.171018  5182 net.cpp:122] Setting up Eltwise1
I1001 10:43:06.171022  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.171025  5182 net.cpp:137] Memory required for data: 71187600
I1001 10:43:06.171026  5182 layer_factory.hpp:77] Creating layer penlu3
I1001 10:43:06.171031  5182 net.cpp:84] Creating Layer penlu3
I1001 10:43:06.171034  5182 net.cpp:406] penlu3 <- Eltwise1
I1001 10:43:06.171037  5182 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1001 10:43:06.171129  5182 net.cpp:122] Setting up penlu3
I1001 10:43:06.171134  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.171145  5182 net.cpp:137] Memory required for data: 76205200
I1001 10:43:06.171150  5182 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1001 10:43:06.171154  5182 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1001 10:43:06.171156  5182 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1001 10:43:06.171159  5182 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1001 10:43:06.171164  5182 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1001 10:43:06.171185  5182 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1001 10:43:06.171190  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.171193  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.171195  5182 net.cpp:137] Memory required for data: 86240400
I1001 10:43:06.171197  5182 layer_factory.hpp:77] Creating layer Convolution4
I1001 10:43:06.171205  5182 net.cpp:84] Creating Layer Convolution4
I1001 10:43:06.171207  5182 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1001 10:43:06.171211  5182 net.cpp:380] Convolution4 -> Convolution4
I1001 10:43:06.172022  5182 net.cpp:122] Setting up Convolution4
I1001 10:43:06.172034  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.172036  5182 net.cpp:137] Memory required for data: 91258000
I1001 10:43:06.172041  5182 layer_factory.hpp:77] Creating layer BatchNorm4
I1001 10:43:06.172046  5182 net.cpp:84] Creating Layer BatchNorm4
I1001 10:43:06.172049  5182 net.cpp:406] BatchNorm4 <- Convolution4
I1001 10:43:06.172053  5182 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1001 10:43:06.172163  5182 net.cpp:122] Setting up BatchNorm4
I1001 10:43:06.172168  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.172170  5182 net.cpp:137] Memory required for data: 96275600
I1001 10:43:06.172178  5182 layer_factory.hpp:77] Creating layer Scale4
I1001 10:43:06.172183  5182 net.cpp:84] Creating Layer Scale4
I1001 10:43:06.172186  5182 net.cpp:406] Scale4 <- Convolution4
I1001 10:43:06.172189  5182 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1001 10:43:06.172212  5182 layer_factory.hpp:77] Creating layer Scale4
I1001 10:43:06.172277  5182 net.cpp:122] Setting up Scale4
I1001 10:43:06.172281  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.172284  5182 net.cpp:137] Memory required for data: 101293200
I1001 10:43:06.172288  5182 layer_factory.hpp:77] Creating layer penlu4
I1001 10:43:06.172294  5182 net.cpp:84] Creating Layer penlu4
I1001 10:43:06.172297  5182 net.cpp:406] penlu4 <- Convolution4
I1001 10:43:06.172300  5182 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1001 10:43:06.172391  5182 net.cpp:122] Setting up penlu4
I1001 10:43:06.172396  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.172399  5182 net.cpp:137] Memory required for data: 106310800
I1001 10:43:06.172402  5182 layer_factory.hpp:77] Creating layer Convolution5
I1001 10:43:06.172410  5182 net.cpp:84] Creating Layer Convolution5
I1001 10:43:06.172412  5182 net.cpp:406] Convolution5 <- Convolution4
I1001 10:43:06.172416  5182 net.cpp:380] Convolution5 -> Convolution5
I1001 10:43:06.173220  5182 net.cpp:122] Setting up Convolution5
I1001 10:43:06.173230  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.173234  5182 net.cpp:137] Memory required for data: 111328400
I1001 10:43:06.173238  5182 layer_factory.hpp:77] Creating layer BatchNorm5
I1001 10:43:06.173243  5182 net.cpp:84] Creating Layer BatchNorm5
I1001 10:43:06.173247  5182 net.cpp:406] BatchNorm5 <- Convolution5
I1001 10:43:06.173250  5182 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1001 10:43:06.173367  5182 net.cpp:122] Setting up BatchNorm5
I1001 10:43:06.173382  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.173385  5182 net.cpp:137] Memory required for data: 116346000
I1001 10:43:06.173390  5182 layer_factory.hpp:77] Creating layer Scale5
I1001 10:43:06.173395  5182 net.cpp:84] Creating Layer Scale5
I1001 10:43:06.173398  5182 net.cpp:406] Scale5 <- Convolution5
I1001 10:43:06.173400  5182 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1001 10:43:06.173442  5182 layer_factory.hpp:77] Creating layer Scale5
I1001 10:43:06.173511  5182 net.cpp:122] Setting up Scale5
I1001 10:43:06.173516  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.173517  5182 net.cpp:137] Memory required for data: 121363600
I1001 10:43:06.173521  5182 layer_factory.hpp:77] Creating layer Eltwise2
I1001 10:43:06.173527  5182 net.cpp:84] Creating Layer Eltwise2
I1001 10:43:06.173529  5182 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1001 10:43:06.173532  5182 net.cpp:406] Eltwise2 <- Convolution5
I1001 10:43:06.173535  5182 net.cpp:380] Eltwise2 -> Eltwise2
I1001 10:43:06.173549  5182 net.cpp:122] Setting up Eltwise2
I1001 10:43:06.173553  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.173555  5182 net.cpp:137] Memory required for data: 126381200
I1001 10:43:06.173557  5182 layer_factory.hpp:77] Creating layer penlu5
I1001 10:43:06.173563  5182 net.cpp:84] Creating Layer penlu5
I1001 10:43:06.173565  5182 net.cpp:406] penlu5 <- Eltwise2
I1001 10:43:06.173568  5182 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1001 10:43:06.173667  5182 net.cpp:122] Setting up penlu5
I1001 10:43:06.173672  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.173674  5182 net.cpp:137] Memory required for data: 131398800
I1001 10:43:06.173679  5182 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1001 10:43:06.173684  5182 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1001 10:43:06.173687  5182 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1001 10:43:06.173691  5182 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1001 10:43:06.173694  5182 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1001 10:43:06.173717  5182 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1001 10:43:06.173722  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.173724  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.173727  5182 net.cpp:137] Memory required for data: 141434000
I1001 10:43:06.173728  5182 layer_factory.hpp:77] Creating layer Convolution6
I1001 10:43:06.173734  5182 net.cpp:84] Creating Layer Convolution6
I1001 10:43:06.173738  5182 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1001 10:43:06.173743  5182 net.cpp:380] Convolution6 -> Convolution6
I1001 10:43:06.174607  5182 net.cpp:122] Setting up Convolution6
I1001 10:43:06.174616  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.174620  5182 net.cpp:137] Memory required for data: 146451600
I1001 10:43:06.174624  5182 layer_factory.hpp:77] Creating layer BatchNorm6
I1001 10:43:06.174630  5182 net.cpp:84] Creating Layer BatchNorm6
I1001 10:43:06.174634  5182 net.cpp:406] BatchNorm6 <- Convolution6
I1001 10:43:06.174638  5182 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1001 10:43:06.174757  5182 net.cpp:122] Setting up BatchNorm6
I1001 10:43:06.174762  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.174764  5182 net.cpp:137] Memory required for data: 151469200
I1001 10:43:06.174769  5182 layer_factory.hpp:77] Creating layer Scale6
I1001 10:43:06.174774  5182 net.cpp:84] Creating Layer Scale6
I1001 10:43:06.174777  5182 net.cpp:406] Scale6 <- Convolution6
I1001 10:43:06.174780  5182 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1001 10:43:06.174805  5182 layer_factory.hpp:77] Creating layer Scale6
I1001 10:43:06.174875  5182 net.cpp:122] Setting up Scale6
I1001 10:43:06.174880  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.174882  5182 net.cpp:137] Memory required for data: 156486800
I1001 10:43:06.174886  5182 layer_factory.hpp:77] Creating layer penlu6
I1001 10:43:06.174892  5182 net.cpp:84] Creating Layer penlu6
I1001 10:43:06.174896  5182 net.cpp:406] penlu6 <- Convolution6
I1001 10:43:06.174899  5182 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1001 10:43:06.175000  5182 net.cpp:122] Setting up penlu6
I1001 10:43:06.175004  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.175014  5182 net.cpp:137] Memory required for data: 161504400
I1001 10:43:06.175019  5182 layer_factory.hpp:77] Creating layer Convolution7
I1001 10:43:06.175026  5182 net.cpp:84] Creating Layer Convolution7
I1001 10:43:06.175030  5182 net.cpp:406] Convolution7 <- Convolution6
I1001 10:43:06.175034  5182 net.cpp:380] Convolution7 -> Convolution7
I1001 10:43:06.175559  5182 net.cpp:122] Setting up Convolution7
I1001 10:43:06.175567  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.175570  5182 net.cpp:137] Memory required for data: 166522000
I1001 10:43:06.175575  5182 layer_factory.hpp:77] Creating layer BatchNorm7
I1001 10:43:06.175580  5182 net.cpp:84] Creating Layer BatchNorm7
I1001 10:43:06.175582  5182 net.cpp:406] BatchNorm7 <- Convolution7
I1001 10:43:06.175586  5182 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1001 10:43:06.175705  5182 net.cpp:122] Setting up BatchNorm7
I1001 10:43:06.175710  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.175712  5182 net.cpp:137] Memory required for data: 171539600
I1001 10:43:06.175722  5182 layer_factory.hpp:77] Creating layer Scale7
I1001 10:43:06.175729  5182 net.cpp:84] Creating Layer Scale7
I1001 10:43:06.175732  5182 net.cpp:406] Scale7 <- Convolution7
I1001 10:43:06.175736  5182 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1001 10:43:06.175760  5182 layer_factory.hpp:77] Creating layer Scale7
I1001 10:43:06.175829  5182 net.cpp:122] Setting up Scale7
I1001 10:43:06.175834  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.175837  5182 net.cpp:137] Memory required for data: 176557200
I1001 10:43:06.175840  5182 layer_factory.hpp:77] Creating layer Eltwise3
I1001 10:43:06.175846  5182 net.cpp:84] Creating Layer Eltwise3
I1001 10:43:06.175849  5182 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1001 10:43:06.175851  5182 net.cpp:406] Eltwise3 <- Convolution7
I1001 10:43:06.175855  5182 net.cpp:380] Eltwise3 -> Eltwise3
I1001 10:43:06.175868  5182 net.cpp:122] Setting up Eltwise3
I1001 10:43:06.175873  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.175874  5182 net.cpp:137] Memory required for data: 181574800
I1001 10:43:06.175876  5182 layer_factory.hpp:77] Creating layer penlu7
I1001 10:43:06.175882  5182 net.cpp:84] Creating Layer penlu7
I1001 10:43:06.175885  5182 net.cpp:406] penlu7 <- Eltwise3
I1001 10:43:06.175889  5182 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1001 10:43:06.176008  5182 net.cpp:122] Setting up penlu7
I1001 10:43:06.176013  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.176015  5182 net.cpp:137] Memory required for data: 186592400
I1001 10:43:06.176020  5182 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1001 10:43:06.176024  5182 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1001 10:43:06.176028  5182 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1001 10:43:06.176031  5182 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1001 10:43:06.176034  5182 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1001 10:43:06.176057  5182 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1001 10:43:06.176061  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.176064  5182 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 10:43:06.176066  5182 net.cpp:137] Memory required for data: 196627600
I1001 10:43:06.176069  5182 layer_factory.hpp:77] Creating layer Convolution8
I1001 10:43:06.176074  5182 net.cpp:84] Creating Layer Convolution8
I1001 10:43:06.176077  5182 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1001 10:43:06.176081  5182 net.cpp:380] Convolution8 -> Convolution8
I1001 10:43:06.177188  5182 net.cpp:122] Setting up Convolution8
I1001 10:43:06.177199  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.177202  5182 net.cpp:137] Memory required for data: 199136400
I1001 10:43:06.177207  5182 layer_factory.hpp:77] Creating layer BatchNorm8
I1001 10:43:06.177212  5182 net.cpp:84] Creating Layer BatchNorm8
I1001 10:43:06.177217  5182 net.cpp:406] BatchNorm8 <- Convolution8
I1001 10:43:06.177227  5182 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1001 10:43:06.177356  5182 net.cpp:122] Setting up BatchNorm8
I1001 10:43:06.177361  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.177362  5182 net.cpp:137] Memory required for data: 201645200
I1001 10:43:06.177367  5182 layer_factory.hpp:77] Creating layer Scale8
I1001 10:43:06.177372  5182 net.cpp:84] Creating Layer Scale8
I1001 10:43:06.177376  5182 net.cpp:406] Scale8 <- Convolution8
I1001 10:43:06.177379  5182 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1001 10:43:06.177423  5182 layer_factory.hpp:77] Creating layer Scale8
I1001 10:43:06.177511  5182 net.cpp:122] Setting up Scale8
I1001 10:43:06.177516  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.177518  5182 net.cpp:137] Memory required for data: 204154000
I1001 10:43:06.177533  5182 layer_factory.hpp:77] Creating layer Convolution9
I1001 10:43:06.177551  5182 net.cpp:84] Creating Layer Convolution9
I1001 10:43:06.177554  5182 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1001 10:43:06.177559  5182 net.cpp:380] Convolution9 -> Convolution9
I1001 10:43:06.178892  5182 net.cpp:122] Setting up Convolution9
I1001 10:43:06.178903  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.178906  5182 net.cpp:137] Memory required for data: 206662800
I1001 10:43:06.178911  5182 layer_factory.hpp:77] Creating layer BatchNorm9
I1001 10:43:06.178918  5182 net.cpp:84] Creating Layer BatchNorm9
I1001 10:43:06.178921  5182 net.cpp:406] BatchNorm9 <- Convolution9
I1001 10:43:06.178925  5182 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1001 10:43:06.179083  5182 net.cpp:122] Setting up BatchNorm9
I1001 10:43:06.179111  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.179126  5182 net.cpp:137] Memory required for data: 209171600
I1001 10:43:06.179144  5182 layer_factory.hpp:77] Creating layer Scale9
I1001 10:43:06.179154  5182 net.cpp:84] Creating Layer Scale9
I1001 10:43:06.179158  5182 net.cpp:406] Scale9 <- Convolution9
I1001 10:43:06.179164  5182 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1001 10:43:06.179225  5182 layer_factory.hpp:77] Creating layer Scale9
I1001 10:43:06.179342  5182 net.cpp:122] Setting up Scale9
I1001 10:43:06.179349  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.179352  5182 net.cpp:137] Memory required for data: 211680400
I1001 10:43:06.179355  5182 layer_factory.hpp:77] Creating layer penlu8
I1001 10:43:06.179361  5182 net.cpp:84] Creating Layer penlu8
I1001 10:43:06.179364  5182 net.cpp:406] penlu8 <- Convolution9
I1001 10:43:06.179369  5182 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1001 10:43:06.179471  5182 net.cpp:122] Setting up penlu8
I1001 10:43:06.179476  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.179478  5182 net.cpp:137] Memory required for data: 214189200
I1001 10:43:06.179483  5182 layer_factory.hpp:77] Creating layer Convolution10
I1001 10:43:06.179491  5182 net.cpp:84] Creating Layer Convolution10
I1001 10:43:06.179494  5182 net.cpp:406] Convolution10 <- Convolution9
I1001 10:43:06.179498  5182 net.cpp:380] Convolution10 -> Convolution10
I1001 10:43:06.180575  5182 net.cpp:122] Setting up Convolution10
I1001 10:43:06.180585  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.180588  5182 net.cpp:137] Memory required for data: 216698000
I1001 10:43:06.180594  5182 layer_factory.hpp:77] Creating layer BatchNorm10
I1001 10:43:06.180599  5182 net.cpp:84] Creating Layer BatchNorm10
I1001 10:43:06.180603  5182 net.cpp:406] BatchNorm10 <- Convolution10
I1001 10:43:06.180606  5182 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1001 10:43:06.180752  5182 net.cpp:122] Setting up BatchNorm10
I1001 10:43:06.180757  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.180759  5182 net.cpp:137] Memory required for data: 219206800
I1001 10:43:06.180764  5182 layer_factory.hpp:77] Creating layer Scale10
I1001 10:43:06.180768  5182 net.cpp:84] Creating Layer Scale10
I1001 10:43:06.180779  5182 net.cpp:406] Scale10 <- Convolution10
I1001 10:43:06.180783  5182 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1001 10:43:06.180811  5182 layer_factory.hpp:77] Creating layer Scale10
I1001 10:43:06.180884  5182 net.cpp:122] Setting up Scale10
I1001 10:43:06.180889  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.180892  5182 net.cpp:137] Memory required for data: 221715600
I1001 10:43:06.180896  5182 layer_factory.hpp:77] Creating layer Eltwise4
I1001 10:43:06.180901  5182 net.cpp:84] Creating Layer Eltwise4
I1001 10:43:06.180903  5182 net.cpp:406] Eltwise4 <- Convolution8
I1001 10:43:06.180907  5182 net.cpp:406] Eltwise4 <- Convolution10
I1001 10:43:06.180910  5182 net.cpp:380] Eltwise4 -> Eltwise4
I1001 10:43:06.180925  5182 net.cpp:122] Setting up Eltwise4
I1001 10:43:06.180930  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.180932  5182 net.cpp:137] Memory required for data: 224224400
I1001 10:43:06.180934  5182 layer_factory.hpp:77] Creating layer penlu9
I1001 10:43:06.180939  5182 net.cpp:84] Creating Layer penlu9
I1001 10:43:06.180943  5182 net.cpp:406] penlu9 <- Eltwise4
I1001 10:43:06.180946  5182 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1001 10:43:06.181077  5182 net.cpp:122] Setting up penlu9
I1001 10:43:06.181082  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.181085  5182 net.cpp:137] Memory required for data: 226733200
I1001 10:43:06.181089  5182 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1001 10:43:06.181093  5182 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1001 10:43:06.181097  5182 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1001 10:43:06.181110  5182 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1001 10:43:06.181115  5182 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1001 10:43:06.181136  5182 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1001 10:43:06.181140  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.181143  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.181145  5182 net.cpp:137] Memory required for data: 231750800
I1001 10:43:06.181147  5182 layer_factory.hpp:77] Creating layer Convolution11
I1001 10:43:06.181154  5182 net.cpp:84] Creating Layer Convolution11
I1001 10:43:06.181157  5182 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1001 10:43:06.181161  5182 net.cpp:380] Convolution11 -> Convolution11
I1001 10:43:06.182194  5182 net.cpp:122] Setting up Convolution11
I1001 10:43:06.182205  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.182209  5182 net.cpp:137] Memory required for data: 234259600
I1001 10:43:06.182214  5182 layer_factory.hpp:77] Creating layer BatchNorm11
I1001 10:43:06.182219  5182 net.cpp:84] Creating Layer BatchNorm11
I1001 10:43:06.182222  5182 net.cpp:406] BatchNorm11 <- Convolution11
I1001 10:43:06.182226  5182 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1001 10:43:06.182354  5182 net.cpp:122] Setting up BatchNorm11
I1001 10:43:06.182359  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.182361  5182 net.cpp:137] Memory required for data: 236768400
I1001 10:43:06.182366  5182 layer_factory.hpp:77] Creating layer Scale11
I1001 10:43:06.182371  5182 net.cpp:84] Creating Layer Scale11
I1001 10:43:06.182374  5182 net.cpp:406] Scale11 <- Convolution11
I1001 10:43:06.182376  5182 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1001 10:43:06.182404  5182 layer_factory.hpp:77] Creating layer Scale11
I1001 10:43:06.182479  5182 net.cpp:122] Setting up Scale11
I1001 10:43:06.182484  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.182487  5182 net.cpp:137] Memory required for data: 239277200
I1001 10:43:06.182490  5182 layer_factory.hpp:77] Creating layer penlu10
I1001 10:43:06.182497  5182 net.cpp:84] Creating Layer penlu10
I1001 10:43:06.182500  5182 net.cpp:406] penlu10 <- Convolution11
I1001 10:43:06.182503  5182 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1001 10:43:06.182637  5182 net.cpp:122] Setting up penlu10
I1001 10:43:06.182651  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.182652  5182 net.cpp:137] Memory required for data: 241786000
I1001 10:43:06.182657  5182 layer_factory.hpp:77] Creating layer Convolution12
I1001 10:43:06.182665  5182 net.cpp:84] Creating Layer Convolution12
I1001 10:43:06.182668  5182 net.cpp:406] Convolution12 <- Convolution11
I1001 10:43:06.182672  5182 net.cpp:380] Convolution12 -> Convolution12
I1001 10:43:06.183709  5182 net.cpp:122] Setting up Convolution12
I1001 10:43:06.183719  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.183722  5182 net.cpp:137] Memory required for data: 244294800
I1001 10:43:06.183727  5182 layer_factory.hpp:77] Creating layer BatchNorm12
I1001 10:43:06.183732  5182 net.cpp:84] Creating Layer BatchNorm12
I1001 10:43:06.183737  5182 net.cpp:406] BatchNorm12 <- Convolution12
I1001 10:43:06.183740  5182 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1001 10:43:06.183866  5182 net.cpp:122] Setting up BatchNorm12
I1001 10:43:06.183871  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.183873  5182 net.cpp:137] Memory required for data: 246803600
I1001 10:43:06.183877  5182 layer_factory.hpp:77] Creating layer Scale12
I1001 10:43:06.183882  5182 net.cpp:84] Creating Layer Scale12
I1001 10:43:06.183885  5182 net.cpp:406] Scale12 <- Convolution12
I1001 10:43:06.183889  5182 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1001 10:43:06.183914  5182 layer_factory.hpp:77] Creating layer Scale12
I1001 10:43:06.183987  5182 net.cpp:122] Setting up Scale12
I1001 10:43:06.183992  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.183995  5182 net.cpp:137] Memory required for data: 249312400
I1001 10:43:06.183998  5182 layer_factory.hpp:77] Creating layer Eltwise5
I1001 10:43:06.184002  5182 net.cpp:84] Creating Layer Eltwise5
I1001 10:43:06.184005  5182 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1001 10:43:06.184008  5182 net.cpp:406] Eltwise5 <- Convolution12
I1001 10:43:06.184011  5182 net.cpp:380] Eltwise5 -> Eltwise5
I1001 10:43:06.184027  5182 net.cpp:122] Setting up Eltwise5
I1001 10:43:06.184031  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.184033  5182 net.cpp:137] Memory required for data: 251821200
I1001 10:43:06.184036  5182 layer_factory.hpp:77] Creating layer penlu11
I1001 10:43:06.184041  5182 net.cpp:84] Creating Layer penlu11
I1001 10:43:06.184044  5182 net.cpp:406] penlu11 <- Eltwise5
I1001 10:43:06.184047  5182 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1001 10:43:06.184152  5182 net.cpp:122] Setting up penlu11
I1001 10:43:06.184157  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.184159  5182 net.cpp:137] Memory required for data: 254330000
I1001 10:43:06.184165  5182 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1001 10:43:06.184168  5182 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1001 10:43:06.184171  5182 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1001 10:43:06.184175  5182 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1001 10:43:06.184180  5182 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1001 10:43:06.184202  5182 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1001 10:43:06.184206  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.184209  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.184211  5182 net.cpp:137] Memory required for data: 259347600
I1001 10:43:06.184214  5182 layer_factory.hpp:77] Creating layer Convolution13
I1001 10:43:06.184221  5182 net.cpp:84] Creating Layer Convolution13
I1001 10:43:06.184223  5182 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1001 10:43:06.184227  5182 net.cpp:380] Convolution13 -> Convolution13
I1001 10:43:06.185281  5182 net.cpp:122] Setting up Convolution13
I1001 10:43:06.185292  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.185295  5182 net.cpp:137] Memory required for data: 261856400
I1001 10:43:06.185299  5182 layer_factory.hpp:77] Creating layer BatchNorm13
I1001 10:43:06.185312  5182 net.cpp:84] Creating Layer BatchNorm13
I1001 10:43:06.185317  5182 net.cpp:406] BatchNorm13 <- Convolution13
I1001 10:43:06.185320  5182 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1001 10:43:06.185448  5182 net.cpp:122] Setting up BatchNorm13
I1001 10:43:06.185453  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.185456  5182 net.cpp:137] Memory required for data: 264365200
I1001 10:43:06.185461  5182 layer_factory.hpp:77] Creating layer Scale13
I1001 10:43:06.185467  5182 net.cpp:84] Creating Layer Scale13
I1001 10:43:06.185469  5182 net.cpp:406] Scale13 <- Convolution13
I1001 10:43:06.185472  5182 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1001 10:43:06.185497  5182 layer_factory.hpp:77] Creating layer Scale13
I1001 10:43:06.185572  5182 net.cpp:122] Setting up Scale13
I1001 10:43:06.185577  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.185580  5182 net.cpp:137] Memory required for data: 266874000
I1001 10:43:06.185583  5182 layer_factory.hpp:77] Creating layer penlu12
I1001 10:43:06.185588  5182 net.cpp:84] Creating Layer penlu12
I1001 10:43:06.185591  5182 net.cpp:406] penlu12 <- Convolution13
I1001 10:43:06.185595  5182 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1001 10:43:06.185698  5182 net.cpp:122] Setting up penlu12
I1001 10:43:06.185703  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.185705  5182 net.cpp:137] Memory required for data: 269382800
I1001 10:43:06.185709  5182 layer_factory.hpp:77] Creating layer Convolution14
I1001 10:43:06.185717  5182 net.cpp:84] Creating Layer Convolution14
I1001 10:43:06.185720  5182 net.cpp:406] Convolution14 <- Convolution13
I1001 10:43:06.185724  5182 net.cpp:380] Convolution14 -> Convolution14
I1001 10:43:06.186800  5182 net.cpp:122] Setting up Convolution14
I1001 10:43:06.186810  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.186815  5182 net.cpp:137] Memory required for data: 271891600
I1001 10:43:06.186830  5182 layer_factory.hpp:77] Creating layer BatchNorm14
I1001 10:43:06.186839  5182 net.cpp:84] Creating Layer BatchNorm14
I1001 10:43:06.186842  5182 net.cpp:406] BatchNorm14 <- Convolution14
I1001 10:43:06.186846  5182 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1001 10:43:06.186975  5182 net.cpp:122] Setting up BatchNorm14
I1001 10:43:06.186981  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.186983  5182 net.cpp:137] Memory required for data: 274400400
I1001 10:43:06.186988  5182 layer_factory.hpp:77] Creating layer Scale14
I1001 10:43:06.186995  5182 net.cpp:84] Creating Layer Scale14
I1001 10:43:06.187000  5182 net.cpp:406] Scale14 <- Convolution14
I1001 10:43:06.187002  5182 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1001 10:43:06.187028  5182 layer_factory.hpp:77] Creating layer Scale14
I1001 10:43:06.187103  5182 net.cpp:122] Setting up Scale14
I1001 10:43:06.187108  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.187111  5182 net.cpp:137] Memory required for data: 276909200
I1001 10:43:06.187114  5182 layer_factory.hpp:77] Creating layer Eltwise6
I1001 10:43:06.187119  5182 net.cpp:84] Creating Layer Eltwise6
I1001 10:43:06.187121  5182 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1001 10:43:06.187124  5182 net.cpp:406] Eltwise6 <- Convolution14
I1001 10:43:06.187129  5182 net.cpp:380] Eltwise6 -> Eltwise6
I1001 10:43:06.187142  5182 net.cpp:122] Setting up Eltwise6
I1001 10:43:06.187146  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.187149  5182 net.cpp:137] Memory required for data: 279418000
I1001 10:43:06.187150  5182 layer_factory.hpp:77] Creating layer penlu13
I1001 10:43:06.187155  5182 net.cpp:84] Creating Layer penlu13
I1001 10:43:06.187158  5182 net.cpp:406] penlu13 <- Eltwise6
I1001 10:43:06.187161  5182 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1001 10:43:06.187263  5182 net.cpp:122] Setting up penlu13
I1001 10:43:06.187266  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.187268  5182 net.cpp:137] Memory required for data: 281926800
I1001 10:43:06.187279  5182 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1001 10:43:06.187283  5182 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1001 10:43:06.187286  5182 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1001 10:43:06.187289  5182 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1001 10:43:06.187294  5182 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1001 10:43:06.187316  5182 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1001 10:43:06.187320  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.187324  5182 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 10:43:06.187325  5182 net.cpp:137] Memory required for data: 286944400
I1001 10:43:06.187327  5182 layer_factory.hpp:77] Creating layer Convolution15
I1001 10:43:06.187333  5182 net.cpp:84] Creating Layer Convolution15
I1001 10:43:06.187336  5182 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1001 10:43:06.187340  5182 net.cpp:380] Convolution15 -> Convolution15
I1001 10:43:06.188251  5182 net.cpp:122] Setting up Convolution15
I1001 10:43:06.188259  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.188262  5182 net.cpp:137] Memory required for data: 288198800
I1001 10:43:06.188266  5182 layer_factory.hpp:77] Creating layer BatchNorm15
I1001 10:43:06.188273  5182 net.cpp:84] Creating Layer BatchNorm15
I1001 10:43:06.188275  5182 net.cpp:406] BatchNorm15 <- Convolution15
I1001 10:43:06.188279  5182 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1001 10:43:06.188405  5182 net.cpp:122] Setting up BatchNorm15
I1001 10:43:06.188408  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.188411  5182 net.cpp:137] Memory required for data: 289453200
I1001 10:43:06.188416  5182 layer_factory.hpp:77] Creating layer Scale15
I1001 10:43:06.188421  5182 net.cpp:84] Creating Layer Scale15
I1001 10:43:06.188422  5182 net.cpp:406] Scale15 <- Convolution15
I1001 10:43:06.188426  5182 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1001 10:43:06.188450  5182 layer_factory.hpp:77] Creating layer Scale15
I1001 10:43:06.188522  5182 net.cpp:122] Setting up Scale15
I1001 10:43:06.188526  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.188529  5182 net.cpp:137] Memory required for data: 290707600
I1001 10:43:06.188532  5182 layer_factory.hpp:77] Creating layer Convolution16
I1001 10:43:06.188539  5182 net.cpp:84] Creating Layer Convolution16
I1001 10:43:06.188541  5182 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1001 10:43:06.188545  5182 net.cpp:380] Convolution16 -> Convolution16
I1001 10:43:06.190263  5182 net.cpp:122] Setting up Convolution16
I1001 10:43:06.190271  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.190274  5182 net.cpp:137] Memory required for data: 291962000
I1001 10:43:06.190279  5182 layer_factory.hpp:77] Creating layer BatchNorm16
I1001 10:43:06.190284  5182 net.cpp:84] Creating Layer BatchNorm16
I1001 10:43:06.190286  5182 net.cpp:406] BatchNorm16 <- Convolution16
I1001 10:43:06.190290  5182 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1001 10:43:06.190416  5182 net.cpp:122] Setting up BatchNorm16
I1001 10:43:06.190420  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.190423  5182 net.cpp:137] Memory required for data: 293216400
I1001 10:43:06.190428  5182 layer_factory.hpp:77] Creating layer Scale16
I1001 10:43:06.190433  5182 net.cpp:84] Creating Layer Scale16
I1001 10:43:06.190434  5182 net.cpp:406] Scale16 <- Convolution16
I1001 10:43:06.190438  5182 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1001 10:43:06.190462  5182 layer_factory.hpp:77] Creating layer Scale16
I1001 10:43:06.190553  5182 net.cpp:122] Setting up Scale16
I1001 10:43:06.190567  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.190569  5182 net.cpp:137] Memory required for data: 294470800
I1001 10:43:06.190573  5182 layer_factory.hpp:77] Creating layer penlu14
I1001 10:43:06.190578  5182 net.cpp:84] Creating Layer penlu14
I1001 10:43:06.190587  5182 net.cpp:406] penlu14 <- Convolution16
I1001 10:43:06.190592  5182 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1001 10:43:06.190695  5182 net.cpp:122] Setting up penlu14
I1001 10:43:06.190699  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.190701  5182 net.cpp:137] Memory required for data: 295725200
I1001 10:43:06.190706  5182 layer_factory.hpp:77] Creating layer Convolution17
I1001 10:43:06.190712  5182 net.cpp:84] Creating Layer Convolution17
I1001 10:43:06.190714  5182 net.cpp:406] Convolution17 <- Convolution16
I1001 10:43:06.190718  5182 net.cpp:380] Convolution17 -> Convolution17
I1001 10:43:06.192369  5182 net.cpp:122] Setting up Convolution17
I1001 10:43:06.192378  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.192380  5182 net.cpp:137] Memory required for data: 296979600
I1001 10:43:06.192385  5182 layer_factory.hpp:77] Creating layer BatchNorm17
I1001 10:43:06.192390  5182 net.cpp:84] Creating Layer BatchNorm17
I1001 10:43:06.192392  5182 net.cpp:406] BatchNorm17 <- Convolution17
I1001 10:43:06.192399  5182 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1001 10:43:06.192528  5182 net.cpp:122] Setting up BatchNorm17
I1001 10:43:06.192533  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.192534  5182 net.cpp:137] Memory required for data: 298234000
I1001 10:43:06.192539  5182 layer_factory.hpp:77] Creating layer Scale17
I1001 10:43:06.192543  5182 net.cpp:84] Creating Layer Scale17
I1001 10:43:06.192546  5182 net.cpp:406] Scale17 <- Convolution17
I1001 10:43:06.192549  5182 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1001 10:43:06.192574  5182 layer_factory.hpp:77] Creating layer Scale17
I1001 10:43:06.192680  5182 net.cpp:122] Setting up Scale17
I1001 10:43:06.192684  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.192687  5182 net.cpp:137] Memory required for data: 299488400
I1001 10:43:06.192690  5182 layer_factory.hpp:77] Creating layer Eltwise7
I1001 10:43:06.192695  5182 net.cpp:84] Creating Layer Eltwise7
I1001 10:43:06.192698  5182 net.cpp:406] Eltwise7 <- Convolution15
I1001 10:43:06.192700  5182 net.cpp:406] Eltwise7 <- Convolution17
I1001 10:43:06.192703  5182 net.cpp:380] Eltwise7 -> Eltwise7
I1001 10:43:06.192718  5182 net.cpp:122] Setting up Eltwise7
I1001 10:43:06.192723  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.192724  5182 net.cpp:137] Memory required for data: 300742800
I1001 10:43:06.192726  5182 layer_factory.hpp:77] Creating layer penlu15
I1001 10:43:06.192731  5182 net.cpp:84] Creating Layer penlu15
I1001 10:43:06.192734  5182 net.cpp:406] penlu15 <- Eltwise7
I1001 10:43:06.192737  5182 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1001 10:43:06.192849  5182 net.cpp:122] Setting up penlu15
I1001 10:43:06.192853  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.192855  5182 net.cpp:137] Memory required for data: 301997200
I1001 10:43:06.192860  5182 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1001 10:43:06.192863  5182 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1001 10:43:06.192867  5182 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1001 10:43:06.192869  5182 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1001 10:43:06.192873  5182 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1001 10:43:06.192894  5182 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1001 10:43:06.192898  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.192900  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.192903  5182 net.cpp:137] Memory required for data: 304506000
I1001 10:43:06.192904  5182 layer_factory.hpp:77] Creating layer Convolution18
I1001 10:43:06.192910  5182 net.cpp:84] Creating Layer Convolution18
I1001 10:43:06.192914  5182 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1001 10:43:06.192916  5182 net.cpp:380] Convolution18 -> Convolution18
I1001 10:43:06.194629  5182 net.cpp:122] Setting up Convolution18
I1001 10:43:06.194638  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.194648  5182 net.cpp:137] Memory required for data: 305760400
I1001 10:43:06.194651  5182 layer_factory.hpp:77] Creating layer BatchNorm18
I1001 10:43:06.194658  5182 net.cpp:84] Creating Layer BatchNorm18
I1001 10:43:06.194660  5182 net.cpp:406] BatchNorm18 <- Convolution18
I1001 10:43:06.194664  5182 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1001 10:43:06.194826  5182 net.cpp:122] Setting up BatchNorm18
I1001 10:43:06.194830  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.194833  5182 net.cpp:137] Memory required for data: 307014800
I1001 10:43:06.194838  5182 layer_factory.hpp:77] Creating layer Scale18
I1001 10:43:06.194842  5182 net.cpp:84] Creating Layer Scale18
I1001 10:43:06.194844  5182 net.cpp:406] Scale18 <- Convolution18
I1001 10:43:06.194849  5182 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1001 10:43:06.194895  5182 layer_factory.hpp:77] Creating layer Scale18
I1001 10:43:06.195017  5182 net.cpp:122] Setting up Scale18
I1001 10:43:06.195024  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.195026  5182 net.cpp:137] Memory required for data: 308269200
I1001 10:43:06.195032  5182 layer_factory.hpp:77] Creating layer penlu16
I1001 10:43:06.195041  5182 net.cpp:84] Creating Layer penlu16
I1001 10:43:06.195046  5182 net.cpp:406] penlu16 <- Convolution18
I1001 10:43:06.195060  5182 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1001 10:43:06.195207  5182 net.cpp:122] Setting up penlu16
I1001 10:43:06.195211  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.195214  5182 net.cpp:137] Memory required for data: 309523600
I1001 10:43:06.195217  5182 layer_factory.hpp:77] Creating layer Convolution19
I1001 10:43:06.195225  5182 net.cpp:84] Creating Layer Convolution19
I1001 10:43:06.195227  5182 net.cpp:406] Convolution19 <- Convolution18
I1001 10:43:06.195231  5182 net.cpp:380] Convolution19 -> Convolution19
I1001 10:43:06.197338  5182 net.cpp:122] Setting up Convolution19
I1001 10:43:06.197350  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.197361  5182 net.cpp:137] Memory required for data: 310778000
I1001 10:43:06.197366  5182 layer_factory.hpp:77] Creating layer BatchNorm19
I1001 10:43:06.197372  5182 net.cpp:84] Creating Layer BatchNorm19
I1001 10:43:06.197376  5182 net.cpp:406] BatchNorm19 <- Convolution19
I1001 10:43:06.197381  5182 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1001 10:43:06.197533  5182 net.cpp:122] Setting up BatchNorm19
I1001 10:43:06.197538  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.197540  5182 net.cpp:137] Memory required for data: 312032400
I1001 10:43:06.197554  5182 layer_factory.hpp:77] Creating layer Scale19
I1001 10:43:06.197559  5182 net.cpp:84] Creating Layer Scale19
I1001 10:43:06.197562  5182 net.cpp:406] Scale19 <- Convolution19
I1001 10:43:06.197567  5182 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1001 10:43:06.197594  5182 layer_factory.hpp:77] Creating layer Scale19
I1001 10:43:06.197691  5182 net.cpp:122] Setting up Scale19
I1001 10:43:06.197697  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.197700  5182 net.cpp:137] Memory required for data: 313286800
I1001 10:43:06.197703  5182 layer_factory.hpp:77] Creating layer Eltwise8
I1001 10:43:06.197710  5182 net.cpp:84] Creating Layer Eltwise8
I1001 10:43:06.197712  5182 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1001 10:43:06.197716  5182 net.cpp:406] Eltwise8 <- Convolution19
I1001 10:43:06.197721  5182 net.cpp:380] Eltwise8 -> Eltwise8
I1001 10:43:06.197737  5182 net.cpp:122] Setting up Eltwise8
I1001 10:43:06.197742  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.197744  5182 net.cpp:137] Memory required for data: 314541200
I1001 10:43:06.197746  5182 layer_factory.hpp:77] Creating layer penlu17
I1001 10:43:06.197752  5182 net.cpp:84] Creating Layer penlu17
I1001 10:43:06.197754  5182 net.cpp:406] penlu17 <- Eltwise8
I1001 10:43:06.197757  5182 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1001 10:43:06.197883  5182 net.cpp:122] Setting up penlu17
I1001 10:43:06.197896  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.197899  5182 net.cpp:137] Memory required for data: 315795600
I1001 10:43:06.197903  5182 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1001 10:43:06.197908  5182 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1001 10:43:06.197911  5182 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1001 10:43:06.197916  5182 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1001 10:43:06.197919  5182 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1001 10:43:06.197944  5182 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1001 10:43:06.197949  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.197953  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.197955  5182 net.cpp:137] Memory required for data: 318304400
I1001 10:43:06.197957  5182 layer_factory.hpp:77] Creating layer Convolution20
I1001 10:43:06.197963  5182 net.cpp:84] Creating Layer Convolution20
I1001 10:43:06.197968  5182 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1001 10:43:06.197971  5182 net.cpp:380] Convolution20 -> Convolution20
I1001 10:43:06.199635  5182 net.cpp:122] Setting up Convolution20
I1001 10:43:06.199646  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.199650  5182 net.cpp:137] Memory required for data: 319558800
I1001 10:43:06.199654  5182 layer_factory.hpp:77] Creating layer BatchNorm20
I1001 10:43:06.199661  5182 net.cpp:84] Creating Layer BatchNorm20
I1001 10:43:06.199663  5182 net.cpp:406] BatchNorm20 <- Convolution20
I1001 10:43:06.199667  5182 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1001 10:43:06.199800  5182 net.cpp:122] Setting up BatchNorm20
I1001 10:43:06.199805  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.199808  5182 net.cpp:137] Memory required for data: 320813200
I1001 10:43:06.199813  5182 layer_factory.hpp:77] Creating layer Scale20
I1001 10:43:06.199817  5182 net.cpp:84] Creating Layer Scale20
I1001 10:43:06.199820  5182 net.cpp:406] Scale20 <- Convolution20
I1001 10:43:06.199826  5182 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1001 10:43:06.199851  5182 layer_factory.hpp:77] Creating layer Scale20
I1001 10:43:06.199929  5182 net.cpp:122] Setting up Scale20
I1001 10:43:06.199934  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.199937  5182 net.cpp:137] Memory required for data: 322067600
I1001 10:43:06.199941  5182 layer_factory.hpp:77] Creating layer penlu18
I1001 10:43:06.199949  5182 net.cpp:84] Creating Layer penlu18
I1001 10:43:06.199951  5182 net.cpp:406] penlu18 <- Convolution20
I1001 10:43:06.199955  5182 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1001 10:43:06.200062  5182 net.cpp:122] Setting up penlu18
I1001 10:43:06.200067  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.200068  5182 net.cpp:137] Memory required for data: 323322000
I1001 10:43:06.200073  5182 layer_factory.hpp:77] Creating layer Convolution21
I1001 10:43:06.200079  5182 net.cpp:84] Creating Layer Convolution21
I1001 10:43:06.200081  5182 net.cpp:406] Convolution21 <- Convolution20
I1001 10:43:06.200085  5182 net.cpp:380] Convolution21 -> Convolution21
I1001 10:43:06.202356  5182 net.cpp:122] Setting up Convolution21
I1001 10:43:06.202364  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.202368  5182 net.cpp:137] Memory required for data: 324576400
I1001 10:43:06.202373  5182 layer_factory.hpp:77] Creating layer BatchNorm21
I1001 10:43:06.202378  5182 net.cpp:84] Creating Layer BatchNorm21
I1001 10:43:06.202380  5182 net.cpp:406] BatchNorm21 <- Convolution21
I1001 10:43:06.202384  5182 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1001 10:43:06.202524  5182 net.cpp:122] Setting up BatchNorm21
I1001 10:43:06.202530  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.202533  5182 net.cpp:137] Memory required for data: 325830800
I1001 10:43:06.202538  5182 layer_factory.hpp:77] Creating layer Scale21
I1001 10:43:06.202543  5182 net.cpp:84] Creating Layer Scale21
I1001 10:43:06.202551  5182 net.cpp:406] Scale21 <- Convolution21
I1001 10:43:06.202554  5182 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1001 10:43:06.202584  5182 layer_factory.hpp:77] Creating layer Scale21
I1001 10:43:06.202661  5182 net.cpp:122] Setting up Scale21
I1001 10:43:06.202666  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.202667  5182 net.cpp:137] Memory required for data: 327085200
I1001 10:43:06.202672  5182 layer_factory.hpp:77] Creating layer Eltwise9
I1001 10:43:06.202677  5182 net.cpp:84] Creating Layer Eltwise9
I1001 10:43:06.202679  5182 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1001 10:43:06.202682  5182 net.cpp:406] Eltwise9 <- Convolution21
I1001 10:43:06.202685  5182 net.cpp:380] Eltwise9 -> Eltwise9
I1001 10:43:06.202702  5182 net.cpp:122] Setting up Eltwise9
I1001 10:43:06.202705  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.202708  5182 net.cpp:137] Memory required for data: 328339600
I1001 10:43:06.202709  5182 layer_factory.hpp:77] Creating layer penlu19
I1001 10:43:06.202714  5182 net.cpp:84] Creating Layer penlu19
I1001 10:43:06.202716  5182 net.cpp:406] penlu19 <- Eltwise9
I1001 10:43:06.202720  5182 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1001 10:43:06.202826  5182 net.cpp:122] Setting up penlu19
I1001 10:43:06.202831  5182 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 10:43:06.202832  5182 net.cpp:137] Memory required for data: 329594000
I1001 10:43:06.202837  5182 layer_factory.hpp:77] Creating layer Pooling1
I1001 10:43:06.202841  5182 net.cpp:84] Creating Layer Pooling1
I1001 10:43:06.202843  5182 net.cpp:406] Pooling1 <- Eltwise9
I1001 10:43:06.202847  5182 net.cpp:380] Pooling1 -> Pooling1
I1001 10:43:06.202992  5182 net.cpp:122] Setting up Pooling1
I1001 10:43:06.202998  5182 net.cpp:129] Top shape: 100 64 1 1 (6400)
I1001 10:43:06.203001  5182 net.cpp:137] Memory required for data: 329619600
I1001 10:43:06.203003  5182 layer_factory.hpp:77] Creating layer InnerProduct1
I1001 10:43:06.203013  5182 net.cpp:84] Creating Layer InnerProduct1
I1001 10:43:06.203016  5182 net.cpp:406] InnerProduct1 <- Pooling1
I1001 10:43:06.203019  5182 net.cpp:380] InnerProduct1 -> InnerProduct1
I1001 10:43:06.203114  5182 net.cpp:122] Setting up InnerProduct1
I1001 10:43:06.203117  5182 net.cpp:129] Top shape: 100 10 (1000)
I1001 10:43:06.203119  5182 net.cpp:137] Memory required for data: 329623600
I1001 10:43:06.203124  5182 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 10:43:06.203127  5182 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1001 10:43:06.203130  5182 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1001 10:43:06.203133  5182 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1001 10:43:06.203137  5182 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1001 10:43:06.203143  5182 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 10:43:06.203697  5182 net.cpp:122] Setting up SoftmaxWithLoss1
I1001 10:43:06.203706  5182 net.cpp:129] Top shape: (1)
I1001 10:43:06.203708  5182 net.cpp:132]     with loss weight 1
I1001 10:43:06.203721  5182 net.cpp:137] Memory required for data: 329623604
I1001 10:43:06.203723  5182 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1001 10:43:06.203725  5182 net.cpp:198] InnerProduct1 needs backward computation.
I1001 10:43:06.203728  5182 net.cpp:198] Pooling1 needs backward computation.
I1001 10:43:06.203730  5182 net.cpp:198] penlu19 needs backward computation.
I1001 10:43:06.203732  5182 net.cpp:198] Eltwise9 needs backward computation.
I1001 10:43:06.203734  5182 net.cpp:198] Scale21 needs backward computation.
I1001 10:43:06.203737  5182 net.cpp:198] BatchNorm21 needs backward computation.
I1001 10:43:06.203738  5182 net.cpp:198] Convolution21 needs backward computation.
I1001 10:43:06.203742  5182 net.cpp:198] penlu18 needs backward computation.
I1001 10:43:06.203743  5182 net.cpp:198] Scale20 needs backward computation.
I1001 10:43:06.203745  5182 net.cpp:198] BatchNorm20 needs backward computation.
I1001 10:43:06.203747  5182 net.cpp:198] Convolution20 needs backward computation.
I1001 10:43:06.203755  5182 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1001 10:43:06.203758  5182 net.cpp:198] penlu17 needs backward computation.
I1001 10:43:06.203760  5182 net.cpp:198] Eltwise8 needs backward computation.
I1001 10:43:06.203763  5182 net.cpp:198] Scale19 needs backward computation.
I1001 10:43:06.203764  5182 net.cpp:198] BatchNorm19 needs backward computation.
I1001 10:43:06.203766  5182 net.cpp:198] Convolution19 needs backward computation.
I1001 10:43:06.203768  5182 net.cpp:198] penlu16 needs backward computation.
I1001 10:43:06.203770  5182 net.cpp:198] Scale18 needs backward computation.
I1001 10:43:06.203773  5182 net.cpp:198] BatchNorm18 needs backward computation.
I1001 10:43:06.203774  5182 net.cpp:198] Convolution18 needs backward computation.
I1001 10:43:06.203778  5182 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1001 10:43:06.203780  5182 net.cpp:198] penlu15 needs backward computation.
I1001 10:43:06.203783  5182 net.cpp:198] Eltwise7 needs backward computation.
I1001 10:43:06.203785  5182 net.cpp:198] Scale17 needs backward computation.
I1001 10:43:06.203788  5182 net.cpp:198] BatchNorm17 needs backward computation.
I1001 10:43:06.203789  5182 net.cpp:198] Convolution17 needs backward computation.
I1001 10:43:06.203791  5182 net.cpp:198] penlu14 needs backward computation.
I1001 10:43:06.203794  5182 net.cpp:198] Scale16 needs backward computation.
I1001 10:43:06.203796  5182 net.cpp:198] BatchNorm16 needs backward computation.
I1001 10:43:06.203799  5182 net.cpp:198] Convolution16 needs backward computation.
I1001 10:43:06.203800  5182 net.cpp:198] Scale15 needs backward computation.
I1001 10:43:06.203802  5182 net.cpp:198] BatchNorm15 needs backward computation.
I1001 10:43:06.203804  5182 net.cpp:198] Convolution15 needs backward computation.
I1001 10:43:06.203807  5182 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1001 10:43:06.203809  5182 net.cpp:198] penlu13 needs backward computation.
I1001 10:43:06.203811  5182 net.cpp:198] Eltwise6 needs backward computation.
I1001 10:43:06.203814  5182 net.cpp:198] Scale14 needs backward computation.
I1001 10:43:06.203816  5182 net.cpp:198] BatchNorm14 needs backward computation.
I1001 10:43:06.203819  5182 net.cpp:198] Convolution14 needs backward computation.
I1001 10:43:06.203821  5182 net.cpp:198] penlu12 needs backward computation.
I1001 10:43:06.203824  5182 net.cpp:198] Scale13 needs backward computation.
I1001 10:43:06.203825  5182 net.cpp:198] BatchNorm13 needs backward computation.
I1001 10:43:06.203827  5182 net.cpp:198] Convolution13 needs backward computation.
I1001 10:43:06.203830  5182 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1001 10:43:06.203833  5182 net.cpp:198] penlu11 needs backward computation.
I1001 10:43:06.203835  5182 net.cpp:198] Eltwise5 needs backward computation.
I1001 10:43:06.203838  5182 net.cpp:198] Scale12 needs backward computation.
I1001 10:43:06.203840  5182 net.cpp:198] BatchNorm12 needs backward computation.
I1001 10:43:06.203842  5182 net.cpp:198] Convolution12 needs backward computation.
I1001 10:43:06.203845  5182 net.cpp:198] penlu10 needs backward computation.
I1001 10:43:06.203846  5182 net.cpp:198] Scale11 needs backward computation.
I1001 10:43:06.203850  5182 net.cpp:198] BatchNorm11 needs backward computation.
I1001 10:43:06.203851  5182 net.cpp:198] Convolution11 needs backward computation.
I1001 10:43:06.203853  5182 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1001 10:43:06.203855  5182 net.cpp:198] penlu9 needs backward computation.
I1001 10:43:06.203858  5182 net.cpp:198] Eltwise4 needs backward computation.
I1001 10:43:06.203860  5182 net.cpp:198] Scale10 needs backward computation.
I1001 10:43:06.203863  5182 net.cpp:198] BatchNorm10 needs backward computation.
I1001 10:43:06.203866  5182 net.cpp:198] Convolution10 needs backward computation.
I1001 10:43:06.203867  5182 net.cpp:198] penlu8 needs backward computation.
I1001 10:43:06.203873  5182 net.cpp:198] Scale9 needs backward computation.
I1001 10:43:06.203876  5182 net.cpp:198] BatchNorm9 needs backward computation.
I1001 10:43:06.203878  5182 net.cpp:198] Convolution9 needs backward computation.
I1001 10:43:06.203881  5182 net.cpp:198] Scale8 needs backward computation.
I1001 10:43:06.203883  5182 net.cpp:198] BatchNorm8 needs backward computation.
I1001 10:43:06.203886  5182 net.cpp:198] Convolution8 needs backward computation.
I1001 10:43:06.203888  5182 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1001 10:43:06.203891  5182 net.cpp:198] penlu7 needs backward computation.
I1001 10:43:06.203892  5182 net.cpp:198] Eltwise3 needs backward computation.
I1001 10:43:06.203896  5182 net.cpp:198] Scale7 needs backward computation.
I1001 10:43:06.203898  5182 net.cpp:198] BatchNorm7 needs backward computation.
I1001 10:43:06.203900  5182 net.cpp:198] Convolution7 needs backward computation.
I1001 10:43:06.203902  5182 net.cpp:198] penlu6 needs backward computation.
I1001 10:43:06.203904  5182 net.cpp:198] Scale6 needs backward computation.
I1001 10:43:06.203907  5182 net.cpp:198] BatchNorm6 needs backward computation.
I1001 10:43:06.203909  5182 net.cpp:198] Convolution6 needs backward computation.
I1001 10:43:06.203912  5182 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1001 10:43:06.203914  5182 net.cpp:198] penlu5 needs backward computation.
I1001 10:43:06.203917  5182 net.cpp:198] Eltwise2 needs backward computation.
I1001 10:43:06.203919  5182 net.cpp:198] Scale5 needs backward computation.
I1001 10:43:06.203922  5182 net.cpp:198] BatchNorm5 needs backward computation.
I1001 10:43:06.203923  5182 net.cpp:198] Convolution5 needs backward computation.
I1001 10:43:06.203927  5182 net.cpp:198] penlu4 needs backward computation.
I1001 10:43:06.203928  5182 net.cpp:198] Scale4 needs backward computation.
I1001 10:43:06.203930  5182 net.cpp:198] BatchNorm4 needs backward computation.
I1001 10:43:06.203933  5182 net.cpp:198] Convolution4 needs backward computation.
I1001 10:43:06.203935  5182 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1001 10:43:06.203938  5182 net.cpp:198] penlu3 needs backward computation.
I1001 10:43:06.203939  5182 net.cpp:198] Eltwise1 needs backward computation.
I1001 10:43:06.203943  5182 net.cpp:198] Scale3 needs backward computation.
I1001 10:43:06.203948  5182 net.cpp:198] BatchNorm3 needs backward computation.
I1001 10:43:06.203949  5182 net.cpp:198] Convolution3 needs backward computation.
I1001 10:43:06.203951  5182 net.cpp:198] penlu2 needs backward computation.
I1001 10:43:06.203953  5182 net.cpp:198] Scale2 needs backward computation.
I1001 10:43:06.203956  5182 net.cpp:198] BatchNorm2 needs backward computation.
I1001 10:43:06.203958  5182 net.cpp:198] Convolution2 needs backward computation.
I1001 10:43:06.203961  5182 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1001 10:43:06.203963  5182 net.cpp:198] penlu1 needs backward computation.
I1001 10:43:06.203966  5182 net.cpp:198] Scale1 needs backward computation.
I1001 10:43:06.203969  5182 net.cpp:198] BatchNorm1 needs backward computation.
I1001 10:43:06.203970  5182 net.cpp:198] Convolution1 needs backward computation.
I1001 10:43:06.203972  5182 net.cpp:200] Data1 does not need backward computation.
I1001 10:43:06.203974  5182 net.cpp:242] This network produces output SoftmaxWithLoss1
I1001 10:43:06.204008  5182 net.cpp:255] Network initialization done.
I1001 10:43:06.205749  5182 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 10:43:06.205757  5182 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1001 10:43:06.205761  5182 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 10:43:06.205838  5182 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I1001 10:43:06.206326  5182 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I1001 10:43:06.206589  5182 layer_factory.hpp:77] Creating layer Data1
I1001 10:43:06.206627  5182 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb
I1001 10:43:06.206637  5182 net.cpp:84] Creating Layer Data1
I1001 10:43:06.206641  5182 net.cpp:380] Data1 -> Data1
I1001 10:43:06.206647  5182 net.cpp:380] Data1 -> Data2
I1001 10:43:06.206652  5182 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I1001 10:43:06.206765  5182 data_layer.cpp:45] output data size: 100,3,32,32
I1001 10:43:06.210606  5182 net.cpp:122] Setting up Data1
I1001 10:43:06.210631  5182 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1001 10:43:06.210638  5182 net.cpp:129] Top shape: 100 (100)
I1001 10:43:06.210639  5182 net.cpp:137] Memory required for data: 1229200
I1001 10:43:06.210642  5182 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1001 10:43:06.210650  5182 net.cpp:84] Creating Layer Data2_Data1_1_split
I1001 10:43:06.210654  5182 net.cpp:406] Data2_Data1_1_split <- Data2
I1001 10:43:06.210657  5182 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1001 10:43:06.210664  5182 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1001 10:43:06.210724  5182 net.cpp:122] Setting up Data2_Data1_1_split
I1001 10:43:06.210731  5182 net.cpp:129] Top shape: 100 (100)
I1001 10:43:06.210733  5182 net.cpp:129] Top shape: 100 (100)
I1001 10:43:06.210736  5182 net.cpp:137] Memory required for data: 1230000
I1001 10:43:06.210737  5182 layer_factory.hpp:77] Creating layer Convolution1
I1001 10:43:06.210747  5182 net.cpp:84] Creating Layer Convolution1
I1001 10:43:06.210749  5182 net.cpp:406] Convolution1 <- Data1
I1001 10:43:06.210753  5182 net.cpp:380] Convolution1 -> Convolution1
I1001 10:43:06.211899  5182 net.cpp:122] Setting up Convolution1
I1001 10:43:06.211910  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.211912  5182 net.cpp:137] Memory required for data: 7783600
I1001 10:43:06.211920  5182 layer_factory.hpp:77] Creating layer BatchNorm1
I1001 10:43:06.211926  5182 net.cpp:84] Creating Layer BatchNorm1
I1001 10:43:06.211932  5182 net.cpp:406] BatchNorm1 <- Convolution1
I1001 10:43:06.211936  5182 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1001 10:43:06.212085  5182 net.cpp:122] Setting up BatchNorm1
I1001 10:43:06.212090  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.212091  5182 net.cpp:137] Memory required for data: 14337200
I1001 10:43:06.212100  5182 layer_factory.hpp:77] Creating layer Scale1
I1001 10:43:06.212106  5182 net.cpp:84] Creating Layer Scale1
I1001 10:43:06.212108  5182 net.cpp:406] Scale1 <- Convolution1
I1001 10:43:06.212116  5182 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1001 10:43:06.212147  5182 layer_factory.hpp:77] Creating layer Scale1
I1001 10:43:06.212227  5182 net.cpp:122] Setting up Scale1
I1001 10:43:06.212231  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.212242  5182 net.cpp:137] Memory required for data: 20890800
I1001 10:43:06.212245  5182 layer_factory.hpp:77] Creating layer penlu1
I1001 10:43:06.212252  5182 net.cpp:84] Creating Layer penlu1
I1001 10:43:06.212255  5182 net.cpp:406] penlu1 <- Convolution1
I1001 10:43:06.212258  5182 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1001 10:43:06.212375  5182 net.cpp:122] Setting up penlu1
I1001 10:43:06.212380  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.212383  5182 net.cpp:137] Memory required for data: 27444400
I1001 10:43:06.212388  5182 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1001 10:43:06.212393  5182 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1001 10:43:06.212394  5182 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1001 10:43:06.212397  5182 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1001 10:43:06.212402  5182 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1001 10:43:06.212425  5182 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1001 10:43:06.212430  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.212432  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.212435  5182 net.cpp:137] Memory required for data: 40551600
I1001 10:43:06.212436  5182 layer_factory.hpp:77] Creating layer Convolution2
I1001 10:43:06.212443  5182 net.cpp:84] Creating Layer Convolution2
I1001 10:43:06.212446  5182 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1001 10:43:06.212450  5182 net.cpp:380] Convolution2 -> Convolution2
I1001 10:43:06.213055  5182 net.cpp:122] Setting up Convolution2
I1001 10:43:06.213063  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.213065  5182 net.cpp:137] Memory required for data: 47105200
I1001 10:43:06.213070  5182 layer_factory.hpp:77] Creating layer BatchNorm2
I1001 10:43:06.213078  5182 net.cpp:84] Creating Layer BatchNorm2
I1001 10:43:06.213080  5182 net.cpp:406] BatchNorm2 <- Convolution2
I1001 10:43:06.213084  5182 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1001 10:43:06.213304  5182 net.cpp:122] Setting up BatchNorm2
I1001 10:43:06.213309  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.213310  5182 net.cpp:137] Memory required for data: 53658800
I1001 10:43:06.213323  5182 layer_factory.hpp:77] Creating layer Scale2
I1001 10:43:06.213328  5182 net.cpp:84] Creating Layer Scale2
I1001 10:43:06.213331  5182 net.cpp:406] Scale2 <- Convolution2
I1001 10:43:06.213335  5182 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1001 10:43:06.213364  5182 layer_factory.hpp:77] Creating layer Scale2
I1001 10:43:06.213438  5182 net.cpp:122] Setting up Scale2
I1001 10:43:06.213441  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.213443  5182 net.cpp:137] Memory required for data: 60212400
I1001 10:43:06.213450  5182 layer_factory.hpp:77] Creating layer penlu2
I1001 10:43:06.213456  5182 net.cpp:84] Creating Layer penlu2
I1001 10:43:06.213459  5182 net.cpp:406] penlu2 <- Convolution2
I1001 10:43:06.213462  5182 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1001 10:43:06.213582  5182 net.cpp:122] Setting up penlu2
I1001 10:43:06.213585  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.213587  5182 net.cpp:137] Memory required for data: 66766000
I1001 10:43:06.213593  5182 layer_factory.hpp:77] Creating layer Convolution3
I1001 10:43:06.213598  5182 net.cpp:84] Creating Layer Convolution3
I1001 10:43:06.213601  5182 net.cpp:406] Convolution3 <- Convolution2
I1001 10:43:06.213605  5182 net.cpp:380] Convolution3 -> Convolution3
I1001 10:43:06.214679  5182 net.cpp:122] Setting up Convolution3
I1001 10:43:06.214689  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.214691  5182 net.cpp:137] Memory required for data: 73319600
I1001 10:43:06.214695  5182 layer_factory.hpp:77] Creating layer BatchNorm3
I1001 10:43:06.214700  5182 net.cpp:84] Creating Layer BatchNorm3
I1001 10:43:06.214704  5182 net.cpp:406] BatchNorm3 <- Convolution3
I1001 10:43:06.214709  5182 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1001 10:43:06.214843  5182 net.cpp:122] Setting up BatchNorm3
I1001 10:43:06.214848  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.214850  5182 net.cpp:137] Memory required for data: 79873200
I1001 10:43:06.214855  5182 layer_factory.hpp:77] Creating layer Scale3
I1001 10:43:06.214860  5182 net.cpp:84] Creating Layer Scale3
I1001 10:43:06.214862  5182 net.cpp:406] Scale3 <- Convolution3
I1001 10:43:06.214865  5182 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1001 10:43:06.214892  5182 layer_factory.hpp:77] Creating layer Scale3
I1001 10:43:06.214967  5182 net.cpp:122] Setting up Scale3
I1001 10:43:06.214973  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.214975  5182 net.cpp:137] Memory required for data: 86426800
I1001 10:43:06.214979  5182 layer_factory.hpp:77] Creating layer Eltwise1
I1001 10:43:06.214984  5182 net.cpp:84] Creating Layer Eltwise1
I1001 10:43:06.214987  5182 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1001 10:43:06.214989  5182 net.cpp:406] Eltwise1 <- Convolution3
I1001 10:43:06.214995  5182 net.cpp:380] Eltwise1 -> Eltwise1
I1001 10:43:06.215011  5182 net.cpp:122] Setting up Eltwise1
I1001 10:43:06.215018  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.215019  5182 net.cpp:137] Memory required for data: 92980400
I1001 10:43:06.215021  5182 layer_factory.hpp:77] Creating layer penlu3
I1001 10:43:06.215026  5182 net.cpp:84] Creating Layer penlu3
I1001 10:43:06.215029  5182 net.cpp:406] penlu3 <- Eltwise1
I1001 10:43:06.215032  5182 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1001 10:43:06.215147  5182 net.cpp:122] Setting up penlu3
I1001 10:43:06.215152  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.215153  5182 net.cpp:137] Memory required for data: 99534000
I1001 10:43:06.215162  5182 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1001 10:43:06.215167  5182 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1001 10:43:06.215171  5182 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1001 10:43:06.215174  5182 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1001 10:43:06.215178  5182 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1001 10:43:06.215204  5182 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1001 10:43:06.215215  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.215219  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.215220  5182 net.cpp:137] Memory required for data: 112641200
I1001 10:43:06.215222  5182 layer_factory.hpp:77] Creating layer Convolution4
I1001 10:43:06.215229  5182 net.cpp:84] Creating Layer Convolution4
I1001 10:43:06.215232  5182 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1001 10:43:06.215236  5182 net.cpp:380] Convolution4 -> Convolution4
I1001 10:43:06.216282  5182 net.cpp:122] Setting up Convolution4
I1001 10:43:06.216291  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.216295  5182 net.cpp:137] Memory required for data: 119194800
I1001 10:43:06.216300  5182 layer_factory.hpp:77] Creating layer BatchNorm4
I1001 10:43:06.216305  5182 net.cpp:84] Creating Layer BatchNorm4
I1001 10:43:06.216306  5182 net.cpp:406] BatchNorm4 <- Convolution4
I1001 10:43:06.216310  5182 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1001 10:43:06.216442  5182 net.cpp:122] Setting up BatchNorm4
I1001 10:43:06.216446  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.216449  5182 net.cpp:137] Memory required for data: 125748400
I1001 10:43:06.216456  5182 layer_factory.hpp:77] Creating layer Scale4
I1001 10:43:06.216460  5182 net.cpp:84] Creating Layer Scale4
I1001 10:43:06.216462  5182 net.cpp:406] Scale4 <- Convolution4
I1001 10:43:06.216467  5182 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1001 10:43:06.216495  5182 layer_factory.hpp:77] Creating layer Scale4
I1001 10:43:06.216573  5182 net.cpp:122] Setting up Scale4
I1001 10:43:06.216578  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.216580  5182 net.cpp:137] Memory required for data: 132302000
I1001 10:43:06.216584  5182 layer_factory.hpp:77] Creating layer penlu4
I1001 10:43:06.216590  5182 net.cpp:84] Creating Layer penlu4
I1001 10:43:06.216593  5182 net.cpp:406] penlu4 <- Convolution4
I1001 10:43:06.216598  5182 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1001 10:43:06.216712  5182 net.cpp:122] Setting up penlu4
I1001 10:43:06.216717  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.216718  5182 net.cpp:137] Memory required for data: 138855600
I1001 10:43:06.216724  5182 layer_factory.hpp:77] Creating layer Convolution5
I1001 10:43:06.216732  5182 net.cpp:84] Creating Layer Convolution5
I1001 10:43:06.216734  5182 net.cpp:406] Convolution5 <- Convolution4
I1001 10:43:06.216739  5182 net.cpp:380] Convolution5 -> Convolution5
I1001 10:43:06.217979  5182 net.cpp:122] Setting up Convolution5
I1001 10:43:06.217988  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.217990  5182 net.cpp:137] Memory required for data: 145409200
I1001 10:43:06.217995  5182 layer_factory.hpp:77] Creating layer BatchNorm5
I1001 10:43:06.218000  5182 net.cpp:84] Creating Layer BatchNorm5
I1001 10:43:06.218003  5182 net.cpp:406] BatchNorm5 <- Convolution5
I1001 10:43:06.218006  5182 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1001 10:43:06.218144  5182 net.cpp:122] Setting up BatchNorm5
I1001 10:43:06.218148  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.218150  5182 net.cpp:137] Memory required for data: 151962800
I1001 10:43:06.218155  5182 layer_factory.hpp:77] Creating layer Scale5
I1001 10:43:06.218159  5182 net.cpp:84] Creating Layer Scale5
I1001 10:43:06.218161  5182 net.cpp:406] Scale5 <- Convolution5
I1001 10:43:06.218165  5182 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1001 10:43:06.218191  5182 layer_factory.hpp:77] Creating layer Scale5
I1001 10:43:06.218266  5182 net.cpp:122] Setting up Scale5
I1001 10:43:06.218271  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.218273  5182 net.cpp:137] Memory required for data: 158516400
I1001 10:43:06.218277  5182 layer_factory.hpp:77] Creating layer Eltwise2
I1001 10:43:06.218281  5182 net.cpp:84] Creating Layer Eltwise2
I1001 10:43:06.218283  5182 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1001 10:43:06.218293  5182 net.cpp:406] Eltwise2 <- Convolution5
I1001 10:43:06.218297  5182 net.cpp:380] Eltwise2 -> Eltwise2
I1001 10:43:06.218314  5182 net.cpp:122] Setting up Eltwise2
I1001 10:43:06.218318  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.218320  5182 net.cpp:137] Memory required for data: 165070000
I1001 10:43:06.218322  5182 layer_factory.hpp:77] Creating layer penlu5
I1001 10:43:06.218329  5182 net.cpp:84] Creating Layer penlu5
I1001 10:43:06.218331  5182 net.cpp:406] penlu5 <- Eltwise2
I1001 10:43:06.218335  5182 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1001 10:43:06.218451  5182 net.cpp:122] Setting up penlu5
I1001 10:43:06.218454  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.218456  5182 net.cpp:137] Memory required for data: 171623600
I1001 10:43:06.218461  5182 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1001 10:43:06.218466  5182 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1001 10:43:06.218468  5182 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1001 10:43:06.218472  5182 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1001 10:43:06.218475  5182 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1001 10:43:06.218499  5182 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1001 10:43:06.218503  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.218506  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.218508  5182 net.cpp:137] Memory required for data: 184730800
I1001 10:43:06.218510  5182 layer_factory.hpp:77] Creating layer Convolution6
I1001 10:43:06.218515  5182 net.cpp:84] Creating Layer Convolution6
I1001 10:43:06.218518  5182 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1001 10:43:06.218544  5182 net.cpp:380] Convolution6 -> Convolution6
I1001 10:43:06.219466  5182 net.cpp:122] Setting up Convolution6
I1001 10:43:06.219475  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.219477  5182 net.cpp:137] Memory required for data: 191284400
I1001 10:43:06.219481  5182 layer_factory.hpp:77] Creating layer BatchNorm6
I1001 10:43:06.219487  5182 net.cpp:84] Creating Layer BatchNorm6
I1001 10:43:06.219489  5182 net.cpp:406] BatchNorm6 <- Convolution6
I1001 10:43:06.219493  5182 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1001 10:43:06.219627  5182 net.cpp:122] Setting up BatchNorm6
I1001 10:43:06.219632  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.219635  5182 net.cpp:137] Memory required for data: 197838000
I1001 10:43:06.219638  5182 layer_factory.hpp:77] Creating layer Scale6
I1001 10:43:06.219643  5182 net.cpp:84] Creating Layer Scale6
I1001 10:43:06.219645  5182 net.cpp:406] Scale6 <- Convolution6
I1001 10:43:06.219650  5182 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1001 10:43:06.219676  5182 layer_factory.hpp:77] Creating layer Scale6
I1001 10:43:06.219750  5182 net.cpp:122] Setting up Scale6
I1001 10:43:06.219754  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.219756  5182 net.cpp:137] Memory required for data: 204391600
I1001 10:43:06.219760  5182 layer_factory.hpp:77] Creating layer penlu6
I1001 10:43:06.219765  5182 net.cpp:84] Creating Layer penlu6
I1001 10:43:06.219769  5182 net.cpp:406] penlu6 <- Convolution6
I1001 10:43:06.219772  5182 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1001 10:43:06.219885  5182 net.cpp:122] Setting up penlu6
I1001 10:43:06.219889  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.219892  5182 net.cpp:137] Memory required for data: 210945200
I1001 10:43:06.219895  5182 layer_factory.hpp:77] Creating layer Convolution7
I1001 10:43:06.219903  5182 net.cpp:84] Creating Layer Convolution7
I1001 10:43:06.219907  5182 net.cpp:406] Convolution7 <- Convolution6
I1001 10:43:06.219909  5182 net.cpp:380] Convolution7 -> Convolution7
I1001 10:43:06.220850  5182 net.cpp:122] Setting up Convolution7
I1001 10:43:06.220860  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.220862  5182 net.cpp:137] Memory required for data: 217498800
I1001 10:43:06.220875  5182 layer_factory.hpp:77] Creating layer BatchNorm7
I1001 10:43:06.220882  5182 net.cpp:84] Creating Layer BatchNorm7
I1001 10:43:06.220885  5182 net.cpp:406] BatchNorm7 <- Convolution7
I1001 10:43:06.220888  5182 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1001 10:43:06.221024  5182 net.cpp:122] Setting up BatchNorm7
I1001 10:43:06.221029  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.221031  5182 net.cpp:137] Memory required for data: 224052400
I1001 10:43:06.221041  5182 layer_factory.hpp:77] Creating layer Scale7
I1001 10:43:06.221045  5182 net.cpp:84] Creating Layer Scale7
I1001 10:43:06.221047  5182 net.cpp:406] Scale7 <- Convolution7
I1001 10:43:06.221051  5182 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1001 10:43:06.221079  5182 layer_factory.hpp:77] Creating layer Scale7
I1001 10:43:06.221155  5182 net.cpp:122] Setting up Scale7
I1001 10:43:06.221159  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.221163  5182 net.cpp:137] Memory required for data: 230606000
I1001 10:43:06.221166  5182 layer_factory.hpp:77] Creating layer Eltwise3
I1001 10:43:06.221170  5182 net.cpp:84] Creating Layer Eltwise3
I1001 10:43:06.221174  5182 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1001 10:43:06.221175  5182 net.cpp:406] Eltwise3 <- Convolution7
I1001 10:43:06.221179  5182 net.cpp:380] Eltwise3 -> Eltwise3
I1001 10:43:06.221194  5182 net.cpp:122] Setting up Eltwise3
I1001 10:43:06.221197  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.221199  5182 net.cpp:137] Memory required for data: 237159600
I1001 10:43:06.221201  5182 layer_factory.hpp:77] Creating layer penlu7
I1001 10:43:06.221207  5182 net.cpp:84] Creating Layer penlu7
I1001 10:43:06.221210  5182 net.cpp:406] penlu7 <- Eltwise3
I1001 10:43:06.221212  5182 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1001 10:43:06.221328  5182 net.cpp:122] Setting up penlu7
I1001 10:43:06.221333  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.221334  5182 net.cpp:137] Memory required for data: 243713200
I1001 10:43:06.221338  5182 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1001 10:43:06.221343  5182 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1001 10:43:06.221344  5182 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1001 10:43:06.221349  5182 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1001 10:43:06.221352  5182 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1001 10:43:06.221374  5182 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1001 10:43:06.221379  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.221380  5182 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 10:43:06.221382  5182 net.cpp:137] Memory required for data: 256820400
I1001 10:43:06.221385  5182 layer_factory.hpp:77] Creating layer Convolution8
I1001 10:43:06.221391  5182 net.cpp:84] Creating Layer Convolution8
I1001 10:43:06.221393  5182 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1001 10:43:06.221397  5182 net.cpp:380] Convolution8 -> Convolution8
I1001 10:43:06.222268  5182 net.cpp:122] Setting up Convolution8
I1001 10:43:06.222277  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.222280  5182 net.cpp:137] Memory required for data: 260097200
I1001 10:43:06.222285  5182 layer_factory.hpp:77] Creating layer BatchNorm8
I1001 10:43:06.222290  5182 net.cpp:84] Creating Layer BatchNorm8
I1001 10:43:06.222293  5182 net.cpp:406] BatchNorm8 <- Convolution8
I1001 10:43:06.222297  5182 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1001 10:43:06.222426  5182 net.cpp:122] Setting up BatchNorm8
I1001 10:43:06.222430  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.222432  5182 net.cpp:137] Memory required for data: 263374000
I1001 10:43:06.222436  5182 layer_factory.hpp:77] Creating layer Scale8
I1001 10:43:06.226068  5182 net.cpp:84] Creating Layer Scale8
I1001 10:43:06.226071  5182 net.cpp:406] Scale8 <- Convolution8
I1001 10:43:06.226083  5182 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1001 10:43:06.226117  5182 layer_factory.hpp:77] Creating layer Scale8
I1001 10:43:06.226202  5182 net.cpp:122] Setting up Scale8
I1001 10:43:06.226207  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.226208  5182 net.cpp:137] Memory required for data: 266650800
I1001 10:43:06.226213  5182 layer_factory.hpp:77] Creating layer Convolution9
I1001 10:43:06.226220  5182 net.cpp:84] Creating Layer Convolution9
I1001 10:43:06.226223  5182 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1001 10:43:06.226228  5182 net.cpp:380] Convolution9 -> Convolution9
I1001 10:43:06.227303  5182 net.cpp:122] Setting up Convolution9
I1001 10:43:06.227311  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.227314  5182 net.cpp:137] Memory required for data: 269927600
I1001 10:43:06.227319  5182 layer_factory.hpp:77] Creating layer BatchNorm9
I1001 10:43:06.227325  5182 net.cpp:84] Creating Layer BatchNorm9
I1001 10:43:06.227327  5182 net.cpp:406] BatchNorm9 <- Convolution9
I1001 10:43:06.227334  5182 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1001 10:43:06.227473  5182 net.cpp:122] Setting up BatchNorm9
I1001 10:43:06.227478  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.227480  5182 net.cpp:137] Memory required for data: 273204400
I1001 10:43:06.227486  5182 layer_factory.hpp:77] Creating layer Scale9
I1001 10:43:06.227490  5182 net.cpp:84] Creating Layer Scale9
I1001 10:43:06.227493  5182 net.cpp:406] Scale9 <- Convolution9
I1001 10:43:06.227496  5182 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1001 10:43:06.227524  5182 layer_factory.hpp:77] Creating layer Scale9
I1001 10:43:06.227607  5182 net.cpp:122] Setting up Scale9
I1001 10:43:06.227610  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.227612  5182 net.cpp:137] Memory required for data: 276481200
I1001 10:43:06.227617  5182 layer_factory.hpp:77] Creating layer penlu8
I1001 10:43:06.227622  5182 net.cpp:84] Creating Layer penlu8
I1001 10:43:06.227624  5182 net.cpp:406] penlu8 <- Convolution9
I1001 10:43:06.227629  5182 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1001 10:43:06.227743  5182 net.cpp:122] Setting up penlu8
I1001 10:43:06.227748  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.227751  5182 net.cpp:137] Memory required for data: 279758000
I1001 10:43:06.227754  5182 layer_factory.hpp:77] Creating layer Convolution10
I1001 10:43:06.227761  5182 net.cpp:84] Creating Layer Convolution10
I1001 10:43:06.227764  5182 net.cpp:406] Convolution10 <- Convolution9
I1001 10:43:06.227768  5182 net.cpp:380] Convolution10 -> Convolution10
I1001 10:43:06.229297  5182 net.cpp:122] Setting up Convolution10
I1001 10:43:06.229306  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.229310  5182 net.cpp:137] Memory required for data: 283034800
I1001 10:43:06.229313  5182 layer_factory.hpp:77] Creating layer BatchNorm10
I1001 10:43:06.229320  5182 net.cpp:84] Creating Layer BatchNorm10
I1001 10:43:06.229321  5182 net.cpp:406] BatchNorm10 <- Convolution10
I1001 10:43:06.229326  5182 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1001 10:43:06.229460  5182 net.cpp:122] Setting up BatchNorm10
I1001 10:43:06.229463  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.229466  5182 net.cpp:137] Memory required for data: 286311600
I1001 10:43:06.229470  5182 layer_factory.hpp:77] Creating layer Scale10
I1001 10:43:06.229475  5182 net.cpp:84] Creating Layer Scale10
I1001 10:43:06.229478  5182 net.cpp:406] Scale10 <- Convolution10
I1001 10:43:06.229481  5182 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1001 10:43:06.229509  5182 layer_factory.hpp:77] Creating layer Scale10
I1001 10:43:06.229588  5182 net.cpp:122] Setting up Scale10
I1001 10:43:06.229593  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.229594  5182 net.cpp:137] Memory required for data: 289588400
I1001 10:43:06.229598  5182 layer_factory.hpp:77] Creating layer Eltwise4
I1001 10:43:06.229604  5182 net.cpp:84] Creating Layer Eltwise4
I1001 10:43:06.229612  5182 net.cpp:406] Eltwise4 <- Convolution8
I1001 10:43:06.229615  5182 net.cpp:406] Eltwise4 <- Convolution10
I1001 10:43:06.229619  5182 net.cpp:380] Eltwise4 -> Eltwise4
I1001 10:43:06.229635  5182 net.cpp:122] Setting up Eltwise4
I1001 10:43:06.229638  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.229640  5182 net.cpp:137] Memory required for data: 292865200
I1001 10:43:06.229642  5182 layer_factory.hpp:77] Creating layer penlu9
I1001 10:43:06.229647  5182 net.cpp:84] Creating Layer penlu9
I1001 10:43:06.229650  5182 net.cpp:406] penlu9 <- Eltwise4
I1001 10:43:06.229655  5182 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1001 10:43:06.229775  5182 net.cpp:122] Setting up penlu9
I1001 10:43:06.229790  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.229792  5182 net.cpp:137] Memory required for data: 296142000
I1001 10:43:06.229797  5182 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1001 10:43:06.229801  5182 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1001 10:43:06.229804  5182 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1001 10:43:06.229806  5182 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1001 10:43:06.229810  5182 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1001 10:43:06.229846  5182 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1001 10:43:06.229849  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.229852  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.229854  5182 net.cpp:137] Memory required for data: 302695600
I1001 10:43:06.229856  5182 layer_factory.hpp:77] Creating layer Convolution11
I1001 10:43:06.229863  5182 net.cpp:84] Creating Layer Convolution11
I1001 10:43:06.229866  5182 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1001 10:43:06.229869  5182 net.cpp:380] Convolution11 -> Convolution11
I1001 10:43:06.231003  5182 net.cpp:122] Setting up Convolution11
I1001 10:43:06.231011  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.231014  5182 net.cpp:137] Memory required for data: 305972400
I1001 10:43:06.231017  5182 layer_factory.hpp:77] Creating layer BatchNorm11
I1001 10:43:06.231022  5182 net.cpp:84] Creating Layer BatchNorm11
I1001 10:43:06.231024  5182 net.cpp:406] BatchNorm11 <- Convolution11
I1001 10:43:06.231029  5182 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1001 10:43:06.231163  5182 net.cpp:122] Setting up BatchNorm11
I1001 10:43:06.231166  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.231168  5182 net.cpp:137] Memory required for data: 309249200
I1001 10:43:06.231173  5182 layer_factory.hpp:77] Creating layer Scale11
I1001 10:43:06.231178  5182 net.cpp:84] Creating Layer Scale11
I1001 10:43:06.231179  5182 net.cpp:406] Scale11 <- Convolution11
I1001 10:43:06.231182  5182 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1001 10:43:06.231209  5182 layer_factory.hpp:77] Creating layer Scale11
I1001 10:43:06.231284  5182 net.cpp:122] Setting up Scale11
I1001 10:43:06.231288  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.231290  5182 net.cpp:137] Memory required for data: 312526000
I1001 10:43:06.231294  5182 layer_factory.hpp:77] Creating layer penlu10
I1001 10:43:06.231299  5182 net.cpp:84] Creating Layer penlu10
I1001 10:43:06.231302  5182 net.cpp:406] penlu10 <- Convolution11
I1001 10:43:06.231305  5182 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1001 10:43:06.231412  5182 net.cpp:122] Setting up penlu10
I1001 10:43:06.231417  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.231420  5182 net.cpp:137] Memory required for data: 315802800
I1001 10:43:06.231423  5182 layer_factory.hpp:77] Creating layer Convolution12
I1001 10:43:06.231431  5182 net.cpp:84] Creating Layer Convolution12
I1001 10:43:06.231433  5182 net.cpp:406] Convolution12 <- Convolution11
I1001 10:43:06.231436  5182 net.cpp:380] Convolution12 -> Convolution12
I1001 10:43:06.232161  5182 net.cpp:122] Setting up Convolution12
I1001 10:43:06.232177  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.232179  5182 net.cpp:137] Memory required for data: 319079600
I1001 10:43:06.232183  5182 layer_factory.hpp:77] Creating layer BatchNorm12
I1001 10:43:06.232188  5182 net.cpp:84] Creating Layer BatchNorm12
I1001 10:43:06.232192  5182 net.cpp:406] BatchNorm12 <- Convolution12
I1001 10:43:06.232194  5182 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1001 10:43:06.232328  5182 net.cpp:122] Setting up BatchNorm12
I1001 10:43:06.232333  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.232336  5182 net.cpp:137] Memory required for data: 322356400
I1001 10:43:06.232339  5182 layer_factory.hpp:77] Creating layer Scale12
I1001 10:43:06.232344  5182 net.cpp:84] Creating Layer Scale12
I1001 10:43:06.232347  5182 net.cpp:406] Scale12 <- Convolution12
I1001 10:43:06.232349  5182 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1001 10:43:06.232376  5182 layer_factory.hpp:77] Creating layer Scale12
I1001 10:43:06.232453  5182 net.cpp:122] Setting up Scale12
I1001 10:43:06.232457  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.232460  5182 net.cpp:137] Memory required for data: 325633200
I1001 10:43:06.232463  5182 layer_factory.hpp:77] Creating layer Eltwise5
I1001 10:43:06.232467  5182 net.cpp:84] Creating Layer Eltwise5
I1001 10:43:06.232470  5182 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1001 10:43:06.232473  5182 net.cpp:406] Eltwise5 <- Convolution12
I1001 10:43:06.232477  5182 net.cpp:380] Eltwise5 -> Eltwise5
I1001 10:43:06.232489  5182 net.cpp:122] Setting up Eltwise5
I1001 10:43:06.232492  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.232494  5182 net.cpp:137] Memory required for data: 328910000
I1001 10:43:06.232496  5182 layer_factory.hpp:77] Creating layer penlu11
I1001 10:43:06.232502  5182 net.cpp:84] Creating Layer penlu11
I1001 10:43:06.232504  5182 net.cpp:406] penlu11 <- Eltwise5
I1001 10:43:06.232507  5182 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1001 10:43:06.232620  5182 net.cpp:122] Setting up penlu11
I1001 10:43:06.232625  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.232626  5182 net.cpp:137] Memory required for data: 332186800
I1001 10:43:06.232630  5182 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1001 10:43:06.232635  5182 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1001 10:43:06.232636  5182 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1001 10:43:06.232640  5182 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1001 10:43:06.232643  5182 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1001 10:43:06.232669  5182 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1001 10:43:06.232673  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.232676  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.232678  5182 net.cpp:137] Memory required for data: 338740400
I1001 10:43:06.232681  5182 layer_factory.hpp:77] Creating layer Convolution13
I1001 10:43:06.232686  5182 net.cpp:84] Creating Layer Convolution13
I1001 10:43:06.232688  5182 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1001 10:43:06.232692  5182 net.cpp:380] Convolution13 -> Convolution13
I1001 10:43:06.233747  5182 net.cpp:122] Setting up Convolution13
I1001 10:43:06.233755  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.233757  5182 net.cpp:137] Memory required for data: 342017200
I1001 10:43:06.233762  5182 layer_factory.hpp:77] Creating layer BatchNorm13
I1001 10:43:06.233767  5182 net.cpp:84] Creating Layer BatchNorm13
I1001 10:43:06.233770  5182 net.cpp:406] BatchNorm13 <- Convolution13
I1001 10:43:06.233774  5182 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1001 10:43:06.233907  5182 net.cpp:122] Setting up BatchNorm13
I1001 10:43:06.233911  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.233913  5182 net.cpp:137] Memory required for data: 345294000
I1001 10:43:06.233918  5182 layer_factory.hpp:77] Creating layer Scale13
I1001 10:43:06.233929  5182 net.cpp:84] Creating Layer Scale13
I1001 10:43:06.233932  5182 net.cpp:406] Scale13 <- Convolution13
I1001 10:43:06.233935  5182 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1001 10:43:06.233963  5182 layer_factory.hpp:77] Creating layer Scale13
I1001 10:43:06.234040  5182 net.cpp:122] Setting up Scale13
I1001 10:43:06.234043  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.234045  5182 net.cpp:137] Memory required for data: 348570800
I1001 10:43:06.234050  5182 layer_factory.hpp:77] Creating layer penlu12
I1001 10:43:06.234055  5182 net.cpp:84] Creating Layer penlu12
I1001 10:43:06.234057  5182 net.cpp:406] penlu12 <- Convolution13
I1001 10:43:06.234061  5182 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1001 10:43:06.234166  5182 net.cpp:122] Setting up penlu12
I1001 10:43:06.234170  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.234172  5182 net.cpp:137] Memory required for data: 351847600
I1001 10:43:06.234176  5182 layer_factory.hpp:77] Creating layer Convolution14
I1001 10:43:06.234187  5182 net.cpp:84] Creating Layer Convolution14
I1001 10:43:06.234189  5182 net.cpp:406] Convolution14 <- Convolution13
I1001 10:43:06.234194  5182 net.cpp:380] Convolution14 -> Convolution14
I1001 10:43:06.235275  5182 net.cpp:122] Setting up Convolution14
I1001 10:43:06.235285  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.235287  5182 net.cpp:137] Memory required for data: 355124400
I1001 10:43:06.235302  5182 layer_factory.hpp:77] Creating layer BatchNorm14
I1001 10:43:06.235307  5182 net.cpp:84] Creating Layer BatchNorm14
I1001 10:43:06.235311  5182 net.cpp:406] BatchNorm14 <- Convolution14
I1001 10:43:06.235313  5182 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1001 10:43:06.235446  5182 net.cpp:122] Setting up BatchNorm14
I1001 10:43:06.235451  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.235453  5182 net.cpp:137] Memory required for data: 358401200
I1001 10:43:06.235458  5182 layer_factory.hpp:77] Creating layer Scale14
I1001 10:43:06.235462  5182 net.cpp:84] Creating Layer Scale14
I1001 10:43:06.235466  5182 net.cpp:406] Scale14 <- Convolution14
I1001 10:43:06.235468  5182 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1001 10:43:06.235496  5182 layer_factory.hpp:77] Creating layer Scale14
I1001 10:43:06.235570  5182 net.cpp:122] Setting up Scale14
I1001 10:43:06.235574  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.235576  5182 net.cpp:137] Memory required for data: 361678000
I1001 10:43:06.235580  5182 layer_factory.hpp:77] Creating layer Eltwise6
I1001 10:43:06.235585  5182 net.cpp:84] Creating Layer Eltwise6
I1001 10:43:06.235587  5182 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1001 10:43:06.235590  5182 net.cpp:406] Eltwise6 <- Convolution14
I1001 10:43:06.235594  5182 net.cpp:380] Eltwise6 -> Eltwise6
I1001 10:43:06.235606  5182 net.cpp:122] Setting up Eltwise6
I1001 10:43:06.235610  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.235611  5182 net.cpp:137] Memory required for data: 364954800
I1001 10:43:06.235613  5182 layer_factory.hpp:77] Creating layer penlu13
I1001 10:43:06.235618  5182 net.cpp:84] Creating Layer penlu13
I1001 10:43:06.235621  5182 net.cpp:406] penlu13 <- Eltwise6
I1001 10:43:06.235625  5182 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1001 10:43:06.235736  5182 net.cpp:122] Setting up penlu13
I1001 10:43:06.235740  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.235743  5182 net.cpp:137] Memory required for data: 368231600
I1001 10:43:06.235746  5182 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1001 10:43:06.235749  5182 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1001 10:43:06.256973  5182 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1001 10:43:06.256984  5182 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1001 10:43:06.256990  5182 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1001 10:43:06.257028  5182 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1001 10:43:06.257041  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.257045  5182 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 10:43:06.257047  5182 net.cpp:137] Memory required for data: 374785200
I1001 10:43:06.257050  5182 layer_factory.hpp:77] Creating layer Convolution15
I1001 10:43:06.257058  5182 net.cpp:84] Creating Layer Convolution15
I1001 10:43:06.257061  5182 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1001 10:43:06.257066  5182 net.cpp:380] Convolution15 -> Convolution15
I1001 10:43:06.258105  5182 net.cpp:122] Setting up Convolution15
I1001 10:43:06.258116  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.258118  5182 net.cpp:137] Memory required for data: 376423600
I1001 10:43:06.258123  5182 layer_factory.hpp:77] Creating layer BatchNorm15
I1001 10:43:06.258129  5182 net.cpp:84] Creating Layer BatchNorm15
I1001 10:43:06.258132  5182 net.cpp:406] BatchNorm15 <- Convolution15
I1001 10:43:06.258138  5182 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1001 10:43:06.258287  5182 net.cpp:122] Setting up BatchNorm15
I1001 10:43:06.258292  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.258294  5182 net.cpp:137] Memory required for data: 378062000
I1001 10:43:06.258301  5182 layer_factory.hpp:77] Creating layer Scale15
I1001 10:43:06.258306  5182 net.cpp:84] Creating Layer Scale15
I1001 10:43:06.258311  5182 net.cpp:406] Scale15 <- Convolution15
I1001 10:43:06.258313  5182 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1001 10:43:06.258344  5182 layer_factory.hpp:77] Creating layer Scale15
I1001 10:43:06.258440  5182 net.cpp:122] Setting up Scale15
I1001 10:43:06.258443  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.258445  5182 net.cpp:137] Memory required for data: 379700400
I1001 10:43:06.258450  5182 layer_factory.hpp:77] Creating layer Convolution16
I1001 10:43:06.258457  5182 net.cpp:84] Creating Layer Convolution16
I1001 10:43:06.258460  5182 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1001 10:43:06.258466  5182 net.cpp:380] Convolution16 -> Convolution16
I1001 10:43:06.260020  5182 net.cpp:122] Setting up Convolution16
I1001 10:43:06.260030  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.260031  5182 net.cpp:137] Memory required for data: 381338800
I1001 10:43:06.260036  5182 layer_factory.hpp:77] Creating layer BatchNorm16
I1001 10:43:06.260042  5182 net.cpp:84] Creating Layer BatchNorm16
I1001 10:43:06.260046  5182 net.cpp:406] BatchNorm16 <- Convolution16
I1001 10:43:06.260049  5182 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1001 10:43:06.260191  5182 net.cpp:122] Setting up BatchNorm16
I1001 10:43:06.260195  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.260197  5182 net.cpp:137] Memory required for data: 382977200
I1001 10:43:06.260202  5182 layer_factory.hpp:77] Creating layer Scale16
I1001 10:43:06.260206  5182 net.cpp:84] Creating Layer Scale16
I1001 10:43:06.260210  5182 net.cpp:406] Scale16 <- Convolution16
I1001 10:43:06.260212  5182 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1001 10:43:06.260241  5182 layer_factory.hpp:77] Creating layer Scale16
I1001 10:43:06.260321  5182 net.cpp:122] Setting up Scale16
I1001 10:43:06.260325  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.260327  5182 net.cpp:137] Memory required for data: 384615600
I1001 10:43:06.260331  5182 layer_factory.hpp:77] Creating layer penlu14
I1001 10:43:06.260337  5182 net.cpp:84] Creating Layer penlu14
I1001 10:43:06.260339  5182 net.cpp:406] penlu14 <- Convolution16
I1001 10:43:06.260344  5182 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1001 10:43:06.260458  5182 net.cpp:122] Setting up penlu14
I1001 10:43:06.260462  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.260464  5182 net.cpp:137] Memory required for data: 386254000
I1001 10:43:06.260469  5182 layer_factory.hpp:77] Creating layer Convolution17
I1001 10:43:06.260478  5182 net.cpp:84] Creating Layer Convolution17
I1001 10:43:06.260479  5182 net.cpp:406] Convolution17 <- Convolution16
I1001 10:43:06.260491  5182 net.cpp:380] Convolution17 -> Convolution17
I1001 10:43:06.262255  5182 net.cpp:122] Setting up Convolution17
I1001 10:43:06.262264  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.262266  5182 net.cpp:137] Memory required for data: 387892400
I1001 10:43:06.262271  5182 layer_factory.hpp:77] Creating layer BatchNorm17
I1001 10:43:06.262276  5182 net.cpp:84] Creating Layer BatchNorm17
I1001 10:43:06.262279  5182 net.cpp:406] BatchNorm17 <- Convolution17
I1001 10:43:06.262284  5182 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1001 10:43:06.262424  5182 net.cpp:122] Setting up BatchNorm17
I1001 10:43:06.262429  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.262431  5182 net.cpp:137] Memory required for data: 389530800
I1001 10:43:06.262436  5182 layer_factory.hpp:77] Creating layer Scale17
I1001 10:43:06.262441  5182 net.cpp:84] Creating Layer Scale17
I1001 10:43:06.262444  5182 net.cpp:406] Scale17 <- Convolution17
I1001 10:43:06.262447  5182 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1001 10:43:06.262475  5182 layer_factory.hpp:77] Creating layer Scale17
I1001 10:43:06.262562  5182 net.cpp:122] Setting up Scale17
I1001 10:43:06.262567  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.262569  5182 net.cpp:137] Memory required for data: 391169200
I1001 10:43:06.262573  5182 layer_factory.hpp:77] Creating layer Eltwise7
I1001 10:43:06.262578  5182 net.cpp:84] Creating Layer Eltwise7
I1001 10:43:06.262581  5182 net.cpp:406] Eltwise7 <- Convolution15
I1001 10:43:06.262584  5182 net.cpp:406] Eltwise7 <- Convolution17
I1001 10:43:06.262588  5182 net.cpp:380] Eltwise7 -> Eltwise7
I1001 10:43:06.262605  5182 net.cpp:122] Setting up Eltwise7
I1001 10:43:06.262609  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.262611  5182 net.cpp:137] Memory required for data: 392807600
I1001 10:43:06.262614  5182 layer_factory.hpp:77] Creating layer penlu15
I1001 10:43:06.262619  5182 net.cpp:84] Creating Layer penlu15
I1001 10:43:06.262621  5182 net.cpp:406] penlu15 <- Eltwise7
I1001 10:43:06.262624  5182 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1001 10:43:06.262737  5182 net.cpp:122] Setting up penlu15
I1001 10:43:06.262742  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.262743  5182 net.cpp:137] Memory required for data: 394446000
I1001 10:43:06.262748  5182 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1001 10:43:06.262751  5182 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1001 10:43:06.262753  5182 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1001 10:43:06.262756  5182 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1001 10:43:06.262761  5182 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1001 10:43:06.262785  5182 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1001 10:43:06.262789  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.262791  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.262794  5182 net.cpp:137] Memory required for data: 397722800
I1001 10:43:06.262795  5182 layer_factory.hpp:77] Creating layer Convolution18
I1001 10:43:06.262804  5182 net.cpp:84] Creating Layer Convolution18
I1001 10:43:06.262806  5182 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1001 10:43:06.262810  5182 net.cpp:380] Convolution18 -> Convolution18
I1001 10:43:06.264509  5182 net.cpp:122] Setting up Convolution18
I1001 10:43:06.264518  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.264520  5182 net.cpp:137] Memory required for data: 399361200
I1001 10:43:06.264525  5182 layer_factory.hpp:77] Creating layer BatchNorm18
I1001 10:43:06.264530  5182 net.cpp:84] Creating Layer BatchNorm18
I1001 10:43:06.264533  5182 net.cpp:406] BatchNorm18 <- Convolution18
I1001 10:43:06.264538  5182 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1001 10:43:06.264678  5182 net.cpp:122] Setting up BatchNorm18
I1001 10:43:06.264683  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.264691  5182 net.cpp:137] Memory required for data: 400999600
I1001 10:43:06.264696  5182 layer_factory.hpp:77] Creating layer Scale18
I1001 10:43:06.264701  5182 net.cpp:84] Creating Layer Scale18
I1001 10:43:06.264704  5182 net.cpp:406] Scale18 <- Convolution18
I1001 10:43:06.264708  5182 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1001 10:43:06.264737  5182 layer_factory.hpp:77] Creating layer Scale18
I1001 10:43:06.264817  5182 net.cpp:122] Setting up Scale18
I1001 10:43:06.264822  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.264824  5182 net.cpp:137] Memory required for data: 402638000
I1001 10:43:06.264828  5182 layer_factory.hpp:77] Creating layer penlu16
I1001 10:43:06.264834  5182 net.cpp:84] Creating Layer penlu16
I1001 10:43:06.264837  5182 net.cpp:406] penlu16 <- Convolution18
I1001 10:43:06.264840  5182 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1001 10:43:06.264953  5182 net.cpp:122] Setting up penlu16
I1001 10:43:06.264957  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.264960  5182 net.cpp:137] Memory required for data: 404276400
I1001 10:43:06.264964  5182 layer_factory.hpp:77] Creating layer Convolution19
I1001 10:43:06.264971  5182 net.cpp:84] Creating Layer Convolution19
I1001 10:43:06.264973  5182 net.cpp:406] Convolution19 <- Convolution18
I1001 10:43:06.264977  5182 net.cpp:380] Convolution19 -> Convolution19
I1001 10:43:06.266676  5182 net.cpp:122] Setting up Convolution19
I1001 10:43:06.266685  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.266687  5182 net.cpp:137] Memory required for data: 405914800
I1001 10:43:06.266692  5182 layer_factory.hpp:77] Creating layer BatchNorm19
I1001 10:43:06.266697  5182 net.cpp:84] Creating Layer BatchNorm19
I1001 10:43:06.266700  5182 net.cpp:406] BatchNorm19 <- Convolution19
I1001 10:43:06.266705  5182 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1001 10:43:06.266849  5182 net.cpp:122] Setting up BatchNorm19
I1001 10:43:06.266852  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.266855  5182 net.cpp:137] Memory required for data: 407553200
I1001 10:43:06.266860  5182 layer_factory.hpp:77] Creating layer Scale19
I1001 10:43:06.266863  5182 net.cpp:84] Creating Layer Scale19
I1001 10:43:06.266866  5182 net.cpp:406] Scale19 <- Convolution19
I1001 10:43:06.266870  5182 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1001 10:43:06.266898  5182 layer_factory.hpp:77] Creating layer Scale19
I1001 10:43:06.266979  5182 net.cpp:122] Setting up Scale19
I1001 10:43:06.266983  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.266985  5182 net.cpp:137] Memory required for data: 409191600
I1001 10:43:06.266988  5182 layer_factory.hpp:77] Creating layer Eltwise8
I1001 10:43:06.266993  5182 net.cpp:84] Creating Layer Eltwise8
I1001 10:43:06.266996  5182 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1001 10:43:06.266999  5182 net.cpp:406] Eltwise8 <- Convolution19
I1001 10:43:06.267002  5182 net.cpp:380] Eltwise8 -> Eltwise8
I1001 10:43:06.267019  5182 net.cpp:122] Setting up Eltwise8
I1001 10:43:06.267024  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.267025  5182 net.cpp:137] Memory required for data: 410830000
I1001 10:43:06.267027  5182 layer_factory.hpp:77] Creating layer penlu17
I1001 10:43:06.267033  5182 net.cpp:84] Creating Layer penlu17
I1001 10:43:06.267035  5182 net.cpp:406] penlu17 <- Eltwise8
I1001 10:43:06.267040  5182 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1001 10:43:06.267151  5182 net.cpp:122] Setting up penlu17
I1001 10:43:06.267155  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.267158  5182 net.cpp:137] Memory required for data: 412468400
I1001 10:43:06.267163  5182 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1001 10:43:06.267166  5182 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1001 10:43:06.267168  5182 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1001 10:43:06.267171  5182 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1001 10:43:06.267182  5182 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1001 10:43:06.267208  5182 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1001 10:43:06.267212  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.267215  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.267217  5182 net.cpp:137] Memory required for data: 415745200
I1001 10:43:06.267220  5182 layer_factory.hpp:77] Creating layer Convolution20
I1001 10:43:06.267226  5182 net.cpp:84] Creating Layer Convolution20
I1001 10:43:06.267228  5182 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1001 10:43:06.267233  5182 net.cpp:380] Convolution20 -> Convolution20
I1001 10:43:06.269250  5182 net.cpp:122] Setting up Convolution20
I1001 10:43:06.269258  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.269261  5182 net.cpp:137] Memory required for data: 417383600
I1001 10:43:06.269266  5182 layer_factory.hpp:77] Creating layer BatchNorm20
I1001 10:43:06.269271  5182 net.cpp:84] Creating Layer BatchNorm20
I1001 10:43:06.269274  5182 net.cpp:406] BatchNorm20 <- Convolution20
I1001 10:43:06.269279  5182 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1001 10:43:06.269424  5182 net.cpp:122] Setting up BatchNorm20
I1001 10:43:06.269429  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.269431  5182 net.cpp:137] Memory required for data: 419022000
I1001 10:43:06.269436  5182 layer_factory.hpp:77] Creating layer Scale20
I1001 10:43:06.269440  5182 net.cpp:84] Creating Layer Scale20
I1001 10:43:06.269443  5182 net.cpp:406] Scale20 <- Convolution20
I1001 10:43:06.269445  5182 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1001 10:43:06.269475  5182 layer_factory.hpp:77] Creating layer Scale20
I1001 10:43:06.269557  5182 net.cpp:122] Setting up Scale20
I1001 10:43:06.269562  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.269563  5182 net.cpp:137] Memory required for data: 420660400
I1001 10:43:06.269567  5182 layer_factory.hpp:77] Creating layer penlu18
I1001 10:43:06.269572  5182 net.cpp:84] Creating Layer penlu18
I1001 10:43:06.269575  5182 net.cpp:406] penlu18 <- Convolution20
I1001 10:43:06.269579  5182 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1001 10:43:06.269692  5182 net.cpp:122] Setting up penlu18
I1001 10:43:06.269697  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.269700  5182 net.cpp:137] Memory required for data: 422298800
I1001 10:43:06.269703  5182 layer_factory.hpp:77] Creating layer Convolution21
I1001 10:43:06.269711  5182 net.cpp:84] Creating Layer Convolution21
I1001 10:43:06.269713  5182 net.cpp:406] Convolution21 <- Convolution20
I1001 10:43:06.269717  5182 net.cpp:380] Convolution21 -> Convolution21
I1001 10:43:06.271772  5182 net.cpp:122] Setting up Convolution21
I1001 10:43:06.271781  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.271785  5182 net.cpp:137] Memory required for data: 423937200
I1001 10:43:06.271788  5182 layer_factory.hpp:77] Creating layer BatchNorm21
I1001 10:43:06.271793  5182 net.cpp:84] Creating Layer BatchNorm21
I1001 10:43:06.271796  5182 net.cpp:406] BatchNorm21 <- Convolution21
I1001 10:43:06.271800  5182 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1001 10:43:06.271944  5182 net.cpp:122] Setting up BatchNorm21
I1001 10:43:06.271948  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.271950  5182 net.cpp:137] Memory required for data: 425575600
I1001 10:43:06.271955  5182 layer_factory.hpp:77] Creating layer Scale21
I1001 10:43:06.271960  5182 net.cpp:84] Creating Layer Scale21
I1001 10:43:06.287838  5182 net.cpp:406] Scale21 <- Convolution21
I1001 10:43:06.287852  5182 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1001 10:43:06.287895  5182 layer_factory.hpp:77] Creating layer Scale21
I1001 10:43:06.287992  5182 net.cpp:122] Setting up Scale21
I1001 10:43:06.287997  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.287998  5182 net.cpp:137] Memory required for data: 427214000
I1001 10:43:06.288003  5182 layer_factory.hpp:77] Creating layer Eltwise9
I1001 10:43:06.288017  5182 net.cpp:84] Creating Layer Eltwise9
I1001 10:43:06.288019  5182 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1001 10:43:06.288023  5182 net.cpp:406] Eltwise9 <- Convolution21
I1001 10:43:06.288028  5182 net.cpp:380] Eltwise9 -> Eltwise9
I1001 10:43:06.288048  5182 net.cpp:122] Setting up Eltwise9
I1001 10:43:06.288053  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.288055  5182 net.cpp:137] Memory required for data: 428852400
I1001 10:43:06.288058  5182 layer_factory.hpp:77] Creating layer penlu19
I1001 10:43:06.288063  5182 net.cpp:84] Creating Layer penlu19
I1001 10:43:06.288065  5182 net.cpp:406] penlu19 <- Eltwise9
I1001 10:43:06.288069  5182 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1001 10:43:06.288198  5182 net.cpp:122] Setting up penlu19
I1001 10:43:06.288203  5182 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 10:43:06.288206  5182 net.cpp:137] Memory required for data: 430490800
I1001 10:43:06.288211  5182 layer_factory.hpp:77] Creating layer Pooling1
I1001 10:43:06.288216  5182 net.cpp:84] Creating Layer Pooling1
I1001 10:43:06.288218  5182 net.cpp:406] Pooling1 <- Eltwise9
I1001 10:43:06.288223  5182 net.cpp:380] Pooling1 -> Pooling1
I1001 10:43:06.288373  5182 net.cpp:122] Setting up Pooling1
I1001 10:43:06.288380  5182 net.cpp:129] Top shape: 100 64 1 1 (6400)
I1001 10:43:06.288383  5182 net.cpp:137] Memory required for data: 430516400
I1001 10:43:06.288385  5182 layer_factory.hpp:77] Creating layer InnerProduct1
I1001 10:43:06.288393  5182 net.cpp:84] Creating Layer InnerProduct1
I1001 10:43:06.288394  5182 net.cpp:406] InnerProduct1 <- Pooling1
I1001 10:43:06.288399  5182 net.cpp:380] InnerProduct1 -> InnerProduct1
I1001 10:43:06.288506  5182 net.cpp:122] Setting up InnerProduct1
I1001 10:43:06.288511  5182 net.cpp:129] Top shape: 100 10 (1000)
I1001 10:43:06.288513  5182 net.cpp:137] Memory required for data: 430520400
I1001 10:43:06.288517  5182 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1001 10:43:06.288522  5182 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1001 10:43:06.288525  5182 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1001 10:43:06.288528  5182 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1001 10:43:06.288533  5182 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1001 10:43:06.288561  5182 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1001 10:43:06.288565  5182 net.cpp:129] Top shape: 100 10 (1000)
I1001 10:43:06.288568  5182 net.cpp:129] Top shape: 100 10 (1000)
I1001 10:43:06.288570  5182 net.cpp:137] Memory required for data: 430528400
I1001 10:43:06.288573  5182 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 10:43:06.288576  5182 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1001 10:43:06.288579  5182 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I1001 10:43:06.288583  5182 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I1001 10:43:06.288586  5182 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1001 10:43:06.288591  5182 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 10:43:06.288789  5182 net.cpp:122] Setting up SoftmaxWithLoss1
I1001 10:43:06.288797  5182 net.cpp:129] Top shape: (1)
I1001 10:43:06.288800  5182 net.cpp:132]     with loss weight 1
I1001 10:43:06.288807  5182 net.cpp:137] Memory required for data: 430528404
I1001 10:43:06.288810  5182 layer_factory.hpp:77] Creating layer Accuracy1
I1001 10:43:06.288815  5182 net.cpp:84] Creating Layer Accuracy1
I1001 10:43:06.288817  5182 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I1001 10:43:06.288820  5182 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I1001 10:43:06.288825  5182 net.cpp:380] Accuracy1 -> Accuracy1
I1001 10:43:06.288830  5182 net.cpp:122] Setting up Accuracy1
I1001 10:43:06.288833  5182 net.cpp:129] Top shape: (1)
I1001 10:43:06.288836  5182 net.cpp:137] Memory required for data: 430528408
I1001 10:43:06.288844  5182 net.cpp:200] Accuracy1 does not need backward computation.
I1001 10:43:06.288847  5182 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1001 10:43:06.288851  5182 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1001 10:43:06.288853  5182 net.cpp:198] InnerProduct1 needs backward computation.
I1001 10:43:06.288856  5182 net.cpp:198] Pooling1 needs backward computation.
I1001 10:43:06.288858  5182 net.cpp:198] penlu19 needs backward computation.
I1001 10:43:06.288861  5182 net.cpp:198] Eltwise9 needs backward computation.
I1001 10:43:06.288863  5182 net.cpp:198] Scale21 needs backward computation.
I1001 10:43:06.288866  5182 net.cpp:198] BatchNorm21 needs backward computation.
I1001 10:43:06.288867  5182 net.cpp:198] Convolution21 needs backward computation.
I1001 10:43:06.288871  5182 net.cpp:198] penlu18 needs backward computation.
I1001 10:43:06.288872  5182 net.cpp:198] Scale20 needs backward computation.
I1001 10:43:06.288874  5182 net.cpp:198] BatchNorm20 needs backward computation.
I1001 10:43:06.288877  5182 net.cpp:198] Convolution20 needs backward computation.
I1001 10:43:06.288878  5182 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1001 10:43:06.288882  5182 net.cpp:198] penlu17 needs backward computation.
I1001 10:43:06.288883  5182 net.cpp:198] Eltwise8 needs backward computation.
I1001 10:43:06.288887  5182 net.cpp:198] Scale19 needs backward computation.
I1001 10:43:06.288890  5182 net.cpp:198] BatchNorm19 needs backward computation.
I1001 10:43:06.288892  5182 net.cpp:198] Convolution19 needs backward computation.
I1001 10:43:06.288894  5182 net.cpp:198] penlu16 needs backward computation.
I1001 10:43:06.288897  5182 net.cpp:198] Scale18 needs backward computation.
I1001 10:43:06.288898  5182 net.cpp:198] BatchNorm18 needs backward computation.
I1001 10:43:06.288900  5182 net.cpp:198] Convolution18 needs backward computation.
I1001 10:43:06.288903  5182 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1001 10:43:06.288905  5182 net.cpp:198] penlu15 needs backward computation.
I1001 10:43:06.288908  5182 net.cpp:198] Eltwise7 needs backward computation.
I1001 10:43:06.288910  5182 net.cpp:198] Scale17 needs backward computation.
I1001 10:43:06.288913  5182 net.cpp:198] BatchNorm17 needs backward computation.
I1001 10:43:06.288915  5182 net.cpp:198] Convolution17 needs backward computation.
I1001 10:43:06.288918  5182 net.cpp:198] penlu14 needs backward computation.
I1001 10:43:06.288919  5182 net.cpp:198] Scale16 needs backward computation.
I1001 10:43:06.288923  5182 net.cpp:198] BatchNorm16 needs backward computation.
I1001 10:43:06.288924  5182 net.cpp:198] Convolution16 needs backward computation.
I1001 10:43:06.288926  5182 net.cpp:198] Scale15 needs backward computation.
I1001 10:43:06.288929  5182 net.cpp:198] BatchNorm15 needs backward computation.
I1001 10:43:06.288931  5182 net.cpp:198] Convolution15 needs backward computation.
I1001 10:43:06.288934  5182 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1001 10:43:06.288938  5182 net.cpp:198] penlu13 needs backward computation.
I1001 10:43:06.288939  5182 net.cpp:198] Eltwise6 needs backward computation.
I1001 10:43:06.288942  5182 net.cpp:198] Scale14 needs backward computation.
I1001 10:43:06.288944  5182 net.cpp:198] BatchNorm14 needs backward computation.
I1001 10:43:06.288946  5182 net.cpp:198] Convolution14 needs backward computation.
I1001 10:43:06.288949  5182 net.cpp:198] penlu12 needs backward computation.
I1001 10:43:06.289719  5182 net.cpp:198] Scale13 needs backward computation.
I1001 10:43:06.289728  5182 net.cpp:198] BatchNorm13 needs backward computation.
I1001 10:43:06.289732  5182 net.cpp:198] Convolution13 needs backward computation.
I1001 10:43:06.289736  5182 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1001 10:43:06.289738  5182 net.cpp:198] penlu11 needs backward computation.
I1001 10:43:06.289741  5182 net.cpp:198] Eltwise5 needs backward computation.
I1001 10:43:06.289743  5182 net.cpp:198] Scale12 needs backward computation.
I1001 10:43:06.289752  5182 net.cpp:198] BatchNorm12 needs backward computation.
I1001 10:43:06.289755  5182 net.cpp:198] Convolution12 needs backward computation.
I1001 10:43:06.289757  5182 net.cpp:198] penlu10 needs backward computation.
I1001 10:43:06.289759  5182 net.cpp:198] Scale11 needs backward computation.
I1001 10:43:06.289762  5182 net.cpp:198] BatchNorm11 needs backward computation.
I1001 10:43:06.289764  5182 net.cpp:198] Convolution11 needs backward computation.
I1001 10:43:06.289767  5182 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1001 10:43:06.289769  5182 net.cpp:198] penlu9 needs backward computation.
I1001 10:43:06.289772  5182 net.cpp:198] Eltwise4 needs backward computation.
I1001 10:43:06.289774  5182 net.cpp:198] Scale10 needs backward computation.
I1001 10:43:06.289777  5182 net.cpp:198] BatchNorm10 needs backward computation.
I1001 10:43:06.289779  5182 net.cpp:198] Convolution10 needs backward computation.
I1001 10:43:06.289782  5182 net.cpp:198] penlu8 needs backward computation.
I1001 10:43:06.289784  5182 net.cpp:198] Scale9 needs backward computation.
I1001 10:43:06.289786  5182 net.cpp:198] BatchNorm9 needs backward computation.
I1001 10:43:06.289788  5182 net.cpp:198] Convolution9 needs backward computation.
I1001 10:43:06.289791  5182 net.cpp:198] Scale8 needs backward computation.
I1001 10:43:06.289793  5182 net.cpp:198] BatchNorm8 needs backward computation.
I1001 10:43:06.289796  5182 net.cpp:198] Convolution8 needs backward computation.
I1001 10:43:06.289798  5182 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1001 10:43:06.289801  5182 net.cpp:198] penlu7 needs backward computation.
I1001 10:43:06.289803  5182 net.cpp:198] Eltwise3 needs backward computation.
I1001 10:43:06.289806  5182 net.cpp:198] Scale7 needs backward computation.
I1001 10:43:06.289808  5182 net.cpp:198] BatchNorm7 needs backward computation.
I1001 10:43:06.289810  5182 net.cpp:198] Convolution7 needs backward computation.
I1001 10:43:06.289813  5182 net.cpp:198] penlu6 needs backward computation.
I1001 10:43:06.289815  5182 net.cpp:198] Scale6 needs backward computation.
I1001 10:43:06.289818  5182 net.cpp:198] BatchNorm6 needs backward computation.
I1001 10:43:06.289819  5182 net.cpp:198] Convolution6 needs backward computation.
I1001 10:43:06.289822  5182 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1001 10:43:06.289824  5182 net.cpp:198] penlu5 needs backward computation.
I1001 10:43:06.289826  5182 net.cpp:198] Eltwise2 needs backward computation.
I1001 10:43:06.289829  5182 net.cpp:198] Scale5 needs backward computation.
I1001 10:43:06.289832  5182 net.cpp:198] BatchNorm5 needs backward computation.
I1001 10:43:06.289834  5182 net.cpp:198] Convolution5 needs backward computation.
I1001 10:43:06.289836  5182 net.cpp:198] penlu4 needs backward computation.
I1001 10:43:06.289839  5182 net.cpp:198] Scale4 needs backward computation.
I1001 10:43:06.289841  5182 net.cpp:198] BatchNorm4 needs backward computation.
I1001 10:43:06.289844  5182 net.cpp:198] Convolution4 needs backward computation.
I1001 10:43:06.289845  5182 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1001 10:43:06.289849  5182 net.cpp:198] penlu3 needs backward computation.
I1001 10:43:06.289852  5182 net.cpp:198] Eltwise1 needs backward computation.
I1001 10:43:06.289855  5182 net.cpp:198] Scale3 needs backward computation.
I1001 10:43:06.289857  5182 net.cpp:198] BatchNorm3 needs backward computation.
I1001 10:43:06.289860  5182 net.cpp:198] Convolution3 needs backward computation.
I1001 10:43:06.289862  5182 net.cpp:198] penlu2 needs backward computation.
I1001 10:43:06.289865  5182 net.cpp:198] Scale2 needs backward computation.
I1001 10:43:06.289866  5182 net.cpp:198] BatchNorm2 needs backward computation.
I1001 10:43:06.289868  5182 net.cpp:198] Convolution2 needs backward computation.
I1001 10:43:06.289871  5182 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1001 10:43:06.289875  5182 net.cpp:198] penlu1 needs backward computation.
I1001 10:43:06.289880  5182 net.cpp:198] Scale1 needs backward computation.
I1001 10:43:06.289881  5182 net.cpp:198] BatchNorm1 needs backward computation.
I1001 10:43:06.289885  5182 net.cpp:198] Convolution1 needs backward computation.
I1001 10:43:06.289887  5182 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1001 10:43:06.289891  5182 net.cpp:200] Data1 does not need backward computation.
I1001 10:43:06.289892  5182 net.cpp:242] This network produces output Accuracy1
I1001 10:43:06.289894  5182 net.cpp:242] This network produces output SoftmaxWithLoss1
I1001 10:43:06.289932  5182 net.cpp:255] Network initialization done.
I1001 10:43:06.290261  5182 solver.cpp:56] Solver scaffolding done.
I1001 10:43:06.295641  5182 caffe.cpp:248] Starting Optimization
I1001 10:43:06.295647  5182 solver.cpp:272] Solving resnet_cifar10
I1001 10:43:06.295650  5182 solver.cpp:273] Learning Rate Policy: multistep
I1001 10:43:06.297595  5182 solver.cpp:330] Iteration 0, Testing net (#0)
I1001 10:43:07.523352  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:43:07.573207  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.1
I1001 10:43:07.573243  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I1001 10:43:07.646704  5182 solver.cpp:218] Iteration 0 (0 iter/s, 1.35099s/100 iters), loss = 2.30324
I1001 10:43:07.646733  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.30324 (* 1 = 2.30324 loss)
I1001 10:43:07.646759  5182 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1001 10:43:12.890859  5182 solver.cpp:218] Iteration 100 (19.069 iter/s, 5.2441s/100 iters), loss = 1.65344
I1001 10:43:12.890900  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.65344 (* 1 = 1.65344 loss)
I1001 10:43:12.890907  5182 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1001 10:43:18.128973  5182 solver.cpp:218] Iteration 200 (19.0911 iter/s, 5.23805s/100 iters), loss = 1.59412
I1001 10:43:18.129014  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.59412 (* 1 = 1.59412 loss)
I1001 10:43:18.129019  5182 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1001 10:43:23.358106  5182 solver.cpp:218] Iteration 300 (19.1239 iter/s, 5.22907s/100 iters), loss = 1.28636
I1001 10:43:23.358145  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.28636 (* 1 = 1.28636 loss)
I1001 10:43:23.358151  5182 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1001 10:43:28.600013  5182 solver.cpp:218] Iteration 400 (19.0773 iter/s, 5.24184s/100 iters), loss = 0.972021
I1001 10:43:28.600041  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.972021 (* 1 = 0.972021 loss)
I1001 10:43:28.600047  5182 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1001 10:43:33.578285  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:43:33.787097  5182 solver.cpp:330] Iteration 500, Testing net (#0)
I1001 10:43:34.975039  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:43:35.024456  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.2394
I1001 10:43:35.024489  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 6.08466 (* 1 = 6.08466 loss)
I1001 10:43:35.076943  5182 solver.cpp:218] Iteration 500 (15.4395 iter/s, 6.47688s/100 iters), loss = 1.15383
I1001 10:43:35.076968  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.15383 (* 1 = 1.15383 loss)
I1001 10:43:35.076974  5182 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1001 10:43:40.314430  5182 solver.cpp:218] Iteration 600 (19.0933 iter/s, 5.23744s/100 iters), loss = 1.03298
I1001 10:43:40.314508  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03298 (* 1 = 1.03298 loss)
I1001 10:43:40.314532  5182 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1001 10:43:45.555446  5182 solver.cpp:218] Iteration 700 (19.0806 iter/s, 5.24092s/100 iters), loss = 1.05207
I1001 10:43:45.555480  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.05207 (* 1 = 1.05207 loss)
I1001 10:43:45.555490  5182 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1001 10:43:50.789243  5182 solver.cpp:218] Iteration 800 (19.1068 iter/s, 5.23374s/100 iters), loss = 1.00458
I1001 10:43:50.789278  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.00458 (* 1 = 1.00458 loss)
I1001 10:43:50.789288  5182 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1001 10:43:56.017669  5182 solver.cpp:218] Iteration 900 (19.1264 iter/s, 5.22837s/100 iters), loss = 0.768241
I1001 10:43:56.017704  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.768241 (* 1 = 0.768241 loss)
I1001 10:43:56.017714  5182 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1001 10:44:00.996275  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:44:01.205330  5182 solver.cpp:330] Iteration 1000, Testing net (#0)
I1001 10:44:02.395346  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:44:02.445992  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3875
I1001 10:44:02.446032  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.4923 (* 1 = 2.4923 loss)
I1001 10:44:02.499061  5182 solver.cpp:218] Iteration 1000 (15.4289 iter/s, 6.48133s/100 iters), loss = 0.898792
I1001 10:44:02.499104  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.898792 (* 1 = 0.898792 loss)
I1001 10:44:02.499111  5182 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1001 10:44:07.751754  5182 solver.cpp:218] Iteration 1100 (19.0384 iter/s, 5.25254s/100 iters), loss = 0.763534
I1001 10:44:07.751793  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.763534 (* 1 = 0.763534 loss)
I1001 10:44:07.751801  5182 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1001 10:44:13.003890  5182 solver.cpp:218] Iteration 1200 (19.0401 iter/s, 5.25208s/100 iters), loss = 0.783583
I1001 10:44:13.004022  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.783583 (* 1 = 0.783583 loss)
I1001 10:44:13.004030  5182 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1001 10:44:18.255774  5182 solver.cpp:218] Iteration 1300 (19.0413 iter/s, 5.25173s/100 iters), loss = 0.797536
I1001 10:44:18.255815  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.797536 (* 1 = 0.797536 loss)
I1001 10:44:18.255821  5182 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1001 10:44:23.505730  5182 solver.cpp:218] Iteration 1400 (19.048 iter/s, 5.24989s/100 iters), loss = 0.646574
I1001 10:44:23.505767  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.646574 (* 1 = 0.646574 loss)
I1001 10:44:23.505775  5182 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1001 10:44:28.489977  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:44:28.700495  5182 solver.cpp:330] Iteration 1500, Testing net (#0)
I1001 10:44:29.898166  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:44:29.948359  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.2415
I1001 10:44:29.948395  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.34949 (* 1 = 4.34949 loss)
I1001 10:44:30.000779  5182 solver.cpp:218] Iteration 1500 (15.3965 iter/s, 6.49499s/100 iters), loss = 0.795694
I1001 10:44:30.000810  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.795694 (* 1 = 0.795694 loss)
I1001 10:44:30.000818  5182 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1001 10:44:35.245087  5182 solver.cpp:218] Iteration 1600 (19.0685 iter/s, 5.24426s/100 iters), loss = 0.669754
I1001 10:44:35.245132  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.669754 (* 1 = 0.669754 loss)
I1001 10:44:35.245151  5182 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1001 10:44:40.492600  5182 solver.cpp:218] Iteration 1700 (19.0569 iter/s, 5.24745s/100 iters), loss = 0.718364
I1001 10:44:40.492640  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.718364 (* 1 = 0.718364 loss)
I1001 10:44:40.492646  5182 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1001 10:44:45.743518  5182 solver.cpp:218] Iteration 1800 (19.0445 iter/s, 5.25086s/100 iters), loss = 0.684573
I1001 10:44:45.743846  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.684573 (* 1 = 0.684573 loss)
I1001 10:44:45.743856  5182 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1001 10:44:50.998572  5182 solver.cpp:218] Iteration 1900 (19.0306 iter/s, 5.2547s/100 iters), loss = 0.650743
I1001 10:44:50.998605  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.650743 (* 1 = 0.650743 loss)
I1001 10:44:50.998620  5182 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1001 10:44:55.980067  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:44:56.189545  5182 solver.cpp:330] Iteration 2000, Testing net (#0)
I1001 10:44:57.387971  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:44:57.438285  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4105
I1001 10:44:57.438310  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.16752 (* 1 = 2.16752 loss)
I1001 10:44:57.490528  5182 solver.cpp:218] Iteration 2000 (15.4038 iter/s, 6.4919s/100 iters), loss = 0.704834
I1001 10:44:57.490557  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.704834 (* 1 = 0.704834 loss)
I1001 10:44:57.490564  5182 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1001 10:45:02.744866  5182 solver.cpp:218] Iteration 2100 (19.0321 iter/s, 5.25429s/100 iters), loss = 0.464082
I1001 10:45:02.744900  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.464082 (* 1 = 0.464082 loss)
I1001 10:45:02.744917  5182 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1001 10:45:07.985946  5182 solver.cpp:218] Iteration 2200 (19.0802 iter/s, 5.24102s/100 iters), loss = 0.648582
I1001 10:45:07.985977  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.648582 (* 1 = 0.648582 loss)
I1001 10:45:07.985993  5182 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1001 10:45:13.234202  5182 solver.cpp:218] Iteration 2300 (19.0541 iter/s, 5.2482s/100 iters), loss = 0.542548
I1001 10:45:13.234232  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.542548 (* 1 = 0.542548 loss)
I1001 10:45:13.234249  5182 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1001 10:45:18.479768  5182 solver.cpp:218] Iteration 2400 (19.0639 iter/s, 5.24551s/100 iters), loss = 0.547054
I1001 10:45:18.479921  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.547054 (* 1 = 0.547054 loss)
I1001 10:45:18.479930  5182 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1001 10:45:23.456506  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:45:23.670037  5182 solver.cpp:330] Iteration 2500, Testing net (#0)
I1001 10:45:24.862496  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:45:24.912422  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6582
I1001 10:45:24.912457  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.997747 (* 1 = 0.997747 loss)
I1001 10:45:24.964498  5182 solver.cpp:218] Iteration 2500 (15.4213 iter/s, 6.48456s/100 iters), loss = 0.572364
I1001 10:45:24.964525  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.572364 (* 1 = 0.572364 loss)
I1001 10:45:24.964532  5182 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1001 10:45:30.214283  5182 solver.cpp:218] Iteration 2600 (19.0486 iter/s, 5.24973s/100 iters), loss = 0.460928
I1001 10:45:30.214329  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.460928 (* 1 = 0.460928 loss)
I1001 10:45:30.214337  5182 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1001 10:45:35.459373  5182 solver.cpp:218] Iteration 2700 (19.0657 iter/s, 5.24502s/100 iters), loss = 0.640629
I1001 10:45:35.459415  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.640629 (* 1 = 0.640629 loss)
I1001 10:45:35.459422  5182 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1001 10:45:40.713505  5182 solver.cpp:218] Iteration 2800 (19.033 iter/s, 5.25403s/100 iters), loss = 0.530354
I1001 10:45:40.713536  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.530354 (* 1 = 0.530354 loss)
I1001 10:45:40.713551  5182 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1001 10:45:45.964216  5182 solver.cpp:218] Iteration 2900 (19.0452 iter/s, 5.25066s/100 iters), loss = 0.36635
I1001 10:45:45.964246  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.36635 (* 1 = 0.36635 loss)
I1001 10:45:45.964263  5182 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1001 10:45:50.956454  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:45:51.166380  5182 solver.cpp:330] Iteration 3000, Testing net (#0)
I1001 10:45:52.357228  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:45:52.407407  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6559
I1001 10:45:52.407430  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.979697 (* 1 = 0.979697 loss)
I1001 10:45:52.459558  5182 solver.cpp:218] Iteration 3000 (15.3958 iter/s, 6.49529s/100 iters), loss = 0.534468
I1001 10:45:52.459583  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.534468 (* 1 = 0.534468 loss)
I1001 10:45:52.459590  5182 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1001 10:45:57.707614  5182 solver.cpp:218] Iteration 3100 (19.0548 iter/s, 5.24801s/100 iters), loss = 0.372812
I1001 10:45:57.707645  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372812 (* 1 = 0.372812 loss)
I1001 10:45:57.707661  5182 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1001 10:46:02.955572  5182 solver.cpp:218] Iteration 3200 (19.0552 iter/s, 5.2479s/100 iters), loss = 0.476097
I1001 10:46:02.955605  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.476097 (* 1 = 0.476097 loss)
I1001 10:46:02.955621  5182 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1001 10:46:08.193358  5182 solver.cpp:218] Iteration 3300 (19.0922 iter/s, 5.23773s/100 iters), loss = 0.515282
I1001 10:46:08.193398  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.515282 (* 1 = 0.515282 loss)
I1001 10:46:08.193404  5182 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1001 10:46:13.444639  5182 solver.cpp:218] Iteration 3400 (19.0432 iter/s, 5.25122s/100 iters), loss = 0.563925
I1001 10:46:13.444679  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.563925 (* 1 = 0.563925 loss)
I1001 10:46:13.444685  5182 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1001 10:46:18.433369  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:46:18.643744  5182 solver.cpp:330] Iteration 3500, Testing net (#0)
I1001 10:46:19.832316  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:46:19.882374  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6029
I1001 10:46:19.882407  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.17126 (* 1 = 1.17126 loss)
I1001 10:46:19.934816  5182 solver.cpp:218] Iteration 3500 (15.408 iter/s, 6.49012s/100 iters), loss = 0.361429
I1001 10:46:19.934855  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.361429 (* 1 = 0.361429 loss)
I1001 10:46:19.934864  5182 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1001 10:46:25.187163  5182 solver.cpp:218] Iteration 3600 (19.0394 iter/s, 5.25228s/100 iters), loss = 0.446563
I1001 10:46:25.187309  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.446563 (* 1 = 0.446563 loss)
I1001 10:46:25.187317  5182 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1001 10:46:30.435993  5182 solver.cpp:218] Iteration 3700 (19.0524 iter/s, 5.24867s/100 iters), loss = 0.478191
I1001 10:46:30.436025  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.478191 (* 1 = 0.478191 loss)
I1001 10:46:30.436033  5182 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1001 10:46:35.685933  5182 solver.cpp:218] Iteration 3800 (19.048 iter/s, 5.24988s/100 iters), loss = 0.469411
I1001 10:46:35.685967  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.469411 (* 1 = 0.469411 loss)
I1001 10:46:35.685986  5182 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1001 10:46:40.930270  5182 solver.cpp:218] Iteration 3900 (19.0685 iter/s, 5.24425s/100 iters), loss = 0.488396
I1001 10:46:40.930300  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.488396 (* 1 = 0.488396 loss)
I1001 10:46:40.930307  5182 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1001 10:46:45.918720  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:46:46.127966  5182 solver.cpp:330] Iteration 4000, Testing net (#0)
I1001 10:46:47.317518  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:46:47.367759  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6338
I1001 10:46:47.367794  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.09631 (* 1 = 1.09631 loss)
I1001 10:46:47.420270  5182 solver.cpp:218] Iteration 4000 (15.4084 iter/s, 6.48995s/100 iters), loss = 0.374301
I1001 10:46:47.420297  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.374301 (* 1 = 0.374301 loss)
I1001 10:46:47.420305  5182 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1001 10:46:52.668932  5182 solver.cpp:218] Iteration 4100 (19.0527 iter/s, 5.24861s/100 iters), loss = 0.418553
I1001 10:46:52.668967  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.418553 (* 1 = 0.418553 loss)
I1001 10:46:52.668977  5182 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1001 10:46:57.917140  5182 solver.cpp:218] Iteration 4200 (19.0543 iter/s, 5.24815s/100 iters), loss = 0.494611
I1001 10:46:57.917263  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.494611 (* 1 = 0.494611 loss)
I1001 10:46:57.917302  5182 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1001 10:47:03.165038  5182 solver.cpp:218] Iteration 4300 (19.0557 iter/s, 5.24777s/100 iters), loss = 0.448433
I1001 10:47:03.165071  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.448433 (* 1 = 0.448433 loss)
I1001 10:47:03.165089  5182 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1001 10:47:08.403087  5182 solver.cpp:218] Iteration 4400 (19.0913 iter/s, 5.238s/100 iters), loss = 0.489538
I1001 10:47:08.403120  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.489538 (* 1 = 0.489538 loss)
I1001 10:47:08.403129  5182 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1001 10:47:13.391937  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:47:13.600888  5182 solver.cpp:330] Iteration 4500, Testing net (#0)
I1001 10:47:14.800472  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:47:14.850330  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6745
I1001 10:47:14.850356  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.881627 (* 1 = 0.881627 loss)
I1001 10:47:14.903012  5182 solver.cpp:218] Iteration 4500 (15.3849 iter/s, 6.49987s/100 iters), loss = 0.46171
I1001 10:47:14.903048  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.46171 (* 1 = 0.46171 loss)
I1001 10:47:14.903059  5182 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1001 10:47:20.147740  5182 solver.cpp:218] Iteration 4600 (19.067 iter/s, 5.24467s/100 iters), loss = 0.339129
I1001 10:47:20.147773  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339129 (* 1 = 0.339129 loss)
I1001 10:47:20.147791  5182 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1001 10:47:25.399801  5182 solver.cpp:218] Iteration 4700 (19.0403 iter/s, 5.25201s/100 iters), loss = 0.437902
I1001 10:47:25.399833  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.437902 (* 1 = 0.437902 loss)
I1001 10:47:25.399852  5182 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1001 10:47:30.648192  5182 solver.cpp:218] Iteration 4800 (19.0537 iter/s, 5.24833s/100 iters), loss = 0.508141
I1001 10:47:30.648314  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.508141 (* 1 = 0.508141 loss)
I1001 10:47:30.648339  5182 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1001 10:47:35.900920  5182 solver.cpp:218] Iteration 4900 (19.0382 iter/s, 5.25259s/100 iters), loss = 0.349258
I1001 10:47:35.900954  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.349258 (* 1 = 0.349258 loss)
I1001 10:47:35.900972  5182 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1001 10:47:40.877123  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:47:41.087251  5182 solver.cpp:330] Iteration 5000, Testing net (#0)
I1001 10:47:42.286247  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:47:42.336138  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5362
I1001 10:47:42.336164  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.56894 (* 1 = 1.56894 loss)
I1001 10:47:42.388351  5182 solver.cpp:218] Iteration 5000 (15.4145 iter/s, 6.48738s/100 iters), loss = 0.370644
I1001 10:47:42.388377  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.370644 (* 1 = 0.370644 loss)
I1001 10:47:42.388386  5182 sgd_solver.cpp:105] Iteration 5000, lr = 0.1
I1001 10:47:47.629530  5182 solver.cpp:218] Iteration 5100 (19.0799 iter/s, 5.24113s/100 iters), loss = 0.380008
I1001 10:47:47.629565  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.380008 (* 1 = 0.380008 loss)
I1001 10:47:47.629575  5182 sgd_solver.cpp:105] Iteration 5100, lr = 0.1
I1001 10:47:52.869729  5182 solver.cpp:218] Iteration 5200 (19.0835 iter/s, 5.24014s/100 iters), loss = 0.469488
I1001 10:47:52.869761  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.469488 (* 1 = 0.469488 loss)
I1001 10:47:52.869779  5182 sgd_solver.cpp:105] Iteration 5200, lr = 0.1
I1001 10:47:58.119505  5182 solver.cpp:218] Iteration 5300 (19.0486 iter/s, 5.24972s/100 iters), loss = 0.475129
I1001 10:47:58.119539  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.475129 (* 1 = 0.475129 loss)
I1001 10:47:58.119556  5182 sgd_solver.cpp:105] Iteration 5300, lr = 0.1
I1001 10:48:03.369619  5182 solver.cpp:218] Iteration 5400 (19.0474 iter/s, 5.25006s/100 iters), loss = 0.394794
I1001 10:48:03.369840  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394794 (* 1 = 0.394794 loss)
I1001 10:48:03.369860  5182 sgd_solver.cpp:105] Iteration 5400, lr = 0.1
I1001 10:48:08.347383  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:48:08.560945  5182 solver.cpp:330] Iteration 5500, Testing net (#0)
I1001 10:48:09.755638  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:48:09.805760  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6337
I1001 10:48:09.805785  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.177 (* 1 = 1.177 loss)
I1001 10:48:09.858574  5182 solver.cpp:218] Iteration 5500 (15.4114 iter/s, 6.48872s/100 iters), loss = 0.354123
I1001 10:48:09.858602  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.354123 (* 1 = 0.354123 loss)
I1001 10:48:09.858621  5182 sgd_solver.cpp:105] Iteration 5500, lr = 0.1
I1001 10:48:15.111632  5182 solver.cpp:218] Iteration 5600 (19.0367 iter/s, 5.253s/100 iters), loss = 0.409874
I1001 10:48:15.111665  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.409874 (* 1 = 0.409874 loss)
I1001 10:48:15.111673  5182 sgd_solver.cpp:105] Iteration 5600, lr = 0.1
I1001 10:48:20.355252  5182 solver.cpp:218] Iteration 5700 (19.071 iter/s, 5.24357s/100 iters), loss = 0.435964
I1001 10:48:20.355284  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.435964 (* 1 = 0.435964 loss)
I1001 10:48:20.355293  5182 sgd_solver.cpp:105] Iteration 5700, lr = 0.1
I1001 10:48:25.604694  5182 solver.cpp:218] Iteration 5800 (19.0498 iter/s, 5.24939s/100 iters), loss = 0.386363
I1001 10:48:25.604727  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.386363 (* 1 = 0.386363 loss)
I1001 10:48:25.604734  5182 sgd_solver.cpp:105] Iteration 5800, lr = 0.1
I1001 10:48:30.853241  5182 solver.cpp:218] Iteration 5900 (19.0531 iter/s, 5.2485s/100 iters), loss = 0.422482
I1001 10:48:30.853273  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.422482 (* 1 = 0.422482 loss)
I1001 10:48:30.853281  5182 sgd_solver.cpp:105] Iteration 5900, lr = 0.1
I1001 10:48:35.839606  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:48:36.052551  5182 solver.cpp:330] Iteration 6000, Testing net (#0)
I1001 10:48:37.242851  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:48:37.293071  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7057
I1001 10:48:37.293095  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.906635 (* 1 = 0.906635 loss)
I1001 10:48:37.345391  5182 solver.cpp:218] Iteration 6000 (15.4033 iter/s, 6.4921s/100 iters), loss = 0.320551
I1001 10:48:37.345423  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320551 (* 1 = 0.320551 loss)
I1001 10:48:37.345433  5182 sgd_solver.cpp:105] Iteration 6000, lr = 0.1
I1001 10:48:42.594501  5182 solver.cpp:218] Iteration 6100 (19.051 iter/s, 5.24906s/100 iters), loss = 0.378611
I1001 10:48:42.594542  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.378611 (* 1 = 0.378611 loss)
I1001 10:48:42.594549  5182 sgd_solver.cpp:105] Iteration 6100, lr = 0.1
I1001 10:48:47.847218  5182 solver.cpp:218] Iteration 6200 (19.038 iter/s, 5.25265s/100 iters), loss = 0.417196
I1001 10:48:47.847250  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.417196 (* 1 = 0.417196 loss)
I1001 10:48:47.847256  5182 sgd_solver.cpp:105] Iteration 6200, lr = 0.1
I1001 10:48:53.091828  5182 solver.cpp:218] Iteration 6300 (19.0674 iter/s, 5.24455s/100 iters), loss = 0.397678
I1001 10:48:53.091869  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.397678 (* 1 = 0.397678 loss)
I1001 10:48:53.091876  5182 sgd_solver.cpp:105] Iteration 6300, lr = 0.1
I1001 10:48:58.348943  5182 solver.cpp:218] Iteration 6400 (19.0221 iter/s, 5.25705s/100 iters), loss = 0.40141
I1001 10:48:58.348973  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.40141 (* 1 = 0.40141 loss)
I1001 10:48:58.348989  5182 sgd_solver.cpp:105] Iteration 6400, lr = 0.1
I1001 10:49:03.343659  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:49:03.553511  5182 solver.cpp:330] Iteration 6500, Testing net (#0)
I1001 10:49:04.743336  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:49:04.793581  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7188
I1001 10:49:04.793615  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.872173 (* 1 = 0.872173 loss)
I1001 10:49:04.845827  5182 solver.cpp:218] Iteration 6500 (15.3921 iter/s, 6.49683s/100 iters), loss = 0.315992
I1001 10:49:04.845850  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.315992 (* 1 = 0.315992 loss)
I1001 10:49:04.845857  5182 sgd_solver.cpp:105] Iteration 6500, lr = 0.1
I1001 10:49:10.101238  5182 solver.cpp:218] Iteration 6600 (19.0282 iter/s, 5.25537s/100 iters), loss = 0.3474
I1001 10:49:10.101359  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.3474 (* 1 = 0.3474 loss)
I1001 10:49:10.101368  5182 sgd_solver.cpp:105] Iteration 6600, lr = 0.1
I1001 10:49:15.350208  5182 solver.cpp:218] Iteration 6700 (19.0519 iter/s, 5.24883s/100 iters), loss = 0.278954
I1001 10:49:15.350239  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278954 (* 1 = 0.278954 loss)
I1001 10:49:15.350244  5182 sgd_solver.cpp:105] Iteration 6700, lr = 0.1
I1001 10:49:20.595396  5182 solver.cpp:218] Iteration 6800 (19.0653 iter/s, 5.24513s/100 iters), loss = 0.354454
I1001 10:49:20.595439  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.354454 (* 1 = 0.354454 loss)
I1001 10:49:20.595448  5182 sgd_solver.cpp:105] Iteration 6800, lr = 0.1
I1001 10:49:25.839865  5182 solver.cpp:218] Iteration 6900 (19.0681 iter/s, 5.24437s/100 iters), loss = 0.389145
I1001 10:49:25.839905  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.389145 (* 1 = 0.389145 loss)
I1001 10:49:25.839911  5182 sgd_solver.cpp:105] Iteration 6900, lr = 0.1
I1001 10:49:30.825855  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:49:31.037071  5182 solver.cpp:330] Iteration 7000, Testing net (#0)
I1001 10:49:32.226884  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:49:32.276899  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6144
I1001 10:49:32.276934  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.36072 (* 1 = 1.36072 loss)
I1001 10:49:32.329195  5182 solver.cpp:218] Iteration 7000 (15.4101 iter/s, 6.48927s/100 iters), loss = 0.28246
I1001 10:49:32.329219  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28246 (* 1 = 0.28246 loss)
I1001 10:49:32.329226  5182 sgd_solver.cpp:105] Iteration 7000, lr = 0.1
I1001 10:49:37.582430  5182 solver.cpp:218] Iteration 7100 (19.0361 iter/s, 5.25319s/100 iters), loss = 0.313232
I1001 10:49:37.582463  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313232 (* 1 = 0.313232 loss)
I1001 10:49:37.582469  5182 sgd_solver.cpp:105] Iteration 7100, lr = 0.1
I1001 10:49:42.833410  5182 solver.cpp:218] Iteration 7200 (19.0443 iter/s, 5.25092s/100 iters), loss = 0.370395
I1001 10:49:42.833518  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.370395 (* 1 = 0.370395 loss)
I1001 10:49:42.833534  5182 sgd_solver.cpp:105] Iteration 7200, lr = 0.1
I1001 10:49:48.089020  5182 solver.cpp:218] Iteration 7300 (19.0278 iter/s, 5.25548s/100 iters), loss = 0.424639
I1001 10:49:48.089062  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.424639 (* 1 = 0.424639 loss)
I1001 10:49:48.089069  5182 sgd_solver.cpp:105] Iteration 7300, lr = 0.1
I1001 10:49:53.336860  5182 solver.cpp:218] Iteration 7400 (19.0557 iter/s, 5.24778s/100 iters), loss = 0.264721
I1001 10:49:53.336891  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264721 (* 1 = 0.264721 loss)
I1001 10:49:53.336899  5182 sgd_solver.cpp:105] Iteration 7400, lr = 0.1
I1001 10:49:58.331373  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:49:58.541566  5182 solver.cpp:330] Iteration 7500, Testing net (#0)
I1001 10:49:59.737102  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:49:59.787895  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7062
I1001 10:49:59.787931  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.888482 (* 1 = 0.888482 loss)
I1001 10:49:59.840495  5182 solver.cpp:218] Iteration 7500 (15.3761 iter/s, 6.50358s/100 iters), loss = 0.276553
I1001 10:49:59.840526  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276553 (* 1 = 0.276553 loss)
I1001 10:49:59.840533  5182 sgd_solver.cpp:105] Iteration 7500, lr = 0.1
I1001 10:50:05.084970  5182 solver.cpp:218] Iteration 7600 (19.0679 iter/s, 5.24442s/100 iters), loss = 0.26872
I1001 10:50:05.085000  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26872 (* 1 = 0.26872 loss)
I1001 10:50:05.085006  5182 sgd_solver.cpp:105] Iteration 7600, lr = 0.1
I1001 10:50:10.338572  5182 solver.cpp:218] Iteration 7700 (19.0348 iter/s, 5.25354s/100 iters), loss = 0.401836
I1001 10:50:10.338605  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.401836 (* 1 = 0.401836 loss)
I1001 10:50:10.338614  5182 sgd_solver.cpp:105] Iteration 7700, lr = 0.1
I1001 10:50:15.589939  5182 solver.cpp:218] Iteration 7800 (19.0428 iter/s, 5.25132s/100 iters), loss = 0.34199
I1001 10:50:15.590044  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.34199 (* 1 = 0.34199 loss)
I1001 10:50:15.590060  5182 sgd_solver.cpp:105] Iteration 7800, lr = 0.1
I1001 10:50:20.843734  5182 solver.cpp:218] Iteration 7900 (19.0343 iter/s, 5.25367s/100 iters), loss = 0.351568
I1001 10:50:20.843765  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.351568 (* 1 = 0.351568 loss)
I1001 10:50:20.843772  5182 sgd_solver.cpp:105] Iteration 7900, lr = 0.1
I1001 10:50:25.826114  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:50:26.036487  5182 solver.cpp:330] Iteration 8000, Testing net (#0)
I1001 10:50:27.235946  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:50:27.285980  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5407
I1001 10:50:27.286015  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.8921 (* 1 = 1.8921 loss)
I1001 10:50:27.338604  5182 solver.cpp:218] Iteration 8000 (15.3969 iter/s, 6.49482s/100 iters), loss = 0.312051
I1001 10:50:27.338637  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312051 (* 1 = 0.312051 loss)
I1001 10:50:27.338644  5182 sgd_solver.cpp:105] Iteration 8000, lr = 0.1
I1001 10:50:32.588321  5182 solver.cpp:218] Iteration 8100 (19.0488 iter/s, 5.24967s/100 iters), loss = 0.293237
I1001 10:50:32.588357  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293238 (* 1 = 0.293238 loss)
I1001 10:50:32.588364  5182 sgd_solver.cpp:105] Iteration 8100, lr = 0.1
I1001 10:50:37.834206  5182 solver.cpp:218] Iteration 8200 (19.0628 iter/s, 5.24583s/100 iters), loss = 0.379635
I1001 10:50:37.834247  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.379635 (* 1 = 0.379635 loss)
I1001 10:50:37.834254  5182 sgd_solver.cpp:105] Iteration 8200, lr = 0.1
I1001 10:50:43.088304  5182 solver.cpp:218] Iteration 8300 (19.033 iter/s, 5.25404s/100 iters), loss = 0.36994
I1001 10:50:43.088333  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.36994 (* 1 = 0.36994 loss)
I1001 10:50:43.088340  5182 sgd_solver.cpp:105] Iteration 8300, lr = 0.1
I1001 10:50:48.342123  5182 solver.cpp:218] Iteration 8400 (19.034 iter/s, 5.25376s/100 iters), loss = 0.266734
I1001 10:50:48.342280  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266734 (* 1 = 0.266734 loss)
I1001 10:50:48.342298  5182 sgd_solver.cpp:105] Iteration 8400, lr = 0.1
I1001 10:50:53.321552  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:50:53.532745  5182 solver.cpp:330] Iteration 8500, Testing net (#0)
I1001 10:50:54.729213  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:50:54.779284  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7646
I1001 10:50:54.779319  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.699218 (* 1 = 0.699218 loss)
I1001 10:50:54.831491  5182 solver.cpp:218] Iteration 8500 (15.4102 iter/s, 6.4892s/100 iters), loss = 0.23842
I1001 10:50:54.831522  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23842 (* 1 = 0.23842 loss)
I1001 10:50:54.831529  5182 sgd_solver.cpp:105] Iteration 8500, lr = 0.1
I1001 10:51:00.074337  5182 solver.cpp:218] Iteration 8600 (19.0738 iter/s, 5.24279s/100 iters), loss = 0.297698
I1001 10:51:00.074367  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297698 (* 1 = 0.297698 loss)
I1001 10:51:00.074373  5182 sgd_solver.cpp:105] Iteration 8600, lr = 0.1
I1001 10:51:05.314906  5182 solver.cpp:218] Iteration 8700 (19.0821 iter/s, 5.24052s/100 iters), loss = 0.380033
I1001 10:51:05.314946  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.380033 (* 1 = 0.380033 loss)
I1001 10:51:05.314952  5182 sgd_solver.cpp:105] Iteration 8700, lr = 0.1
I1001 10:51:10.565454  5182 solver.cpp:218] Iteration 8800 (19.0459 iter/s, 5.25048s/100 iters), loss = 0.384248
I1001 10:51:10.565485  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.384248 (* 1 = 0.384248 loss)
I1001 10:51:10.565491  5182 sgd_solver.cpp:105] Iteration 8800, lr = 0.1
I1001 10:51:15.821739  5182 solver.cpp:218] Iteration 8900 (19.025 iter/s, 5.25623s/100 iters), loss = 0.321868
I1001 10:51:15.821781  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.321868 (* 1 = 0.321868 loss)
I1001 10:51:15.821789  5182 sgd_solver.cpp:105] Iteration 8900, lr = 0.1
I1001 10:51:20.810322  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:51:21.020198  5182 solver.cpp:330] Iteration 9000, Testing net (#0)
I1001 10:51:22.211803  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:51:22.262001  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7183
I1001 10:51:22.262037  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.839857 (* 1 = 0.839857 loss)
I1001 10:51:22.314129  5182 solver.cpp:218] Iteration 9000 (15.4028 iter/s, 6.49233s/100 iters), loss = 0.310574
I1001 10:51:22.314167  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310574 (* 1 = 0.310574 loss)
I1001 10:51:22.314174  5182 sgd_solver.cpp:105] Iteration 9000, lr = 0.1
I1001 10:51:27.568704  5182 solver.cpp:218] Iteration 9100 (19.0312 iter/s, 5.25452s/100 iters), loss = 0.345596
I1001 10:51:27.568743  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.345596 (* 1 = 0.345596 loss)
I1001 10:51:27.568749  5182 sgd_solver.cpp:105] Iteration 9100, lr = 0.1
I1001 10:51:32.821740  5182 solver.cpp:218] Iteration 9200 (19.0369 iter/s, 5.25297s/100 iters), loss = 0.292766
I1001 10:51:32.821784  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292766 (* 1 = 0.292766 loss)
I1001 10:51:32.821790  5182 sgd_solver.cpp:105] Iteration 9200, lr = 0.1
I1001 10:51:38.062651  5182 solver.cpp:218] Iteration 9300 (19.0809 iter/s, 5.24085s/100 iters), loss = 0.356772
I1001 10:51:38.062680  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.356772 (* 1 = 0.356772 loss)
I1001 10:51:38.062686  5182 sgd_solver.cpp:105] Iteration 9300, lr = 0.1
I1001 10:51:43.309061  5182 solver.cpp:218] Iteration 9400 (19.0608 iter/s, 5.24636s/100 iters), loss = 0.301375
I1001 10:51:43.309089  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301375 (* 1 = 0.301375 loss)
I1001 10:51:43.309094  5182 sgd_solver.cpp:105] Iteration 9400, lr = 0.1
I1001 10:51:48.296022  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:51:48.505278  5182 solver.cpp:330] Iteration 9500, Testing net (#0)
I1001 10:51:49.694329  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:51:49.744053  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6875
I1001 10:51:49.744088  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.08153 (* 1 = 1.08153 loss)
I1001 10:51:49.796785  5182 solver.cpp:218] Iteration 9500 (15.4138 iter/s, 6.48768s/100 iters), loss = 0.265873
I1001 10:51:49.796813  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265873 (* 1 = 0.265873 loss)
I1001 10:51:49.796819  5182 sgd_solver.cpp:105] Iteration 9500, lr = 0.1
I1001 10:51:55.046977  5182 solver.cpp:218] Iteration 9600 (19.0471 iter/s, 5.25014s/100 iters), loss = 0.263876
I1001 10:51:55.047106  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263876 (* 1 = 0.263876 loss)
I1001 10:51:55.047114  5182 sgd_solver.cpp:105] Iteration 9600, lr = 0.1
I1001 10:52:00.292810  5182 solver.cpp:218] Iteration 9700 (19.0633 iter/s, 5.24569s/100 iters), loss = 0.388327
I1001 10:52:00.292842  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.388327 (* 1 = 0.388327 loss)
I1001 10:52:00.292851  5182 sgd_solver.cpp:105] Iteration 9700, lr = 0.1
I1001 10:52:05.537149  5182 solver.cpp:218] Iteration 9800 (19.0684 iter/s, 5.24428s/100 iters), loss = 0.390679
I1001 10:52:05.537184  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390679 (* 1 = 0.390679 loss)
I1001 10:52:05.537191  5182 sgd_solver.cpp:105] Iteration 9800, lr = 0.1
I1001 10:52:10.788187  5182 solver.cpp:218] Iteration 9900 (19.0441 iter/s, 5.25098s/100 iters), loss = 0.314596
I1001 10:52:10.788226  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.314596 (* 1 = 0.314596 loss)
I1001 10:52:10.788233  5182 sgd_solver.cpp:105] Iteration 9900, lr = 0.1
I1001 10:52:15.776912  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:52:15.986805  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_10000.caffemodel
I1001 10:52:15.995714  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_10000.solverstate
I1001 10:52:15.997153  5182 solver.cpp:330] Iteration 10000, Testing net (#0)
I1001 10:52:17.188045  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:52:17.238507  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7088
I1001 10:52:17.238538  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.98799 (* 1 = 0.98799 loss)
I1001 10:52:17.291025  5182 solver.cpp:218] Iteration 10000 (15.378 iter/s, 6.50278s/100 iters), loss = 0.253318
I1001 10:52:17.291052  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253318 (* 1 = 0.253318 loss)
I1001 10:52:17.291062  5182 sgd_solver.cpp:105] Iteration 10000, lr = 0.1
I1001 10:52:22.544215  5182 solver.cpp:218] Iteration 10100 (19.0362 iter/s, 5.25314s/100 iters), loss = 0.262326
I1001 10:52:22.544247  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262326 (* 1 = 0.262326 loss)
I1001 10:52:22.544266  5182 sgd_solver.cpp:105] Iteration 10100, lr = 0.1
I1001 10:52:27.792488  5182 solver.cpp:218] Iteration 10200 (19.0541 iter/s, 5.24822s/100 iters), loss = 0.235705
I1001 10:52:27.792659  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235705 (* 1 = 0.235705 loss)
I1001 10:52:27.792671  5182 sgd_solver.cpp:105] Iteration 10200, lr = 0.1
I1001 10:52:33.040048  5182 solver.cpp:218] Iteration 10300 (19.0571 iter/s, 5.24738s/100 iters), loss = 0.330207
I1001 10:52:33.040086  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.330207 (* 1 = 0.330207 loss)
I1001 10:52:33.040094  5182 sgd_solver.cpp:105] Iteration 10300, lr = 0.1
I1001 10:52:38.281653  5182 solver.cpp:218] Iteration 10400 (19.0783 iter/s, 5.24155s/100 iters), loss = 0.348583
I1001 10:52:38.281687  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348583 (* 1 = 0.348583 loss)
I1001 10:52:38.281694  5182 sgd_solver.cpp:105] Iteration 10400, lr = 0.1
I1001 10:52:43.267421  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:52:43.478518  5182 solver.cpp:330] Iteration 10500, Testing net (#0)
I1001 10:52:44.673774  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:52:44.725551  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7556
I1001 10:52:44.725579  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.715374 (* 1 = 0.715374 loss)
I1001 10:52:44.779669  5182 solver.cpp:218] Iteration 10500 (15.3894 iter/s, 6.49796s/100 iters), loss = 0.284235
I1001 10:52:44.779706  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284235 (* 1 = 0.284235 loss)
I1001 10:52:44.779716  5182 sgd_solver.cpp:105] Iteration 10500, lr = 0.1
I1001 10:52:50.026108  5182 solver.cpp:218] Iteration 10600 (19.0609 iter/s, 5.24635s/100 iters), loss = 0.363446
I1001 10:52:50.026139  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.363446 (* 1 = 0.363446 loss)
I1001 10:52:50.026159  5182 sgd_solver.cpp:105] Iteration 10600, lr = 0.1
I1001 10:52:55.280272  5182 solver.cpp:218] Iteration 10700 (19.0327 iter/s, 5.25411s/100 iters), loss = 0.392587
I1001 10:52:55.280300  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.392587 (* 1 = 0.392587 loss)
I1001 10:52:55.280305  5182 sgd_solver.cpp:105] Iteration 10700, lr = 0.1
I1001 10:53:00.535722  5182 solver.cpp:218] Iteration 10800 (19.028 iter/s, 5.2554s/100 iters), loss = 0.367813
I1001 10:53:00.535837  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.367813 (* 1 = 0.367813 loss)
I1001 10:53:00.535856  5182 sgd_solver.cpp:105] Iteration 10800, lr = 0.1
I1001 10:53:05.788543  5182 solver.cpp:218] Iteration 10900 (19.0378 iter/s, 5.2527s/100 iters), loss = 0.368747
I1001 10:53:05.788586  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.368747 (* 1 = 0.368747 loss)
I1001 10:53:05.788594  5182 sgd_solver.cpp:105] Iteration 10900, lr = 0.1
I1001 10:53:10.768638  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:53:10.978534  5182 solver.cpp:330] Iteration 11000, Testing net (#0)
I1001 10:53:12.177912  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:53:12.228346  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7636
I1001 10:53:12.228381  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.753326 (* 1 = 0.753326 loss)
I1001 10:53:12.280735  5182 solver.cpp:218] Iteration 11000 (15.4033 iter/s, 6.49213s/100 iters), loss = 0.233882
I1001 10:53:12.280761  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233882 (* 1 = 0.233882 loss)
I1001 10:53:12.280767  5182 sgd_solver.cpp:105] Iteration 11000, lr = 0.1
I1001 10:53:17.519429  5182 solver.cpp:218] Iteration 11100 (19.0889 iter/s, 5.23865s/100 iters), loss = 0.328961
I1001 10:53:17.519462  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.328961 (* 1 = 0.328961 loss)
I1001 10:53:17.519469  5182 sgd_solver.cpp:105] Iteration 11100, lr = 0.1
I1001 10:53:22.765359  5182 solver.cpp:218] Iteration 11200 (19.0627 iter/s, 5.24584s/100 iters), loss = 0.37721
I1001 10:53:22.765389  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.37721 (* 1 = 0.37721 loss)
I1001 10:53:22.765395  5182 sgd_solver.cpp:105] Iteration 11200, lr = 0.1
I1001 10:53:28.011978  5182 solver.cpp:218] Iteration 11300 (19.0601 iter/s, 5.24657s/100 iters), loss = 0.294555
I1001 10:53:28.012008  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.294555 (* 1 = 0.294555 loss)
I1001 10:53:28.012014  5182 sgd_solver.cpp:105] Iteration 11300, lr = 0.1
I1001 10:53:33.260632  5182 solver.cpp:218] Iteration 11400 (19.0527 iter/s, 5.2486s/100 iters), loss = 0.233437
I1001 10:53:33.260766  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233437 (* 1 = 0.233437 loss)
I1001 10:53:33.260773  5182 sgd_solver.cpp:105] Iteration 11400, lr = 0.1
I1001 10:53:38.240391  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:53:38.450258  5182 solver.cpp:330] Iteration 11500, Testing net (#0)
I1001 10:53:39.650285  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:53:39.700415  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7153
I1001 10:53:39.700450  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.929636 (* 1 = 0.929636 loss)
I1001 10:53:39.753154  5182 solver.cpp:218] Iteration 11500 (15.4027 iter/s, 6.49237s/100 iters), loss = 0.218157
I1001 10:53:39.753183  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218157 (* 1 = 0.218157 loss)
I1001 10:53:39.753190  5182 sgd_solver.cpp:105] Iteration 11500, lr = 0.1
I1001 10:53:45.005106  5182 solver.cpp:218] Iteration 11600 (19.0407 iter/s, 5.25191s/100 iters), loss = 0.332046
I1001 10:53:45.005137  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332046 (* 1 = 0.332046 loss)
I1001 10:53:45.005153  5182 sgd_solver.cpp:105] Iteration 11600, lr = 0.1
I1001 10:53:50.246455  5182 solver.cpp:218] Iteration 11700 (19.0793 iter/s, 5.2413s/100 iters), loss = 0.332362
I1001 10:53:50.246487  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332362 (* 1 = 0.332362 loss)
I1001 10:53:50.246503  5182 sgd_solver.cpp:105] Iteration 11700, lr = 0.1
I1001 10:53:55.498337  5182 solver.cpp:218] Iteration 11800 (19.041 iter/s, 5.25182s/100 iters), loss = 0.307619
I1001 10:53:55.498364  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.307619 (* 1 = 0.307619 loss)
I1001 10:53:55.498370  5182 sgd_solver.cpp:105] Iteration 11800, lr = 0.1
I1001 10:54:00.751379  5182 solver.cpp:218] Iteration 11900 (19.0368 iter/s, 5.25299s/100 iters), loss = 0.265799
I1001 10:54:00.751408  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265799 (* 1 = 0.265799 loss)
I1001 10:54:00.751425  5182 sgd_solver.cpp:105] Iteration 11900, lr = 0.1
I1001 10:54:05.735757  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:54:05.946627  5182 solver.cpp:330] Iteration 12000, Testing net (#0)
I1001 10:54:07.137497  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:54:07.187670  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7898
I1001 10:54:07.187695  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.625983 (* 1 = 0.625983 loss)
I1001 10:54:07.239557  5182 solver.cpp:218] Iteration 12000 (15.4128 iter/s, 6.48813s/100 iters), loss = 0.186604
I1001 10:54:07.239580  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186604 (* 1 = 0.186604 loss)
I1001 10:54:07.239588  5182 sgd_solver.cpp:105] Iteration 12000, lr = 0.1
I1001 10:54:12.485739  5182 solver.cpp:218] Iteration 12100 (19.0616 iter/s, 5.24614s/100 iters), loss = 0.205897
I1001 10:54:12.485769  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205897 (* 1 = 0.205897 loss)
I1001 10:54:12.485775  5182 sgd_solver.cpp:105] Iteration 12100, lr = 0.1
I1001 10:54:17.728366  5182 solver.cpp:218] Iteration 12200 (19.0746 iter/s, 5.24257s/100 iters), loss = 0.386218
I1001 10:54:17.728412  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.386218 (* 1 = 0.386218 loss)
I1001 10:54:17.728420  5182 sgd_solver.cpp:105] Iteration 12200, lr = 0.1
I1001 10:54:22.974248  5182 solver.cpp:218] Iteration 12300 (19.0629 iter/s, 5.24578s/100 iters), loss = 0.289045
I1001 10:54:22.974278  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289045 (* 1 = 0.289045 loss)
I1001 10:54:22.974284  5182 sgd_solver.cpp:105] Iteration 12300, lr = 0.1
I1001 10:54:28.227461  5182 solver.cpp:218] Iteration 12400 (19.0362 iter/s, 5.25316s/100 iters), loss = 0.297571
I1001 10:54:28.227493  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297571 (* 1 = 0.297571 loss)
I1001 10:54:28.227499  5182 sgd_solver.cpp:105] Iteration 12400, lr = 0.1
I1001 10:54:33.218366  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:54:33.428310  5182 solver.cpp:330] Iteration 12500, Testing net (#0)
I1001 10:54:34.617110  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:54:34.667122  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7206
I1001 10:54:34.667158  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.865827 (* 1 = 0.865827 loss)
I1001 10:54:34.719563  5182 solver.cpp:218] Iteration 12500 (15.4035 iter/s, 6.49205s/100 iters), loss = 0.320007
I1001 10:54:34.719591  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320007 (* 1 = 0.320007 loss)
I1001 10:54:34.719597  5182 sgd_solver.cpp:105] Iteration 12500, lr = 0.1
I1001 10:54:39.968113  5182 solver.cpp:218] Iteration 12600 (19.0531 iter/s, 5.2485s/100 iters), loss = 0.329085
I1001 10:54:39.968245  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329086 (* 1 = 0.329086 loss)
I1001 10:54:39.968262  5182 sgd_solver.cpp:105] Iteration 12600, lr = 0.1
I1001 10:54:45.218314  5182 solver.cpp:218] Iteration 12700 (19.0474 iter/s, 5.25006s/100 iters), loss = 0.339354
I1001 10:54:45.218345  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339354 (* 1 = 0.339354 loss)
I1001 10:54:45.218351  5182 sgd_solver.cpp:105] Iteration 12700, lr = 0.1
I1001 10:54:50.457671  5182 solver.cpp:218] Iteration 12800 (19.0865 iter/s, 5.23931s/100 iters), loss = 0.320167
I1001 10:54:50.457700  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320167 (* 1 = 0.320167 loss)
I1001 10:54:50.457705  5182 sgd_solver.cpp:105] Iteration 12800, lr = 0.1
I1001 10:54:55.706202  5182 solver.cpp:218] Iteration 12900 (19.0531 iter/s, 5.24848s/100 iters), loss = 0.368706
I1001 10:54:55.706241  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.368706 (* 1 = 0.368706 loss)
I1001 10:54:55.706248  5182 sgd_solver.cpp:105] Iteration 12900, lr = 0.1
I1001 10:55:00.693660  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:55:00.904249  5182 solver.cpp:330] Iteration 13000, Testing net (#0)
I1001 10:55:02.096266  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:55:02.146394  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7825
I1001 10:55:02.146419  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.679728 (* 1 = 0.679728 loss)
I1001 10:55:02.198614  5182 solver.cpp:218] Iteration 13000 (15.4027 iter/s, 6.49235s/100 iters), loss = 0.242787
I1001 10:55:02.198642  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242787 (* 1 = 0.242787 loss)
I1001 10:55:02.198647  5182 sgd_solver.cpp:105] Iteration 13000, lr = 0.1
I1001 10:55:07.449950  5182 solver.cpp:218] Iteration 13100 (19.0429 iter/s, 5.25129s/100 iters), loss = 0.28866
I1001 10:55:07.449981  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.288661 (* 1 = 0.288661 loss)
I1001 10:55:07.449987  5182 sgd_solver.cpp:105] Iteration 13100, lr = 0.1
I1001 10:55:12.701201  5182 solver.cpp:218] Iteration 13200 (19.0433 iter/s, 5.2512s/100 iters), loss = 0.367575
I1001 10:55:12.701342  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.367576 (* 1 = 0.367576 loss)
I1001 10:55:12.701392  5182 sgd_solver.cpp:105] Iteration 13200, lr = 0.1
I1001 10:55:17.956055  5182 solver.cpp:218] Iteration 13300 (19.0306 iter/s, 5.25469s/100 iters), loss = 0.290978
I1001 10:55:17.956089  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.290978 (* 1 = 0.290978 loss)
I1001 10:55:17.956095  5182 sgd_solver.cpp:105] Iteration 13300, lr = 0.1
I1001 10:55:23.199671  5182 solver.cpp:218] Iteration 13400 (19.071 iter/s, 5.24356s/100 iters), loss = 0.226419
I1001 10:55:23.199700  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226419 (* 1 = 0.226419 loss)
I1001 10:55:23.199717  5182 sgd_solver.cpp:105] Iteration 13400, lr = 0.1
I1001 10:55:28.195822  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:55:28.405290  5182 solver.cpp:330] Iteration 13500, Testing net (#0)
I1001 10:55:29.596813  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:55:29.648321  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7918
I1001 10:55:29.648357  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.625933 (* 1 = 0.625933 loss)
I1001 10:55:29.703063  5182 solver.cpp:218] Iteration 13500 (15.3767 iter/s, 6.50334s/100 iters), loss = 0.341357
I1001 10:55:29.703107  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.341357 (* 1 = 0.341357 loss)
I1001 10:55:29.703125  5182 sgd_solver.cpp:105] Iteration 13500, lr = 0.1
I1001 10:55:34.949265  5182 solver.cpp:218] Iteration 13600 (19.0618 iter/s, 5.2461s/100 iters), loss = 0.312753
I1001 10:55:34.949304  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312753 (* 1 = 0.312753 loss)
I1001 10:55:34.949311  5182 sgd_solver.cpp:105] Iteration 13600, lr = 0.1
I1001 10:55:40.197254  5182 solver.cpp:218] Iteration 13700 (19.0551 iter/s, 5.24793s/100 iters), loss = 0.278384
I1001 10:55:40.197293  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278384 (* 1 = 0.278384 loss)
I1001 10:55:40.197299  5182 sgd_solver.cpp:105] Iteration 13700, lr = 0.1
I1001 10:55:45.446858  5182 solver.cpp:218] Iteration 13800 (19.0493 iter/s, 5.24954s/100 iters), loss = 0.320266
I1001 10:55:45.446998  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320266 (* 1 = 0.320266 loss)
I1001 10:55:45.447007  5182 sgd_solver.cpp:105] Iteration 13800, lr = 0.1
I1001 10:55:50.690215  5182 solver.cpp:218] Iteration 13900 (19.0723 iter/s, 5.24319s/100 iters), loss = 0.231265
I1001 10:55:50.690254  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231265 (* 1 = 0.231265 loss)
I1001 10:55:50.690273  5182 sgd_solver.cpp:105] Iteration 13900, lr = 0.1
I1001 10:55:55.671077  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:55:55.880424  5182 solver.cpp:330] Iteration 14000, Testing net (#0)
I1001 10:55:57.078512  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:55:57.128684  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6232
I1001 10:55:57.128718  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.39111 (* 1 = 1.39111 loss)
I1001 10:55:57.181023  5182 solver.cpp:218] Iteration 14000 (15.4066 iter/s, 6.49072s/100 iters), loss = 0.211971
I1001 10:55:57.181053  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.211971 (* 1 = 0.211971 loss)
I1001 10:55:57.181061  5182 sgd_solver.cpp:105] Iteration 14000, lr = 0.1
I1001 10:56:02.426262  5182 solver.cpp:218] Iteration 14100 (19.0651 iter/s, 5.24519s/100 iters), loss = 0.205926
I1001 10:56:02.426290  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205926 (* 1 = 0.205926 loss)
I1001 10:56:02.426306  5182 sgd_solver.cpp:105] Iteration 14100, lr = 0.1
I1001 10:56:07.685117  5182 solver.cpp:218] Iteration 14200 (19.0157 iter/s, 5.25881s/100 iters), loss = 0.246857
I1001 10:56:07.685147  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246857 (* 1 = 0.246857 loss)
I1001 10:56:07.685153  5182 sgd_solver.cpp:105] Iteration 14200, lr = 0.1
I1001 10:56:12.939801  5182 solver.cpp:218] Iteration 14300 (19.0308 iter/s, 5.25463s/100 iters), loss = 0.315423
I1001 10:56:12.939831  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.315423 (* 1 = 0.315423 loss)
I1001 10:56:12.939836  5182 sgd_solver.cpp:105] Iteration 14300, lr = 0.1
I1001 10:56:18.193298  5182 solver.cpp:218] Iteration 14400 (19.0351 iter/s, 5.25345s/100 iters), loss = 0.257491
I1001 10:56:18.193436  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257492 (* 1 = 0.257492 loss)
I1001 10:56:18.193455  5182 sgd_solver.cpp:105] Iteration 14400, lr = 0.1
I1001 10:56:23.171244  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:56:23.380923  5182 solver.cpp:330] Iteration 14500, Testing net (#0)
I1001 10:56:24.578902  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:56:24.629154  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6636
I1001 10:56:24.629190  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.10961 (* 1 = 1.10961 loss)
I1001 10:56:24.681367  5182 solver.cpp:218] Iteration 14500 (15.4133 iter/s, 6.48791s/100 iters), loss = 0.221475
I1001 10:56:24.681398  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221475 (* 1 = 0.221475 loss)
I1001 10:56:24.681406  5182 sgd_solver.cpp:105] Iteration 14500, lr = 0.1
I1001 10:56:29.933439  5182 solver.cpp:218] Iteration 14600 (19.0403 iter/s, 5.25202s/100 iters), loss = 0.25596
I1001 10:56:29.933478  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255961 (* 1 = 0.255961 loss)
I1001 10:56:29.933485  5182 sgd_solver.cpp:105] Iteration 14600, lr = 0.1
I1001 10:56:35.172657  5182 solver.cpp:218] Iteration 14700 (19.087 iter/s, 5.23916s/100 iters), loss = 0.350546
I1001 10:56:35.172684  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.350546 (* 1 = 0.350546 loss)
I1001 10:56:35.172690  5182 sgd_solver.cpp:105] Iteration 14700, lr = 0.1
I1001 10:56:40.419759  5182 solver.cpp:218] Iteration 14800 (19.0583 iter/s, 5.24705s/100 iters), loss = 0.273738
I1001 10:56:40.419798  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273738 (* 1 = 0.273738 loss)
I1001 10:56:40.419805  5182 sgd_solver.cpp:105] Iteration 14800, lr = 0.1
I1001 10:56:45.665587  5182 solver.cpp:218] Iteration 14900 (19.063 iter/s, 5.24577s/100 iters), loss = 0.291922
I1001 10:56:45.665617  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.291922 (* 1 = 0.291922 loss)
I1001 10:56:45.665623  5182 sgd_solver.cpp:105] Iteration 14900, lr = 0.1
I1001 10:56:50.652886  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:56:50.867378  5182 solver.cpp:330] Iteration 15000, Testing net (#0)
I1001 10:56:52.060214  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:56:52.110553  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7604
I1001 10:56:52.110577  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.752057 (* 1 = 0.752057 loss)
I1001 10:56:52.163153  5182 solver.cpp:218] Iteration 15000 (15.3905 iter/s, 6.49751s/100 iters), loss = 0.254356
I1001 10:56:52.163185  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254357 (* 1 = 0.254357 loss)
I1001 10:56:52.163193  5182 sgd_solver.cpp:105] Iteration 15000, lr = 0.1
I1001 10:56:57.413568  5182 solver.cpp:218] Iteration 15100 (19.0463 iter/s, 5.25036s/100 iters), loss = 0.22284
I1001 10:56:57.413600  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22284 (* 1 = 0.22284 loss)
I1001 10:56:57.413609  5182 sgd_solver.cpp:105] Iteration 15100, lr = 0.1
I1001 10:57:02.656779  5182 solver.cpp:218] Iteration 15200 (19.0725 iter/s, 5.24316s/100 iters), loss = 0.283845
I1001 10:57:02.656813  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283845 (* 1 = 0.283845 loss)
I1001 10:57:02.656831  5182 sgd_solver.cpp:105] Iteration 15200, lr = 0.1
I1001 10:57:07.901410  5182 solver.cpp:218] Iteration 15300 (19.0674 iter/s, 5.24455s/100 iters), loss = 0.309436
I1001 10:57:07.901439  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309436 (* 1 = 0.309436 loss)
I1001 10:57:07.901445  5182 sgd_solver.cpp:105] Iteration 15300, lr = 0.1
I1001 10:57:13.154299  5182 solver.cpp:218] Iteration 15400 (19.0373 iter/s, 5.25284s/100 iters), loss = 0.313265
I1001 10:57:13.154328  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313265 (* 1 = 0.313265 loss)
I1001 10:57:13.154333  5182 sgd_solver.cpp:105] Iteration 15400, lr = 0.1
I1001 10:57:18.142366  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:57:18.352231  5182 solver.cpp:330] Iteration 15500, Testing net (#0)
I1001 10:57:19.543516  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:57:19.593698  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7973
I1001 10:57:19.593732  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.597687 (* 1 = 0.597687 loss)
I1001 10:57:19.646194  5182 solver.cpp:218] Iteration 15500 (15.4039 iter/s, 6.49185s/100 iters), loss = 0.219476
I1001 10:57:19.646220  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219476 (* 1 = 0.219476 loss)
I1001 10:57:19.646227  5182 sgd_solver.cpp:105] Iteration 15500, lr = 0.1
I1001 10:57:24.894318  5182 solver.cpp:218] Iteration 15600 (19.0546 iter/s, 5.24808s/100 iters), loss = 0.269845
I1001 10:57:24.894455  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269846 (* 1 = 0.269846 loss)
I1001 10:57:24.894464  5182 sgd_solver.cpp:105] Iteration 15600, lr = 0.1
I1001 10:57:30.144397  5182 solver.cpp:218] Iteration 15700 (19.0479 iter/s, 5.24992s/100 iters), loss = 0.427683
I1001 10:57:30.144425  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.427683 (* 1 = 0.427683 loss)
I1001 10:57:30.144431  5182 sgd_solver.cpp:105] Iteration 15700, lr = 0.1
I1001 10:57:35.385669  5182 solver.cpp:218] Iteration 15800 (19.0795 iter/s, 5.24122s/100 iters), loss = 0.315455
I1001 10:57:35.385710  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.315455 (* 1 = 0.315455 loss)
I1001 10:57:35.385715  5182 sgd_solver.cpp:105] Iteration 15800, lr = 0.1
I1001 10:57:40.639346  5182 solver.cpp:218] Iteration 15900 (19.0345 iter/s, 5.25362s/100 iters), loss = 0.237854
I1001 10:57:40.639375  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237854 (* 1 = 0.237854 loss)
I1001 10:57:40.639380  5182 sgd_solver.cpp:105] Iteration 15900, lr = 0.1
I1001 10:57:45.630148  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:57:45.839491  5182 solver.cpp:330] Iteration 16000, Testing net (#0)
I1001 10:57:47.029325  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:57:47.079579  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7681
I1001 10:57:47.079613  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.698597 (* 1 = 0.698597 loss)
I1001 10:57:47.132081  5182 solver.cpp:218] Iteration 16000 (15.402 iter/s, 6.49268s/100 iters), loss = 0.183681
I1001 10:57:47.132108  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183681 (* 1 = 0.183681 loss)
I1001 10:57:47.132115  5182 sgd_solver.cpp:105] Iteration 16000, lr = 0.1
I1001 10:57:52.395118  5182 solver.cpp:218] Iteration 16100 (19.0006 iter/s, 5.26299s/100 iters), loss = 0.223628
I1001 10:57:52.395149  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223628 (* 1 = 0.223628 loss)
I1001 10:57:52.395156  5182 sgd_solver.cpp:105] Iteration 16100, lr = 0.1
I1001 10:57:57.656714  5182 solver.cpp:218] Iteration 16200 (19.0058 iter/s, 5.26154s/100 iters), loss = 0.305901
I1001 10:57:57.656839  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305901 (* 1 = 0.305901 loss)
I1001 10:57:57.656847  5182 sgd_solver.cpp:105] Iteration 16200, lr = 0.1
I1001 10:58:02.910125  5182 solver.cpp:218] Iteration 16300 (19.0358 iter/s, 5.25327s/100 iters), loss = 0.29602
I1001 10:58:02.910156  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.29602 (* 1 = 0.29602 loss)
I1001 10:58:02.910162  5182 sgd_solver.cpp:105] Iteration 16300, lr = 0.1
I1001 10:58:08.147415  5182 solver.cpp:218] Iteration 16400 (19.094 iter/s, 5.23724s/100 iters), loss = 0.255547
I1001 10:58:08.147444  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255547 (* 1 = 0.255547 loss)
I1001 10:58:08.147450  5182 sgd_solver.cpp:105] Iteration 16400, lr = 0.1
I1001 10:58:13.134104  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:58:13.343667  5182 solver.cpp:330] Iteration 16500, Testing net (#0)
I1001 10:58:14.533149  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:58:14.583772  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.757
I1001 10:58:14.583797  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.870502 (* 1 = 0.870502 loss)
I1001 10:58:14.637583  5182 solver.cpp:218] Iteration 16500 (15.408 iter/s, 6.49012s/100 iters), loss = 0.1921
I1001 10:58:14.637629  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1921 (* 1 = 0.1921 loss)
I1001 10:58:14.637635  5182 sgd_solver.cpp:105] Iteration 16500, lr = 0.1
I1001 10:58:19.884289  5182 solver.cpp:218] Iteration 16600 (19.0598 iter/s, 5.24664s/100 iters), loss = 0.254964
I1001 10:58:19.884318  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254964 (* 1 = 0.254964 loss)
I1001 10:58:19.884323  5182 sgd_solver.cpp:105] Iteration 16600, lr = 0.1
I1001 10:58:25.139551  5182 solver.cpp:218] Iteration 16700 (19.0287 iter/s, 5.25521s/100 iters), loss = 0.319247
I1001 10:58:25.139580  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319247 (* 1 = 0.319247 loss)
I1001 10:58:25.139585  5182 sgd_solver.cpp:105] Iteration 16700, lr = 0.1
I1001 10:58:30.394922  5182 solver.cpp:218] Iteration 16800 (19.0283 iter/s, 5.25532s/100 iters), loss = 0.415905
I1001 10:58:30.395036  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.415906 (* 1 = 0.415906 loss)
I1001 10:58:30.395043  5182 sgd_solver.cpp:105] Iteration 16800, lr = 0.1
I1001 10:58:35.644827  5182 solver.cpp:218] Iteration 16900 (19.0484 iter/s, 5.24978s/100 iters), loss = 0.273109
I1001 10:58:35.644865  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273109 (* 1 = 0.273109 loss)
I1001 10:58:35.644871  5182 sgd_solver.cpp:105] Iteration 16900, lr = 0.1
I1001 10:58:40.628510  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:58:40.837340  5182 solver.cpp:330] Iteration 17000, Testing net (#0)
I1001 10:58:42.036762  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:58:42.086771  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6655
I1001 10:58:42.086805  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.27672 (* 1 = 1.27672 loss)
I1001 10:58:42.139619  5182 solver.cpp:218] Iteration 17000 (15.3971 iter/s, 6.49474s/100 iters), loss = 0.216847
I1001 10:58:42.139649  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216847 (* 1 = 0.216847 loss)
I1001 10:58:42.139657  5182 sgd_solver.cpp:105] Iteration 17000, lr = 0.1
I1001 10:58:47.379058  5182 solver.cpp:218] Iteration 17100 (19.0862 iter/s, 5.23938s/100 iters), loss = 0.232311
I1001 10:58:47.379089  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232312 (* 1 = 0.232312 loss)
I1001 10:58:47.379106  5182 sgd_solver.cpp:105] Iteration 17100, lr = 0.1
I1001 10:58:52.627112  5182 solver.cpp:218] Iteration 17200 (19.0549 iter/s, 5.248s/100 iters), loss = 0.318883
I1001 10:58:52.627141  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.318883 (* 1 = 0.318883 loss)
I1001 10:58:52.627147  5182 sgd_solver.cpp:105] Iteration 17200, lr = 0.1
I1001 10:58:57.874352  5182 solver.cpp:218] Iteration 17300 (19.0578 iter/s, 5.24719s/100 iters), loss = 0.278921
I1001 10:58:57.874382  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278921 (* 1 = 0.278921 loss)
I1001 10:58:57.874388  5182 sgd_solver.cpp:105] Iteration 17300, lr = 0.1
I1001 10:59:03.121886  5182 solver.cpp:218] Iteration 17400 (19.0568 iter/s, 5.24748s/100 iters), loss = 0.19806
I1001 10:59:03.121975  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19806 (* 1 = 0.19806 loss)
I1001 10:59:03.121984  5182 sgd_solver.cpp:105] Iteration 17400, lr = 0.1
I1001 10:59:08.097800  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:59:08.307499  5182 solver.cpp:330] Iteration 17500, Testing net (#0)
I1001 10:59:09.503808  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:59:09.554072  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7474
I1001 10:59:09.554097  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.800755 (* 1 = 0.800755 loss)
I1001 10:59:09.606673  5182 solver.cpp:218] Iteration 17500 (15.421 iter/s, 6.48468s/100 iters), loss = 0.272719
I1001 10:59:09.606699  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272719 (* 1 = 0.272719 loss)
I1001 10:59:09.606705  5182 sgd_solver.cpp:105] Iteration 17500, lr = 0.1
I1001 10:59:14.858165  5182 solver.cpp:218] Iteration 17600 (19.0424 iter/s, 5.25144s/100 iters), loss = 0.224031
I1001 10:59:14.858196  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224031 (* 1 = 0.224031 loss)
I1001 10:59:14.858203  5182 sgd_solver.cpp:105] Iteration 17600, lr = 0.1
I1001 10:59:20.098070  5182 solver.cpp:218] Iteration 17700 (19.0845 iter/s, 5.23985s/100 iters), loss = 0.29108
I1001 10:59:20.098099  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.29108 (* 1 = 0.29108 loss)
I1001 10:59:20.098105  5182 sgd_solver.cpp:105] Iteration 17700, lr = 0.1
I1001 10:59:25.353382  5182 solver.cpp:218] Iteration 17800 (19.0285 iter/s, 5.25526s/100 iters), loss = 0.265653
I1001 10:59:25.353421  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265653 (* 1 = 0.265653 loss)
I1001 10:59:25.353427  5182 sgd_solver.cpp:105] Iteration 17800, lr = 0.1
I1001 10:59:30.608165  5182 solver.cpp:218] Iteration 17900 (19.0305 iter/s, 5.25472s/100 iters), loss = 0.243093
I1001 10:59:30.608193  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243093 (* 1 = 0.243093 loss)
I1001 10:59:30.608199  5182 sgd_solver.cpp:105] Iteration 17900, lr = 0.1
I1001 10:59:35.590597  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:59:35.804857  5182 solver.cpp:330] Iteration 18000, Testing net (#0)
I1001 10:59:36.996721  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 10:59:37.046169  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7957
I1001 10:59:37.046205  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.580499 (* 1 = 0.580499 loss)
I1001 10:59:37.098875  5182 solver.cpp:218] Iteration 18000 (15.4068 iter/s, 6.49066s/100 iters), loss = 0.275585
I1001 10:59:37.098904  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.275585 (* 1 = 0.275585 loss)
I1001 10:59:37.098911  5182 sgd_solver.cpp:105] Iteration 18000, lr = 0.1
I1001 10:59:42.348697  5182 solver.cpp:218] Iteration 18100 (19.0485 iter/s, 5.24977s/100 iters), loss = 0.293466
I1001 10:59:42.348734  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293466 (* 1 = 0.293466 loss)
I1001 10:59:42.348742  5182 sgd_solver.cpp:105] Iteration 18100, lr = 0.1
I1001 10:59:47.585295  5182 solver.cpp:218] Iteration 18200 (19.0966 iter/s, 5.23653s/100 iters), loss = 0.32026
I1001 10:59:47.585325  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.32026 (* 1 = 0.32026 loss)
I1001 10:59:47.585332  5182 sgd_solver.cpp:105] Iteration 18200, lr = 0.1
I1001 10:59:52.831279  5182 solver.cpp:218] Iteration 18300 (19.0624 iter/s, 5.24593s/100 iters), loss = 0.247825
I1001 10:59:52.831307  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247825 (* 1 = 0.247825 loss)
I1001 10:59:52.831313  5182 sgd_solver.cpp:105] Iteration 18300, lr = 0.1
I1001 10:59:58.079721  5182 solver.cpp:218] Iteration 18400 (19.0535 iter/s, 5.24839s/100 iters), loss = 0.385126
I1001 10:59:58.079751  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.385126 (* 1 = 0.385126 loss)
I1001 10:59:58.079756  5182 sgd_solver.cpp:105] Iteration 18400, lr = 0.1
I1001 11:00:03.070518  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:00:03.279922  5182 solver.cpp:330] Iteration 18500, Testing net (#0)
I1001 11:00:04.468757  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:00:04.518882  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7384
I1001 11:00:04.518906  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.828437 (* 1 = 0.828437 loss)
I1001 11:00:04.571486  5182 solver.cpp:218] Iteration 18500 (15.4042 iter/s, 6.49172s/100 iters), loss = 0.346367
I1001 11:00:04.571512  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.346367 (* 1 = 0.346367 loss)
I1001 11:00:04.571518  5182 sgd_solver.cpp:105] Iteration 18500, lr = 0.1
I1001 11:00:09.824998  5182 solver.cpp:218] Iteration 18600 (19.0351 iter/s, 5.25346s/100 iters), loss = 0.239806
I1001 11:00:09.825155  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239806 (* 1 = 0.239806 loss)
I1001 11:00:09.825162  5182 sgd_solver.cpp:105] Iteration 18600, lr = 0.1
I1001 11:00:15.078163  5182 solver.cpp:218] Iteration 18700 (19.0368 iter/s, 5.25299s/100 iters), loss = 0.255591
I1001 11:00:15.078193  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255591 (* 1 = 0.255591 loss)
I1001 11:00:15.078199  5182 sgd_solver.cpp:105] Iteration 18700, lr = 0.1
I1001 11:00:20.319375  5182 solver.cpp:218] Iteration 18800 (19.0798 iter/s, 5.24116s/100 iters), loss = 0.222873
I1001 11:00:20.319403  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222873 (* 1 = 0.222873 loss)
I1001 11:00:20.319408  5182 sgd_solver.cpp:105] Iteration 18800, lr = 0.1
I1001 11:00:25.576946  5182 solver.cpp:218] Iteration 18900 (19.0204 iter/s, 5.25751s/100 iters), loss = 0.189547
I1001 11:00:25.576994  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.189547 (* 1 = 0.189547 loss)
I1001 11:00:25.577000  5182 sgd_solver.cpp:105] Iteration 18900, lr = 0.1
I1001 11:00:30.565923  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:00:30.776037  5182 solver.cpp:330] Iteration 19000, Testing net (#0)
I1001 11:00:31.966763  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:00:32.016634  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7767
I1001 11:00:32.016659  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.691088 (* 1 = 0.691088 loss)
I1001 11:00:32.069190  5182 solver.cpp:218] Iteration 19000 (15.4031 iter/s, 6.49219s/100 iters), loss = 0.175863
I1001 11:00:32.069221  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175863 (* 1 = 0.175863 loss)
I1001 11:00:32.069227  5182 sgd_solver.cpp:105] Iteration 19000, lr = 0.1
I1001 11:00:37.322895  5182 solver.cpp:218] Iteration 19100 (19.0344 iter/s, 5.25366s/100 iters), loss = 0.238132
I1001 11:00:37.322923  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238132 (* 1 = 0.238132 loss)
I1001 11:00:37.322929  5182 sgd_solver.cpp:105] Iteration 19100, lr = 0.1
I1001 11:00:42.572139  5182 solver.cpp:218] Iteration 19200 (19.0505 iter/s, 5.24919s/100 iters), loss = 0.322946
I1001 11:00:42.572311  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.322946 (* 1 = 0.322946 loss)
I1001 11:00:42.572319  5182 sgd_solver.cpp:105] Iteration 19200, lr = 0.1
I1001 11:00:47.824354  5182 solver.cpp:218] Iteration 19300 (19.0403 iter/s, 5.25202s/100 iters), loss = 0.222163
I1001 11:00:47.824386  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222163 (* 1 = 0.222163 loss)
I1001 11:00:47.824404  5182 sgd_solver.cpp:105] Iteration 19300, lr = 0.1
I1001 11:00:53.073374  5182 solver.cpp:218] Iteration 19400 (19.0515 iter/s, 5.24893s/100 iters), loss = 0.261954
I1001 11:00:53.073403  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261954 (* 1 = 0.261954 loss)
I1001 11:00:53.073410  5182 sgd_solver.cpp:105] Iteration 19400, lr = 0.1
I1001 11:00:58.066812  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:00:58.277212  5182 solver.cpp:330] Iteration 19500, Testing net (#0)
I1001 11:00:59.466439  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:00:59.516674  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7419
I1001 11:00:59.516696  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.834215 (* 1 = 0.834215 loss)
I1001 11:00:59.570291  5182 solver.cpp:218] Iteration 19500 (15.392 iter/s, 6.49686s/100 iters), loss = 0.184946
I1001 11:00:59.570338  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184946 (* 1 = 0.184946 loss)
I1001 11:00:59.570356  5182 sgd_solver.cpp:105] Iteration 19500, lr = 0.1
I1001 11:01:04.822767  5182 solver.cpp:218] Iteration 19600 (19.039 iter/s, 5.25238s/100 iters), loss = 0.301995
I1001 11:01:04.822798  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301995 (* 1 = 0.301995 loss)
I1001 11:01:04.822803  5182 sgd_solver.cpp:105] Iteration 19600, lr = 0.1
I1001 11:01:10.074198  5182 solver.cpp:218] Iteration 19700 (19.0426 iter/s, 5.25138s/100 iters), loss = 0.328858
I1001 11:01:10.074226  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.328858 (* 1 = 0.328858 loss)
I1001 11:01:10.074232  5182 sgd_solver.cpp:105] Iteration 19700, lr = 0.1
I1001 11:01:15.327865  5182 solver.cpp:218] Iteration 19800 (19.0345 iter/s, 5.25362s/100 iters), loss = 0.18482
I1001 11:01:15.328016  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18482 (* 1 = 0.18482 loss)
I1001 11:01:15.328037  5182 sgd_solver.cpp:105] Iteration 19800, lr = 0.1
I1001 11:01:20.572280  5182 solver.cpp:218] Iteration 19900 (19.0685 iter/s, 5.24426s/100 iters), loss = 0.196254
I1001 11:01:20.572314  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196254 (* 1 = 0.196254 loss)
I1001 11:01:20.572321  5182 sgd_solver.cpp:105] Iteration 19900, lr = 0.1
I1001 11:01:25.562520  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:01:25.772197  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_20000.caffemodel
I1001 11:01:25.777261  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_20000.solverstate
I1001 11:01:25.778625  5182 solver.cpp:330] Iteration 20000, Testing net (#0)
I1001 11:01:26.973839  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:01:27.024068  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7872
I1001 11:01:27.024104  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.620756 (* 1 = 0.620756 loss)
I1001 11:01:27.077042  5182 solver.cpp:218] Iteration 20000 (15.3736 iter/s, 6.50468s/100 iters), loss = 0.239426
I1001 11:01:27.077074  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239426 (* 1 = 0.239426 loss)
I1001 11:01:27.077080  5182 sgd_solver.cpp:105] Iteration 20000, lr = 0.1
I1001 11:01:32.316988  5182 solver.cpp:218] Iteration 20100 (19.0843 iter/s, 5.2399s/100 iters), loss = 0.275729
I1001 11:01:32.317018  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.275729 (* 1 = 0.275729 loss)
I1001 11:01:32.317034  5182 sgd_solver.cpp:105] Iteration 20100, lr = 0.1
I1001 11:01:37.568279  5182 solver.cpp:218] Iteration 20200 (19.0431 iter/s, 5.25124s/100 iters), loss = 0.260116
I1001 11:01:37.568308  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260116 (* 1 = 0.260116 loss)
I1001 11:01:37.568325  5182 sgd_solver.cpp:105] Iteration 20200, lr = 0.1
I1001 11:01:42.820129  5182 solver.cpp:218] Iteration 20300 (19.0411 iter/s, 5.2518s/100 iters), loss = 0.234088
I1001 11:01:42.820168  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234088 (* 1 = 0.234088 loss)
I1001 11:01:42.820174  5182 sgd_solver.cpp:105] Iteration 20300, lr = 0.1
I1001 11:01:48.073103  5182 solver.cpp:218] Iteration 20400 (19.037 iter/s, 5.25292s/100 iters), loss = 0.305231
I1001 11:01:48.073253  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305231 (* 1 = 0.305231 loss)
I1001 11:01:48.073272  5182 sgd_solver.cpp:105] Iteration 20400, lr = 0.1
I1001 11:01:53.058236  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:01:53.268268  5182 solver.cpp:330] Iteration 20500, Testing net (#0)
I1001 11:01:54.467774  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:01:54.517786  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7547
I1001 11:01:54.517810  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.71799 (* 1 = 0.71799 loss)
I1001 11:01:54.570515  5182 solver.cpp:218] Iteration 20500 (15.3911 iter/s, 6.49724s/100 iters), loss = 0.224984
I1001 11:01:54.570547  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224984 (* 1 = 0.224984 loss)
I1001 11:01:54.570554  5182 sgd_solver.cpp:105] Iteration 20500, lr = 0.1
I1001 11:01:59.818527  5182 solver.cpp:218] Iteration 20600 (19.055 iter/s, 5.24796s/100 iters), loss = 0.234977
I1001 11:01:59.818560  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234977 (* 1 = 0.234977 loss)
I1001 11:01:59.818567  5182 sgd_solver.cpp:105] Iteration 20600, lr = 0.1
I1001 11:02:05.060585  5182 solver.cpp:218] Iteration 20700 (19.0767 iter/s, 5.24201s/100 iters), loss = 0.2832
I1001 11:02:05.060612  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2832 (* 1 = 0.2832 loss)
I1001 11:02:05.060618  5182 sgd_solver.cpp:105] Iteration 20700, lr = 0.1
I1001 11:02:10.311630  5182 solver.cpp:218] Iteration 20800 (19.044 iter/s, 5.251s/100 iters), loss = 0.243133
I1001 11:02:10.311658  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243133 (* 1 = 0.243133 loss)
I1001 11:02:10.311663  5182 sgd_solver.cpp:105] Iteration 20800, lr = 0.1
I1001 11:02:15.561142  5182 solver.cpp:218] Iteration 20900 (19.0496 iter/s, 5.24946s/100 iters), loss = 0.25352
I1001 11:02:15.561192  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25352 (* 1 = 0.25352 loss)
I1001 11:02:15.561206  5182 sgd_solver.cpp:105] Iteration 20900, lr = 0.1
I1001 11:02:20.538349  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:02:20.754264  5182 solver.cpp:330] Iteration 21000, Testing net (#0)
I1001 11:02:21.942726  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:02:21.992597  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7467
I1001 11:02:21.992632  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.840524 (* 1 = 0.840524 loss)
I1001 11:02:22.045687  5182 solver.cpp:218] Iteration 21000 (15.4214 iter/s, 6.48448s/100 iters), loss = 0.197515
I1001 11:02:22.045716  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197515 (* 1 = 0.197515 loss)
I1001 11:02:22.045722  5182 sgd_solver.cpp:105] Iteration 21000, lr = 0.1
I1001 11:02:27.296522  5182 solver.cpp:218] Iteration 21100 (19.0448 iter/s, 5.25079s/100 iters), loss = 0.221362
I1001 11:02:27.296561  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221362 (* 1 = 0.221362 loss)
I1001 11:02:27.296567  5182 sgd_solver.cpp:105] Iteration 21100, lr = 0.1
I1001 11:02:32.539250  5182 solver.cpp:218] Iteration 21200 (19.0742 iter/s, 5.24267s/100 iters), loss = 0.24034
I1001 11:02:32.539278  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24034 (* 1 = 0.24034 loss)
I1001 11:02:32.539284  5182 sgd_solver.cpp:105] Iteration 21200, lr = 0.1
I1001 11:02:37.790868  5182 solver.cpp:218] Iteration 21300 (19.0419 iter/s, 5.25157s/100 iters), loss = 0.273133
I1001 11:02:37.790897  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273133 (* 1 = 0.273133 loss)
I1001 11:02:37.790904  5182 sgd_solver.cpp:105] Iteration 21300, lr = 0.1
I1001 11:02:43.044976  5182 solver.cpp:218] Iteration 21400 (19.0329 iter/s, 5.25406s/100 iters), loss = 0.212691
I1001 11:02:43.045006  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212692 (* 1 = 0.212692 loss)
I1001 11:02:43.045011  5182 sgd_solver.cpp:105] Iteration 21400, lr = 0.1
I1001 11:02:48.036391  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:02:48.246274  5182 solver.cpp:330] Iteration 21500, Testing net (#0)
I1001 11:02:49.435873  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:02:49.485841  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7329
I1001 11:02:49.485877  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.950646 (* 1 = 0.950646 loss)
I1001 11:02:49.538167  5182 solver.cpp:218] Iteration 21500 (15.4009 iter/s, 6.49314s/100 iters), loss = 0.267896
I1001 11:02:49.538192  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267896 (* 1 = 0.267896 loss)
I1001 11:02:49.538198  5182 sgd_solver.cpp:105] Iteration 21500, lr = 0.1
I1001 11:02:54.792914  5182 solver.cpp:218] Iteration 21600 (19.0306 iter/s, 5.2547s/100 iters), loss = 0.254135
I1001 11:02:54.793040  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254135 (* 1 = 0.254135 loss)
I1001 11:02:54.793047  5182 sgd_solver.cpp:105] Iteration 21600, lr = 0.1
I1001 11:03:00.044661  5182 solver.cpp:218] Iteration 21700 (19.0418 iter/s, 5.2516s/100 iters), loss = 0.296077
I1001 11:03:00.044701  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296077 (* 1 = 0.296077 loss)
I1001 11:03:00.044718  5182 sgd_solver.cpp:105] Iteration 21700, lr = 0.1
I1001 11:03:05.284862  5182 solver.cpp:218] Iteration 21800 (19.0835 iter/s, 5.24014s/100 iters), loss = 0.292426
I1001 11:03:05.284900  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292426 (* 1 = 0.292426 loss)
I1001 11:03:05.284906  5182 sgd_solver.cpp:105] Iteration 21800, lr = 0.1
I1001 11:03:10.535169  5182 solver.cpp:218] Iteration 21900 (19.0467 iter/s, 5.25025s/100 iters), loss = 0.245048
I1001 11:03:10.535208  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245048 (* 1 = 0.245048 loss)
I1001 11:03:10.535214  5182 sgd_solver.cpp:105] Iteration 21900, lr = 0.1
I1001 11:03:15.524384  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:03:15.734633  5182 solver.cpp:330] Iteration 22000, Testing net (#0)
I1001 11:03:16.924059  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:03:16.974159  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7943
I1001 11:03:16.974194  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.680614 (* 1 = 0.680614 loss)
I1001 11:03:17.026602  5182 solver.cpp:218] Iteration 22000 (15.4051 iter/s, 6.49138s/100 iters), loss = 0.199229
I1001 11:03:17.026628  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199229 (* 1 = 0.199229 loss)
I1001 11:03:17.026634  5182 sgd_solver.cpp:105] Iteration 22000, lr = 0.1
I1001 11:03:22.282327  5182 solver.cpp:218] Iteration 22100 (19.0271 iter/s, 5.25567s/100 iters), loss = 0.309806
I1001 11:03:22.282356  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309807 (* 1 = 0.309807 loss)
I1001 11:03:22.282362  5182 sgd_solver.cpp:105] Iteration 22100, lr = 0.1
I1001 11:03:27.540204  5182 solver.cpp:218] Iteration 22200 (19.0193 iter/s, 5.25783s/100 iters), loss = 0.216941
I1001 11:03:27.540367  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216941 (* 1 = 0.216941 loss)
I1001 11:03:27.540376  5182 sgd_solver.cpp:105] Iteration 22200, lr = 0.1
I1001 11:03:32.795549  5182 solver.cpp:218] Iteration 22300 (19.0289 iter/s, 5.25517s/100 iters), loss = 0.18047
I1001 11:03:32.795595  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18047 (* 1 = 0.18047 loss)
I1001 11:03:32.795603  5182 sgd_solver.cpp:105] Iteration 22300, lr = 0.1
I1001 11:03:38.038117  5182 solver.cpp:218] Iteration 22400 (19.075 iter/s, 5.24247s/100 iters), loss = 0.199742
I1001 11:03:38.038146  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199743 (* 1 = 0.199743 loss)
I1001 11:03:38.038162  5182 sgd_solver.cpp:105] Iteration 22400, lr = 0.1
I1001 11:03:43.026698  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:03:43.237009  5182 solver.cpp:330] Iteration 22500, Testing net (#0)
I1001 11:03:44.425809  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:03:44.475823  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6361
I1001 11:03:44.475858  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.6072 (* 1 = 1.6072 loss)
I1001 11:03:44.528813  5182 solver.cpp:218] Iteration 22500 (15.4068 iter/s, 6.49065s/100 iters), loss = 0.232645
I1001 11:03:44.528836  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232645 (* 1 = 0.232645 loss)
I1001 11:03:44.528843  5182 sgd_solver.cpp:105] Iteration 22500, lr = 0.1
I1001 11:03:49.778209  5182 solver.cpp:218] Iteration 22600 (19.05 iter/s, 5.24935s/100 iters), loss = 0.312422
I1001 11:03:49.778249  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312422 (* 1 = 0.312422 loss)
I1001 11:03:49.778254  5182 sgd_solver.cpp:105] Iteration 22600, lr = 0.1
I1001 11:03:55.027055  5182 solver.cpp:218] Iteration 22700 (19.052 iter/s, 5.24879s/100 iters), loss = 0.348885
I1001 11:03:55.027083  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348886 (* 1 = 0.348886 loss)
I1001 11:03:55.027099  5182 sgd_solver.cpp:105] Iteration 22700, lr = 0.1
I1001 11:04:00.283634  5182 solver.cpp:218] Iteration 22800 (19.024 iter/s, 5.25653s/100 iters), loss = 0.180355
I1001 11:04:00.283735  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180355 (* 1 = 0.180355 loss)
I1001 11:04:00.283751  5182 sgd_solver.cpp:105] Iteration 22800, lr = 0.1
I1001 11:04:05.532958  5182 solver.cpp:218] Iteration 22900 (19.0505 iter/s, 5.2492s/100 iters), loss = 0.198319
I1001 11:04:05.532986  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198319 (* 1 = 0.198319 loss)
I1001 11:04:05.532992  5182 sgd_solver.cpp:105] Iteration 22900, lr = 0.1
I1001 11:04:10.525416  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:04:10.735170  5182 solver.cpp:330] Iteration 23000, Testing net (#0)
I1001 11:04:11.933584  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:04:11.983616  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7787
I1001 11:04:11.983650  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.666628 (* 1 = 0.666628 loss)
I1001 11:04:12.035955  5182 solver.cpp:218] Iteration 23000 (15.3776 iter/s, 6.50295s/100 iters), loss = 0.21556
I1001 11:04:12.035984  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21556 (* 1 = 0.21556 loss)
I1001 11:04:12.035991  5182 sgd_solver.cpp:105] Iteration 23000, lr = 0.1
I1001 11:04:17.280076  5182 solver.cpp:218] Iteration 23100 (19.0692 iter/s, 5.24407s/100 iters), loss = 0.238998
I1001 11:04:17.280105  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238998 (* 1 = 0.238998 loss)
I1001 11:04:17.280112  5182 sgd_solver.cpp:105] Iteration 23100, lr = 0.1
I1001 11:04:22.533862  5182 solver.cpp:218] Iteration 23200 (19.0341 iter/s, 5.25374s/100 iters), loss = 0.323476
I1001 11:04:22.533892  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.323476 (* 1 = 0.323476 loss)
I1001 11:04:22.533907  5182 sgd_solver.cpp:105] Iteration 23200, lr = 0.1
I1001 11:04:27.783622  5182 solver.cpp:218] Iteration 23300 (19.0487 iter/s, 5.24971s/100 iters), loss = 0.214252
I1001 11:04:27.783661  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214252 (* 1 = 0.214252 loss)
I1001 11:04:27.783668  5182 sgd_solver.cpp:105] Iteration 23300, lr = 0.1
I1001 11:04:33.035550  5182 solver.cpp:218] Iteration 23400 (19.0408 iter/s, 5.25187s/100 iters), loss = 0.179252
I1001 11:04:33.035709  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179252 (* 1 = 0.179252 loss)
I1001 11:04:33.035718  5182 sgd_solver.cpp:105] Iteration 23400, lr = 0.1
I1001 11:04:38.015398  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:04:38.225925  5182 solver.cpp:330] Iteration 23500, Testing net (#0)
I1001 11:04:39.419900  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:04:39.469828  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8162
I1001 11:04:39.469852  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.557308 (* 1 = 0.557308 loss)
I1001 11:04:39.522047  5182 solver.cpp:218] Iteration 23500 (15.417 iter/s, 6.48633s/100 iters), loss = 0.225281
I1001 11:04:39.522074  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.225281 (* 1 = 0.225281 loss)
I1001 11:04:39.522081  5182 sgd_solver.cpp:105] Iteration 23500, lr = 0.1
I1001 11:04:44.772249  5182 solver.cpp:218] Iteration 23600 (19.0471 iter/s, 5.25014s/100 iters), loss = 0.246181
I1001 11:04:44.772305  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246181 (* 1 = 0.246181 loss)
I1001 11:04:44.772311  5182 sgd_solver.cpp:105] Iteration 23600, lr = 0.1
I1001 11:04:50.016703  5182 solver.cpp:218] Iteration 23700 (19.068 iter/s, 5.24438s/100 iters), loss = 0.219878
I1001 11:04:50.016731  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219878 (* 1 = 0.219878 loss)
I1001 11:04:50.016737  5182 sgd_solver.cpp:105] Iteration 23700, lr = 0.1
I1001 11:04:55.273629  5182 solver.cpp:218] Iteration 23800 (19.0227 iter/s, 5.25688s/100 iters), loss = 0.249753
I1001 11:04:55.273669  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249753 (* 1 = 0.249753 loss)
I1001 11:04:55.273674  5182 sgd_solver.cpp:105] Iteration 23800, lr = 0.1
I1001 11:05:00.533748  5182 solver.cpp:218] Iteration 23900 (19.0112 iter/s, 5.26006s/100 iters), loss = 0.245432
I1001 11:05:00.533777  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245432 (* 1 = 0.245432 loss)
I1001 11:05:00.533792  5182 sgd_solver.cpp:105] Iteration 23900, lr = 0.1
I1001 11:05:05.514246  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:05:05.730234  5182 solver.cpp:330] Iteration 24000, Testing net (#0)
I1001 11:05:06.923061  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:05:06.973448  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8099
I1001 11:05:06.973471  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.610248 (* 1 = 0.610248 loss)
I1001 11:05:07.026281  5182 solver.cpp:218] Iteration 24000 (15.4024 iter/s, 6.49248s/100 iters), loss = 0.238573
I1001 11:05:07.026312  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238573 (* 1 = 0.238573 loss)
I1001 11:05:07.026319  5182 sgd_solver.cpp:105] Iteration 24000, lr = 0.1
I1001 11:05:12.280594  5182 solver.cpp:218] Iteration 24100 (19.0322 iter/s, 5.25426s/100 iters), loss = 0.235026
I1001 11:05:12.280623  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235026 (* 1 = 0.235026 loss)
I1001 11:05:12.280629  5182 sgd_solver.cpp:105] Iteration 24100, lr = 0.1
I1001 11:05:17.525863  5182 solver.cpp:218] Iteration 24200 (19.065 iter/s, 5.24522s/100 iters), loss = 0.297399
I1001 11:05:17.525903  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297399 (* 1 = 0.297399 loss)
I1001 11:05:17.525908  5182 sgd_solver.cpp:105] Iteration 24200, lr = 0.1
I1001 11:05:22.774555  5182 solver.cpp:218] Iteration 24300 (19.0526 iter/s, 5.24863s/100 iters), loss = 0.284536
I1001 11:05:22.774585  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284536 (* 1 = 0.284536 loss)
I1001 11:05:22.774590  5182 sgd_solver.cpp:105] Iteration 24300, lr = 0.1
I1001 11:05:28.025090  5182 solver.cpp:218] Iteration 24400 (19.0459 iter/s, 5.25048s/100 iters), loss = 0.167608
I1001 11:05:28.025120  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167609 (* 1 = 0.167609 loss)
I1001 11:05:28.025136  5182 sgd_solver.cpp:105] Iteration 24400, lr = 0.1
I1001 11:05:33.011883  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:05:33.222009  5182 solver.cpp:330] Iteration 24500, Testing net (#0)
I1001 11:05:34.411975  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:05:34.462198  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8053
I1001 11:05:34.462222  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.571082 (* 1 = 0.571082 loss)
I1001 11:05:34.514788  5182 solver.cpp:218] Iteration 24500 (15.4092 iter/s, 6.48965s/100 iters), loss = 0.235296
I1001 11:05:34.514813  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235296 (* 1 = 0.235296 loss)
I1001 11:05:34.514820  5182 sgd_solver.cpp:105] Iteration 24500, lr = 0.1
I1001 11:05:39.767938  5182 solver.cpp:218] Iteration 24600 (19.0364 iter/s, 5.2531s/100 iters), loss = 0.301006
I1001 11:05:39.768069  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301006 (* 1 = 0.301006 loss)
I1001 11:05:39.768086  5182 sgd_solver.cpp:105] Iteration 24600, lr = 0.1
I1001 11:05:45.025218  5182 solver.cpp:218] Iteration 24700 (19.0218 iter/s, 5.25712s/100 iters), loss = 0.272425
I1001 11:05:45.025248  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272425 (* 1 = 0.272425 loss)
I1001 11:05:45.025254  5182 sgd_solver.cpp:105] Iteration 24700, lr = 0.1
I1001 11:05:50.270628  5182 solver.cpp:218] Iteration 24800 (19.0645 iter/s, 5.24536s/100 iters), loss = 0.213066
I1001 11:05:50.270668  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213066 (* 1 = 0.213066 loss)
I1001 11:05:50.270674  5182 sgd_solver.cpp:105] Iteration 24800, lr = 0.1
I1001 11:05:55.531539  5182 solver.cpp:218] Iteration 24900 (19.0083 iter/s, 5.26085s/100 iters), loss = 0.281259
I1001 11:05:55.531579  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28126 (* 1 = 0.28126 loss)
I1001 11:05:55.531585  5182 sgd_solver.cpp:105] Iteration 24900, lr = 0.1
I1001 11:06:00.525503  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:06:00.735245  5182 solver.cpp:330] Iteration 25000, Testing net (#0)
I1001 11:06:01.925029  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:06:01.975284  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8291
I1001 11:06:01.975309  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.516439 (* 1 = 0.516439 loss)
I1001 11:06:02.027976  5182 solver.cpp:218] Iteration 25000 (15.3932 iter/s, 6.49638s/100 iters), loss = 0.206547
I1001 11:06:02.028002  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206547 (* 1 = 0.206547 loss)
I1001 11:06:02.028008  5182 sgd_solver.cpp:105] Iteration 25000, lr = 0.1
I1001 11:06:07.283165  5182 solver.cpp:218] Iteration 25100 (19.029 iter/s, 5.25514s/100 iters), loss = 0.277667
I1001 11:06:07.283195  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277668 (* 1 = 0.277668 loss)
I1001 11:06:07.283201  5182 sgd_solver.cpp:105] Iteration 25100, lr = 0.1
I1001 11:06:12.536502  5182 solver.cpp:218] Iteration 25200 (19.0357 iter/s, 5.25328s/100 iters), loss = 0.30666
I1001 11:06:12.536614  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.30666 (* 1 = 0.30666 loss)
I1001 11:06:12.536633  5182 sgd_solver.cpp:105] Iteration 25200, lr = 0.1
I1001 11:06:17.782351  5182 solver.cpp:218] Iteration 25300 (19.0632 iter/s, 5.24572s/100 iters), loss = 0.176962
I1001 11:06:17.782388  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176962 (* 1 = 0.176962 loss)
I1001 11:06:17.782395  5182 sgd_solver.cpp:105] Iteration 25300, lr = 0.1
I1001 11:06:23.034087  5182 solver.cpp:218] Iteration 25400 (19.0417 iter/s, 5.25164s/100 iters), loss = 0.164284
I1001 11:06:23.034127  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164284 (* 1 = 0.164284 loss)
I1001 11:06:23.034134  5182 sgd_solver.cpp:105] Iteration 25400, lr = 0.1
I1001 11:06:28.031519  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:06:28.240680  5182 solver.cpp:330] Iteration 25500, Testing net (#0)
I1001 11:06:29.431530  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:06:29.481674  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7956
I1001 11:06:29.481709  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.607212 (* 1 = 0.607212 loss)
I1001 11:06:29.534355  5182 solver.cpp:218] Iteration 25500 (15.3841 iter/s, 6.50021s/100 iters), loss = 0.185129
I1001 11:06:29.534381  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185129 (* 1 = 0.185129 loss)
I1001 11:06:29.534389  5182 sgd_solver.cpp:105] Iteration 25500, lr = 0.1
I1001 11:06:34.782476  5182 solver.cpp:218] Iteration 25600 (19.0546 iter/s, 5.24807s/100 iters), loss = 0.228761
I1001 11:06:34.782506  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.228762 (* 1 = 0.228762 loss)
I1001 11:06:34.782523  5182 sgd_solver.cpp:105] Iteration 25600, lr = 0.1
I1001 11:06:40.033697  5182 solver.cpp:218] Iteration 25700 (19.0434 iter/s, 5.25117s/100 iters), loss = 0.277087
I1001 11:06:40.033726  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277088 (* 1 = 0.277088 loss)
I1001 11:06:40.033742  5182 sgd_solver.cpp:105] Iteration 25700, lr = 0.1
I1001 11:06:45.286352  5182 solver.cpp:218] Iteration 25800 (19.0382 iter/s, 5.2526s/100 iters), loss = 0.159274
I1001 11:06:45.286485  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159274 (* 1 = 0.159274 loss)
I1001 11:06:45.286494  5182 sgd_solver.cpp:105] Iteration 25800, lr = 0.1
I1001 11:06:50.525486  5182 solver.cpp:218] Iteration 25900 (19.0876 iter/s, 5.23899s/100 iters), loss = 0.270487
I1001 11:06:50.525514  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270487 (* 1 = 0.270487 loss)
I1001 11:06:50.525521  5182 sgd_solver.cpp:105] Iteration 25900, lr = 0.1
I1001 11:06:55.510653  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:06:55.719985  5182 solver.cpp:330] Iteration 26000, Testing net (#0)
I1001 11:06:56.915683  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:06:56.965517  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7597
I1001 11:06:56.965543  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.783117 (* 1 = 0.783117 loss)
I1001 11:06:57.018389  5182 solver.cpp:218] Iteration 26000 (15.4015 iter/s, 6.49285s/100 iters), loss = 0.16195
I1001 11:06:57.018419  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.161951 (* 1 = 0.161951 loss)
I1001 11:06:57.018425  5182 sgd_solver.cpp:105] Iteration 26000, lr = 0.1
I1001 11:07:02.262943  5182 solver.cpp:218] Iteration 26100 (19.0676 iter/s, 5.2445s/100 iters), loss = 0.337271
I1001 11:07:02.262982  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.337271 (* 1 = 0.337271 loss)
I1001 11:07:02.262989  5182 sgd_solver.cpp:105] Iteration 26100, lr = 0.1
I1001 11:07:07.517767  5182 solver.cpp:218] Iteration 26200 (19.0303 iter/s, 5.25476s/100 iters), loss = 0.383476
I1001 11:07:07.517796  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.383476 (* 1 = 0.383476 loss)
I1001 11:07:07.517802  5182 sgd_solver.cpp:105] Iteration 26200, lr = 0.1
I1001 11:07:12.771394  5182 solver.cpp:218] Iteration 26300 (19.0347 iter/s, 5.25358s/100 iters), loss = 0.228253
I1001 11:07:12.771425  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.228253 (* 1 = 0.228253 loss)
I1001 11:07:12.771431  5182 sgd_solver.cpp:105] Iteration 26300, lr = 0.1
I1001 11:07:18.026924  5182 solver.cpp:218] Iteration 26400 (19.0278 iter/s, 5.25548s/100 iters), loss = 0.24296
I1001 11:07:18.027062  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24296 (* 1 = 0.24296 loss)
I1001 11:07:18.027071  5182 sgd_solver.cpp:105] Iteration 26400, lr = 0.1
I1001 11:07:23.007854  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:07:23.218361  5182 solver.cpp:330] Iteration 26500, Testing net (#0)
I1001 11:07:24.417451  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:07:24.467861  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7948
I1001 11:07:24.467896  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.612812 (* 1 = 0.612812 loss)
I1001 11:07:24.520483  5182 solver.cpp:218] Iteration 26500 (15.4002 iter/s, 6.49341s/100 iters), loss = 0.232652
I1001 11:07:24.520515  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232652 (* 1 = 0.232652 loss)
I1001 11:07:24.520522  5182 sgd_solver.cpp:105] Iteration 26500, lr = 0.1
I1001 11:07:29.771989  5182 solver.cpp:218] Iteration 26600 (19.0424 iter/s, 5.25145s/100 iters), loss = 0.163998
I1001 11:07:29.772038  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163998 (* 1 = 0.163998 loss)
I1001 11:07:29.772058  5182 sgd_solver.cpp:105] Iteration 26600, lr = 0.1
I1001 11:07:35.016608  5182 solver.cpp:218] Iteration 26700 (19.0675 iter/s, 5.24452s/100 iters), loss = 0.206781
I1001 11:07:35.016649  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206781 (* 1 = 0.206781 loss)
I1001 11:07:35.016654  5182 sgd_solver.cpp:105] Iteration 26700, lr = 0.1
I1001 11:07:40.262094  5182 solver.cpp:218] Iteration 26800 (19.0642 iter/s, 5.24542s/100 iters), loss = 0.289008
I1001 11:07:40.262123  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289008 (* 1 = 0.289008 loss)
I1001 11:07:40.262140  5182 sgd_solver.cpp:105] Iteration 26800, lr = 0.1
I1001 11:07:45.515365  5182 solver.cpp:218] Iteration 26900 (19.0359 iter/s, 5.25322s/100 iters), loss = 0.209982
I1001 11:07:45.515396  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209982 (* 1 = 0.209982 loss)
I1001 11:07:45.515413  5182 sgd_solver.cpp:105] Iteration 26900, lr = 0.1
I1001 11:07:50.492489  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:07:50.705013  5182 solver.cpp:330] Iteration 27000, Testing net (#0)
I1001 11:07:51.898203  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:07:51.947933  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8075
I1001 11:07:51.947959  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.580667 (* 1 = 0.580667 loss)
I1001 11:07:52.000777  5182 solver.cpp:218] Iteration 27000 (15.4193 iter/s, 6.48536s/100 iters), loss = 0.141757
I1001 11:07:52.000804  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141757 (* 1 = 0.141757 loss)
I1001 11:07:52.000811  5182 sgd_solver.cpp:105] Iteration 27000, lr = 0.1
I1001 11:07:57.251698  5182 solver.cpp:218] Iteration 27100 (19.0445 iter/s, 5.25087s/100 iters), loss = 0.18572
I1001 11:07:57.251726  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18572 (* 1 = 0.18572 loss)
I1001 11:07:57.251734  5182 sgd_solver.cpp:105] Iteration 27100, lr = 0.1
I1001 11:08:02.495442  5182 solver.cpp:218] Iteration 27200 (19.0705 iter/s, 5.24369s/100 iters), loss = 0.375294
I1001 11:08:02.495471  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.375294 (* 1 = 0.375294 loss)
I1001 11:08:02.495477  5182 sgd_solver.cpp:105] Iteration 27200, lr = 0.1
I1001 11:08:07.751549  5182 solver.cpp:218] Iteration 27300 (19.0257 iter/s, 5.25605s/100 iters), loss = 0.292728
I1001 11:08:07.751579  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292728 (* 1 = 0.292728 loss)
I1001 11:08:07.751595  5182 sgd_solver.cpp:105] Iteration 27300, lr = 0.1
I1001 11:08:13.011153  5182 solver.cpp:218] Iteration 27400 (19.013 iter/s, 5.25955s/100 iters), loss = 0.213058
I1001 11:08:13.011194  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213058 (* 1 = 0.213058 loss)
I1001 11:08:13.011200  5182 sgd_solver.cpp:105] Iteration 27400, lr = 0.1
I1001 11:08:18.005801  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:08:18.216218  5182 solver.cpp:330] Iteration 27500, Testing net (#0)
I1001 11:08:19.405498  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:08:19.455957  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7886
I1001 11:08:19.455991  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.686768 (* 1 = 0.686768 loss)
I1001 11:08:19.508548  5182 solver.cpp:218] Iteration 27500 (15.3909 iter/s, 6.49733s/100 iters), loss = 0.181646
I1001 11:08:19.508582  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181646 (* 1 = 0.181646 loss)
I1001 11:08:19.508589  5182 sgd_solver.cpp:105] Iteration 27500, lr = 0.1
I1001 11:08:24.767179  5182 solver.cpp:218] Iteration 27600 (19.0166 iter/s, 5.25857s/100 iters), loss = 0.240536
I1001 11:08:24.767334  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240536 (* 1 = 0.240536 loss)
I1001 11:08:24.767343  5182 sgd_solver.cpp:105] Iteration 27600, lr = 0.1
I1001 11:08:30.018681  5182 solver.cpp:218] Iteration 27700 (19.0428 iter/s, 5.25132s/100 iters), loss = 0.286975
I1001 11:08:30.018710  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286975 (* 1 = 0.286975 loss)
I1001 11:08:30.018718  5182 sgd_solver.cpp:105] Iteration 27700, lr = 0.1
I1001 11:08:35.263144  5182 solver.cpp:218] Iteration 27800 (19.0679 iter/s, 5.24441s/100 iters), loss = 0.185214
I1001 11:08:35.263173  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185214 (* 1 = 0.185214 loss)
I1001 11:08:35.263178  5182 sgd_solver.cpp:105] Iteration 27800, lr = 0.1
I1001 11:08:40.517503  5182 solver.cpp:218] Iteration 27900 (19.032 iter/s, 5.25431s/100 iters), loss = 0.174961
I1001 11:08:40.517532  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.174961 (* 1 = 0.174961 loss)
I1001 11:08:40.517539  5182 sgd_solver.cpp:105] Iteration 27900, lr = 0.1
I1001 11:08:45.509420  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:08:45.719286  5182 solver.cpp:330] Iteration 28000, Testing net (#0)
I1001 11:08:46.905398  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:08:46.955575  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7435
I1001 11:08:46.955601  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.916436 (* 1 = 0.916436 loss)
I1001 11:08:47.008074  5182 solver.cpp:218] Iteration 28000 (15.4071 iter/s, 6.49052s/100 iters), loss = 0.203823
I1001 11:08:47.008101  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203823 (* 1 = 0.203823 loss)
I1001 11:08:47.008110  5182 sgd_solver.cpp:105] Iteration 28000, lr = 0.1
I1001 11:08:52.264577  5182 solver.cpp:218] Iteration 28100 (19.0242 iter/s, 5.25645s/100 iters), loss = 0.251918
I1001 11:08:52.264616  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.251918 (* 1 = 0.251918 loss)
I1001 11:08:52.264621  5182 sgd_solver.cpp:105] Iteration 28100, lr = 0.1
I1001 11:08:57.522402  5182 solver.cpp:218] Iteration 28200 (19.0195 iter/s, 5.25776s/100 iters), loss = 0.322531
I1001 11:08:57.522550  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.322531 (* 1 = 0.322531 loss)
I1001 11:08:57.522569  5182 sgd_solver.cpp:105] Iteration 28200, lr = 0.1
I1001 11:09:02.777851  5182 solver.cpp:218] Iteration 28300 (19.0285 iter/s, 5.25528s/100 iters), loss = 0.22106
I1001 11:09:02.777887  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22106 (* 1 = 0.22106 loss)
I1001 11:09:02.777896  5182 sgd_solver.cpp:105] Iteration 28300, lr = 0.1
I1001 11:09:08.019878  5182 solver.cpp:218] Iteration 28400 (19.0768 iter/s, 5.24197s/100 iters), loss = 0.219581
I1001 11:09:08.019907  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219581 (* 1 = 0.219581 loss)
I1001 11:09:08.019913  5182 sgd_solver.cpp:105] Iteration 28400, lr = 0.1
I1001 11:09:13.012596  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:09:13.222302  5182 solver.cpp:330] Iteration 28500, Testing net (#0)
I1001 11:09:14.411990  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:09:14.462278  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7554
I1001 11:09:14.462302  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.767177 (* 1 = 0.767177 loss)
I1001 11:09:14.514967  5182 solver.cpp:218] Iteration 28500 (15.3964 iter/s, 6.49504s/100 iters), loss = 0.227425
I1001 11:09:14.514993  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227425 (* 1 = 0.227425 loss)
I1001 11:09:14.515000  5182 sgd_solver.cpp:105] Iteration 28500, lr = 0.1
I1001 11:09:19.766983  5182 solver.cpp:218] Iteration 28600 (19.0405 iter/s, 5.25197s/100 iters), loss = 0.338943
I1001 11:09:19.767022  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.338942 (* 1 = 0.338942 loss)
I1001 11:09:19.767029  5182 sgd_solver.cpp:105] Iteration 28600, lr = 0.1
I1001 11:09:25.022248  5182 solver.cpp:218] Iteration 28700 (19.0288 iter/s, 5.2552s/100 iters), loss = 0.287331
I1001 11:09:25.022289  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.287331 (* 1 = 0.287331 loss)
I1001 11:09:25.022295  5182 sgd_solver.cpp:105] Iteration 28700, lr = 0.1
I1001 11:09:30.275662  5182 solver.cpp:218] Iteration 28800 (19.0355 iter/s, 5.25335s/100 iters), loss = 0.197102
I1001 11:09:30.275785  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197102 (* 1 = 0.197102 loss)
I1001 11:09:30.275794  5182 sgd_solver.cpp:105] Iteration 28800, lr = 0.1
I1001 11:09:35.521180  5182 solver.cpp:218] Iteration 28900 (19.0644 iter/s, 5.24539s/100 iters), loss = 0.286657
I1001 11:09:35.521221  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286657 (* 1 = 0.286657 loss)
I1001 11:09:35.521227  5182 sgd_solver.cpp:105] Iteration 28900, lr = 0.1
I1001 11:09:40.516782  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:09:40.726881  5182 solver.cpp:330] Iteration 29000, Testing net (#0)
I1001 11:09:41.925644  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:09:41.975622  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7328
I1001 11:09:41.975657  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.779132 (* 1 = 0.779132 loss)
I1001 11:09:42.028107  5182 solver.cpp:218] Iteration 29000 (15.3684 iter/s, 6.50687s/100 iters), loss = 0.209299
I1001 11:09:42.028136  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209299 (* 1 = 0.209299 loss)
I1001 11:09:42.028154  5182 sgd_solver.cpp:105] Iteration 29000, lr = 0.1
I1001 11:09:47.274660  5182 solver.cpp:218] Iteration 29100 (19.0603 iter/s, 5.2465s/100 iters), loss = 0.196779
I1001 11:09:47.274700  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196779 (* 1 = 0.196779 loss)
I1001 11:09:47.274708  5182 sgd_solver.cpp:105] Iteration 29100, lr = 0.1
I1001 11:09:52.530274  5182 solver.cpp:218] Iteration 29200 (19.0275 iter/s, 5.25555s/100 iters), loss = 0.29939
I1001 11:09:52.530304  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.29939 (* 1 = 0.29939 loss)
I1001 11:09:52.530310  5182 sgd_solver.cpp:105] Iteration 29200, lr = 0.1
I1001 11:09:57.789598  5182 solver.cpp:218] Iteration 29300 (19.0141 iter/s, 5.25927s/100 iters), loss = 0.145378
I1001 11:09:57.789638  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145378 (* 1 = 0.145378 loss)
I1001 11:09:57.789644  5182 sgd_solver.cpp:105] Iteration 29300, lr = 0.1
I1001 11:10:03.047343  5182 solver.cpp:218] Iteration 29400 (19.0198 iter/s, 5.25769s/100 iters), loss = 0.169383
I1001 11:10:03.047457  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169383 (* 1 = 0.169383 loss)
I1001 11:10:03.047466  5182 sgd_solver.cpp:105] Iteration 29400, lr = 0.1
I1001 11:10:08.026660  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:10:08.236701  5182 solver.cpp:330] Iteration 29500, Testing net (#0)
I1001 11:10:09.434105  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:10:09.484212  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8034
I1001 11:10:09.484246  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.607915 (* 1 = 0.607915 loss)
I1001 11:10:09.536947  5182 solver.cpp:218] Iteration 29500 (15.4096 iter/s, 6.48947s/100 iters), loss = 0.194156
I1001 11:10:09.536994  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194156 (* 1 = 0.194156 loss)
I1001 11:10:09.537017  5182 sgd_solver.cpp:105] Iteration 29500, lr = 0.1
I1001 11:10:14.788820  5182 solver.cpp:218] Iteration 29600 (19.041 iter/s, 5.25182s/100 iters), loss = 0.350139
I1001 11:10:14.788866  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.350139 (* 1 = 0.350139 loss)
I1001 11:10:14.788874  5182 sgd_solver.cpp:105] Iteration 29600, lr = 0.1
I1001 11:10:20.034816  5182 solver.cpp:218] Iteration 29700 (19.0625 iter/s, 5.24589s/100 iters), loss = 0.185555
I1001 11:10:20.034845  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185555 (* 1 = 0.185555 loss)
I1001 11:10:20.034852  5182 sgd_solver.cpp:105] Iteration 29700, lr = 0.1
I1001 11:10:25.292104  5182 solver.cpp:218] Iteration 29800 (19.0214 iter/s, 5.25724s/100 iters), loss = 0.149647
I1001 11:10:25.292143  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149647 (* 1 = 0.149647 loss)
I1001 11:10:25.292150  5182 sgd_solver.cpp:105] Iteration 29800, lr = 0.1
I1001 11:10:30.548918  5182 solver.cpp:218] Iteration 29900 (19.0231 iter/s, 5.25675s/100 iters), loss = 0.257022
I1001 11:10:30.548956  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257022 (* 1 = 0.257022 loss)
I1001 11:10:30.548964  5182 sgd_solver.cpp:105] Iteration 29900, lr = 0.1
I1001 11:10:35.531065  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:10:35.746556  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_30000.caffemodel
I1001 11:10:35.751857  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_30000.solverstate
I1001 11:10:35.753252  5182 solver.cpp:330] Iteration 30000, Testing net (#0)
I1001 11:10:36.947149  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:10:36.997278  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7234
I1001 11:10:36.997314  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.927875 (* 1 = 0.927875 loss)
I1001 11:10:37.049612  5182 solver.cpp:218] Iteration 30000 (15.3831 iter/s, 6.50063s/100 iters), loss = 0.266176
I1001 11:10:37.049643  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266175 (* 1 = 0.266175 loss)
I1001 11:10:37.049649  5182 sgd_solver.cpp:105] Iteration 30000, lr = 0.1
I1001 11:10:42.306612  5182 solver.cpp:218] Iteration 30100 (19.0224 iter/s, 5.25695s/100 iters), loss = 0.312754
I1001 11:10:42.306650  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312753 (* 1 = 0.312753 loss)
I1001 11:10:42.306658  5182 sgd_solver.cpp:105] Iteration 30100, lr = 0.1
I1001 11:10:47.554170  5182 solver.cpp:218] Iteration 30200 (19.0567 iter/s, 5.2475s/100 iters), loss = 0.248513
I1001 11:10:47.554209  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248513 (* 1 = 0.248513 loss)
I1001 11:10:47.554215  5182 sgd_solver.cpp:105] Iteration 30200, lr = 0.1
I1001 11:10:52.811961  5182 solver.cpp:218] Iteration 30300 (19.0196 iter/s, 5.25773s/100 iters), loss = 0.19983
I1001 11:10:52.811990  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19983 (* 1 = 0.19983 loss)
I1001 11:10:52.811997  5182 sgd_solver.cpp:105] Iteration 30300, lr = 0.1
I1001 11:10:58.063699  5182 solver.cpp:218] Iteration 30400 (19.0415 iter/s, 5.25169s/100 iters), loss = 0.244962
I1001 11:10:58.063741  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244962 (* 1 = 0.244962 loss)
I1001 11:10:58.063748  5182 sgd_solver.cpp:105] Iteration 30400, lr = 0.1
I1001 11:11:03.053472  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:11:03.263655  5182 solver.cpp:330] Iteration 30500, Testing net (#0)
I1001 11:11:04.453344  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:11:04.503358  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7726
I1001 11:11:04.503391  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.706307 (* 1 = 0.706307 loss)
I1001 11:11:04.555843  5182 solver.cpp:218] Iteration 30500 (15.4034 iter/s, 6.49208s/100 iters), loss = 0.168468
I1001 11:11:04.555869  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168467 (* 1 = 0.168467 loss)
I1001 11:11:04.555876  5182 sgd_solver.cpp:105] Iteration 30500, lr = 0.1
I1001 11:11:09.812732  5182 solver.cpp:218] Iteration 30600 (19.0228 iter/s, 5.25684s/100 iters), loss = 0.220551
I1001 11:11:09.812903  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220551 (* 1 = 0.220551 loss)
I1001 11:11:09.812912  5182 sgd_solver.cpp:105] Iteration 30600, lr = 0.1
I1001 11:11:15.071323  5182 solver.cpp:218] Iteration 30700 (19.0172 iter/s, 5.25841s/100 iters), loss = 0.28693
I1001 11:11:15.071354  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28693 (* 1 = 0.28693 loss)
I1001 11:11:15.071360  5182 sgd_solver.cpp:105] Iteration 30700, lr = 0.1
I1001 11:11:20.320041  5182 solver.cpp:218] Iteration 30800 (19.0525 iter/s, 5.24867s/100 iters), loss = 0.244116
I1001 11:11:20.320081  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244116 (* 1 = 0.244116 loss)
I1001 11:11:20.320087  5182 sgd_solver.cpp:105] Iteration 30800, lr = 0.1
I1001 11:11:25.584543  5182 solver.cpp:218] Iteration 30900 (18.9954 iter/s, 5.26444s/100 iters), loss = 0.212613
I1001 11:11:25.584583  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212613 (* 1 = 0.212613 loss)
I1001 11:11:25.584589  5182 sgd_solver.cpp:105] Iteration 30900, lr = 0.1
I1001 11:11:30.569649  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:11:30.778867  5182 solver.cpp:330] Iteration 31000, Testing net (#0)
I1001 11:11:31.968585  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:11:32.018892  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.832
I1001 11:11:32.018928  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.508035 (* 1 = 0.508035 loss)
I1001 11:11:32.071503  5182 solver.cpp:218] Iteration 31000 (15.4157 iter/s, 6.4869s/100 iters), loss = 0.268928
I1001 11:11:32.071542  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268928 (* 1 = 0.268928 loss)
I1001 11:11:32.071548  5182 sgd_solver.cpp:105] Iteration 31000, lr = 0.1
I1001 11:11:37.330023  5182 solver.cpp:218] Iteration 31100 (19.017 iter/s, 5.25846s/100 iters), loss = 0.283679
I1001 11:11:37.330061  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283679 (* 1 = 0.283679 loss)
I1001 11:11:37.330067  5182 sgd_solver.cpp:105] Iteration 31100, lr = 0.1
I1001 11:11:42.575902  5182 solver.cpp:218] Iteration 31200 (19.0628 iter/s, 5.24582s/100 iters), loss = 0.269354
I1001 11:11:42.576014  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269354 (* 1 = 0.269354 loss)
I1001 11:11:42.576032  5182 sgd_solver.cpp:105] Iteration 31200, lr = 0.1
I1001 11:11:47.825348  5182 solver.cpp:218] Iteration 31300 (19.0501 iter/s, 5.24931s/100 iters), loss = 0.225554
I1001 11:11:47.825395  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.225554 (* 1 = 0.225554 loss)
I1001 11:11:47.825402  5182 sgd_solver.cpp:105] Iteration 31300, lr = 0.1
I1001 11:11:53.075723  5182 solver.cpp:218] Iteration 31400 (19.0465 iter/s, 5.25031s/100 iters), loss = 0.175029
I1001 11:11:53.075755  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175029 (* 1 = 0.175029 loss)
I1001 11:11:53.075762  5182 sgd_solver.cpp:105] Iteration 31400, lr = 0.1
I1001 11:11:58.072718  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:11:58.282817  5182 solver.cpp:330] Iteration 31500, Testing net (#0)
I1001 11:11:59.472298  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:11:59.522538  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8045
I1001 11:11:59.522572  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.641504 (* 1 = 0.641504 loss)
I1001 11:11:59.574889  5182 solver.cpp:218] Iteration 31500 (15.3867 iter/s, 6.49911s/100 iters), loss = 0.154048
I1001 11:11:59.574914  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154048 (* 1 = 0.154048 loss)
I1001 11:11:59.574921  5182 sgd_solver.cpp:105] Iteration 31500, lr = 0.1
I1001 11:12:04.835218  5182 solver.cpp:218] Iteration 31600 (19.0104 iter/s, 5.26028s/100 iters), loss = 0.255778
I1001 11:12:04.835258  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255778 (* 1 = 0.255778 loss)
I1001 11:12:04.835263  5182 sgd_solver.cpp:105] Iteration 31600, lr = 0.1
I1001 11:12:10.095612  5182 solver.cpp:218] Iteration 31700 (19.0102 iter/s, 5.26033s/100 iters), loss = 0.191315
I1001 11:12:10.095650  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191315 (* 1 = 0.191315 loss)
I1001 11:12:10.095656  5182 sgd_solver.cpp:105] Iteration 31700, lr = 0.1
I1001 11:12:15.353261  5182 solver.cpp:218] Iteration 31800 (19.0201 iter/s, 5.25759s/100 iters), loss = 0.237738
I1001 11:12:15.353384  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237737 (* 1 = 0.237737 loss)
I1001 11:12:15.353392  5182 sgd_solver.cpp:105] Iteration 31800, lr = 0.1
I1001 11:12:20.602370  5182 solver.cpp:218] Iteration 31900 (19.0514 iter/s, 5.24897s/100 iters), loss = 0.127123
I1001 11:12:20.602408  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127122 (* 1 = 0.127122 loss)
I1001 11:12:20.602413  5182 sgd_solver.cpp:105] Iteration 31900, lr = 0.1
I1001 11:12:25.599850  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:12:25.810590  5182 solver.cpp:330] Iteration 32000, Testing net (#0)
I1001 11:12:27.006165  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:12:27.056092  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8092
I1001 11:12:27.056128  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.569501 (* 1 = 0.569501 loss)
I1001 11:12:27.108847  5182 solver.cpp:218] Iteration 32000 (15.3695 iter/s, 6.50641s/100 iters), loss = 0.242173
I1001 11:12:27.108887  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242173 (* 1 = 0.242173 loss)
I1001 11:12:27.108894  5182 sgd_solver.cpp:105] Iteration 32000, lr = 0.1
I1001 11:12:32.355684  5182 solver.cpp:218] Iteration 32100 (19.0593 iter/s, 5.24677s/100 iters), loss = 0.242555
I1001 11:12:32.355723  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242554 (* 1 = 0.242554 loss)
I1001 11:12:32.355729  5182 sgd_solver.cpp:105] Iteration 32100, lr = 0.1
I1001 11:12:37.607091  5182 solver.cpp:218] Iteration 32200 (19.0428 iter/s, 5.25134s/100 iters), loss = 0.241783
I1001 11:12:37.607131  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241783 (* 1 = 0.241783 loss)
I1001 11:12:37.607136  5182 sgd_solver.cpp:105] Iteration 32200, lr = 0.1
I1001 11:12:42.860386  5182 solver.cpp:218] Iteration 32300 (19.0359 iter/s, 5.25324s/100 iters), loss = 0.236906
I1001 11:12:42.860425  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236906 (* 1 = 0.236906 loss)
I1001 11:12:42.860431  5182 sgd_solver.cpp:105] Iteration 32300, lr = 0.1
I1001 11:12:48.119704  5182 solver.cpp:218] Iteration 32400 (19.0141 iter/s, 5.25926s/100 iters), loss = 0.18253
I1001 11:12:48.119817  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18253 (* 1 = 0.18253 loss)
I1001 11:12:48.119834  5182 sgd_solver.cpp:105] Iteration 32400, lr = 0.1
I1001 11:12:53.105368  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:12:53.316301  5182 solver.cpp:330] Iteration 32500, Testing net (#0)
I1001 11:12:54.516058  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:12:54.566200  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6962
I1001 11:12:54.566236  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.01628 (* 1 = 1.01628 loss)
I1001 11:12:54.618988  5182 solver.cpp:218] Iteration 32500 (15.3866 iter/s, 6.49916s/100 iters), loss = 0.160255
I1001 11:12:54.619031  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160255 (* 1 = 0.160255 loss)
I1001 11:12:54.619040  5182 sgd_solver.cpp:105] Iteration 32500, lr = 0.1
I1001 11:12:59.878221  5182 solver.cpp:218] Iteration 32600 (19.0144 iter/s, 5.25916s/100 iters), loss = 0.219444
I1001 11:12:59.878263  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219443 (* 1 = 0.219443 loss)
I1001 11:12:59.878270  5182 sgd_solver.cpp:105] Iteration 32600, lr = 0.1
I1001 11:13:05.128582  5182 solver.cpp:218] Iteration 32700 (19.0465 iter/s, 5.2503s/100 iters), loss = 0.254801
I1001 11:13:05.128618  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254801 (* 1 = 0.254801 loss)
I1001 11:13:05.128635  5182 sgd_solver.cpp:105] Iteration 32700, lr = 0.1
I1001 11:13:10.388669  5182 solver.cpp:218] Iteration 32800 (19.0113 iter/s, 5.26003s/100 iters), loss = 0.162997
I1001 11:13:10.388701  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.162996 (* 1 = 0.162996 loss)
I1001 11:13:10.388710  5182 sgd_solver.cpp:105] Iteration 32800, lr = 0.1
I1001 11:13:15.645803  5182 solver.cpp:218] Iteration 32900 (19.022 iter/s, 5.25708s/100 iters), loss = 0.186304
I1001 11:13:15.645836  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186304 (* 1 = 0.186304 loss)
I1001 11:13:15.645854  5182 sgd_solver.cpp:105] Iteration 32900, lr = 0.1
I1001 11:13:20.630079  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:13:20.844956  5182 solver.cpp:330] Iteration 33000, Testing net (#0)
I1001 11:13:22.033706  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:13:22.083878  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7342
I1001 11:13:22.083904  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.866503 (* 1 = 0.866503 loss)
I1001 11:13:22.136394  5182 solver.cpp:218] Iteration 33000 (15.4071 iter/s, 6.49053s/100 iters), loss = 0.216262
I1001 11:13:22.136427  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216262 (* 1 = 0.216262 loss)
I1001 11:13:22.136436  5182 sgd_solver.cpp:105] Iteration 33000, lr = 0.1
I1001 11:13:27.390945  5182 solver.cpp:218] Iteration 33100 (19.0313 iter/s, 5.2545s/100 iters), loss = 0.218464
I1001 11:13:27.390985  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218464 (* 1 = 0.218464 loss)
I1001 11:13:27.390991  5182 sgd_solver.cpp:105] Iteration 33100, lr = 0.1
I1001 11:13:32.640455  5182 solver.cpp:218] Iteration 33200 (19.0496 iter/s, 5.24945s/100 iters), loss = 0.264654
I1001 11:13:32.640499  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264654 (* 1 = 0.264654 loss)
I1001 11:13:32.640507  5182 sgd_solver.cpp:105] Iteration 33200, lr = 0.1
I1001 11:13:37.893934  5182 solver.cpp:218] Iteration 33300 (19.0353 iter/s, 5.25339s/100 iters), loss = 0.150771
I1001 11:13:37.893962  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15077 (* 1 = 0.15077 loss)
I1001 11:13:37.893968  5182 sgd_solver.cpp:105] Iteration 33300, lr = 0.1
I1001 11:13:43.151093  5182 solver.cpp:218] Iteration 33400 (19.0219 iter/s, 5.25711s/100 iters), loss = 0.18776
I1001 11:13:43.151130  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18776 (* 1 = 0.18776 loss)
I1001 11:13:43.151139  5182 sgd_solver.cpp:105] Iteration 33400, lr = 0.1
I1001 11:13:48.142488  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:13:48.350806  5182 solver.cpp:330] Iteration 33500, Testing net (#0)
I1001 11:13:49.540081  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:13:49.590361  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7951
I1001 11:13:49.590386  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.646398 (* 1 = 0.646398 loss)
I1001 11:13:49.643007  5182 solver.cpp:218] Iteration 33500 (15.4039 iter/s, 6.49186s/100 iters), loss = 0.227643
I1001 11:13:49.643033  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227642 (* 1 = 0.227642 loss)
I1001 11:13:49.643040  5182 sgd_solver.cpp:105] Iteration 33500, lr = 0.1
I1001 11:13:54.908864  5182 solver.cpp:218] Iteration 33600 (18.9904 iter/s, 5.26581s/100 iters), loss = 0.237649
I1001 11:13:54.908963  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237649 (* 1 = 0.237649 loss)
I1001 11:13:54.908970  5182 sgd_solver.cpp:105] Iteration 33600, lr = 0.1
I1001 11:14:00.172430  5182 solver.cpp:218] Iteration 33700 (18.999 iter/s, 5.26345s/100 iters), loss = 0.216854
I1001 11:14:00.172462  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216854 (* 1 = 0.216854 loss)
I1001 11:14:00.172469  5182 sgd_solver.cpp:105] Iteration 33700, lr = 0.1
I1001 11:14:05.418006  5182 solver.cpp:218] Iteration 33800 (19.0639 iter/s, 5.24552s/100 iters), loss = 0.201544
I1001 11:14:05.418040  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201544 (* 1 = 0.201544 loss)
I1001 11:14:05.418047  5182 sgd_solver.cpp:105] Iteration 33800, lr = 0.1
I1001 11:14:10.672329  5182 solver.cpp:218] Iteration 33900 (19.0321 iter/s, 5.25427s/100 iters), loss = 0.219674
I1001 11:14:10.672358  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219674 (* 1 = 0.219674 loss)
I1001 11:14:10.672364  5182 sgd_solver.cpp:105] Iteration 33900, lr = 0.1
I1001 11:14:15.663187  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:14:15.873481  5182 solver.cpp:330] Iteration 34000, Testing net (#0)
I1001 11:14:17.062899  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:14:17.112637  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.776
I1001 11:14:17.112671  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.659984 (* 1 = 0.659984 loss)
I1001 11:14:17.164875  5182 solver.cpp:218] Iteration 34000 (15.4024 iter/s, 6.4925s/100 iters), loss = 0.215563
I1001 11:14:17.164899  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215563 (* 1 = 0.215563 loss)
I1001 11:14:17.164906  5182 sgd_solver.cpp:105] Iteration 34000, lr = 0.1
I1001 11:14:22.420409  5182 solver.cpp:218] Iteration 34100 (19.0277 iter/s, 5.25549s/100 iters), loss = 0.2539
I1001 11:14:22.420437  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253899 (* 1 = 0.253899 loss)
I1001 11:14:22.420444  5182 sgd_solver.cpp:105] Iteration 34100, lr = 0.1
I1001 11:14:27.678802  5182 solver.cpp:218] Iteration 34200 (19.0174 iter/s, 5.25834s/100 iters), loss = 0.173316
I1001 11:14:27.678880  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173316 (* 1 = 0.173316 loss)
I1001 11:14:27.678889  5182 sgd_solver.cpp:105] Iteration 34200, lr = 0.1
I1001 11:14:32.937528  5182 solver.cpp:218] Iteration 34300 (19.0164 iter/s, 5.25863s/100 iters), loss = 0.260611
I1001 11:14:32.937562  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260611 (* 1 = 0.260611 loss)
I1001 11:14:32.937568  5182 sgd_solver.cpp:105] Iteration 34300, lr = 0.1
I1001 11:14:38.188555  5182 solver.cpp:218] Iteration 34400 (19.0441 iter/s, 5.25097s/100 iters), loss = 0.127943
I1001 11:14:38.188585  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127943 (* 1 = 0.127943 loss)
I1001 11:14:38.188592  5182 sgd_solver.cpp:105] Iteration 34400, lr = 0.1
I1001 11:14:43.187883  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:14:43.397420  5182 solver.cpp:330] Iteration 34500, Testing net (#0)
I1001 11:14:44.586374  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:14:44.636271  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7363
I1001 11:14:44.636308  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.859116 (* 1 = 0.859116 loss)
I1001 11:14:44.690557  5182 solver.cpp:218] Iteration 34500 (15.38 iter/s, 6.50195s/100 iters), loss = 0.191697
I1001 11:14:44.690590  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191696 (* 1 = 0.191696 loss)
I1001 11:14:44.690598  5182 sgd_solver.cpp:105] Iteration 34500, lr = 0.1
I1001 11:14:49.944250  5182 solver.cpp:218] Iteration 34600 (19.0344 iter/s, 5.25363s/100 iters), loss = 0.304421
I1001 11:14:49.944289  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.304421 (* 1 = 0.304421 loss)
I1001 11:14:49.944295  5182 sgd_solver.cpp:105] Iteration 34600, lr = 0.1
I1001 11:14:55.197230  5182 solver.cpp:218] Iteration 34700 (19.037 iter/s, 5.25292s/100 iters), loss = 0.222936
I1001 11:14:55.197270  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222936 (* 1 = 0.222936 loss)
I1001 11:14:55.197276  5182 sgd_solver.cpp:105] Iteration 34700, lr = 0.1
I1001 11:15:00.448674  5182 solver.cpp:218] Iteration 34800 (19.0426 iter/s, 5.25138s/100 iters), loss = 0.231342
I1001 11:15:00.448825  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231342 (* 1 = 0.231342 loss)
I1001 11:15:00.448843  5182 sgd_solver.cpp:105] Iteration 34800, lr = 0.1
I1001 11:15:05.694308  5182 solver.cpp:218] Iteration 34900 (19.0641 iter/s, 5.24547s/100 iters), loss = 0.205613
I1001 11:15:05.694350  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205613 (* 1 = 0.205613 loss)
I1001 11:15:05.694366  5182 sgd_solver.cpp:105] Iteration 34900, lr = 0.1
I1001 11:15:10.678627  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:15:10.888679  5182 solver.cpp:330] Iteration 35000, Testing net (#0)
I1001 11:15:12.086289  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:15:12.136415  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7417
I1001 11:15:12.136451  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.848218 (* 1 = 0.848218 loss)
I1001 11:15:12.189213  5182 solver.cpp:218] Iteration 35000 (15.3968 iter/s, 6.49484s/100 iters), loss = 0.211111
I1001 11:15:12.189250  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.211111 (* 1 = 0.211111 loss)
I1001 11:15:12.189257  5182 sgd_solver.cpp:105] Iteration 35000, lr = 0.1
I1001 11:15:17.438854  5182 solver.cpp:218] Iteration 35100 (19.0491 iter/s, 5.24959s/100 iters), loss = 0.255927
I1001 11:15:17.438895  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255927 (* 1 = 0.255927 loss)
I1001 11:15:17.438900  5182 sgd_solver.cpp:105] Iteration 35100, lr = 0.1
I1001 11:15:22.698601  5182 solver.cpp:218] Iteration 35200 (19.0125 iter/s, 5.25968s/100 iters), loss = 0.320587
I1001 11:15:22.698629  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320586 (* 1 = 0.320586 loss)
I1001 11:15:22.698635  5182 sgd_solver.cpp:105] Iteration 35200, lr = 0.1
I1001 11:15:27.961171  5182 solver.cpp:218] Iteration 35300 (19.0023 iter/s, 5.26252s/100 iters), loss = 0.184505
I1001 11:15:27.961200  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184504 (* 1 = 0.184504 loss)
I1001 11:15:27.961206  5182 sgd_solver.cpp:105] Iteration 35300, lr = 0.1
I1001 11:15:33.223215  5182 solver.cpp:218] Iteration 35400 (19.0042 iter/s, 5.26199s/100 iters), loss = 0.172313
I1001 11:15:33.223326  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.172313 (* 1 = 0.172313 loss)
I1001 11:15:33.223345  5182 sgd_solver.cpp:105] Iteration 35400, lr = 0.1
I1001 11:15:38.206583  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:15:38.416152  5182 solver.cpp:330] Iteration 35500, Testing net (#0)
I1001 11:15:39.610918  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:15:39.660887  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8316
I1001 11:15:39.660920  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.509399 (* 1 = 0.509399 loss)
I1001 11:15:39.713245  5182 solver.cpp:218] Iteration 35500 (15.4085 iter/s, 6.48991s/100 iters), loss = 0.240124
I1001 11:15:39.713271  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240124 (* 1 = 0.240124 loss)
I1001 11:15:39.713279  5182 sgd_solver.cpp:105] Iteration 35500, lr = 0.1
I1001 11:15:44.958644  5182 solver.cpp:218] Iteration 35600 (19.0645 iter/s, 5.24535s/100 iters), loss = 0.171379
I1001 11:15:44.958676  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171378 (* 1 = 0.171378 loss)
I1001 11:15:44.958683  5182 sgd_solver.cpp:105] Iteration 35600, lr = 0.1
I1001 11:15:50.201969  5182 solver.cpp:218] Iteration 35700 (19.0721 iter/s, 5.24327s/100 iters), loss = 0.266087
I1001 11:15:50.202009  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266087 (* 1 = 0.266087 loss)
I1001 11:15:50.202015  5182 sgd_solver.cpp:105] Iteration 35700, lr = 0.1
I1001 11:15:55.453373  5182 solver.cpp:218] Iteration 35800 (19.0427 iter/s, 5.25134s/100 iters), loss = 0.288796
I1001 11:15:55.453403  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.288796 (* 1 = 0.288796 loss)
I1001 11:15:55.453408  5182 sgd_solver.cpp:105] Iteration 35800, lr = 0.1
I1001 11:16:00.709444  5182 solver.cpp:218] Iteration 35900 (19.0258 iter/s, 5.25602s/100 iters), loss = 0.253507
I1001 11:16:00.709475  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253507 (* 1 = 0.253507 loss)
I1001 11:16:00.709483  5182 sgd_solver.cpp:105] Iteration 35900, lr = 0.1
I1001 11:16:05.698662  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:16:05.914602  5182 solver.cpp:330] Iteration 36000, Testing net (#0)
I1001 11:16:07.103297  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:16:07.152887  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7834
I1001 11:16:07.152915  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.693352 (* 1 = 0.693352 loss)
I1001 11:16:07.205360  5182 solver.cpp:218] Iteration 36000 (15.3944 iter/s, 6.49586s/100 iters), loss = 0.1898
I1001 11:16:07.205389  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1898 (* 1 = 0.1898 loss)
I1001 11:16:07.205399  5182 sgd_solver.cpp:105] Iteration 36000, lr = 0.1
I1001 11:16:12.467600  5182 solver.cpp:218] Iteration 36100 (19.0035 iter/s, 5.26219s/100 iters), loss = 0.172628
I1001 11:16:12.467640  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.172628 (* 1 = 0.172628 loss)
I1001 11:16:12.467648  5182 sgd_solver.cpp:105] Iteration 36100, lr = 0.1
I1001 11:16:17.717978  5182 solver.cpp:218] Iteration 36200 (19.0465 iter/s, 5.25031s/100 iters), loss = 0.243083
I1001 11:16:17.718013  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243083 (* 1 = 0.243083 loss)
I1001 11:16:17.718019  5182 sgd_solver.cpp:105] Iteration 36200, lr = 0.1
I1001 11:16:22.974059  5182 solver.cpp:218] Iteration 36300 (19.0258 iter/s, 5.25603s/100 iters), loss = 0.307359
I1001 11:16:22.974102  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.307359 (* 1 = 0.307359 loss)
I1001 11:16:22.974107  5182 sgd_solver.cpp:105] Iteration 36300, lr = 0.1
I1001 11:16:28.225767  5182 solver.cpp:218] Iteration 36400 (19.0416 iter/s, 5.25165s/100 iters), loss = 0.21963
I1001 11:16:28.225802  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21963 (* 1 = 0.21963 loss)
I1001 11:16:28.225821  5182 sgd_solver.cpp:105] Iteration 36400, lr = 0.1
I1001 11:16:33.220188  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:16:33.432083  5182 solver.cpp:330] Iteration 36500, Testing net (#0)
I1001 11:16:34.621086  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:16:34.671115  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7712
I1001 11:16:34.671141  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.73948 (* 1 = 0.73948 loss)
I1001 11:16:34.723554  5182 solver.cpp:218] Iteration 36500 (15.39 iter/s, 6.49774s/100 iters), loss = 0.196132
I1001 11:16:34.723582  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196132 (* 1 = 0.196132 loss)
I1001 11:16:34.723592  5182 sgd_solver.cpp:105] Iteration 36500, lr = 0.1
I1001 11:16:39.979360  5182 solver.cpp:218] Iteration 36600 (19.0268 iter/s, 5.25575s/100 iters), loss = 0.220378
I1001 11:16:39.979523  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220378 (* 1 = 0.220378 loss)
I1001 11:16:39.979567  5182 sgd_solver.cpp:105] Iteration 36600, lr = 0.1
I1001 11:16:45.228368  5182 solver.cpp:218] Iteration 36700 (19.0519 iter/s, 5.24883s/100 iters), loss = 0.217625
I1001 11:16:45.228402  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217625 (* 1 = 0.217625 loss)
I1001 11:16:45.228412  5182 sgd_solver.cpp:105] Iteration 36700, lr = 0.1
I1001 11:16:50.481793  5182 solver.cpp:218] Iteration 36800 (19.0354 iter/s, 5.25337s/100 iters), loss = 0.24521
I1001 11:16:50.481825  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24521 (* 1 = 0.24521 loss)
I1001 11:16:50.481844  5182 sgd_solver.cpp:105] Iteration 36800, lr = 0.1
I1001 11:16:55.746281  5182 solver.cpp:218] Iteration 36900 (18.9954 iter/s, 5.26444s/100 iters), loss = 0.208774
I1001 11:16:55.746312  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208773 (* 1 = 0.208773 loss)
I1001 11:16:55.746331  5182 sgd_solver.cpp:105] Iteration 36900, lr = 0.1
I1001 11:17:00.743777  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:17:00.954509  5182 solver.cpp:330] Iteration 37000, Testing net (#0)
I1001 11:17:02.144615  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:17:02.195137  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6684
I1001 11:17:02.195169  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.37394 (* 1 = 1.37394 loss)
I1001 11:17:02.247500  5182 solver.cpp:218] Iteration 37000 (15.3819 iter/s, 6.50116s/100 iters), loss = 0.240898
I1001 11:17:02.247525  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240898 (* 1 = 0.240898 loss)
I1001 11:17:02.247531  5182 sgd_solver.cpp:105] Iteration 37000, lr = 0.1
I1001 11:17:07.508020  5182 solver.cpp:218] Iteration 37100 (19.0097 iter/s, 5.26047s/100 iters), loss = 0.197631
I1001 11:17:07.508050  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19763 (* 1 = 0.19763 loss)
I1001 11:17:07.508066  5182 sgd_solver.cpp:105] Iteration 37100, lr = 0.1
I1001 11:17:12.767127  5182 solver.cpp:218] Iteration 37200 (19.0148 iter/s, 5.25906s/100 iters), loss = 0.211442
I1001 11:17:12.767215  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.211442 (* 1 = 0.211442 loss)
I1001 11:17:12.767231  5182 sgd_solver.cpp:105] Iteration 37200, lr = 0.1
I1001 11:17:18.020848  5182 solver.cpp:218] Iteration 37300 (19.0345 iter/s, 5.25361s/100 iters), loss = 0.160699
I1001 11:17:18.020884  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160698 (* 1 = 0.160698 loss)
I1001 11:17:18.020890  5182 sgd_solver.cpp:105] Iteration 37300, lr = 0.1
I1001 11:17:23.266202  5182 solver.cpp:218] Iteration 37400 (19.0647 iter/s, 5.2453s/100 iters), loss = 0.207041
I1001 11:17:23.266232  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207041 (* 1 = 0.207041 loss)
I1001 11:17:23.266248  5182 sgd_solver.cpp:105] Iteration 37400, lr = 0.1
I1001 11:17:28.257580  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:17:28.469125  5182 solver.cpp:330] Iteration 37500, Testing net (#0)
I1001 11:17:29.660140  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:17:29.711246  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7126
I1001 11:17:29.711280  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.853232 (* 1 = 0.853232 loss)
I1001 11:17:29.764820  5182 solver.cpp:218] Iteration 37500 (15.388 iter/s, 6.49856s/100 iters), loss = 0.251629
I1001 11:17:29.764885  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.251628 (* 1 = 0.251628 loss)
I1001 11:17:29.764892  5182 sgd_solver.cpp:105] Iteration 37500, lr = 0.1
I1001 11:17:35.019814  5182 solver.cpp:218] Iteration 37600 (19.0298 iter/s, 5.25491s/100 iters), loss = 0.26561
I1001 11:17:35.019853  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265609 (* 1 = 0.265609 loss)
I1001 11:17:35.019860  5182 sgd_solver.cpp:105] Iteration 37600, lr = 0.1
I1001 11:17:40.278367  5182 solver.cpp:218] Iteration 37700 (19.0168 iter/s, 5.25849s/100 iters), loss = 0.242139
I1001 11:17:40.278406  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242139 (* 1 = 0.242139 loss)
I1001 11:17:40.278412  5182 sgd_solver.cpp:105] Iteration 37700, lr = 0.1
I1001 11:17:45.536670  5182 solver.cpp:218] Iteration 37800 (19.0178 iter/s, 5.25824s/100 iters), loss = 0.170165
I1001 11:17:45.536805  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170165 (* 1 = 0.170165 loss)
I1001 11:17:45.536823  5182 sgd_solver.cpp:105] Iteration 37800, lr = 0.1
I1001 11:17:50.793576  5182 solver.cpp:218] Iteration 37900 (19.0231 iter/s, 5.25675s/100 iters), loss = 0.171555
I1001 11:17:50.793625  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171555 (* 1 = 0.171555 loss)
I1001 11:17:50.793633  5182 sgd_solver.cpp:105] Iteration 37900, lr = 0.1
I1001 11:17:55.785877  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:17:55.995476  5182 solver.cpp:330] Iteration 38000, Testing net (#0)
I1001 11:17:57.194402  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:17:57.244438  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8035
I1001 11:17:57.244463  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.594697 (* 1 = 0.594697 loss)
I1001 11:17:57.297202  5182 solver.cpp:218] Iteration 38000 (15.3763 iter/s, 6.50352s/100 iters), loss = 0.189368
I1001 11:17:57.297228  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.189368 (* 1 = 0.189368 loss)
I1001 11:17:57.297235  5182 sgd_solver.cpp:105] Iteration 38000, lr = 0.1
I1001 11:18:02.544400  5182 solver.cpp:218] Iteration 38100 (19.058 iter/s, 5.24715s/100 iters), loss = 0.190597
I1001 11:18:02.544428  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190597 (* 1 = 0.190597 loss)
I1001 11:18:02.544435  5182 sgd_solver.cpp:105] Iteration 38100, lr = 0.1
I1001 11:18:07.798810  5182 solver.cpp:218] Iteration 38200 (19.0318 iter/s, 5.25436s/100 iters), loss = 0.281295
I1001 11:18:07.798847  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281294 (* 1 = 0.281294 loss)
I1001 11:18:07.798853  5182 sgd_solver.cpp:105] Iteration 38200, lr = 0.1
I1001 11:18:13.052980  5182 solver.cpp:218] Iteration 38300 (19.0327 iter/s, 5.25411s/100 iters), loss = 0.238581
I1001 11:18:13.053014  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238581 (* 1 = 0.238581 loss)
I1001 11:18:13.053023  5182 sgd_solver.cpp:105] Iteration 38300, lr = 0.1
I1001 11:18:18.311197  5182 solver.cpp:218] Iteration 38400 (19.018 iter/s, 5.25816s/100 iters), loss = 0.286248
I1001 11:18:18.311319  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286248 (* 1 = 0.286248 loss)
I1001 11:18:18.311327  5182 sgd_solver.cpp:105] Iteration 38400, lr = 0.1
I1001 11:18:23.300518  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:18:23.512483  5182 solver.cpp:330] Iteration 38500, Testing net (#0)
I1001 11:18:24.711630  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:18:24.761838  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8029
I1001 11:18:24.761874  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.609636 (* 1 = 0.609636 loss)
I1001 11:18:24.814461  5182 solver.cpp:218] Iteration 38500 (15.3772 iter/s, 6.50312s/100 iters), loss = 0.229254
I1001 11:18:24.814491  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229254 (* 1 = 0.229254 loss)
I1001 11:18:24.814498  5182 sgd_solver.cpp:105] Iteration 38500, lr = 0.1
I1001 11:18:30.073618  5182 solver.cpp:218] Iteration 38600 (19.0147 iter/s, 5.2591s/100 iters), loss = 0.212225
I1001 11:18:30.073647  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212225 (* 1 = 0.212225 loss)
I1001 11:18:30.073653  5182 sgd_solver.cpp:105] Iteration 38600, lr = 0.1
I1001 11:18:35.321285  5182 solver.cpp:218] Iteration 38700 (19.0563 iter/s, 5.24762s/100 iters), loss = 0.349976
I1001 11:18:35.321323  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.349976 (* 1 = 0.349976 loss)
I1001 11:18:35.321329  5182 sgd_solver.cpp:105] Iteration 38700, lr = 0.1
I1001 11:18:40.579260  5182 solver.cpp:218] Iteration 38800 (19.0189 iter/s, 5.25792s/100 iters), loss = 0.167283
I1001 11:18:40.579289  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167282 (* 1 = 0.167282 loss)
I1001 11:18:40.579295  5182 sgd_solver.cpp:105] Iteration 38800, lr = 0.1
I1001 11:18:45.833336  5182 solver.cpp:218] Iteration 38900 (19.033 iter/s, 5.25402s/100 iters), loss = 0.194185
I1001 11:18:45.833371  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194184 (* 1 = 0.194184 loss)
I1001 11:18:45.833379  5182 sgd_solver.cpp:105] Iteration 38900, lr = 0.1
I1001 11:18:50.822291  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:18:51.036168  5182 solver.cpp:330] Iteration 39000, Testing net (#0)
I1001 11:18:52.224845  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:18:52.274883  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6981
I1001 11:18:52.274916  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.04486 (* 1 = 1.04486 loss)
I1001 11:18:52.327600  5182 solver.cpp:218] Iteration 39000 (15.3983 iter/s, 6.49421s/100 iters), loss = 0.164304
I1001 11:18:52.327622  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164303 (* 1 = 0.164303 loss)
I1001 11:18:52.327630  5182 sgd_solver.cpp:105] Iteration 39000, lr = 0.1
I1001 11:18:57.586220  5182 solver.cpp:218] Iteration 39100 (19.0166 iter/s, 5.25858s/100 iters), loss = 0.214152
I1001 11:18:57.586252  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214152 (* 1 = 0.214152 loss)
I1001 11:18:57.586258  5182 sgd_solver.cpp:105] Iteration 39100, lr = 0.1
I1001 11:19:02.835876  5182 solver.cpp:218] Iteration 39200 (19.0491 iter/s, 5.2496s/100 iters), loss = 0.227368
I1001 11:19:02.835911  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227367 (* 1 = 0.227367 loss)
I1001 11:19:02.835918  5182 sgd_solver.cpp:105] Iteration 39200, lr = 0.1
I1001 11:19:08.084671  5182 solver.cpp:218] Iteration 39300 (19.0522 iter/s, 5.24874s/100 iters), loss = 0.168009
I1001 11:19:08.084699  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168009 (* 1 = 0.168009 loss)
I1001 11:19:08.084705  5182 sgd_solver.cpp:105] Iteration 39300, lr = 0.1
I1001 11:19:13.347522  5182 solver.cpp:218] Iteration 39400 (19.0013 iter/s, 5.2628s/100 iters), loss = 0.11445
I1001 11:19:13.347561  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.11445 (* 1 = 0.11445 loss)
I1001 11:19:13.347568  5182 sgd_solver.cpp:105] Iteration 39400, lr = 0.1
I1001 11:19:18.340911  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:19:18.552433  5182 solver.cpp:330] Iteration 39500, Testing net (#0)
I1001 11:19:19.742861  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:19:19.792829  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7619
I1001 11:19:19.792863  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.786846 (* 1 = 0.786846 loss)
I1001 11:19:19.845584  5182 solver.cpp:218] Iteration 39500 (15.3893 iter/s, 6.498s/100 iters), loss = 0.190278
I1001 11:19:19.845610  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190278 (* 1 = 0.190278 loss)
I1001 11:19:19.845618  5182 sgd_solver.cpp:105] Iteration 39500, lr = 0.1
I1001 11:19:25.105507  5182 solver.cpp:218] Iteration 39600 (19.0119 iter/s, 5.25988s/100 iters), loss = 0.227669
I1001 11:19:25.105640  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227669 (* 1 = 0.227669 loss)
I1001 11:19:25.105661  5182 sgd_solver.cpp:105] Iteration 39600, lr = 0.1
I1001 11:19:30.364449  5182 solver.cpp:218] Iteration 39700 (19.0158 iter/s, 5.25879s/100 iters), loss = 0.363025
I1001 11:19:30.364487  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.363024 (* 1 = 0.363024 loss)
I1001 11:19:30.364493  5182 sgd_solver.cpp:105] Iteration 39700, lr = 0.1
I1001 11:19:35.612041  5182 solver.cpp:218] Iteration 39800 (19.0566 iter/s, 5.24754s/100 iters), loss = 0.194648
I1001 11:19:35.612082  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194648 (* 1 = 0.194648 loss)
I1001 11:19:35.612088  5182 sgd_solver.cpp:105] Iteration 39800, lr = 0.1
I1001 11:19:40.865252  5182 solver.cpp:218] Iteration 39900 (19.0362 iter/s, 5.25315s/100 iters), loss = 0.264092
I1001 11:19:40.865283  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264092 (* 1 = 0.264092 loss)
I1001 11:19:40.865288  5182 sgd_solver.cpp:105] Iteration 39900, lr = 0.1
I1001 11:19:45.855914  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:19:46.066437  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_40000.caffemodel
I1001 11:19:46.071386  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_40000.solverstate
I1001 11:19:46.072712  5182 solver.cpp:330] Iteration 40000, Testing net (#0)
I1001 11:19:47.262761  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:19:47.313249  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8033
I1001 11:19:47.313284  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.597462 (* 1 = 0.597462 loss)
I1001 11:19:47.365747  5182 solver.cpp:218] Iteration 40000 (15.3836 iter/s, 6.50045s/100 iters), loss = 0.181187
I1001 11:19:47.365773  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181186 (* 1 = 0.181186 loss)
I1001 11:19:47.365779  5182 sgd_solver.cpp:46] MultiStep Status: Iteration 40000, step = 1
I1001 11:19:47.365782  5182 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I1001 11:19:52.613385  5182 solver.cpp:218] Iteration 40100 (19.0564 iter/s, 5.24759s/100 iters), loss = 0.171846
I1001 11:19:52.613425  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171846 (* 1 = 0.171846 loss)
I1001 11:19:52.613431  5182 sgd_solver.cpp:105] Iteration 40100, lr = 0.01
I1001 11:19:57.860791  5182 solver.cpp:218] Iteration 40200 (19.0572 iter/s, 5.24735s/100 iters), loss = 0.168627
I1001 11:19:57.860949  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168627 (* 1 = 0.168627 loss)
I1001 11:19:57.860955  5182 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I1001 11:20:03.120590  5182 solver.cpp:218] Iteration 40300 (19.0127 iter/s, 5.25964s/100 iters), loss = 0.118457
I1001 11:20:03.120622  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118457 (* 1 = 0.118457 loss)
I1001 11:20:03.120630  5182 sgd_solver.cpp:105] Iteration 40300, lr = 0.01
I1001 11:20:08.370661  5182 solver.cpp:218] Iteration 40400 (19.0475 iter/s, 5.25002s/100 iters), loss = 0.0932802
I1001 11:20:08.370692  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0932798 (* 1 = 0.0932798 loss)
I1001 11:20:08.370700  5182 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I1001 11:20:13.361438  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:20:13.572809  5182 solver.cpp:330] Iteration 40500, Testing net (#0)
I1001 11:20:14.766852  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:20:14.818099  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8929
I1001 11:20:14.818125  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.316409 (* 1 = 0.316409 loss)
I1001 11:20:14.873041  5182 solver.cpp:218] Iteration 40500 (15.3791 iter/s, 6.50233s/100 iters), loss = 0.0963862
I1001 11:20:14.873078  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0963859 (* 1 = 0.0963859 loss)
I1001 11:20:14.873087  5182 sgd_solver.cpp:105] Iteration 40500, lr = 0.01
I1001 11:20:20.127233  5182 solver.cpp:218] Iteration 40600 (19.0326 iter/s, 5.25414s/100 iters), loss = 0.147955
I1001 11:20:20.127274  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.147955 (* 1 = 0.147955 loss)
I1001 11:20:20.127279  5182 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I1001 11:20:25.390120  5182 solver.cpp:218] Iteration 40700 (19.0012 iter/s, 5.26283s/100 iters), loss = 0.0958532
I1001 11:20:25.390157  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0958528 (* 1 = 0.0958528 loss)
I1001 11:20:25.390163  5182 sgd_solver.cpp:105] Iteration 40700, lr = 0.01
I1001 11:20:30.645695  5182 solver.cpp:218] Iteration 40800 (19.0276 iter/s, 5.25552s/100 iters), loss = 0.149645
I1001 11:20:30.645866  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149644 (* 1 = 0.149644 loss)
I1001 11:20:30.645874  5182 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I1001 11:20:35.898980  5182 solver.cpp:218] Iteration 40900 (19.0364 iter/s, 5.25309s/100 iters), loss = 0.0727552
I1001 11:20:35.899014  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0727549 (* 1 = 0.0727549 loss)
I1001 11:20:35.899021  5182 sgd_solver.cpp:105] Iteration 40900, lr = 0.01
I1001 11:20:40.885987  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:20:41.096526  5182 solver.cpp:330] Iteration 41000, Testing net (#0)
I1001 11:20:42.293681  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:20:42.344007  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8938
I1001 11:20:42.344040  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.306621 (* 1 = 0.306621 loss)
I1001 11:20:42.396200  5182 solver.cpp:218] Iteration 41000 (15.3913 iter/s, 6.49717s/100 iters), loss = 0.077757
I1001 11:20:42.396229  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0777567 (* 1 = 0.0777567 loss)
I1001 11:20:42.396236  5182 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I1001 11:20:47.646023  5182 solver.cpp:218] Iteration 41100 (19.0484 iter/s, 5.24977s/100 iters), loss = 0.191133
I1001 11:20:47.646054  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191133 (* 1 = 0.191133 loss)
I1001 11:20:47.646071  5182 sgd_solver.cpp:105] Iteration 41100, lr = 0.01
I1001 11:20:52.904556  5182 solver.cpp:218] Iteration 41200 (19.0169 iter/s, 5.25849s/100 iters), loss = 0.0961753
I1001 11:20:52.904588  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.096175 (* 1 = 0.096175 loss)
I1001 11:20:52.904606  5182 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I1001 11:20:58.155752  5182 solver.cpp:218] Iteration 41300 (19.0435 iter/s, 5.25115s/100 iters), loss = 0.110678
I1001 11:20:58.155784  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110678 (* 1 = 0.110678 loss)
I1001 11:20:58.155802  5182 sgd_solver.cpp:105] Iteration 41300, lr = 0.01
I1001 11:21:03.415839  5182 solver.cpp:218] Iteration 41400 (19.0113 iter/s, 5.26004s/100 iters), loss = 0.0723053
I1001 11:21:03.415961  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.072305 (* 1 = 0.072305 loss)
I1001 11:21:03.415982  5182 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I1001 11:21:08.399550  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:21:08.610208  5182 solver.cpp:330] Iteration 41500, Testing net (#0)
I1001 11:21:09.810108  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:21:09.860260  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8926
I1001 11:21:09.860294  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.311559 (* 1 = 0.311559 loss)
I1001 11:21:09.913257  5182 solver.cpp:218] Iteration 41500 (15.391 iter/s, 6.49729s/100 iters), loss = 0.0682742
I1001 11:21:09.913281  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0682739 (* 1 = 0.0682739 loss)
I1001 11:21:09.913287  5182 sgd_solver.cpp:105] Iteration 41500, lr = 0.01
I1001 11:21:15.171967  5182 solver.cpp:218] Iteration 41600 (19.0162 iter/s, 5.25866s/100 iters), loss = 0.0905678
I1001 11:21:15.171999  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0905675 (* 1 = 0.0905675 loss)
I1001 11:21:15.172015  5182 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I1001 11:21:20.418656  5182 solver.cpp:218] Iteration 41700 (19.0598 iter/s, 5.24664s/100 iters), loss = 0.133049
I1001 11:21:20.418695  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133049 (* 1 = 0.133049 loss)
I1001 11:21:20.418701  5182 sgd_solver.cpp:105] Iteration 41700, lr = 0.01
I1001 11:21:25.672336  5182 solver.cpp:218] Iteration 41800 (19.0345 iter/s, 5.25362s/100 iters), loss = 0.0837929
I1001 11:21:25.672365  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0837926 (* 1 = 0.0837926 loss)
I1001 11:21:25.672371  5182 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I1001 11:21:30.921185  5182 solver.cpp:218] Iteration 41900 (19.052 iter/s, 5.2488s/100 iters), loss = 0.110624
I1001 11:21:30.921214  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110624 (* 1 = 0.110624 loss)
I1001 11:21:30.921221  5182 sgd_solver.cpp:105] Iteration 41900, lr = 0.01
I1001 11:21:35.910771  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:21:36.122581  5182 solver.cpp:330] Iteration 42000, Testing net (#0)
I1001 11:21:37.313133  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:21:37.363487  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8877
I1001 11:21:37.363512  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.322713 (* 1 = 0.322713 loss)
I1001 11:21:37.416450  5182 solver.cpp:218] Iteration 42000 (15.396 iter/s, 6.49521s/100 iters), loss = 0.0824814
I1001 11:21:37.416478  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0824812 (* 1 = 0.0824812 loss)
I1001 11:21:37.416487  5182 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I1001 11:21:42.674208  5182 solver.cpp:218] Iteration 42100 (19.0197 iter/s, 5.25771s/100 iters), loss = 0.107692
I1001 11:21:42.674240  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107692 (* 1 = 0.107692 loss)
I1001 11:21:42.674248  5182 sgd_solver.cpp:105] Iteration 42100, lr = 0.01
I1001 11:21:47.931519  5182 solver.cpp:218] Iteration 42200 (19.0213 iter/s, 5.25726s/100 iters), loss = 0.136062
I1001 11:21:47.931552  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.136062 (* 1 = 0.136062 loss)
I1001 11:21:47.931560  5182 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I1001 11:21:53.181602  5182 solver.cpp:218] Iteration 42300 (19.0475 iter/s, 5.25003s/100 iters), loss = 0.0711815
I1001 11:21:53.181641  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0711813 (* 1 = 0.0711813 loss)
I1001 11:21:53.181648  5182 sgd_solver.cpp:105] Iteration 42300, lr = 0.01
I1001 11:21:58.443298  5182 solver.cpp:218] Iteration 42400 (19.0055 iter/s, 5.26164s/100 iters), loss = 0.102434
I1001 11:21:58.443338  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.102434 (* 1 = 0.102434 loss)
I1001 11:21:58.443344  5182 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I1001 11:22:03.435622  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:22:03.646673  5182 solver.cpp:330] Iteration 42500, Testing net (#0)
I1001 11:22:04.836449  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:22:04.886550  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8957
I1001 11:22:04.886585  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.305198 (* 1 = 0.305198 loss)
I1001 11:22:04.939018  5182 solver.cpp:218] Iteration 42500 (15.3949 iter/s, 6.49566s/100 iters), loss = 0.0689701
I1001 11:22:04.939043  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0689699 (* 1 = 0.0689699 loss)
I1001 11:22:04.939049  5182 sgd_solver.cpp:105] Iteration 42500, lr = 0.01
I1001 11:22:10.187417  5182 solver.cpp:218] Iteration 42600 (19.0536 iter/s, 5.24835s/100 iters), loss = 0.0894157
I1001 11:22:10.187572  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0894155 (* 1 = 0.0894155 loss)
I1001 11:22:10.187578  5182 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I1001 11:22:15.441170  5182 solver.cpp:218] Iteration 42700 (19.0346 iter/s, 5.25358s/100 iters), loss = 0.121082
I1001 11:22:15.441205  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121082 (* 1 = 0.121082 loss)
I1001 11:22:15.441212  5182 sgd_solver.cpp:105] Iteration 42700, lr = 0.01
I1001 11:22:20.681287  5182 solver.cpp:218] Iteration 42800 (19.0838 iter/s, 5.24006s/100 iters), loss = 0.0571518
I1001 11:22:20.681318  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0571516 (* 1 = 0.0571516 loss)
I1001 11:22:20.681324  5182 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I1001 11:22:25.936344  5182 solver.cpp:218] Iteration 42900 (19.0295 iter/s, 5.25501s/100 iters), loss = 0.0871035
I1001 11:22:25.936385  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0871033 (* 1 = 0.0871033 loss)
I1001 11:22:25.936391  5182 sgd_solver.cpp:105] Iteration 42900, lr = 0.01
I1001 11:22:30.928589  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:22:31.138731  5182 solver.cpp:330] Iteration 43000, Testing net (#0)
I1001 11:22:32.327620  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:22:32.377812  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8971
I1001 11:22:32.377837  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.308554 (* 1 = 0.308554 loss)
I1001 11:22:32.430665  5182 solver.cpp:218] Iteration 43000 (15.3982 iter/s, 6.49426s/100 iters), loss = 0.0652532
I1001 11:22:32.430691  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.065253 (* 1 = 0.065253 loss)
I1001 11:22:32.430698  5182 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I1001 11:22:37.685989  5182 solver.cpp:218] Iteration 43100 (19.0285 iter/s, 5.25528s/100 iters), loss = 0.138428
I1001 11:22:37.686028  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138428 (* 1 = 0.138428 loss)
I1001 11:22:37.686034  5182 sgd_solver.cpp:105] Iteration 43100, lr = 0.01
I1001 11:22:42.943986  5182 solver.cpp:218] Iteration 43200 (19.0189 iter/s, 5.25793s/100 iters), loss = 0.0932887
I1001 11:22:42.944125  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0932885 (* 1 = 0.0932885 loss)
I1001 11:22:42.944133  5182 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I1001 11:22:48.201401  5182 solver.cpp:218] Iteration 43300 (19.0213 iter/s, 5.25727s/100 iters), loss = 0.0598071
I1001 11:22:48.201442  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0598069 (* 1 = 0.0598069 loss)
I1001 11:22:48.201448  5182 sgd_solver.cpp:105] Iteration 43300, lr = 0.01
I1001 11:22:53.445852  5182 solver.cpp:218] Iteration 43400 (19.068 iter/s, 5.24439s/100 iters), loss = 0.0410893
I1001 11:22:53.445880  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.041089 (* 1 = 0.041089 loss)
I1001 11:22:53.445886  5182 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I1001 11:22:58.436328  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:22:58.646924  5182 solver.cpp:330] Iteration 43500, Testing net (#0)
I1001 11:22:59.838590  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:22:59.889575  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8973
I1001 11:22:59.889601  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.306689 (* 1 = 0.306689 loss)
I1001 11:22:59.943348  5182 solver.cpp:218] Iteration 43500 (15.3907 iter/s, 6.49745s/100 iters), loss = 0.0514502
I1001 11:22:59.943380  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.05145 (* 1 = 0.05145 loss)
I1001 11:22:59.943387  5182 sgd_solver.cpp:105] Iteration 43500, lr = 0.01
I1001 11:23:05.192369  5182 solver.cpp:218] Iteration 43600 (19.0514 iter/s, 5.24896s/100 iters), loss = 0.100534
I1001 11:23:05.192399  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100534 (* 1 = 0.100534 loss)
I1001 11:23:05.192415  5182 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I1001 11:23:10.448202  5182 solver.cpp:218] Iteration 43700 (19.0267 iter/s, 5.25578s/100 iters), loss = 0.107468
I1001 11:23:10.448231  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107468 (* 1 = 0.107468 loss)
I1001 11:23:10.448237  5182 sgd_solver.cpp:105] Iteration 43700, lr = 0.01
I1001 11:23:15.713310  5182 solver.cpp:218] Iteration 43800 (18.9932 iter/s, 5.26506s/100 iters), loss = 0.0711278
I1001 11:23:15.713452  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0711276 (* 1 = 0.0711276 loss)
I1001 11:23:15.713470  5182 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I1001 11:23:20.973806  5182 solver.cpp:218] Iteration 43900 (19.0102 iter/s, 5.26033s/100 iters), loss = 0.062867
I1001 11:23:20.973852  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0628668 (* 1 = 0.0628668 loss)
I1001 11:23:20.973860  5182 sgd_solver.cpp:105] Iteration 43900, lr = 0.01
I1001 11:23:25.963042  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:23:26.174535  5182 solver.cpp:330] Iteration 44000, Testing net (#0)
I1001 11:23:27.373798  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:23:27.423902  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9003
I1001 11:23:27.423936  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.303067 (* 1 = 0.303067 loss)
I1001 11:23:27.476704  5182 solver.cpp:218] Iteration 44000 (15.378 iter/s, 6.5028s/100 iters), loss = 0.0750049
I1001 11:23:27.476730  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0750047 (* 1 = 0.0750047 loss)
I1001 11:23:27.476737  5182 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I1001 11:23:32.717921  5182 solver.cpp:218] Iteration 44100 (19.0797 iter/s, 5.24117s/100 iters), loss = 0.0783857
I1001 11:23:32.717955  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0783855 (* 1 = 0.0783855 loss)
I1001 11:23:32.717962  5182 sgd_solver.cpp:105] Iteration 44100, lr = 0.01
I1001 11:23:37.969194  5182 solver.cpp:218] Iteration 44200 (19.0433 iter/s, 5.25118s/100 iters), loss = 0.13491
I1001 11:23:37.969223  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.13491 (* 1 = 0.13491 loss)
I1001 11:23:37.969230  5182 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I1001 11:23:43.221302  5182 solver.cpp:218] Iteration 44300 (19.0402 iter/s, 5.25206s/100 iters), loss = 0.0640129
I1001 11:23:43.221343  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0640127 (* 1 = 0.0640127 loss)
I1001 11:23:43.221349  5182 sgd_solver.cpp:105] Iteration 44300, lr = 0.01
I1001 11:23:48.476488  5182 solver.cpp:218] Iteration 44400 (19.029 iter/s, 5.25512s/100 iters), loss = 0.0499609
I1001 11:23:48.476645  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0499607 (* 1 = 0.0499607 loss)
I1001 11:23:48.476665  5182 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I1001 11:23:53.452985  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:23:53.662504  5182 solver.cpp:330] Iteration 44500, Testing net (#0)
I1001 11:23:54.861994  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:23:54.912202  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8958
I1001 11:23:54.912237  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.321363 (* 1 = 0.321363 loss)
I1001 11:23:54.964820  5182 solver.cpp:218] Iteration 44500 (15.4127 iter/s, 6.48816s/100 iters), loss = 0.0510012
I1001 11:23:54.964849  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0510009 (* 1 = 0.0510009 loss)
I1001 11:23:54.964856  5182 sgd_solver.cpp:105] Iteration 44500, lr = 0.01
I1001 11:24:00.229272  5182 solver.cpp:218] Iteration 44600 (18.9955 iter/s, 5.2644s/100 iters), loss = 0.108845
I1001 11:24:00.229305  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108845 (* 1 = 0.108845 loss)
I1001 11:24:00.229311  5182 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I1001 11:24:05.479311  5182 solver.cpp:218] Iteration 44700 (19.0477 iter/s, 5.24998s/100 iters), loss = 0.105089
I1001 11:24:05.479341  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105089 (* 1 = 0.105089 loss)
I1001 11:24:05.479346  5182 sgd_solver.cpp:105] Iteration 44700, lr = 0.01
I1001 11:24:10.737745  5182 solver.cpp:218] Iteration 44800 (19.0173 iter/s, 5.25838s/100 iters), loss = 0.0508754
I1001 11:24:10.737772  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0508751 (* 1 = 0.0508751 loss)
I1001 11:24:10.737778  5182 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I1001 11:24:15.998050  5182 solver.cpp:218] Iteration 44900 (19.0105 iter/s, 5.26026s/100 iters), loss = 0.0456406
I1001 11:24:15.998078  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0456403 (* 1 = 0.0456403 loss)
I1001 11:24:15.998083  5182 sgd_solver.cpp:105] Iteration 44900, lr = 0.01
I1001 11:24:20.993358  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:24:21.204043  5182 solver.cpp:330] Iteration 45000, Testing net (#0)
I1001 11:24:22.395740  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:24:22.446111  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.899
I1001 11:24:22.446135  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.311115 (* 1 = 0.311115 loss)
I1001 11:24:22.498831  5182 solver.cpp:218] Iteration 45000 (15.3829 iter/s, 6.50073s/100 iters), loss = 0.0331937
I1001 11:24:22.498857  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0331935 (* 1 = 0.0331935 loss)
I1001 11:24:22.498864  5182 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I1001 11:24:27.751150  5182 solver.cpp:218] Iteration 45100 (19.0394 iter/s, 5.25227s/100 iters), loss = 0.146447
I1001 11:24:27.751180  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.146447 (* 1 = 0.146447 loss)
I1001 11:24:27.751186  5182 sgd_solver.cpp:105] Iteration 45100, lr = 0.01
I1001 11:24:33.003159  5182 solver.cpp:218] Iteration 45200 (19.0405 iter/s, 5.25195s/100 iters), loss = 0.0673132
I1001 11:24:33.003192  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.067313 (* 1 = 0.067313 loss)
I1001 11:24:33.003199  5182 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I1001 11:24:38.247362  5182 solver.cpp:218] Iteration 45300 (19.0689 iter/s, 5.24415s/100 iters), loss = 0.111284
I1001 11:24:38.247391  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111283 (* 1 = 0.111283 loss)
I1001 11:24:38.247397  5182 sgd_solver.cpp:105] Iteration 45300, lr = 0.01
I1001 11:24:43.498567  5182 solver.cpp:218] Iteration 45400 (19.0434 iter/s, 5.25116s/100 iters), loss = 0.0462767
I1001 11:24:43.498597  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0462765 (* 1 = 0.0462765 loss)
I1001 11:24:43.498603  5182 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I1001 11:24:48.494163  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:24:48.704267  5182 solver.cpp:330] Iteration 45500, Testing net (#0)
I1001 11:24:49.894450  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:24:49.944622  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9021
I1001 11:24:49.944656  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.309522 (* 1 = 0.309522 loss)
I1001 11:24:49.997619  5182 solver.cpp:218] Iteration 45500 (15.387 iter/s, 6.499s/100 iters), loss = 0.04382
I1001 11:24:49.997645  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0438198 (* 1 = 0.0438198 loss)
I1001 11:24:49.997651  5182 sgd_solver.cpp:105] Iteration 45500, lr = 0.01
I1001 11:24:55.251708  5182 solver.cpp:218] Iteration 45600 (19.033 iter/s, 5.25404s/100 iters), loss = 0.170441
I1001 11:24:55.251844  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170441 (* 1 = 0.170441 loss)
I1001 11:24:55.251863  5182 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I1001 11:25:00.508141  5182 solver.cpp:218] Iteration 45700 (19.0248 iter/s, 5.25629s/100 iters), loss = 0.0494141
I1001 11:25:00.508169  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0494139 (* 1 = 0.0494139 loss)
I1001 11:25:00.508177  5182 sgd_solver.cpp:105] Iteration 45700, lr = 0.01
I1001 11:25:05.752732  5182 solver.cpp:218] Iteration 45800 (19.0675 iter/s, 5.24454s/100 iters), loss = 0.0411458
I1001 11:25:05.752765  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0411455 (* 1 = 0.0411455 loss)
I1001 11:25:05.752773  5182 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I1001 11:25:11.003201  5182 solver.cpp:218] Iteration 45900 (19.0461 iter/s, 5.25041s/100 iters), loss = 0.0437329
I1001 11:25:11.003239  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0437327 (* 1 = 0.0437327 loss)
I1001 11:25:11.003245  5182 sgd_solver.cpp:105] Iteration 45900, lr = 0.01
I1001 11:25:15.996491  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:25:16.206555  5182 solver.cpp:330] Iteration 46000, Testing net (#0)
I1001 11:25:17.395750  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:25:17.446161  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9012
I1001 11:25:17.446195  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.30898 (* 1 = 0.30898 loss)
I1001 11:25:17.498579  5182 solver.cpp:218] Iteration 46000 (15.3957 iter/s, 6.49532s/100 iters), loss = 0.063152
I1001 11:25:17.498603  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0631518 (* 1 = 0.0631518 loss)
I1001 11:25:17.498610  5182 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I1001 11:25:22.754217  5182 solver.cpp:218] Iteration 46100 (19.0273 iter/s, 5.25559s/100 iters), loss = 0.0861214
I1001 11:25:22.754258  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0861212 (* 1 = 0.0861212 loss)
I1001 11:25:22.754264  5182 sgd_solver.cpp:105] Iteration 46100, lr = 0.01
I1001 11:25:28.009872  5182 solver.cpp:218] Iteration 46200 (19.0273 iter/s, 5.25559s/100 iters), loss = 0.0556959
I1001 11:25:28.009974  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0556956 (* 1 = 0.0556956 loss)
I1001 11:25:28.009984  5182 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I1001 11:25:33.266451  5182 solver.cpp:218] Iteration 46300 (19.0242 iter/s, 5.25646s/100 iters), loss = 0.0634938
I1001 11:25:33.266491  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0634935 (* 1 = 0.0634935 loss)
I1001 11:25:33.266497  5182 sgd_solver.cpp:105] Iteration 46300, lr = 0.01
I1001 11:25:38.514058  5182 solver.cpp:218] Iteration 46400 (19.0565 iter/s, 5.24755s/100 iters), loss = 0.0190524
I1001 11:25:38.514099  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0190522 (* 1 = 0.0190522 loss)
I1001 11:25:38.514106  5182 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I1001 11:25:43.506152  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:25:43.716595  5182 solver.cpp:330] Iteration 46500, Testing net (#0)
I1001 11:25:44.910573  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:25:44.960995  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9081
I1001 11:25:44.961020  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.285299 (* 1 = 0.285299 loss)
I1001 11:25:45.014600  5182 solver.cpp:218] Iteration 46500 (15.3835 iter/s, 6.50048s/100 iters), loss = 0.043997
I1001 11:25:45.014633  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0439967 (* 1 = 0.0439967 loss)
I1001 11:25:45.014642  5182 sgd_solver.cpp:105] Iteration 46500, lr = 0.01
I1001 11:25:50.261624  5182 solver.cpp:218] Iteration 46600 (19.0586 iter/s, 5.24697s/100 iters), loss = 0.0657893
I1001 11:25:50.261664  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0657891 (* 1 = 0.0657891 loss)
I1001 11:25:50.261672  5182 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I1001 11:25:55.518411  5182 solver.cpp:218] Iteration 46700 (19.0232 iter/s, 5.25673s/100 iters), loss = 0.0552949
I1001 11:25:55.518451  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0552946 (* 1 = 0.0552946 loss)
I1001 11:25:55.518458  5182 sgd_solver.cpp:105] Iteration 46700, lr = 0.01
I1001 11:26:00.775625  5182 solver.cpp:218] Iteration 46800 (19.0217 iter/s, 5.25715s/100 iters), loss = 0.0798348
I1001 11:26:00.775753  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0798346 (* 1 = 0.0798346 loss)
I1001 11:26:00.775760  5182 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I1001 11:26:06.027304  5182 solver.cpp:218] Iteration 46900 (19.0421 iter/s, 5.25153s/100 iters), loss = 0.0451051
I1001 11:26:06.027336  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0451049 (* 1 = 0.0451049 loss)
I1001 11:26:06.027343  5182 sgd_solver.cpp:105] Iteration 46900, lr = 0.01
I1001 11:26:11.011190  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:26:11.221633  5182 solver.cpp:330] Iteration 47000, Testing net (#0)
I1001 11:26:12.417915  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:26:12.468204  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.895
I1001 11:26:12.468237  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.339103 (* 1 = 0.339103 loss)
I1001 11:26:12.520747  5182 solver.cpp:218] Iteration 47000 (15.4003 iter/s, 6.49339s/100 iters), loss = 0.0377666
I1001 11:26:12.520777  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0377664 (* 1 = 0.0377664 loss)
I1001 11:26:12.520784  5182 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I1001 11:26:17.768188  5182 solver.cpp:218] Iteration 47100 (19.0571 iter/s, 5.24739s/100 iters), loss = 0.122398
I1001 11:26:17.768219  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.122397 (* 1 = 0.122397 loss)
I1001 11:26:17.768226  5182 sgd_solver.cpp:105] Iteration 47100, lr = 0.01
I1001 11:26:23.020259  5182 solver.cpp:218] Iteration 47200 (19.0403 iter/s, 5.25201s/100 iters), loss = 0.0530698
I1001 11:26:23.020300  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0530696 (* 1 = 0.0530696 loss)
I1001 11:26:23.020306  5182 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I1001 11:26:28.277256  5182 solver.cpp:218] Iteration 47300 (19.0225 iter/s, 5.25694s/100 iters), loss = 0.0894499
I1001 11:26:28.277312  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0894498 (* 1 = 0.0894498 loss)
I1001 11:26:28.277320  5182 sgd_solver.cpp:105] Iteration 47300, lr = 0.01
I1001 11:26:33.540403  5182 solver.cpp:218] Iteration 47400 (19.0003 iter/s, 5.26307s/100 iters), loss = 0.0300474
I1001 11:26:33.540549  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0300472 (* 1 = 0.0300472 loss)
I1001 11:26:33.540556  5182 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I1001 11:26:38.520813  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:26:38.731591  5182 solver.cpp:330] Iteration 47500, Testing net (#0)
I1001 11:26:39.930248  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:26:39.980494  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8965
I1001 11:26:39.980516  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.325515 (* 1 = 0.325515 loss)
I1001 11:26:40.033085  5182 solver.cpp:218] Iteration 47500 (15.4023 iter/s, 6.49252s/100 iters), loss = 0.0396247
I1001 11:26:40.033113  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0396245 (* 1 = 0.0396245 loss)
I1001 11:26:40.033118  5182 sgd_solver.cpp:105] Iteration 47500, lr = 0.01
I1001 11:26:45.287518  5182 solver.cpp:218] Iteration 47600 (19.0317 iter/s, 5.25438s/100 iters), loss = 0.0737989
I1001 11:26:45.287556  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0737986 (* 1 = 0.0737986 loss)
I1001 11:26:45.287562  5182 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I1001 11:26:50.534533  5182 solver.cpp:218] Iteration 47700 (19.0587 iter/s, 5.24695s/100 iters), loss = 0.0474736
I1001 11:26:50.534562  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0474734 (* 1 = 0.0474734 loss)
I1001 11:26:50.534569  5182 sgd_solver.cpp:105] Iteration 47700, lr = 0.01
I1001 11:26:55.787356  5182 solver.cpp:218] Iteration 47800 (19.0376 iter/s, 5.25277s/100 iters), loss = 0.0564668
I1001 11:26:55.787397  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0564666 (* 1 = 0.0564666 loss)
I1001 11:26:55.787403  5182 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I1001 11:27:01.041240  5182 solver.cpp:218] Iteration 47900 (19.0338 iter/s, 5.25382s/100 iters), loss = 0.0592851
I1001 11:27:01.041270  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0592849 (* 1 = 0.0592849 loss)
I1001 11:27:01.041276  5182 sgd_solver.cpp:105] Iteration 47900, lr = 0.01
I1001 11:27:06.030360  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:27:06.240622  5182 solver.cpp:330] Iteration 48000, Testing net (#0)
I1001 11:27:07.428961  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:27:07.479143  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8838
I1001 11:27:07.479178  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.381866 (* 1 = 0.381866 loss)
I1001 11:27:07.531707  5182 solver.cpp:218] Iteration 48000 (15.4073 iter/s, 6.49042s/100 iters), loss = 0.0408218
I1001 11:27:07.531731  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0408215 (* 1 = 0.0408215 loss)
I1001 11:27:07.531738  5182 sgd_solver.cpp:105] Iteration 48000, lr = 0.01
I1001 11:27:12.786586  5182 solver.cpp:218] Iteration 48100 (19.0301 iter/s, 5.25483s/100 iters), loss = 0.0552055
I1001 11:27:12.786626  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0552053 (* 1 = 0.0552053 loss)
I1001 11:27:12.786633  5182 sgd_solver.cpp:105] Iteration 48100, lr = 0.01
I1001 11:27:18.043952  5182 solver.cpp:218] Iteration 48200 (19.0212 iter/s, 5.2573s/100 iters), loss = 0.0632416
I1001 11:27:18.043983  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0632413 (* 1 = 0.0632413 loss)
I1001 11:27:18.043990  5182 sgd_solver.cpp:105] Iteration 48200, lr = 0.01
I1001 11:27:23.290038  5182 solver.cpp:218] Iteration 48300 (19.062 iter/s, 5.24603s/100 iters), loss = 0.0589964
I1001 11:27:23.290067  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0589961 (* 1 = 0.0589961 loss)
I1001 11:27:23.290083  5182 sgd_solver.cpp:105] Iteration 48300, lr = 0.01
I1001 11:27:28.550096  5182 solver.cpp:218] Iteration 48400 (19.0114 iter/s, 5.26001s/100 iters), loss = 0.0710896
I1001 11:27:28.550127  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0710894 (* 1 = 0.0710894 loss)
I1001 11:27:28.550132  5182 sgd_solver.cpp:105] Iteration 48400, lr = 0.01
I1001 11:27:33.548641  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:27:33.758329  5182 solver.cpp:330] Iteration 48500, Testing net (#0)
I1001 11:27:34.948762  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:27:34.999312  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8974
I1001 11:27:34.999346  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.345348 (* 1 = 0.345348 loss)
I1001 11:27:35.051693  5182 solver.cpp:218] Iteration 48500 (15.381 iter/s, 6.50155s/100 iters), loss = 0.0303522
I1001 11:27:35.051726  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.030352 (* 1 = 0.030352 loss)
I1001 11:27:35.051733  5182 sgd_solver.cpp:105] Iteration 48500, lr = 0.01
I1001 11:27:40.306573  5182 solver.cpp:218] Iteration 48600 (19.0301 iter/s, 5.25483s/100 iters), loss = 0.0826858
I1001 11:27:40.306668  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0826856 (* 1 = 0.0826856 loss)
I1001 11:27:40.306685  5182 sgd_solver.cpp:105] Iteration 48600, lr = 0.01
I1001 11:27:45.559216  5182 solver.cpp:218] Iteration 48700 (19.0385 iter/s, 5.25253s/100 iters), loss = 0.0999284
I1001 11:27:45.559244  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0999282 (* 1 = 0.0999282 loss)
I1001 11:27:45.559250  5182 sgd_solver.cpp:105] Iteration 48700, lr = 0.01
I1001 11:27:50.803006  5182 solver.cpp:218] Iteration 48800 (19.0704 iter/s, 5.24374s/100 iters), loss = 0.0537067
I1001 11:27:50.803051  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0537066 (* 1 = 0.0537066 loss)
I1001 11:27:50.803058  5182 sgd_solver.cpp:105] Iteration 48800, lr = 0.01
I1001 11:27:56.061492  5182 solver.cpp:218] Iteration 48900 (19.0172 iter/s, 5.25839s/100 iters), loss = 0.0541111
I1001 11:27:56.061523  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.054111 (* 1 = 0.054111 loss)
I1001 11:27:56.061529  5182 sgd_solver.cpp:105] Iteration 48900, lr = 0.01
I1001 11:28:01.058328  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:28:01.268985  5182 solver.cpp:330] Iteration 49000, Testing net (#0)
I1001 11:28:02.458504  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:28:02.508656  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9018
I1001 11:28:02.508680  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.325808 (* 1 = 0.325808 loss)
I1001 11:28:02.561116  5182 solver.cpp:218] Iteration 49000 (15.3856 iter/s, 6.49957s/100 iters), loss = 0.0486297
I1001 11:28:02.561143  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0486296 (* 1 = 0.0486296 loss)
I1001 11:28:02.561149  5182 sgd_solver.cpp:105] Iteration 49000, lr = 0.01
I1001 11:28:07.806607  5182 solver.cpp:218] Iteration 49100 (19.0642 iter/s, 5.24544s/100 iters), loss = 0.0966303
I1001 11:28:07.806637  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0966301 (* 1 = 0.0966301 loss)
I1001 11:28:07.806643  5182 sgd_solver.cpp:105] Iteration 49100, lr = 0.01
I1001 11:28:13.070456  5182 solver.cpp:218] Iteration 49200 (18.9977 iter/s, 5.2638s/100 iters), loss = 0.122874
I1001 11:28:13.070549  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.122874 (* 1 = 0.122874 loss)
I1001 11:28:13.070556  5182 sgd_solver.cpp:105] Iteration 49200, lr = 0.01
I1001 11:28:18.337209  5182 solver.cpp:218] Iteration 49300 (18.9874 iter/s, 5.26664s/100 iters), loss = 0.0281902
I1001 11:28:18.337250  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.02819 (* 1 = 0.02819 loss)
I1001 11:28:18.337256  5182 sgd_solver.cpp:105] Iteration 49300, lr = 0.01
I1001 11:28:23.588876  5182 solver.cpp:218] Iteration 49400 (19.0418 iter/s, 5.2516s/100 iters), loss = 0.0253542
I1001 11:28:23.588918  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.025354 (* 1 = 0.025354 loss)
I1001 11:28:23.588925  5182 sgd_solver.cpp:105] Iteration 49400, lr = 0.01
I1001 11:28:28.588085  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:28:28.798451  5182 solver.cpp:330] Iteration 49500, Testing net (#0)
I1001 11:28:29.992733  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:28:30.043584  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9031
I1001 11:28:30.043609  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.323444 (* 1 = 0.323444 loss)
I1001 11:28:30.096256  5182 solver.cpp:218] Iteration 49500 (15.3673 iter/s, 6.50731s/100 iters), loss = 0.0269612
I1001 11:28:30.096288  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.026961 (* 1 = 0.026961 loss)
I1001 11:28:30.096295  5182 sgd_solver.cpp:105] Iteration 49500, lr = 0.01
I1001 11:28:35.346405  5182 solver.cpp:218] Iteration 49600 (19.0473 iter/s, 5.2501s/100 iters), loss = 0.0495873
I1001 11:28:35.346434  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0495872 (* 1 = 0.0495872 loss)
I1001 11:28:35.346441  5182 sgd_solver.cpp:105] Iteration 49600, lr = 0.01
I1001 11:28:40.603459  5182 solver.cpp:218] Iteration 49700 (19.0222 iter/s, 5.257s/100 iters), loss = 0.0312606
I1001 11:28:40.603490  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0312604 (* 1 = 0.0312604 loss)
I1001 11:28:40.603497  5182 sgd_solver.cpp:105] Iteration 49700, lr = 0.01
I1001 11:28:45.862232  5182 solver.cpp:218] Iteration 49800 (19.016 iter/s, 5.25872s/100 iters), loss = 0.031448
I1001 11:28:45.862341  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0314479 (* 1 = 0.0314479 loss)
I1001 11:28:45.862360  5182 sgd_solver.cpp:105] Iteration 49800, lr = 0.01
I1001 11:28:51.124502  5182 solver.cpp:218] Iteration 49900 (19.0036 iter/s, 5.26215s/100 iters), loss = 0.0159164
I1001 11:28:51.124534  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159163 (* 1 = 0.0159163 loss)
I1001 11:28:51.124552  5182 sgd_solver.cpp:105] Iteration 49900, lr = 0.01
I1001 11:28:56.111114  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:28:56.321069  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_50000.caffemodel
I1001 11:28:56.326030  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_50000.solverstate
I1001 11:28:56.327425  5182 solver.cpp:330] Iteration 50000, Testing net (#0)
I1001 11:28:57.525713  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:28:57.575971  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8976
I1001 11:28:57.575997  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.347387 (* 1 = 0.347387 loss)
I1001 11:28:57.628819  5182 solver.cpp:218] Iteration 50000 (15.3745 iter/s, 6.50427s/100 iters), loss = 0.0303945
I1001 11:28:57.628851  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0303943 (* 1 = 0.0303943 loss)
I1001 11:28:57.628860  5182 sgd_solver.cpp:105] Iteration 50000, lr = 0.01
I1001 11:29:02.877526  5182 solver.cpp:218] Iteration 50100 (19.0525 iter/s, 5.24866s/100 iters), loss = 0.0922229
I1001 11:29:02.877563  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0922227 (* 1 = 0.0922227 loss)
I1001 11:29:02.877573  5182 sgd_solver.cpp:105] Iteration 50100, lr = 0.01
I1001 11:29:08.124392  5182 solver.cpp:218] Iteration 50200 (19.0592 iter/s, 5.24681s/100 iters), loss = 0.062551
I1001 11:29:08.124433  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0625509 (* 1 = 0.0625509 loss)
I1001 11:29:08.124439  5182 sgd_solver.cpp:105] Iteration 50200, lr = 0.01
I1001 11:29:13.368424  5182 solver.cpp:218] Iteration 50300 (19.0695 iter/s, 5.24397s/100 iters), loss = 0.0496237
I1001 11:29:13.368455  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0496235 (* 1 = 0.0496235 loss)
I1001 11:29:13.368461  5182 sgd_solver.cpp:105] Iteration 50300, lr = 0.01
I1001 11:29:18.620270  5182 solver.cpp:218] Iteration 50400 (19.0411 iter/s, 5.2518s/100 iters), loss = 0.0201516
I1001 11:29:18.620383  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0201514 (* 1 = 0.0201514 loss)
I1001 11:29:18.620390  5182 sgd_solver.cpp:105] Iteration 50400, lr = 0.01
I1001 11:29:23.602774  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:29:23.814990  5182 solver.cpp:330] Iteration 50500, Testing net (#0)
I1001 11:29:25.008920  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:29:25.059319  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9014
I1001 11:29:25.059345  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329733 (* 1 = 0.329733 loss)
I1001 11:29:25.112174  5182 solver.cpp:218] Iteration 50500 (15.4041 iter/s, 6.49178s/100 iters), loss = 0.0455281
I1001 11:29:25.112200  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.045528 (* 1 = 0.045528 loss)
I1001 11:29:25.112206  5182 sgd_solver.cpp:105] Iteration 50500, lr = 0.01
I1001 11:29:30.367964  5182 solver.cpp:218] Iteration 50600 (19.0268 iter/s, 5.25574s/100 iters), loss = 0.047094
I1001 11:29:30.367993  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0470938 (* 1 = 0.0470938 loss)
I1001 11:29:30.368000  5182 sgd_solver.cpp:105] Iteration 50600, lr = 0.01
I1001 11:29:35.615789  5182 solver.cpp:218] Iteration 50700 (19.0557 iter/s, 5.24778s/100 iters), loss = 0.0208474
I1001 11:29:35.615828  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0208473 (* 1 = 0.0208473 loss)
I1001 11:29:35.615834  5182 sgd_solver.cpp:105] Iteration 50700, lr = 0.01
I1001 11:29:40.875461  5182 solver.cpp:218] Iteration 50800 (19.0128 iter/s, 5.25962s/100 iters), loss = 0.0822581
I1001 11:29:40.875490  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0822579 (* 1 = 0.0822579 loss)
I1001 11:29:40.875496  5182 sgd_solver.cpp:105] Iteration 50800, lr = 0.01
I1001 11:29:46.137434  5182 solver.cpp:218] Iteration 50900 (19.0045 iter/s, 5.26192s/100 iters), loss = 0.0425921
I1001 11:29:46.137462  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.042592 (* 1 = 0.042592 loss)
I1001 11:29:46.137478  5182 sgd_solver.cpp:105] Iteration 50900, lr = 0.01
I1001 11:29:51.132853  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:29:51.343051  5182 solver.cpp:330] Iteration 51000, Testing net (#0)
I1001 11:29:52.532250  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:29:52.582690  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8958
I1001 11:29:52.582725  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356175 (* 1 = 0.356175 loss)
I1001 11:29:52.635177  5182 solver.cpp:218] Iteration 51000 (15.3901 iter/s, 6.4977s/100 iters), loss = 0.0241508
I1001 11:29:52.635202  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0241506 (* 1 = 0.0241506 loss)
I1001 11:29:52.635218  5182 sgd_solver.cpp:105] Iteration 51000, lr = 0.01
I1001 11:29:57.895063  5182 solver.cpp:218] Iteration 51100 (19.012 iter/s, 5.25984s/100 iters), loss = 0.0660441
I1001 11:29:57.895092  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.066044 (* 1 = 0.066044 loss)
I1001 11:29:57.895098  5182 sgd_solver.cpp:105] Iteration 51100, lr = 0.01
I1001 11:30:03.154847  5182 solver.cpp:218] Iteration 51200 (19.0124 iter/s, 5.25973s/100 iters), loss = 0.0572708
I1001 11:30:03.154876  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0572707 (* 1 = 0.0572707 loss)
I1001 11:30:03.154882  5182 sgd_solver.cpp:105] Iteration 51200, lr = 0.01
I1001 11:30:08.405033  5182 solver.cpp:218] Iteration 51300 (19.0471 iter/s, 5.25014s/100 iters), loss = 0.0869701
I1001 11:30:08.405062  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.08697 (* 1 = 0.08697 loss)
I1001 11:30:08.405069  5182 sgd_solver.cpp:105] Iteration 51300, lr = 0.01
I1001 11:30:13.662986  5182 solver.cpp:218] Iteration 51400 (19.019 iter/s, 5.2579s/100 iters), loss = 0.0290759
I1001 11:30:13.663017  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0290758 (* 1 = 0.0290758 loss)
I1001 11:30:13.663022  5182 sgd_solver.cpp:105] Iteration 51400, lr = 0.01
I1001 11:30:18.659629  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:30:18.870326  5182 solver.cpp:330] Iteration 51500, Testing net (#0)
I1001 11:30:20.060575  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:30:20.110656  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8943
I1001 11:30:20.110690  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.368639 (* 1 = 0.368639 loss)
I1001 11:30:20.163367  5182 solver.cpp:218] Iteration 51500 (15.3838 iter/s, 6.50033s/100 iters), loss = 0.0216283
I1001 11:30:20.163395  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216281 (* 1 = 0.0216281 loss)
I1001 11:30:20.163401  5182 sgd_solver.cpp:105] Iteration 51500, lr = 0.01
I1001 11:30:25.423387  5182 solver.cpp:218] Iteration 51600 (19.0115 iter/s, 5.25997s/100 iters), loss = 0.0377739
I1001 11:30:25.423501  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0377737 (* 1 = 0.0377737 loss)
I1001 11:30:25.423519  5182 sgd_solver.cpp:105] Iteration 51600, lr = 0.01
I1001 11:30:30.682394  5182 solver.cpp:218] Iteration 51700 (19.0155 iter/s, 5.25888s/100 iters), loss = 0.0626823
I1001 11:30:30.682423  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0626822 (* 1 = 0.0626822 loss)
I1001 11:30:30.682430  5182 sgd_solver.cpp:105] Iteration 51700, lr = 0.01
I1001 11:30:35.942098  5182 solver.cpp:218] Iteration 51800 (19.0127 iter/s, 5.25965s/100 iters), loss = 0.0348893
I1001 11:30:35.942134  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0348892 (* 1 = 0.0348892 loss)
I1001 11:30:35.942142  5182 sgd_solver.cpp:105] Iteration 51800, lr = 0.01
I1001 11:30:41.192747  5182 solver.cpp:218] Iteration 51900 (19.0455 iter/s, 5.25059s/100 iters), loss = 0.0315304
I1001 11:30:41.192787  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0315302 (* 1 = 0.0315302 loss)
I1001 11:30:41.192795  5182 sgd_solver.cpp:105] Iteration 51900, lr = 0.01
I1001 11:30:46.179775  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:30:46.388895  5182 solver.cpp:330] Iteration 52000, Testing net (#0)
I1001 11:30:47.579005  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:30:47.629339  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8957
I1001 11:30:47.629374  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.360935 (* 1 = 0.360935 loss)
I1001 11:30:47.682056  5182 solver.cpp:218] Iteration 52000 (15.4101 iter/s, 6.48925s/100 iters), loss = 0.0356601
I1001 11:30:47.682080  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0356599 (* 1 = 0.0356599 loss)
I1001 11:30:47.682087  5182 sgd_solver.cpp:105] Iteration 52000, lr = 0.01
I1001 11:30:52.938103  5182 solver.cpp:218] Iteration 52100 (19.0259 iter/s, 5.256s/100 iters), loss = 0.0545896
I1001 11:30:52.938143  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0545894 (* 1 = 0.0545894 loss)
I1001 11:30:52.938149  5182 sgd_solver.cpp:105] Iteration 52100, lr = 0.01
I1001 11:30:58.191256  5182 solver.cpp:218] Iteration 52200 (19.0364 iter/s, 5.25309s/100 iters), loss = 0.0417417
I1001 11:30:58.191409  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0417416 (* 1 = 0.0417416 loss)
I1001 11:30:58.191417  5182 sgd_solver.cpp:105] Iteration 52200, lr = 0.01
I1001 11:31:03.449872  5182 solver.cpp:218] Iteration 52300 (19.017 iter/s, 5.25845s/100 iters), loss = 0.0250393
I1001 11:31:03.449913  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250391 (* 1 = 0.0250391 loss)
I1001 11:31:03.449918  5182 sgd_solver.cpp:105] Iteration 52300, lr = 0.01
I1001 11:31:08.696523  5182 solver.cpp:218] Iteration 52400 (19.06 iter/s, 5.24659s/100 iters), loss = 0.0222188
I1001 11:31:08.696552  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0222186 (* 1 = 0.0222186 loss)
I1001 11:31:08.696558  5182 sgd_solver.cpp:105] Iteration 52400, lr = 0.01
I1001 11:31:13.690381  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:31:13.900655  5182 solver.cpp:330] Iteration 52500, Testing net (#0)
I1001 11:31:15.101673  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:31:15.152101  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8994
I1001 11:31:15.152137  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.350155 (* 1 = 0.350155 loss)
I1001 11:31:15.204843  5182 solver.cpp:218] Iteration 52500 (15.3651 iter/s, 6.50827s/100 iters), loss = 0.038025
I1001 11:31:15.204876  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380249 (* 1 = 0.0380249 loss)
I1001 11:31:15.204883  5182 sgd_solver.cpp:105] Iteration 52500, lr = 0.01
I1001 11:31:20.458434  5182 solver.cpp:218] Iteration 52600 (19.0348 iter/s, 5.25354s/100 iters), loss = 0.0481231
I1001 11:31:20.458463  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0481229 (* 1 = 0.0481229 loss)
I1001 11:31:20.458469  5182 sgd_solver.cpp:105] Iteration 52600, lr = 0.01
I1001 11:31:25.716950  5182 solver.cpp:218] Iteration 52700 (19.017 iter/s, 5.25847s/100 iters), loss = 0.040259
I1001 11:31:25.716984  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0402589 (* 1 = 0.0402589 loss)
I1001 11:31:25.716991  5182 sgd_solver.cpp:105] Iteration 52700, lr = 0.01
I1001 11:31:30.974633  5182 solver.cpp:218] Iteration 52800 (19.02 iter/s, 5.25763s/100 iters), loss = 0.0310876
I1001 11:31:30.974797  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0310874 (* 1 = 0.0310874 loss)
I1001 11:31:30.974804  5182 sgd_solver.cpp:105] Iteration 52800, lr = 0.01
I1001 11:31:36.234042  5182 solver.cpp:218] Iteration 52900 (19.0142 iter/s, 5.25923s/100 iters), loss = 0.00851739
I1001 11:31:36.234082  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00851723 (* 1 = 0.00851723 loss)
I1001 11:31:36.234089  5182 sgd_solver.cpp:105] Iteration 52900, lr = 0.01
I1001 11:31:41.220998  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:31:41.430728  5182 solver.cpp:330] Iteration 53000, Testing net (#0)
I1001 11:31:42.626812  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:31:42.677120  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8971
I1001 11:31:42.677155  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.362863 (* 1 = 0.362863 loss)
I1001 11:31:42.730020  5182 solver.cpp:218] Iteration 53000 (15.3943 iter/s, 6.49591s/100 iters), loss = 0.0665082
I1001 11:31:42.730048  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0665081 (* 1 = 0.0665081 loss)
I1001 11:31:42.730054  5182 sgd_solver.cpp:105] Iteration 53000, lr = 0.01
I1001 11:31:47.985430  5182 solver.cpp:218] Iteration 53100 (19.0282 iter/s, 5.25535s/100 iters), loss = 0.0439132
I1001 11:31:47.985466  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0439131 (* 1 = 0.0439131 loss)
I1001 11:31:47.985472  5182 sgd_solver.cpp:105] Iteration 53100, lr = 0.01
I1001 11:31:53.230619  5182 solver.cpp:218] Iteration 53200 (19.0654 iter/s, 5.24509s/100 iters), loss = 0.0317066
I1001 11:31:53.230649  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0317065 (* 1 = 0.0317065 loss)
I1001 11:31:53.230656  5182 sgd_solver.cpp:105] Iteration 53200, lr = 0.01
I1001 11:31:58.483822  5182 solver.cpp:218] Iteration 53300 (19.0362 iter/s, 5.25315s/100 iters), loss = 0.0378969
I1001 11:31:58.483863  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0378968 (* 1 = 0.0378968 loss)
I1001 11:31:58.483870  5182 sgd_solver.cpp:105] Iteration 53300, lr = 0.01
I1001 11:32:03.738363  5182 solver.cpp:218] Iteration 53400 (19.0314 iter/s, 5.25448s/100 iters), loss = 0.0225938
I1001 11:32:03.738507  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0225936 (* 1 = 0.0225936 loss)
I1001 11:32:03.738518  5182 sgd_solver.cpp:105] Iteration 53400, lr = 0.01
I1001 11:32:08.725121  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:32:08.943383  5182 solver.cpp:330] Iteration 53500, Testing net (#0)
I1001 11:32:10.137326  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:32:10.187660  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9075
I1001 11:32:10.187686  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.330867 (* 1 = 0.330867 loss)
I1001 11:32:10.240566  5182 solver.cpp:218] Iteration 53500 (15.3798 iter/s, 6.50205s/100 iters), loss = 0.017626
I1001 11:32:10.240593  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0176259 (* 1 = 0.0176259 loss)
I1001 11:32:10.240602  5182 sgd_solver.cpp:105] Iteration 53500, lr = 0.01
I1001 11:32:15.497936  5182 solver.cpp:218] Iteration 53600 (19.0211 iter/s, 5.25732s/100 iters), loss = 0.0921576
I1001 11:32:15.497967  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0921574 (* 1 = 0.0921574 loss)
I1001 11:32:15.497987  5182 sgd_solver.cpp:105] Iteration 53600, lr = 0.01
I1001 11:32:20.745409  5182 solver.cpp:218] Iteration 53700 (19.057 iter/s, 5.24742s/100 iters), loss = 0.055094
I1001 11:32:20.745440  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0550939 (* 1 = 0.0550939 loss)
I1001 11:32:20.745447  5182 sgd_solver.cpp:105] Iteration 53700, lr = 0.01
I1001 11:32:26.001909  5182 solver.cpp:218] Iteration 53800 (19.0243 iter/s, 5.25645s/100 iters), loss = 0.0303486
I1001 11:32:26.001938  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0303485 (* 1 = 0.0303485 loss)
I1001 11:32:26.001945  5182 sgd_solver.cpp:105] Iteration 53800, lr = 0.01
I1001 11:32:31.257433  5182 solver.cpp:218] Iteration 53900 (19.0278 iter/s, 5.25547s/100 iters), loss = 0.0157241
I1001 11:32:31.257463  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015724 (* 1 = 0.015724 loss)
I1001 11:32:31.257480  5182 sgd_solver.cpp:105] Iteration 53900, lr = 0.01
I1001 11:32:36.247639  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:32:36.458415  5182 solver.cpp:330] Iteration 54000, Testing net (#0)
I1001 11:32:37.647893  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:32:37.698271  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9005
I1001 11:32:37.698305  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.344272 (* 1 = 0.344272 loss)
I1001 11:32:37.751207  5182 solver.cpp:218] Iteration 54000 (15.3995 iter/s, 6.49372s/100 iters), loss = 0.0484379
I1001 11:32:37.751232  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0484378 (* 1 = 0.0484378 loss)
I1001 11:32:37.751240  5182 sgd_solver.cpp:105] Iteration 54000, lr = 0.01
I1001 11:32:43.006460  5182 solver.cpp:218] Iteration 54100 (19.0287 iter/s, 5.25521s/100 iters), loss = 0.0809482
I1001 11:32:43.006490  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0809481 (* 1 = 0.0809481 loss)
I1001 11:32:43.006496  5182 sgd_solver.cpp:105] Iteration 54100, lr = 0.01
I1001 11:32:48.264205  5182 solver.cpp:218] Iteration 54200 (19.0198 iter/s, 5.25769s/100 iters), loss = 0.0396456
I1001 11:32:48.264236  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0396455 (* 1 = 0.0396455 loss)
I1001 11:32:48.264243  5182 sgd_solver.cpp:105] Iteration 54200, lr = 0.01
I1001 11:32:53.514037  5182 solver.cpp:218] Iteration 54300 (19.0484 iter/s, 5.24978s/100 iters), loss = 0.0304652
I1001 11:32:53.514067  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0304651 (* 1 = 0.0304651 loss)
I1001 11:32:53.514084  5182 sgd_solver.cpp:105] Iteration 54300, lr = 0.01
I1001 11:32:58.778543  5182 solver.cpp:218] Iteration 54400 (18.9954 iter/s, 5.26444s/100 iters), loss = 0.0231874
I1001 11:32:58.778581  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0231873 (* 1 = 0.0231873 loss)
I1001 11:32:58.778597  5182 sgd_solver.cpp:105] Iteration 54400, lr = 0.01
I1001 11:33:03.777992  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:33:03.988550  5182 solver.cpp:330] Iteration 54500, Testing net (#0)
I1001 11:33:05.178704  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:33:05.229238  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8979
I1001 11:33:05.229264  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355235 (* 1 = 0.355235 loss)
I1001 11:33:05.281766  5182 solver.cpp:218] Iteration 54500 (15.3771 iter/s, 6.50316s/100 iters), loss = 0.022111
I1001 11:33:05.281793  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0221109 (* 1 = 0.0221109 loss)
I1001 11:33:05.281800  5182 sgd_solver.cpp:105] Iteration 54500, lr = 0.01
I1001 11:33:10.542343  5182 solver.cpp:218] Iteration 54600 (19.0095 iter/s, 5.26053s/100 iters), loss = 0.0541569
I1001 11:33:10.542433  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0541568 (* 1 = 0.0541568 loss)
I1001 11:33:10.542440  5182 sgd_solver.cpp:105] Iteration 54600, lr = 0.01
I1001 11:33:15.800320  5182 solver.cpp:218] Iteration 54700 (19.0191 iter/s, 5.25787s/100 iters), loss = 0.0329783
I1001 11:33:15.800360  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0329782 (* 1 = 0.0329782 loss)
I1001 11:33:15.800366  5182 sgd_solver.cpp:105] Iteration 54700, lr = 0.01
I1001 11:33:21.060365  5182 solver.cpp:218] Iteration 54800 (19.0115 iter/s, 5.25998s/100 iters), loss = 0.0466024
I1001 11:33:21.060401  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0466023 (* 1 = 0.0466023 loss)
I1001 11:33:21.060408  5182 sgd_solver.cpp:105] Iteration 54800, lr = 0.01
I1001 11:33:26.312141  5182 solver.cpp:218] Iteration 54900 (19.0414 iter/s, 5.25172s/100 iters), loss = 0.0377955
I1001 11:33:26.312181  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0377954 (* 1 = 0.0377954 loss)
I1001 11:33:26.312187  5182 sgd_solver.cpp:105] Iteration 54900, lr = 0.01
I1001 11:33:31.307442  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:33:31.517771  5182 solver.cpp:330] Iteration 55000, Testing net (#0)
I1001 11:33:32.707826  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:33:32.758168  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8927
I1001 11:33:32.758204  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.376032 (* 1 = 0.376032 loss)
I1001 11:33:32.812767  5182 solver.cpp:218] Iteration 55000 (15.3833 iter/s, 6.50056s/100 iters), loss = 0.0315223
I1001 11:33:32.812799  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0315222 (* 1 = 0.0315222 loss)
I1001 11:33:32.812806  5182 sgd_solver.cpp:105] Iteration 55000, lr = 0.01
I1001 11:33:38.068225  5182 solver.cpp:218] Iteration 55100 (19.028 iter/s, 5.25541s/100 iters), loss = 0.0346439
I1001 11:33:38.068255  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0346437 (* 1 = 0.0346437 loss)
I1001 11:33:38.068261  5182 sgd_solver.cpp:105] Iteration 55100, lr = 0.01
I1001 11:33:43.327447  5182 solver.cpp:218] Iteration 55200 (19.0144 iter/s, 5.25917s/100 iters), loss = 0.040039
I1001 11:33:43.327623  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0400388 (* 1 = 0.0400388 loss)
I1001 11:33:43.327632  5182 sgd_solver.cpp:105] Iteration 55200, lr = 0.01
I1001 11:33:48.587759  5182 solver.cpp:218] Iteration 55300 (19.0109 iter/s, 5.26013s/100 iters), loss = 0.0212171
I1001 11:33:48.587788  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212169 (* 1 = 0.0212169 loss)
I1001 11:33:48.587795  5182 sgd_solver.cpp:105] Iteration 55300, lr = 0.01
I1001 11:33:53.840739  5182 solver.cpp:218] Iteration 55400 (19.037 iter/s, 5.25293s/100 iters), loss = 0.0376012
I1001 11:33:53.840786  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0376011 (* 1 = 0.0376011 loss)
I1001 11:33:53.840795  5182 sgd_solver.cpp:105] Iteration 55400, lr = 0.01
I1001 11:33:58.836513  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:33:59.046079  5182 solver.cpp:330] Iteration 55500, Testing net (#0)
I1001 11:34:00.247779  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:34:00.298158  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8946
I1001 11:34:00.298192  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.375243 (* 1 = 0.375243 loss)
I1001 11:34:00.350641  5182 solver.cpp:218] Iteration 55500 (15.3614 iter/s, 6.5098s/100 iters), loss = 0.0178976
I1001 11:34:00.350677  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0178975 (* 1 = 0.0178975 loss)
I1001 11:34:00.350682  5182 sgd_solver.cpp:105] Iteration 55500, lr = 0.01
I1001 11:34:05.592936  5182 solver.cpp:218] Iteration 55600 (19.0758 iter/s, 5.24224s/100 iters), loss = 0.0239721
I1001 11:34:05.592964  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.023972 (* 1 = 0.023972 loss)
I1001 11:34:05.592969  5182 sgd_solver.cpp:105] Iteration 55600, lr = 0.01
I1001 11:34:10.846801  5182 solver.cpp:218] Iteration 55700 (19.0338 iter/s, 5.25382s/100 iters), loss = 0.0442514
I1001 11:34:10.846829  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0442513 (* 1 = 0.0442513 loss)
I1001 11:34:10.846835  5182 sgd_solver.cpp:105] Iteration 55700, lr = 0.01
I1001 11:34:16.097048  5182 solver.cpp:218] Iteration 55800 (19.0469 iter/s, 5.2502s/100 iters), loss = 0.129446
I1001 11:34:16.097205  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.129446 (* 1 = 0.129446 loss)
I1001 11:34:16.097215  5182 sgd_solver.cpp:105] Iteration 55800, lr = 0.01
I1001 11:34:21.355159  5182 solver.cpp:218] Iteration 55900 (19.0189 iter/s, 5.25794s/100 iters), loss = 0.0183465
I1001 11:34:21.355199  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0183464 (* 1 = 0.0183464 loss)
I1001 11:34:21.355206  5182 sgd_solver.cpp:105] Iteration 55900, lr = 0.01
I1001 11:34:26.340656  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:34:26.549988  5182 solver.cpp:330] Iteration 56000, Testing net (#0)
I1001 11:34:27.748874  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:34:27.799278  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9005
I1001 11:34:27.799309  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356934 (* 1 = 0.356934 loss)
I1001 11:34:27.851704  5182 solver.cpp:218] Iteration 56000 (15.393 iter/s, 6.49648s/100 iters), loss = 0.0364796
I1001 11:34:27.851730  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0364795 (* 1 = 0.0364795 loss)
I1001 11:34:27.851737  5182 sgd_solver.cpp:105] Iteration 56000, lr = 0.01
I1001 11:34:33.109724  5182 solver.cpp:218] Iteration 56100 (19.0187 iter/s, 5.25797s/100 iters), loss = 0.0353555
I1001 11:34:33.109755  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0353554 (* 1 = 0.0353554 loss)
I1001 11:34:33.109771  5182 sgd_solver.cpp:105] Iteration 56100, lr = 0.01
I1001 11:34:38.363665  5182 solver.cpp:218] Iteration 56200 (19.0335 iter/s, 5.25389s/100 iters), loss = 0.0217083
I1001 11:34:38.363695  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0217082 (* 1 = 0.0217082 loss)
I1001 11:34:38.363711  5182 sgd_solver.cpp:105] Iteration 56200, lr = 0.01
I1001 11:34:43.623731  5182 solver.cpp:218] Iteration 56300 (19.0114 iter/s, 5.26001s/100 iters), loss = 0.0372455
I1001 11:34:43.623760  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0372454 (* 1 = 0.0372454 loss)
I1001 11:34:43.623777  5182 sgd_solver.cpp:105] Iteration 56300, lr = 0.01
I1001 11:34:48.879444  5182 solver.cpp:218] Iteration 56400 (19.0271 iter/s, 5.25566s/100 iters), loss = 0.015024
I1001 11:34:48.879554  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0150239 (* 1 = 0.0150239 loss)
I1001 11:34:48.879562  5182 sgd_solver.cpp:105] Iteration 56400, lr = 0.01
I1001 11:34:53.867787  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:34:54.083739  5182 solver.cpp:330] Iteration 56500, Testing net (#0)
I1001 11:34:55.275240  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:34:55.325271  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8992
I1001 11:34:55.325295  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361618 (* 1 = 0.361618 loss)
I1001 11:34:55.377650  5182 solver.cpp:218] Iteration 56500 (15.3892 iter/s, 6.49808s/100 iters), loss = 0.0233012
I1001 11:34:55.377678  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0233011 (* 1 = 0.0233011 loss)
I1001 11:34:55.377686  5182 sgd_solver.cpp:105] Iteration 56500, lr = 0.01
I1001 11:35:00.633744  5182 solver.cpp:218] Iteration 56600 (19.0257 iter/s, 5.25604s/100 iters), loss = 0.0420856
I1001 11:35:00.633771  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0420855 (* 1 = 0.0420855 loss)
I1001 11:35:00.633779  5182 sgd_solver.cpp:105] Iteration 56600, lr = 0.01
I1001 11:35:05.883785  5182 solver.cpp:218] Iteration 56700 (19.0477 iter/s, 5.24999s/100 iters), loss = 0.0177345
I1001 11:35:05.883821  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0177344 (* 1 = 0.0177344 loss)
I1001 11:35:05.883828  5182 sgd_solver.cpp:105] Iteration 56700, lr = 0.01
I1001 11:35:11.139223  5182 solver.cpp:218] Iteration 56800 (19.0281 iter/s, 5.25538s/100 iters), loss = 0.103833
I1001 11:35:11.139252  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103833 (* 1 = 0.103833 loss)
I1001 11:35:11.139258  5182 sgd_solver.cpp:105] Iteration 56800, lr = 0.01
I1001 11:35:16.392030  5182 solver.cpp:218] Iteration 56900 (19.0376 iter/s, 5.25276s/100 iters), loss = 0.0181273
I1001 11:35:16.392071  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0181272 (* 1 = 0.0181272 loss)
I1001 11:35:16.392077  5182 sgd_solver.cpp:105] Iteration 56900, lr = 0.01
I1001 11:35:21.381912  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:35:21.593192  5182 solver.cpp:330] Iteration 57000, Testing net (#0)
I1001 11:35:22.782030  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:35:22.832240  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9009
I1001 11:35:22.832265  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358407 (* 1 = 0.358407 loss)
I1001 11:35:22.884831  5182 solver.cpp:218] Iteration 57000 (15.4018 iter/s, 6.49274s/100 iters), loss = 0.0687123
I1001 11:35:22.884856  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0687122 (* 1 = 0.0687122 loss)
I1001 11:35:22.884863  5182 sgd_solver.cpp:105] Iteration 57000, lr = 0.01
I1001 11:35:28.141839  5182 solver.cpp:218] Iteration 57100 (19.0224 iter/s, 5.25696s/100 iters), loss = 0.0656997
I1001 11:35:28.141880  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0656996 (* 1 = 0.0656996 loss)
I1001 11:35:28.141886  5182 sgd_solver.cpp:105] Iteration 57100, lr = 0.01
I1001 11:35:33.397195  5182 solver.cpp:218] Iteration 57200 (19.0285 iter/s, 5.25529s/100 iters), loss = 0.0194097
I1001 11:35:33.397225  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194096 (* 1 = 0.0194096 loss)
I1001 11:35:33.397231  5182 sgd_solver.cpp:105] Iteration 57200, lr = 0.01
I1001 11:35:38.635454  5182 solver.cpp:218] Iteration 57300 (19.0905 iter/s, 5.23821s/100 iters), loss = 0.0541573
I1001 11:35:38.635483  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0541572 (* 1 = 0.0541572 loss)
I1001 11:35:38.635489  5182 sgd_solver.cpp:105] Iteration 57300, lr = 0.01
I1001 11:35:43.887087  5182 solver.cpp:218] Iteration 57400 (19.0419 iter/s, 5.25157s/100 iters), loss = 0.0204627
I1001 11:35:43.887116  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0204626 (* 1 = 0.0204626 loss)
I1001 11:35:43.887123  5182 sgd_solver.cpp:105] Iteration 57400, lr = 0.01
I1001 11:35:48.870906  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:35:49.081496  5182 solver.cpp:330] Iteration 57500, Testing net (#0)
I1001 11:35:50.271379  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:35:50.321555  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8918
I1001 11:35:50.321589  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386415 (* 1 = 0.386415 loss)
I1001 11:35:50.374013  5182 solver.cpp:218] Iteration 57500 (15.4157 iter/s, 6.48688s/100 iters), loss = 0.0255252
I1001 11:35:50.374043  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0255251 (* 1 = 0.0255251 loss)
I1001 11:35:50.374049  5182 sgd_solver.cpp:105] Iteration 57500, lr = 0.01
I1001 11:35:55.633291  5182 solver.cpp:218] Iteration 57600 (19.0142 iter/s, 5.25922s/100 iters), loss = 0.0227546
I1001 11:35:55.633401  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0227545 (* 1 = 0.0227545 loss)
I1001 11:35:55.633409  5182 sgd_solver.cpp:105] Iteration 57600, lr = 0.01
I1001 11:36:00.888928  5182 solver.cpp:218] Iteration 57700 (19.0277 iter/s, 5.25551s/100 iters), loss = 0.021457
I1001 11:36:00.888957  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0214569 (* 1 = 0.0214569 loss)
I1001 11:36:00.888963  5182 sgd_solver.cpp:105] Iteration 57700, lr = 0.01
I1001 11:36:06.145676  5182 solver.cpp:218] Iteration 57800 (19.0234 iter/s, 5.2567s/100 iters), loss = 0.0253248
I1001 11:36:06.145707  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0253247 (* 1 = 0.0253247 loss)
I1001 11:36:06.145714  5182 sgd_solver.cpp:105] Iteration 57800, lr = 0.01
I1001 11:36:11.398692  5182 solver.cpp:218] Iteration 57900 (19.0369 iter/s, 5.25296s/100 iters), loss = 0.0200439
I1001 11:36:11.398732  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0200437 (* 1 = 0.0200437 loss)
I1001 11:36:11.398738  5182 sgd_solver.cpp:105] Iteration 57900, lr = 0.01
I1001 11:36:16.396523  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:36:16.608110  5182 solver.cpp:330] Iteration 58000, Testing net (#0)
I1001 11:36:17.795323  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:36:17.846177  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8996
I1001 11:36:17.846204  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361924 (* 1 = 0.361924 loss)
I1001 11:36:17.900257  5182 solver.cpp:218] Iteration 58000 (15.3811 iter/s, 6.5015s/100 iters), loss = 0.021123
I1001 11:36:17.900307  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0211228 (* 1 = 0.0211228 loss)
I1001 11:36:17.900315  5182 sgd_solver.cpp:105] Iteration 58000, lr = 0.01
I1001 11:36:23.156628  5182 solver.cpp:218] Iteration 58100 (19.0249 iter/s, 5.25627s/100 iters), loss = 0.031476
I1001 11:36:23.156659  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0314758 (* 1 = 0.0314758 loss)
I1001 11:36:23.156675  5182 sgd_solver.cpp:105] Iteration 58100, lr = 0.01
I1001 11:36:28.412736  5182 solver.cpp:218] Iteration 58200 (19.0257 iter/s, 5.25606s/100 iters), loss = 0.0640562
I1001 11:36:28.412868  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0640561 (* 1 = 0.0640561 loss)
I1001 11:36:28.412875  5182 sgd_solver.cpp:105] Iteration 58200, lr = 0.01
I1001 11:36:33.666002  5182 solver.cpp:218] Iteration 58300 (19.0363 iter/s, 5.25312s/100 iters), loss = 0.0337327
I1001 11:36:33.666030  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0337326 (* 1 = 0.0337326 loss)
I1001 11:36:33.666046  5182 sgd_solver.cpp:105] Iteration 58300, lr = 0.01
I1001 11:36:38.918179  5182 solver.cpp:218] Iteration 58400 (19.0399 iter/s, 5.25212s/100 iters), loss = 0.0242314
I1001 11:36:38.918212  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0242312 (* 1 = 0.0242312 loss)
I1001 11:36:38.918220  5182 sgd_solver.cpp:105] Iteration 58400, lr = 0.01
I1001 11:36:43.906136  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:36:44.116451  5182 solver.cpp:330] Iteration 58500, Testing net (#0)
I1001 11:36:45.313786  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:36:45.364356  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.902
I1001 11:36:45.364392  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.363691 (* 1 = 0.363691 loss)
I1001 11:36:45.416909  5182 solver.cpp:218] Iteration 58500 (15.3877 iter/s, 6.49868s/100 iters), loss = 0.0177411
I1001 11:36:45.416935  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.017741 (* 1 = 0.017741 loss)
I1001 11:36:45.416941  5182 sgd_solver.cpp:105] Iteration 58500, lr = 0.01
I1001 11:36:50.663797  5182 solver.cpp:218] Iteration 58600 (19.0591 iter/s, 5.24684s/100 iters), loss = 0.0725993
I1001 11:36:50.663826  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0725992 (* 1 = 0.0725992 loss)
I1001 11:36:50.663832  5182 sgd_solver.cpp:105] Iteration 58600, lr = 0.01
I1001 11:36:55.918653  5182 solver.cpp:218] Iteration 58700 (19.0302 iter/s, 5.25481s/100 iters), loss = 0.041049
I1001 11:36:55.918682  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0410488 (* 1 = 0.0410488 loss)
I1001 11:36:55.918699  5182 sgd_solver.cpp:105] Iteration 58700, lr = 0.01
I1001 11:37:01.180200  5182 solver.cpp:218] Iteration 58800 (19.006 iter/s, 5.26149s/100 iters), loss = 0.0227396
I1001 11:37:01.180305  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0227395 (* 1 = 0.0227395 loss)
I1001 11:37:01.180322  5182 sgd_solver.cpp:105] Iteration 58800, lr = 0.01
I1001 11:37:06.438655  5182 solver.cpp:218] Iteration 58900 (19.0174 iter/s, 5.25833s/100 iters), loss = 0.0208856
I1001 11:37:06.438695  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0208855 (* 1 = 0.0208855 loss)
I1001 11:37:06.438702  5182 sgd_solver.cpp:105] Iteration 58900, lr = 0.01
I1001 11:37:11.424516  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:37:11.634578  5182 solver.cpp:330] Iteration 59000, Testing net (#0)
I1001 11:37:12.831930  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:37:12.882086  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.899
I1001 11:37:12.882122  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.368402 (* 1 = 0.368402 loss)
I1001 11:37:12.934856  5182 solver.cpp:218] Iteration 59000 (15.3938 iter/s, 6.49614s/100 iters), loss = 0.0293952
I1001 11:37:12.934895  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0293951 (* 1 = 0.0293951 loss)
I1001 11:37:12.934903  5182 sgd_solver.cpp:105] Iteration 59000, lr = 0.01
I1001 11:37:18.190492  5182 solver.cpp:218] Iteration 59100 (19.0274 iter/s, 5.25557s/100 iters), loss = 0.0569658
I1001 11:37:18.190536  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0569656 (* 1 = 0.0569656 loss)
I1001 11:37:18.190542  5182 sgd_solver.cpp:105] Iteration 59100, lr = 0.01
I1001 11:37:23.438817  5182 solver.cpp:218] Iteration 59200 (19.0539 iter/s, 5.24826s/100 iters), loss = 0.0243769
I1001 11:37:23.438848  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0243768 (* 1 = 0.0243768 loss)
I1001 11:37:23.438864  5182 sgd_solver.cpp:105] Iteration 59200, lr = 0.01
I1001 11:37:28.696373  5182 solver.cpp:218] Iteration 59300 (19.0204 iter/s, 5.2575s/100 iters), loss = 0.0331595
I1001 11:37:28.696404  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0331594 (* 1 = 0.0331594 loss)
I1001 11:37:28.696420  5182 sgd_solver.cpp:105] Iteration 59300, lr = 0.01
I1001 11:37:33.948402  5182 solver.cpp:218] Iteration 59400 (19.0405 iter/s, 5.25198s/100 iters), loss = 0.0253235
I1001 11:37:33.948521  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0253233 (* 1 = 0.0253233 loss)
I1001 11:37:33.948529  5182 sgd_solver.cpp:105] Iteration 59400, lr = 0.01
I1001 11:37:38.939462  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:37:39.153635  5182 solver.cpp:330] Iteration 59500, Testing net (#0)
I1001 11:37:40.344127  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:37:40.394244  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8895
I1001 11:37:40.394279  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.429657 (* 1 = 0.429657 loss)
I1001 11:37:40.447099  5182 solver.cpp:218] Iteration 59500 (15.388 iter/s, 6.49856s/100 iters), loss = 0.0206307
I1001 11:37:40.447124  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0206306 (* 1 = 0.0206306 loss)
I1001 11:37:40.447130  5182 sgd_solver.cpp:105] Iteration 59500, lr = 0.01
I1001 11:37:45.702934  5182 solver.cpp:218] Iteration 59600 (19.0267 iter/s, 5.25578s/100 iters), loss = 0.0548167
I1001 11:37:45.702963  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0548166 (* 1 = 0.0548166 loss)
I1001 11:37:45.702970  5182 sgd_solver.cpp:105] Iteration 59600, lr = 0.01
I1001 11:37:50.952345  5182 solver.cpp:218] Iteration 59700 (19.05 iter/s, 5.24936s/100 iters), loss = 0.0205309
I1001 11:37:50.952401  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0205308 (* 1 = 0.0205308 loss)
I1001 11:37:50.952419  5182 sgd_solver.cpp:105] Iteration 59700, lr = 0.01
I1001 11:37:56.203132  5182 solver.cpp:218] Iteration 59800 (19.0453 iter/s, 5.25064s/100 iters), loss = 0.0081855
I1001 11:37:56.203162  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00818534 (* 1 = 0.00818534 loss)
I1001 11:37:56.203179  5182 sgd_solver.cpp:105] Iteration 59800, lr = 0.01
I1001 11:38:01.462169  5182 solver.cpp:218] Iteration 59900 (19.0151 iter/s, 5.25899s/100 iters), loss = 0.00952944
I1001 11:38:01.462198  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00952928 (* 1 = 0.00952928 loss)
I1001 11:38:01.462204  5182 sgd_solver.cpp:105] Iteration 59900, lr = 0.01
I1001 11:38:06.456130  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:38:06.666218  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_60000.caffemodel
I1001 11:38:06.671351  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_60000.solverstate
I1001 11:38:06.672683  5182 solver.cpp:330] Iteration 60000, Testing net (#0)
I1001 11:38:07.862669  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:38:07.913123  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8958
I1001 11:38:07.913157  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.388381 (* 1 = 0.388381 loss)
I1001 11:38:07.966043  5182 solver.cpp:218] Iteration 60000 (15.3756 iter/s, 6.50382s/100 iters), loss = 0.0130778
I1001 11:38:07.966078  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130776 (* 1 = 0.0130776 loss)
I1001 11:38:07.966085  5182 sgd_solver.cpp:105] Iteration 60000, lr = 0.01
I1001 11:38:13.218555  5182 solver.cpp:218] Iteration 60100 (19.0387 iter/s, 5.25246s/100 iters), loss = 0.0581018
I1001 11:38:13.218583  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0581017 (* 1 = 0.0581017 loss)
I1001 11:38:13.218590  5182 sgd_solver.cpp:105] Iteration 60100, lr = 0.01
I1001 11:38:18.465579  5182 solver.cpp:218] Iteration 60200 (19.0586 iter/s, 5.24698s/100 iters), loss = 0.0546126
I1001 11:38:18.465617  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0546124 (* 1 = 0.0546124 loss)
I1001 11:38:18.465623  5182 sgd_solver.cpp:105] Iteration 60200, lr = 0.01
I1001 11:38:23.707491  5182 solver.cpp:218] Iteration 60300 (19.0772 iter/s, 5.24186s/100 iters), loss = 0.0165813
I1001 11:38:23.707530  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0165812 (* 1 = 0.0165812 loss)
I1001 11:38:23.707536  5182 sgd_solver.cpp:105] Iteration 60300, lr = 0.01
I1001 11:38:28.962255  5182 solver.cpp:218] Iteration 60400 (19.0306 iter/s, 5.25471s/100 iters), loss = 0.012366
I1001 11:38:28.962282  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123658 (* 1 = 0.0123658 loss)
I1001 11:38:28.962288  5182 sgd_solver.cpp:105] Iteration 60400, lr = 0.01
I1001 11:38:33.955459  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:38:34.165968  5182 solver.cpp:330] Iteration 60500, Testing net (#0)
I1001 11:38:35.355779  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:38:35.405859  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9038
I1001 11:38:35.405884  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.362254 (* 1 = 0.362254 loss)
I1001 11:38:35.458688  5182 solver.cpp:218] Iteration 60500 (15.3932 iter/s, 6.49639s/100 iters), loss = 0.0319032
I1001 11:38:35.458714  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0319031 (* 1 = 0.0319031 loss)
I1001 11:38:35.458720  5182 sgd_solver.cpp:105] Iteration 60500, lr = 0.01
I1001 11:38:40.716163  5182 solver.cpp:218] Iteration 60600 (19.0207 iter/s, 5.25743s/100 iters), loss = 0.00956965
I1001 11:38:40.716307  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00956945 (* 1 = 0.00956945 loss)
I1001 11:38:40.716316  5182 sgd_solver.cpp:105] Iteration 60600, lr = 0.01
I1001 11:38:45.975047  5182 solver.cpp:218] Iteration 60700 (19.016 iter/s, 5.25873s/100 iters), loss = 0.0627225
I1001 11:38:45.975086  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0627223 (* 1 = 0.0627223 loss)
I1001 11:38:45.975092  5182 sgd_solver.cpp:105] Iteration 60700, lr = 0.01
I1001 11:38:51.228543  5182 solver.cpp:218] Iteration 60800 (19.0351 iter/s, 5.25344s/100 iters), loss = 0.0177911
I1001 11:38:51.228571  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0177909 (* 1 = 0.0177909 loss)
I1001 11:38:51.228577  5182 sgd_solver.cpp:105] Iteration 60800, lr = 0.01
I1001 11:38:56.475055  5182 solver.cpp:218] Iteration 60900 (19.0605 iter/s, 5.24646s/100 iters), loss = 0.0193831
I1001 11:38:56.475095  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0193829 (* 1 = 0.0193829 loss)
I1001 11:38:56.475100  5182 sgd_solver.cpp:105] Iteration 60900, lr = 0.01
I1001 11:39:01.465845  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:39:01.676237  5182 solver.cpp:330] Iteration 61000, Testing net (#0)
I1001 11:39:02.866117  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:39:02.917153  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8973
I1001 11:39:02.917188  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.37764 (* 1 = 0.37764 loss)
I1001 11:39:02.970937  5182 solver.cpp:218] Iteration 61000 (15.3945 iter/s, 6.49582s/100 iters), loss = 0.019113
I1001 11:39:02.970983  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0191128 (* 1 = 0.0191128 loss)
I1001 11:39:02.970989  5182 sgd_solver.cpp:105] Iteration 61000, lr = 0.01
I1001 11:39:08.218447  5182 solver.cpp:218] Iteration 61100 (19.057 iter/s, 5.24742s/100 iters), loss = 0.028988
I1001 11:39:08.218487  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0289878 (* 1 = 0.0289878 loss)
I1001 11:39:08.218492  5182 sgd_solver.cpp:105] Iteration 61100, lr = 0.01
I1001 11:39:13.472687  5182 solver.cpp:218] Iteration 61200 (19.0325 iter/s, 5.25418s/100 iters), loss = 0.0514548
I1001 11:39:13.472817  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0514547 (* 1 = 0.0514547 loss)
I1001 11:39:13.472834  5182 sgd_solver.cpp:105] Iteration 61200, lr = 0.01
I1001 11:39:18.729982  5182 solver.cpp:218] Iteration 61300 (19.0217 iter/s, 5.25716s/100 iters), loss = 0.0850206
I1001 11:39:18.730011  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0850204 (* 1 = 0.0850204 loss)
I1001 11:39:18.730017  5182 sgd_solver.cpp:105] Iteration 61300, lr = 0.01
I1001 11:39:23.987445  5182 solver.cpp:218] Iteration 61400 (19.0208 iter/s, 5.25741s/100 iters), loss = 0.045923
I1001 11:39:23.987493  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0459228 (* 1 = 0.0459228 loss)
I1001 11:39:23.987500  5182 sgd_solver.cpp:105] Iteration 61400, lr = 0.01
I1001 11:39:28.976528  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:39:29.186568  5182 solver.cpp:330] Iteration 61500, Testing net (#0)
I1001 11:39:30.387415  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:39:30.437989  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8545
I1001 11:39:30.438024  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.644926 (* 1 = 0.644926 loss)
I1001 11:39:30.490710  5182 solver.cpp:218] Iteration 61500 (15.3772 iter/s, 6.50314s/100 iters), loss = 0.0345844
I1001 11:39:30.490737  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0345842 (* 1 = 0.0345842 loss)
I1001 11:39:30.490743  5182 sgd_solver.cpp:105] Iteration 61500, lr = 0.01
I1001 11:39:35.742838  5182 solver.cpp:218] Iteration 61600 (19.0401 iter/s, 5.25208s/100 iters), loss = 0.0274146
I1001 11:39:35.742878  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0274144 (* 1 = 0.0274144 loss)
I1001 11:39:35.742885  5182 sgd_solver.cpp:105] Iteration 61600, lr = 0.01
I1001 11:39:41.000569  5182 solver.cpp:218] Iteration 61700 (19.0198 iter/s, 5.25767s/100 iters), loss = 0.0314354
I1001 11:39:41.000607  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0314351 (* 1 = 0.0314351 loss)
I1001 11:39:41.000614  5182 sgd_solver.cpp:105] Iteration 61700, lr = 0.01
I1001 11:39:46.255795  5182 solver.cpp:218] Iteration 61800 (19.0289 iter/s, 5.25517s/100 iters), loss = 0.0444815
I1001 11:39:46.255954  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0444813 (* 1 = 0.0444813 loss)
I1001 11:39:46.255972  5182 sgd_solver.cpp:105] Iteration 61800, lr = 0.01
I1001 11:39:51.514763  5182 solver.cpp:218] Iteration 61900 (19.0157 iter/s, 5.2588s/100 iters), loss = 0.0153341
I1001 11:39:51.514794  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0153339 (* 1 = 0.0153339 loss)
I1001 11:39:51.514801  5182 sgd_solver.cpp:105] Iteration 61900, lr = 0.01
I1001 11:39:56.499140  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:39:56.709506  5182 solver.cpp:330] Iteration 62000, Testing net (#0)
I1001 11:39:57.906769  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:39:57.956729  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8996
I1001 11:39:57.956763  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.382494 (* 1 = 0.382494 loss)
I1001 11:39:58.009534  5182 solver.cpp:218] Iteration 62000 (15.3971 iter/s, 6.49472s/100 iters), loss = 0.00400339
I1001 11:39:58.009562  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00400316 (* 1 = 0.00400316 loss)
I1001 11:39:58.009569  5182 sgd_solver.cpp:105] Iteration 62000, lr = 0.01
I1001 11:40:03.272855  5182 solver.cpp:218] Iteration 62100 (18.9996 iter/s, 5.26327s/100 iters), loss = 0.0474373
I1001 11:40:03.272897  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0474371 (* 1 = 0.0474371 loss)
I1001 11:40:03.272904  5182 sgd_solver.cpp:105] Iteration 62100, lr = 0.01
I1001 11:40:08.524109  5182 solver.cpp:218] Iteration 62200 (19.0433 iter/s, 5.25119s/100 iters), loss = 0.0336776
I1001 11:40:08.524139  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0336774 (* 1 = 0.0336774 loss)
I1001 11:40:08.524155  5182 sgd_solver.cpp:105] Iteration 62200, lr = 0.01
I1001 11:40:13.782058  5182 solver.cpp:218] Iteration 62300 (19.019 iter/s, 5.2579s/100 iters), loss = 0.0187007
I1001 11:40:13.782099  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0187005 (* 1 = 0.0187005 loss)
I1001 11:40:13.782105  5182 sgd_solver.cpp:105] Iteration 62300, lr = 0.01
I1001 11:40:19.044363  5182 solver.cpp:218] Iteration 62400 (19.0033 iter/s, 5.26224s/100 iters), loss = 0.029469
I1001 11:40:19.044481  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0294687 (* 1 = 0.0294687 loss)
I1001 11:40:19.044487  5182 sgd_solver.cpp:105] Iteration 62400, lr = 0.01
I1001 11:40:24.039907  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:40:24.252621  5182 solver.cpp:330] Iteration 62500, Testing net (#0)
I1001 11:40:25.444097  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:40:25.494436  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.887
I1001 11:40:25.494469  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.437541 (* 1 = 0.437541 loss)
I1001 11:40:25.547075  5182 solver.cpp:218] Iteration 62500 (15.3785 iter/s, 6.50258s/100 iters), loss = 0.0313238
I1001 11:40:25.547101  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313235 (* 1 = 0.0313235 loss)
I1001 11:40:25.547107  5182 sgd_solver.cpp:105] Iteration 62500, lr = 0.01
I1001 11:40:30.800034  5182 solver.cpp:218] Iteration 62600 (19.0371 iter/s, 5.25291s/100 iters), loss = 0.0354096
I1001 11:40:30.800062  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0354094 (* 1 = 0.0354094 loss)
I1001 11:40:30.800068  5182 sgd_solver.cpp:105] Iteration 62600, lr = 0.01
I1001 11:40:36.051501  5182 solver.cpp:218] Iteration 62700 (19.0425 iter/s, 5.25141s/100 iters), loss = 0.0111054
I1001 11:40:36.051535  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0111052 (* 1 = 0.0111052 loss)
I1001 11:40:36.051553  5182 sgd_solver.cpp:105] Iteration 62700, lr = 0.01
I1001 11:40:41.300846  5182 solver.cpp:218] Iteration 62800 (19.0502 iter/s, 5.24929s/100 iters), loss = 0.0456111
I1001 11:40:41.300874  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0456109 (* 1 = 0.0456109 loss)
I1001 11:40:41.300880  5182 sgd_solver.cpp:105] Iteration 62800, lr = 0.01
I1001 11:40:46.557374  5182 solver.cpp:218] Iteration 62900 (19.0241 iter/s, 5.25648s/100 iters), loss = 0.0130845
I1001 11:40:46.557404  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130843 (* 1 = 0.0130843 loss)
I1001 11:40:46.557409  5182 sgd_solver.cpp:105] Iteration 62900, lr = 0.01
I1001 11:40:51.553401  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:40:51.763617  5182 solver.cpp:330] Iteration 63000, Testing net (#0)
I1001 11:40:52.952185  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:40:53.002584  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9019
I1001 11:40:53.002609  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.372781 (* 1 = 0.372781 loss)
I1001 11:40:53.055004  5182 solver.cpp:218] Iteration 63000 (15.3903 iter/s, 6.49758s/100 iters), loss = 0.026029
I1001 11:40:53.055030  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0260288 (* 1 = 0.0260288 loss)
I1001 11:40:53.055037  5182 sgd_solver.cpp:105] Iteration 63000, lr = 0.01
I1001 11:40:58.312201  5182 solver.cpp:218] Iteration 63100 (19.0217 iter/s, 5.25715s/100 iters), loss = 0.0586783
I1001 11:40:58.312240  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0586781 (* 1 = 0.0586781 loss)
I1001 11:40:58.312247  5182 sgd_solver.cpp:105] Iteration 63100, lr = 0.01
I1001 11:41:03.572749  5182 solver.cpp:218] Iteration 63200 (19.0097 iter/s, 5.26048s/100 iters), loss = 0.0500651
I1001 11:41:03.572780  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0500649 (* 1 = 0.0500649 loss)
I1001 11:41:03.572788  5182 sgd_solver.cpp:105] Iteration 63200, lr = 0.01
I1001 11:41:08.817701  5182 solver.cpp:218] Iteration 63300 (19.0661 iter/s, 5.2449s/100 iters), loss = 0.0518667
I1001 11:41:08.817742  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0518665 (* 1 = 0.0518665 loss)
I1001 11:41:08.817749  5182 sgd_solver.cpp:105] Iteration 63300, lr = 0.01
I1001 11:41:14.075296  5182 solver.cpp:218] Iteration 63400 (19.0204 iter/s, 5.2575s/100 iters), loss = 0.00615036
I1001 11:41:14.075325  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00615016 (* 1 = 0.00615016 loss)
I1001 11:41:14.075331  5182 sgd_solver.cpp:105] Iteration 63400, lr = 0.01
I1001 11:41:19.063197  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:41:19.273246  5182 solver.cpp:330] Iteration 63500, Testing net (#0)
I1001 11:41:20.464025  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:41:20.514287  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8967
I1001 11:41:20.514322  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.403557 (* 1 = 0.403557 loss)
I1001 11:41:20.566817  5182 solver.cpp:218] Iteration 63500 (15.4048 iter/s, 6.49147s/100 iters), loss = 0.0216158
I1001 11:41:20.566841  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216156 (* 1 = 0.0216156 loss)
I1001 11:41:20.566848  5182 sgd_solver.cpp:105] Iteration 63500, lr = 0.01
I1001 11:41:25.818105  5182 solver.cpp:218] Iteration 63600 (19.0431 iter/s, 5.25124s/100 iters), loss = 0.0446507
I1001 11:41:25.818215  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446505 (* 1 = 0.0446505 loss)
I1001 11:41:25.818222  5182 sgd_solver.cpp:105] Iteration 63600, lr = 0.01
I1001 11:41:31.068652  5182 solver.cpp:218] Iteration 63700 (19.0461 iter/s, 5.25042s/100 iters), loss = 0.0312078
I1001 11:41:31.068682  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0312076 (* 1 = 0.0312076 loss)
I1001 11:41:31.068688  5182 sgd_solver.cpp:105] Iteration 63700, lr = 0.01
I1001 11:41:36.323352  5182 solver.cpp:218] Iteration 63800 (19.0308 iter/s, 5.25465s/100 iters), loss = 0.0276662
I1001 11:41:36.323382  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0276661 (* 1 = 0.0276661 loss)
I1001 11:41:36.323390  5182 sgd_solver.cpp:105] Iteration 63800, lr = 0.01
I1001 11:41:41.573236  5182 solver.cpp:218] Iteration 63900 (19.0482 iter/s, 5.24983s/100 iters), loss = 0.0159075
I1001 11:41:41.573266  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159074 (* 1 = 0.0159074 loss)
I1001 11:41:41.573271  5182 sgd_solver.cpp:105] Iteration 63900, lr = 0.01
I1001 11:41:46.567647  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:41:46.777040  5182 solver.cpp:330] Iteration 64000, Testing net (#0)
I1001 11:41:47.972944  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:41:48.024087  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8905
I1001 11:41:48.024113  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.425159 (* 1 = 0.425159 loss)
I1001 11:41:48.077618  5182 solver.cpp:218] Iteration 64000 (15.3744 iter/s, 6.50433s/100 iters), loss = 0.0168456
I1001 11:41:48.077652  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168455 (* 1 = 0.0168455 loss)
I1001 11:41:48.077659  5182 sgd_solver.cpp:105] Iteration 64000, lr = 0.01
I1001 11:41:53.329908  5182 solver.cpp:218] Iteration 64100 (19.0395 iter/s, 5.25224s/100 iters), loss = 0.0621759
I1001 11:41:53.329937  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0621758 (* 1 = 0.0621758 loss)
I1001 11:41:53.329944  5182 sgd_solver.cpp:105] Iteration 64100, lr = 0.01
I1001 11:41:58.586951  5182 solver.cpp:218] Iteration 64200 (19.0223 iter/s, 5.25699s/100 iters), loss = 0.0179439
I1001 11:41:58.587095  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0179437 (* 1 = 0.0179437 loss)
I1001 11:41:58.587102  5182 sgd_solver.cpp:105] Iteration 64200, lr = 0.01
I1001 11:42:03.843457  5182 solver.cpp:218] Iteration 64300 (19.0246 iter/s, 5.25634s/100 iters), loss = 0.0317908
I1001 11:42:03.843487  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0317907 (* 1 = 0.0317907 loss)
I1001 11:42:03.843493  5182 sgd_solver.cpp:105] Iteration 64300, lr = 0.01
I1001 11:42:09.095260  5182 solver.cpp:218] Iteration 64400 (19.0413 iter/s, 5.25175s/100 iters), loss = 0.0293407
I1001 11:42:09.095304  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0293405 (* 1 = 0.0293405 loss)
I1001 11:42:09.095310  5182 sgd_solver.cpp:105] Iteration 64400, lr = 0.01
I1001 11:42:14.079126  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:42:14.289110  5182 solver.cpp:330] Iteration 64500, Testing net (#0)
I1001 11:42:15.487393  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:42:15.537272  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8912
I1001 11:42:15.537307  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.408218 (* 1 = 0.408218 loss)
I1001 11:42:15.590771  5182 solver.cpp:218] Iteration 64500 (15.3955 iter/s, 6.49541s/100 iters), loss = 0.0117289
I1001 11:42:15.590797  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117287 (* 1 = 0.0117287 loss)
I1001 11:42:15.590804  5182 sgd_solver.cpp:105] Iteration 64500, lr = 0.01
I1001 11:42:20.832984  5182 solver.cpp:218] Iteration 64600 (19.0761 iter/s, 5.24216s/100 iters), loss = 0.058317
I1001 11:42:20.833025  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0583168 (* 1 = 0.0583168 loss)
I1001 11:42:20.833034  5182 sgd_solver.cpp:105] Iteration 64600, lr = 0.01
I1001 11:42:26.085448  5182 solver.cpp:218] Iteration 64700 (19.039 iter/s, 5.25237s/100 iters), loss = 0.0317649
I1001 11:42:26.085476  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0317647 (* 1 = 0.0317647 loss)
I1001 11:42:26.085481  5182 sgd_solver.cpp:105] Iteration 64700, lr = 0.01
I1001 11:42:31.340339  5182 solver.cpp:218] Iteration 64800 (19.0301 iter/s, 5.25484s/100 iters), loss = 0.0252292
I1001 11:42:31.340517  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.025229 (* 1 = 0.025229 loss)
I1001 11:42:31.340524  5182 sgd_solver.cpp:105] Iteration 64800, lr = 0.01
I1001 11:42:36.597951  5182 solver.cpp:218] Iteration 64900 (19.0208 iter/s, 5.25742s/100 iters), loss = 0.031254
I1001 11:42:36.597992  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0312538 (* 1 = 0.0312538 loss)
I1001 11:42:36.597998  5182 sgd_solver.cpp:105] Iteration 64900, lr = 0.01
I1001 11:42:41.579979  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:42:41.790737  5182 solver.cpp:330] Iteration 65000, Testing net (#0)
I1001 11:42:42.988883  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:42:43.038658  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8884
I1001 11:42:43.038683  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.426125 (* 1 = 0.426125 loss)
I1001 11:42:43.091298  5182 solver.cpp:218] Iteration 65000 (15.4005 iter/s, 6.49329s/100 iters), loss = 0.00672897
I1001 11:42:43.091326  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00672879 (* 1 = 0.00672879 loss)
I1001 11:42:43.091336  5182 sgd_solver.cpp:105] Iteration 65000, lr = 0.01
I1001 11:42:48.346349  5182 solver.cpp:218] Iteration 65100 (19.0295 iter/s, 5.255s/100 iters), loss = 0.0300965
I1001 11:42:48.346388  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0300963 (* 1 = 0.0300963 loss)
I1001 11:42:48.346395  5182 sgd_solver.cpp:105] Iteration 65100, lr = 0.01
I1001 11:42:53.588994  5182 solver.cpp:218] Iteration 65200 (19.0746 iter/s, 5.24258s/100 iters), loss = 0.0202995
I1001 11:42:53.589035  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0202993 (* 1 = 0.0202993 loss)
I1001 11:42:53.589041  5182 sgd_solver.cpp:105] Iteration 65200, lr = 0.01
I1001 11:42:58.849654  5182 solver.cpp:218] Iteration 65300 (19.0092 iter/s, 5.2606s/100 iters), loss = 0.0223197
I1001 11:42:58.849694  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0223195 (* 1 = 0.0223195 loss)
I1001 11:42:58.849699  5182 sgd_solver.cpp:105] Iteration 65300, lr = 0.01
I1001 11:43:04.106014  5182 solver.cpp:218] Iteration 65400 (19.0248 iter/s, 5.2563s/100 iters), loss = 0.0467384
I1001 11:43:04.106148  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0467382 (* 1 = 0.0467382 loss)
I1001 11:43:04.106155  5182 sgd_solver.cpp:105] Iteration 65400, lr = 0.01
I1001 11:43:09.097677  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:43:09.308542  5182 solver.cpp:330] Iteration 65500, Testing net (#0)
I1001 11:43:10.498437  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:43:10.551617  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9028
I1001 11:43:10.551651  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.366714 (* 1 = 0.366714 loss)
I1001 11:43:10.604473  5182 solver.cpp:218] Iteration 65500 (15.3886 iter/s, 6.49832s/100 iters), loss = 0.0327429
I1001 11:43:10.604498  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0327427 (* 1 = 0.0327427 loss)
I1001 11:43:10.604504  5182 sgd_solver.cpp:105] Iteration 65500, lr = 0.01
I1001 11:43:15.861299  5182 solver.cpp:218] Iteration 65600 (19.0231 iter/s, 5.25678s/100 iters), loss = 0.0285736
I1001 11:43:15.861330  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0285734 (* 1 = 0.0285734 loss)
I1001 11:43:15.861348  5182 sgd_solver.cpp:105] Iteration 65600, lr = 0.01
I1001 11:43:21.116086  5182 solver.cpp:218] Iteration 65700 (19.0305 iter/s, 5.25473s/100 iters), loss = 0.0219969
I1001 11:43:21.116122  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0219967 (* 1 = 0.0219967 loss)
I1001 11:43:21.116140  5182 sgd_solver.cpp:105] Iteration 65700, lr = 0.01
I1001 11:43:26.370769  5182 solver.cpp:218] Iteration 65800 (19.0308 iter/s, 5.25463s/100 iters), loss = 0.0117569
I1001 11:43:26.370800  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117567 (* 1 = 0.0117567 loss)
I1001 11:43:26.370807  5182 sgd_solver.cpp:105] Iteration 65800, lr = 0.01
I1001 11:43:31.635314  5182 solver.cpp:218] Iteration 65900 (18.9952 iter/s, 5.2645s/100 iters), loss = 0.0061891
I1001 11:43:31.635346  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00618889 (* 1 = 0.00618889 loss)
I1001 11:43:31.635355  5182 sgd_solver.cpp:105] Iteration 65900, lr = 0.01
I1001 11:43:36.633954  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:43:36.844243  5182 solver.cpp:330] Iteration 66000, Testing net (#0)
I1001 11:43:38.033262  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:43:38.083461  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8982
I1001 11:43:38.083487  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.397566 (* 1 = 0.397566 loss)
I1001 11:43:38.136230  5182 solver.cpp:218] Iteration 66000 (15.3826 iter/s, 6.50087s/100 iters), loss = 0.0160318
I1001 11:43:38.136260  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0160316 (* 1 = 0.0160316 loss)
I1001 11:43:38.136270  5182 sgd_solver.cpp:105] Iteration 66000, lr = 0.01
I1001 11:43:43.393836  5182 solver.cpp:218] Iteration 66100 (19.0203 iter/s, 5.25755s/100 iters), loss = 0.0171365
I1001 11:43:43.393868  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171363 (* 1 = 0.0171363 loss)
I1001 11:43:43.393887  5182 sgd_solver.cpp:105] Iteration 66100, lr = 0.01
I1001 11:43:48.649993  5182 solver.cpp:218] Iteration 66200 (19.0255 iter/s, 5.25611s/100 iters), loss = 0.0870152
I1001 11:43:48.650027  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0870149 (* 1 = 0.0870149 loss)
I1001 11:43:48.650035  5182 sgd_solver.cpp:105] Iteration 66200, lr = 0.01
I1001 11:43:53.895179  5182 solver.cpp:218] Iteration 66300 (19.0653 iter/s, 5.24513s/100 iters), loss = 0.0543414
I1001 11:43:53.895225  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0543412 (* 1 = 0.0543412 loss)
I1001 11:43:53.895231  5182 sgd_solver.cpp:105] Iteration 66300, lr = 0.01
I1001 11:43:59.149624  5182 solver.cpp:218] Iteration 66400 (19.0317 iter/s, 5.25438s/100 iters), loss = 0.0437642
I1001 11:43:59.149664  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.043764 (* 1 = 0.043764 loss)
I1001 11:43:59.149670  5182 sgd_solver.cpp:105] Iteration 66400, lr = 0.01
I1001 11:44:04.148020  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:44:04.357146  5182 solver.cpp:330] Iteration 66500, Testing net (#0)
I1001 11:44:05.546604  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:44:05.596976  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8891
I1001 11:44:05.597002  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.425095 (* 1 = 0.425095 loss)
I1001 11:44:05.650332  5182 solver.cpp:218] Iteration 66500 (15.3831 iter/s, 6.50065s/100 iters), loss = 0.0720467
I1001 11:44:05.650359  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0720464 (* 1 = 0.0720464 loss)
I1001 11:44:05.650367  5182 sgd_solver.cpp:105] Iteration 66500, lr = 0.01
I1001 11:44:10.909445  5182 solver.cpp:218] Iteration 66600 (19.0148 iter/s, 5.25906s/100 iters), loss = 0.0475247
I1001 11:44:10.909566  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0475245 (* 1 = 0.0475245 loss)
I1001 11:44:10.909577  5182 sgd_solver.cpp:105] Iteration 66600, lr = 0.01
I1001 11:44:16.170727  5182 solver.cpp:218] Iteration 66700 (19.0073 iter/s, 5.26115s/100 iters), loss = 0.0119779
I1001 11:44:16.170759  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119777 (* 1 = 0.0119777 loss)
I1001 11:44:16.170765  5182 sgd_solver.cpp:105] Iteration 66700, lr = 0.01
I1001 11:44:21.423135  5182 solver.cpp:218] Iteration 66800 (19.0391 iter/s, 5.25235s/100 iters), loss = 0.0242172
I1001 11:44:21.423176  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.024217 (* 1 = 0.024217 loss)
I1001 11:44:21.423182  5182 sgd_solver.cpp:105] Iteration 66800, lr = 0.01
I1001 11:44:26.673822  5182 solver.cpp:218] Iteration 66900 (19.0454 iter/s, 5.25062s/100 iters), loss = 0.0275651
I1001 11:44:26.673861  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275648 (* 1 = 0.0275648 loss)
I1001 11:44:26.673867  5182 sgd_solver.cpp:105] Iteration 66900, lr = 0.01
I1001 11:44:31.663828  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:44:31.873991  5182 solver.cpp:330] Iteration 67000, Testing net (#0)
I1001 11:44:33.067975  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:44:33.118679  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8997
I1001 11:44:33.118713  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.396617 (* 1 = 0.396617 loss)
I1001 11:44:33.171700  5182 solver.cpp:218] Iteration 67000 (15.3898 iter/s, 6.49782s/100 iters), loss = 0.019484
I1001 11:44:33.171741  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194837 (* 1 = 0.0194837 loss)
I1001 11:44:33.171748  5182 sgd_solver.cpp:105] Iteration 67000, lr = 0.01
I1001 11:44:38.421666  5182 solver.cpp:218] Iteration 67100 (19.048 iter/s, 5.2499s/100 iters), loss = 0.0976143
I1001 11:44:38.421706  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.097614 (* 1 = 0.097614 loss)
I1001 11:44:38.421712  5182 sgd_solver.cpp:105] Iteration 67100, lr = 0.01
I1001 11:44:43.675751  5182 solver.cpp:218] Iteration 67200 (19.033 iter/s, 5.25402s/100 iters), loss = 0.10491
I1001 11:44:43.675915  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.10491 (* 1 = 0.10491 loss)
I1001 11:44:43.675922  5182 sgd_solver.cpp:105] Iteration 67200, lr = 0.01
I1001 11:44:48.935376  5182 solver.cpp:218] Iteration 67300 (19.0134 iter/s, 5.25944s/100 iters), loss = 0.0479542
I1001 11:44:48.935416  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0479539 (* 1 = 0.0479539 loss)
I1001 11:44:48.935422  5182 sgd_solver.cpp:105] Iteration 67300, lr = 0.01
I1001 11:44:54.196125  5182 solver.cpp:218] Iteration 67400 (19.0089 iter/s, 5.26069s/100 iters), loss = 0.033553
I1001 11:44:54.196166  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0335527 (* 1 = 0.0335527 loss)
I1001 11:44:54.196172  5182 sgd_solver.cpp:105] Iteration 67400, lr = 0.01
I1001 11:44:59.187697  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:44:59.398102  5182 solver.cpp:330] Iteration 67500, Testing net (#0)
I1001 11:45:00.596037  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:45:00.646384  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.895
I1001 11:45:00.646419  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.402409 (* 1 = 0.402409 loss)
I1001 11:45:00.699807  5182 solver.cpp:218] Iteration 67500 (15.3761 iter/s, 6.50362s/100 iters), loss = 0.0264634
I1001 11:45:00.699836  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0264632 (* 1 = 0.0264632 loss)
I1001 11:45:00.699842  5182 sgd_solver.cpp:105] Iteration 67500, lr = 0.01
I1001 11:45:05.947016  5182 solver.cpp:218] Iteration 67600 (19.058 iter/s, 5.24715s/100 iters), loss = 0.0183587
I1001 11:45:05.947051  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0183584 (* 1 = 0.0183584 loss)
I1001 11:45:05.947058  5182 sgd_solver.cpp:105] Iteration 67600, lr = 0.01
I1001 11:45:11.200776  5182 solver.cpp:218] Iteration 67700 (19.0342 iter/s, 5.25371s/100 iters), loss = 0.0168369
I1001 11:45:11.200819  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168367 (* 1 = 0.0168367 loss)
I1001 11:45:11.200825  5182 sgd_solver.cpp:105] Iteration 67700, lr = 0.01
I1001 11:45:16.455560  5182 solver.cpp:218] Iteration 67800 (19.0305 iter/s, 5.25472s/100 iters), loss = 0.00861648
I1001 11:45:16.455685  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00861622 (* 1 = 0.00861622 loss)
I1001 11:45:16.455703  5182 sgd_solver.cpp:105] Iteration 67800, lr = 0.01
I1001 11:45:21.713069  5182 solver.cpp:218] Iteration 67900 (19.0209 iter/s, 5.25736s/100 iters), loss = 0.0127539
I1001 11:45:21.713109  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0127536 (* 1 = 0.0127536 loss)
I1001 11:45:21.713115  5182 sgd_solver.cpp:105] Iteration 67900, lr = 0.01
I1001 11:45:26.700186  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:45:26.912503  5182 solver.cpp:330] Iteration 68000, Testing net (#0)
I1001 11:45:28.107136  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:45:28.157073  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8962
I1001 11:45:28.157096  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.405184 (* 1 = 0.405184 loss)
I1001 11:45:28.209831  5182 solver.cpp:218] Iteration 68000 (15.3924 iter/s, 6.4967s/100 iters), loss = 0.00554831
I1001 11:45:28.209856  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00554805 (* 1 = 0.00554805 loss)
I1001 11:45:28.209862  5182 sgd_solver.cpp:105] Iteration 68000, lr = 0.01
I1001 11:45:33.466838  5182 solver.cpp:218] Iteration 68100 (19.0224 iter/s, 5.25696s/100 iters), loss = 0.0589436
I1001 11:45:33.466879  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0589433 (* 1 = 0.0589433 loss)
I1001 11:45:33.466886  5182 sgd_solver.cpp:105] Iteration 68100, lr = 0.01
I1001 11:45:38.719209  5182 solver.cpp:218] Iteration 68200 (19.0392 iter/s, 5.25231s/100 iters), loss = 0.0199261
I1001 11:45:38.719249  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0199259 (* 1 = 0.0199259 loss)
I1001 11:45:38.719254  5182 sgd_solver.cpp:105] Iteration 68200, lr = 0.01
I1001 11:45:43.975881  5182 solver.cpp:218] Iteration 68300 (19.0237 iter/s, 5.25661s/100 iters), loss = 0.03758
I1001 11:45:43.975920  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0375798 (* 1 = 0.0375798 loss)
I1001 11:45:43.975926  5182 sgd_solver.cpp:105] Iteration 68300, lr = 0.01
I1001 11:45:49.238127  5182 solver.cpp:218] Iteration 68400 (19.0035 iter/s, 5.26218s/100 iters), loss = 0.0242414
I1001 11:45:49.238248  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0242411 (* 1 = 0.0242411 loss)
I1001 11:45:49.238267  5182 sgd_solver.cpp:105] Iteration 68400, lr = 0.01
I1001 11:45:54.234413  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:45:54.444138  5182 solver.cpp:330] Iteration 68500, Testing net (#0)
I1001 11:45:55.634263  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:45:55.684553  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8939
I1001 11:45:55.684587  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.412606 (* 1 = 0.412606 loss)
I1001 11:45:55.737196  5182 solver.cpp:218] Iteration 68500 (15.3871 iter/s, 6.49893s/100 iters), loss = 0.016976
I1001 11:45:55.737233  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0169757 (* 1 = 0.0169757 loss)
I1001 11:45:55.737241  5182 sgd_solver.cpp:105] Iteration 68500, lr = 0.01
I1001 11:46:00.993487  5182 solver.cpp:218] Iteration 68600 (19.025 iter/s, 5.25624s/100 iters), loss = 0.0488376
I1001 11:46:00.993518  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0488373 (* 1 = 0.0488373 loss)
I1001 11:46:00.993525  5182 sgd_solver.cpp:105] Iteration 68600, lr = 0.01
I1001 11:46:06.247308  5182 solver.cpp:218] Iteration 68700 (19.034 iter/s, 5.25377s/100 iters), loss = 0.0329643
I1001 11:46:06.247339  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.032964 (* 1 = 0.032964 loss)
I1001 11:46:06.247344  5182 sgd_solver.cpp:105] Iteration 68700, lr = 0.01
I1001 11:46:11.492431  5182 solver.cpp:218] Iteration 68800 (19.0655 iter/s, 5.24507s/100 iters), loss = 0.0284371
I1001 11:46:11.492471  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0284369 (* 1 = 0.0284369 loss)
I1001 11:46:11.492476  5182 sgd_solver.cpp:105] Iteration 68800, lr = 0.01
I1001 11:46:16.745736  5182 solver.cpp:218] Iteration 68900 (19.0359 iter/s, 5.25324s/100 iters), loss = 0.00756229
I1001 11:46:16.745775  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00756203 (* 1 = 0.00756203 loss)
I1001 11:46:16.745780  5182 sgd_solver.cpp:105] Iteration 68900, lr = 0.01
I1001 11:46:21.737234  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:46:21.947351  5182 solver.cpp:330] Iteration 69000, Testing net (#0)
I1001 11:46:23.135817  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:46:23.186209  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8929
I1001 11:46:23.186244  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417956 (* 1 = 0.417956 loss)
I1001 11:46:23.238394  5182 solver.cpp:218] Iteration 69000 (15.4022 iter/s, 6.4926s/100 iters), loss = 0.0151829
I1001 11:46:23.238420  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0151827 (* 1 = 0.0151827 loss)
I1001 11:46:23.238425  5182 sgd_solver.cpp:105] Iteration 69000, lr = 0.01
I1001 11:46:28.500222  5182 solver.cpp:218] Iteration 69100 (19.005 iter/s, 5.26178s/100 iters), loss = 0.0267061
I1001 11:46:28.500252  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0267058 (* 1 = 0.0267058 loss)
I1001 11:46:28.500257  5182 sgd_solver.cpp:105] Iteration 69100, lr = 0.01
I1001 11:46:33.758571  5182 solver.cpp:218] Iteration 69200 (19.0176 iter/s, 5.25829s/100 iters), loss = 0.0337804
I1001 11:46:33.758601  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0337802 (* 1 = 0.0337802 loss)
I1001 11:46:33.758617  5182 sgd_solver.cpp:105] Iteration 69200, lr = 0.01
I1001 11:46:39.017408  5182 solver.cpp:218] Iteration 69300 (19.0158 iter/s, 5.25878s/100 iters), loss = 0.035466
I1001 11:46:39.017480  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0354657 (* 1 = 0.0354657 loss)
I1001 11:46:39.017493  5182 sgd_solver.cpp:105] Iteration 69300, lr = 0.01
I1001 11:46:44.269062  5182 solver.cpp:218] Iteration 69400 (19.0421 iter/s, 5.25153s/100 iters), loss = 0.0222446
I1001 11:46:44.269103  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0222443 (* 1 = 0.0222443 loss)
I1001 11:46:44.269109  5182 sgd_solver.cpp:105] Iteration 69400, lr = 0.01
I1001 11:46:49.265462  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:46:49.474714  5182 solver.cpp:330] Iteration 69500, Testing net (#0)
I1001 11:46:50.662570  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:46:50.712733  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9019
I1001 11:46:50.712769  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385466 (* 1 = 0.385466 loss)
I1001 11:46:50.765125  5182 solver.cpp:218] Iteration 69500 (15.3941 iter/s, 6.496s/100 iters), loss = 0.0196213
I1001 11:46:50.765148  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.019621 (* 1 = 0.019621 loss)
I1001 11:46:50.765154  5182 sgd_solver.cpp:105] Iteration 69500, lr = 0.01
I1001 11:46:56.020115  5182 solver.cpp:218] Iteration 69600 (19.0297 iter/s, 5.25494s/100 iters), loss = 0.009638
I1001 11:46:56.020251  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00963776 (* 1 = 0.00963776 loss)
I1001 11:46:56.020262  5182 sgd_solver.cpp:105] Iteration 69600, lr = 0.01
I1001 11:47:01.272363  5182 solver.cpp:218] Iteration 69700 (19.04 iter/s, 5.2521s/100 iters), loss = 0.0216483
I1001 11:47:01.272393  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216481 (* 1 = 0.0216481 loss)
I1001 11:47:01.272402  5182 sgd_solver.cpp:105] Iteration 69700, lr = 0.01
I1001 11:47:06.526932  5182 solver.cpp:218] Iteration 69800 (19.0312 iter/s, 5.25452s/100 iters), loss = 0.020449
I1001 11:47:06.526963  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0204488 (* 1 = 0.0204488 loss)
I1001 11:47:06.526971  5182 sgd_solver.cpp:105] Iteration 69800, lr = 0.01
I1001 11:47:11.777066  5182 solver.cpp:218] Iteration 69900 (19.0473 iter/s, 5.25008s/100 iters), loss = 0.0330364
I1001 11:47:11.777098  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0330362 (* 1 = 0.0330362 loss)
I1001 11:47:11.777107  5182 sgd_solver.cpp:105] Iteration 69900, lr = 0.01
I1001 11:47:16.771800  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:47:16.981585  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_70000.caffemodel
I1001 11:47:16.986501  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_70000.solverstate
I1001 11:47:16.987849  5182 solver.cpp:330] Iteration 70000, Testing net (#0)
I1001 11:47:18.186573  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:47:18.236944  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8998
I1001 11:47:18.236970  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.397576 (* 1 = 0.397576 loss)
I1001 11:47:18.289523  5182 solver.cpp:218] Iteration 70000 (15.3553 iter/s, 6.5124s/100 iters), loss = 0.057535
I1001 11:47:18.289558  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0575348 (* 1 = 0.0575348 loss)
I1001 11:47:18.289577  5182 sgd_solver.cpp:105] Iteration 70000, lr = 0.01
I1001 11:47:23.536707  5182 solver.cpp:218] Iteration 70100 (19.058 iter/s, 5.24713s/100 iters), loss = 0.0149926
I1001 11:47:23.536736  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0149923 (* 1 = 0.0149923 loss)
I1001 11:47:23.536742  5182 sgd_solver.cpp:105] Iteration 70100, lr = 0.01
I1001 11:47:28.795619  5182 solver.cpp:218] Iteration 70200 (19.0155 iter/s, 5.25886s/100 iters), loss = 0.0529971
I1001 11:47:28.795785  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0529969 (* 1 = 0.0529969 loss)
I1001 11:47:28.795807  5182 sgd_solver.cpp:105] Iteration 70200, lr = 0.01
I1001 11:47:34.053443  5182 solver.cpp:218] Iteration 70300 (19.0199 iter/s, 5.25765s/100 iters), loss = 0.0485936
I1001 11:47:34.053483  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0485934 (* 1 = 0.0485934 loss)
I1001 11:47:34.053489  5182 sgd_solver.cpp:105] Iteration 70300, lr = 0.01
I1001 11:47:39.307224  5182 solver.cpp:218] Iteration 70400 (19.0341 iter/s, 5.25372s/100 iters), loss = 0.0162026
I1001 11:47:39.307253  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0162024 (* 1 = 0.0162024 loss)
I1001 11:47:39.307260  5182 sgd_solver.cpp:105] Iteration 70400, lr = 0.01
I1001 11:47:44.285795  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:47:44.496471  5182 solver.cpp:330] Iteration 70500, Testing net (#0)
I1001 11:47:45.692649  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:47:45.742748  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8964
I1001 11:47:45.742781  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.408975 (* 1 = 0.408975 loss)
I1001 11:47:45.795516  5182 solver.cpp:218] Iteration 70500 (15.4125 iter/s, 6.48824s/100 iters), loss = 0.0368063
I1001 11:47:45.795552  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0368061 (* 1 = 0.0368061 loss)
I1001 11:47:45.795558  5182 sgd_solver.cpp:105] Iteration 70500, lr = 0.01
I1001 11:47:51.056147  5182 solver.cpp:218] Iteration 70600 (19.0093 iter/s, 5.26058s/100 iters), loss = 0.0216303
I1001 11:47:51.056182  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216301 (* 1 = 0.0216301 loss)
I1001 11:47:51.056188  5182 sgd_solver.cpp:105] Iteration 70600, lr = 0.01
I1001 11:47:56.305127  5182 solver.cpp:218] Iteration 70700 (19.0515 iter/s, 5.24893s/100 iters), loss = 0.0121458
I1001 11:47:56.305156  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121456 (* 1 = 0.0121456 loss)
I1001 11:47:56.305162  5182 sgd_solver.cpp:105] Iteration 70700, lr = 0.01
I1001 11:48:01.565389  5182 solver.cpp:218] Iteration 70800 (19.0106 iter/s, 5.26021s/100 iters), loss = 0.023038
I1001 11:48:01.565496  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0230378 (* 1 = 0.0230378 loss)
I1001 11:48:01.565513  5182 sgd_solver.cpp:105] Iteration 70800, lr = 0.01
I1001 11:48:06.818032  5182 solver.cpp:218] Iteration 70900 (19.0385 iter/s, 5.25252s/100 iters), loss = 0.0205267
I1001 11:48:06.818071  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0205265 (* 1 = 0.0205265 loss)
I1001 11:48:06.818078  5182 sgd_solver.cpp:105] Iteration 70900, lr = 0.01
I1001 11:48:11.802220  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:48:12.018745  5182 solver.cpp:330] Iteration 71000, Testing net (#0)
I1001 11:48:13.212136  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:48:13.262243  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9017
I1001 11:48:13.262269  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.384766 (* 1 = 0.384766 loss)
I1001 11:48:13.314721  5182 solver.cpp:218] Iteration 71000 (15.3926 iter/s, 6.49663s/100 iters), loss = 0.0279834
I1001 11:48:13.314749  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0279832 (* 1 = 0.0279832 loss)
I1001 11:48:13.314759  5182 sgd_solver.cpp:105] Iteration 71000, lr = 0.01
I1001 11:48:18.574290  5182 solver.cpp:218] Iteration 71100 (19.0131 iter/s, 5.25952s/100 iters), loss = 0.0351853
I1001 11:48:18.574319  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0351851 (* 1 = 0.0351851 loss)
I1001 11:48:18.574326  5182 sgd_solver.cpp:105] Iteration 71100, lr = 0.01
I1001 11:48:23.825078  5182 solver.cpp:218] Iteration 71200 (19.0449 iter/s, 5.25074s/100 iters), loss = 0.0270417
I1001 11:48:23.825111  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0270415 (* 1 = 0.0270415 loss)
I1001 11:48:23.825129  5182 sgd_solver.cpp:105] Iteration 71200, lr = 0.01
I1001 11:48:29.077355  5182 solver.cpp:218] Iteration 71300 (19.0395 iter/s, 5.25223s/100 iters), loss = 0.0298277
I1001 11:48:29.077385  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0298275 (* 1 = 0.0298275 loss)
I1001 11:48:29.077391  5182 sgd_solver.cpp:105] Iteration 71300, lr = 0.01
I1001 11:48:34.333454  5182 solver.cpp:218] Iteration 71400 (19.0257 iter/s, 5.25605s/100 iters), loss = 0.0381668
I1001 11:48:34.333575  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0381665 (* 1 = 0.0381665 loss)
I1001 11:48:34.333595  5182 sgd_solver.cpp:105] Iteration 71400, lr = 0.01
I1001 11:48:39.325532  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:48:39.536399  5182 solver.cpp:330] Iteration 71500, Testing net (#0)
I1001 11:48:40.724637  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:48:40.774729  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8936
I1001 11:48:40.774763  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.421036 (* 1 = 0.421036 loss)
I1001 11:48:40.827378  5182 solver.cpp:218] Iteration 71500 (15.3993 iter/s, 6.4938s/100 iters), loss = 0.037027
I1001 11:48:40.827404  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0370268 (* 1 = 0.0370268 loss)
I1001 11:48:40.827409  5182 sgd_solver.cpp:105] Iteration 71500, lr = 0.01
I1001 11:48:46.087163  5182 solver.cpp:218] Iteration 71600 (19.0124 iter/s, 5.25974s/100 iters), loss = 0.0133684
I1001 11:48:46.087193  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0133682 (* 1 = 0.0133682 loss)
I1001 11:48:46.087199  5182 sgd_solver.cpp:105] Iteration 71600, lr = 0.01
I1001 11:48:51.348417  5182 solver.cpp:218] Iteration 71700 (19.0071 iter/s, 5.2612s/100 iters), loss = 0.0112506
I1001 11:48:51.348448  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112503 (* 1 = 0.0112503 loss)
I1001 11:48:51.348453  5182 sgd_solver.cpp:105] Iteration 71700, lr = 0.01
I1001 11:48:56.595559  5182 solver.cpp:218] Iteration 71800 (19.0582 iter/s, 5.24709s/100 iters), loss = 0.0121306
I1001 11:48:56.595588  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121303 (* 1 = 0.0121303 loss)
I1001 11:48:56.595594  5182 sgd_solver.cpp:105] Iteration 71800, lr = 0.01
I1001 11:49:01.855243  5182 solver.cpp:218] Iteration 71900 (19.0127 iter/s, 5.25963s/100 iters), loss = 0.00410866
I1001 11:49:01.855283  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00410842 (* 1 = 0.00410842 loss)
I1001 11:49:01.855288  5182 sgd_solver.cpp:105] Iteration 71900, lr = 0.01
I1001 11:49:06.852615  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:49:07.063032  5182 solver.cpp:330] Iteration 72000, Testing net (#0)
I1001 11:49:08.250959  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:49:08.301252  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8942
I1001 11:49:08.301287  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.406948 (* 1 = 0.406948 loss)
I1001 11:49:08.354190  5182 solver.cpp:218] Iteration 72000 (15.3873 iter/s, 6.49888s/100 iters), loss = 0.0283871
I1001 11:49:08.354228  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0283868 (* 1 = 0.0283868 loss)
I1001 11:49:08.354244  5182 sgd_solver.cpp:105] Iteration 72000, lr = 0.01
I1001 11:49:13.612784  5182 solver.cpp:218] Iteration 72100 (19.0167 iter/s, 5.25853s/100 iters), loss = 0.0300279
I1001 11:49:13.612813  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0300277 (* 1 = 0.0300277 loss)
I1001 11:49:13.612819  5182 sgd_solver.cpp:105] Iteration 72100, lr = 0.01
I1001 11:49:18.866480  5182 solver.cpp:218] Iteration 72200 (19.0344 iter/s, 5.25365s/100 iters), loss = 0.024419
I1001 11:49:18.866509  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0244188 (* 1 = 0.0244188 loss)
I1001 11:49:18.866528  5182 sgd_solver.cpp:105] Iteration 72200, lr = 0.01
I1001 11:49:24.117910  5182 solver.cpp:218] Iteration 72300 (19.0426 iter/s, 5.25138s/100 iters), loss = 0.0316708
I1001 11:49:24.117944  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0316706 (* 1 = 0.0316706 loss)
I1001 11:49:24.117950  5182 sgd_solver.cpp:105] Iteration 72300, lr = 0.01
I1001 11:49:29.365470  5182 solver.cpp:218] Iteration 72400 (19.0567 iter/s, 5.24751s/100 iters), loss = 0.0237786
I1001 11:49:29.365499  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0237783 (* 1 = 0.0237783 loss)
I1001 11:49:29.365505  5182 sgd_solver.cpp:105] Iteration 72400, lr = 0.01
I1001 11:49:34.362843  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:49:34.573488  5182 solver.cpp:330] Iteration 72500, Testing net (#0)
I1001 11:49:35.762895  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:49:35.813045  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8938
I1001 11:49:35.813079  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.414976 (* 1 = 0.414976 loss)
I1001 11:49:35.865984  5182 solver.cpp:218] Iteration 72500 (15.3835 iter/s, 6.50046s/100 iters), loss = 0.0220344
I1001 11:49:35.866015  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0220342 (* 1 = 0.0220342 loss)
I1001 11:49:35.866024  5182 sgd_solver.cpp:105] Iteration 72500, lr = 0.01
I1001 11:49:41.124500  5182 solver.cpp:218] Iteration 72600 (19.017 iter/s, 5.25846s/100 iters), loss = 0.0224321
I1001 11:49:41.124649  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0224318 (* 1 = 0.0224318 loss)
I1001 11:49:41.124657  5182 sgd_solver.cpp:105] Iteration 72600, lr = 0.01
I1001 11:49:46.384220  5182 solver.cpp:218] Iteration 72700 (19.013 iter/s, 5.25955s/100 iters), loss = 0.00762698
I1001 11:49:46.384261  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00762673 (* 1 = 0.00762673 loss)
I1001 11:49:46.384276  5182 sgd_solver.cpp:105] Iteration 72700, lr = 0.01
I1001 11:49:51.642820  5182 solver.cpp:218] Iteration 72800 (19.0167 iter/s, 5.25854s/100 iters), loss = 0.0175706
I1001 11:49:51.642859  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175704 (* 1 = 0.0175704 loss)
I1001 11:49:51.642866  5182 sgd_solver.cpp:105] Iteration 72800, lr = 0.01
I1001 11:49:56.891633  5182 solver.cpp:218] Iteration 72900 (19.0522 iter/s, 5.24875s/100 iters), loss = 0.0139532
I1001 11:49:56.891670  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0139529 (* 1 = 0.0139529 loss)
I1001 11:49:56.891677  5182 sgd_solver.cpp:105] Iteration 72900, lr = 0.01
I1001 11:50:01.885043  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:50:02.095424  5182 solver.cpp:330] Iteration 73000, Testing net (#0)
I1001 11:50:03.290947  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:50:03.341163  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8943
I1001 11:50:03.341198  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.42959 (* 1 = 0.42959 loss)
I1001 11:50:03.394203  5182 solver.cpp:218] Iteration 73000 (15.3787 iter/s, 6.50251s/100 iters), loss = 0.0244675
I1001 11:50:03.394248  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0244673 (* 1 = 0.0244673 loss)
I1001 11:50:03.394254  5182 sgd_solver.cpp:105] Iteration 73000, lr = 0.01
I1001 11:50:08.641149  5182 solver.cpp:218] Iteration 73100 (19.0589 iter/s, 5.24688s/100 iters), loss = 0.0120353
I1001 11:50:08.641183  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0120351 (* 1 = 0.0120351 loss)
I1001 11:50:08.641191  5182 sgd_solver.cpp:105] Iteration 73100, lr = 0.01
I1001 11:50:13.896634  5182 solver.cpp:218] Iteration 73200 (19.0279 iter/s, 5.25543s/100 iters), loss = 0.0449686
I1001 11:50:13.896818  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0449684 (* 1 = 0.0449684 loss)
I1001 11:50:13.896832  5182 sgd_solver.cpp:105] Iteration 73200, lr = 0.01
I1001 11:50:19.151226  5182 solver.cpp:218] Iteration 73300 (19.0316 iter/s, 5.25442s/100 iters), loss = 0.00499455
I1001 11:50:19.151257  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00499428 (* 1 = 0.00499428 loss)
I1001 11:50:19.151265  5182 sgd_solver.cpp:105] Iteration 73300, lr = 0.01
I1001 11:50:24.404191  5182 solver.cpp:218] Iteration 73400 (19.0371 iter/s, 5.25291s/100 iters), loss = 0.0100707
I1001 11:50:24.404224  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0100704 (* 1 = 0.0100704 loss)
I1001 11:50:24.404243  5182 sgd_solver.cpp:105] Iteration 73400, lr = 0.01
I1001 11:50:29.388460  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:50:29.598783  5182 solver.cpp:330] Iteration 73500, Testing net (#0)
I1001 11:50:30.795254  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:50:30.845304  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9024
I1001 11:50:30.845329  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38431 (* 1 = 0.38431 loss)
I1001 11:50:30.897780  5182 solver.cpp:218] Iteration 73500 (15.3999 iter/s, 6.49354s/100 iters), loss = 0.0105688
I1001 11:50:30.897805  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0105686 (* 1 = 0.0105686 loss)
I1001 11:50:30.897811  5182 sgd_solver.cpp:105] Iteration 73500, lr = 0.01
I1001 11:50:36.155364  5182 solver.cpp:218] Iteration 73600 (19.0203 iter/s, 5.25753s/100 iters), loss = 0.00575065
I1001 11:50:36.155400  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00575038 (* 1 = 0.00575038 loss)
I1001 11:50:36.155417  5182 sgd_solver.cpp:105] Iteration 73600, lr = 0.01
I1001 11:50:41.400668  5182 solver.cpp:218] Iteration 73700 (19.065 iter/s, 5.24521s/100 iters), loss = 0.0259115
I1001 11:50:41.400698  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0259113 (* 1 = 0.0259113 loss)
I1001 11:50:41.400715  5182 sgd_solver.cpp:105] Iteration 73700, lr = 0.01
I1001 11:50:46.656731  5182 solver.cpp:218] Iteration 73800 (19.0258 iter/s, 5.25601s/100 iters), loss = 0.0224962
I1001 11:50:46.656829  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.022496 (* 1 = 0.022496 loss)
I1001 11:50:46.656836  5182 sgd_solver.cpp:105] Iteration 73800, lr = 0.01
I1001 11:50:51.911324  5182 solver.cpp:218] Iteration 73900 (19.0314 iter/s, 5.25449s/100 iters), loss = 0.0228335
I1001 11:50:51.911365  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0228332 (* 1 = 0.0228332 loss)
I1001 11:50:51.911371  5182 sgd_solver.cpp:105] Iteration 73900, lr = 0.01
I1001 11:50:56.893985  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:50:57.109828  5182 solver.cpp:330] Iteration 74000, Testing net (#0)
I1001 11:50:58.298542  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:50:58.348924  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8872
I1001 11:50:58.348949  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.457442 (* 1 = 0.457442 loss)
I1001 11:50:58.401466  5182 solver.cpp:218] Iteration 74000 (15.4081 iter/s, 6.49008s/100 iters), loss = 0.0454103
I1001 11:50:58.401491  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.04541 (* 1 = 0.04541 loss)
I1001 11:50:58.401497  5182 sgd_solver.cpp:105] Iteration 74000, lr = 0.01
I1001 11:51:03.659332  5182 solver.cpp:218] Iteration 74100 (19.0193 iter/s, 5.25781s/100 iters), loss = 0.0167594
I1001 11:51:03.659360  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0167591 (* 1 = 0.0167591 loss)
I1001 11:51:03.659366  5182 sgd_solver.cpp:105] Iteration 74100, lr = 0.01
I1001 11:51:08.906376  5182 solver.cpp:218] Iteration 74200 (19.0585 iter/s, 5.24699s/100 iters), loss = 0.0189237
I1001 11:51:08.906411  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0189235 (* 1 = 0.0189235 loss)
I1001 11:51:08.906419  5182 sgd_solver.cpp:105] Iteration 74200, lr = 0.01
I1001 11:51:14.162798  5182 solver.cpp:218] Iteration 74300 (19.0245 iter/s, 5.25637s/100 iters), loss = 0.026804
I1001 11:51:14.162837  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0268037 (* 1 = 0.0268037 loss)
I1001 11:51:14.162843  5182 sgd_solver.cpp:105] Iteration 74300, lr = 0.01
I1001 11:51:19.420490  5182 solver.cpp:218] Iteration 74400 (19.02 iter/s, 5.25763s/100 iters), loss = 0.00763223
I1001 11:51:19.420655  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00763196 (* 1 = 0.00763196 loss)
I1001 11:51:19.420665  5182 sgd_solver.cpp:105] Iteration 74400, lr = 0.01
I1001 11:51:24.416792  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:51:24.627524  5182 solver.cpp:330] Iteration 74500, Testing net (#0)
I1001 11:51:25.815516  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:51:25.865768  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8942
I1001 11:51:25.865803  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.421582 (* 1 = 0.421582 loss)
I1001 11:51:25.918442  5182 solver.cpp:218] Iteration 74500 (15.3899 iter/s, 6.49777s/100 iters), loss = 0.0104432
I1001 11:51:25.918467  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0104429 (* 1 = 0.0104429 loss)
I1001 11:51:25.918473  5182 sgd_solver.cpp:105] Iteration 74500, lr = 0.01
I1001 11:51:31.177650  5182 solver.cpp:218] Iteration 74600 (19.0144 iter/s, 5.25916s/100 iters), loss = 0.0113424
I1001 11:51:31.177678  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0113421 (* 1 = 0.0113421 loss)
I1001 11:51:31.177695  5182 sgd_solver.cpp:105] Iteration 74600, lr = 0.01
I1001 11:51:36.431008  5182 solver.cpp:218] Iteration 74700 (19.0356 iter/s, 5.25331s/100 iters), loss = 0.0140441
I1001 11:51:36.431041  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140438 (* 1 = 0.0140438 loss)
I1001 11:51:36.431047  5182 sgd_solver.cpp:105] Iteration 74700, lr = 0.01
I1001 11:51:41.677263  5182 solver.cpp:218] Iteration 74800 (19.0614 iter/s, 5.2462s/100 iters), loss = 0.0211861
I1001 11:51:41.677301  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0211859 (* 1 = 0.0211859 loss)
I1001 11:51:41.677307  5182 sgd_solver.cpp:105] Iteration 74800, lr = 0.01
I1001 11:51:46.931320  5182 solver.cpp:218] Iteration 74900 (19.0331 iter/s, 5.25399s/100 iters), loss = 0.019627
I1001 11:51:46.931350  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0196268 (* 1 = 0.0196268 loss)
I1001 11:51:46.931355  5182 sgd_solver.cpp:105] Iteration 74900, lr = 0.01
I1001 11:51:51.922426  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:51:52.133277  5182 solver.cpp:330] Iteration 75000, Testing net (#0)
I1001 11:51:53.322326  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:51:53.372364  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8895
I1001 11:51:53.372397  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.436257 (* 1 = 0.436257 loss)
I1001 11:51:53.424947  5182 solver.cpp:218] Iteration 75000 (15.3998 iter/s, 6.49358s/100 iters), loss = 0.0242498
I1001 11:51:53.424973  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0242495 (* 1 = 0.0242495 loss)
I1001 11:51:53.424978  5182 sgd_solver.cpp:105] Iteration 75000, lr = 0.01
I1001 11:51:58.683753  5182 solver.cpp:218] Iteration 75100 (19.0159 iter/s, 5.25876s/100 iters), loss = 0.0260347
I1001 11:51:58.683781  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0260344 (* 1 = 0.0260344 loss)
I1001 11:51:58.683797  5182 sgd_solver.cpp:105] Iteration 75100, lr = 0.01
I1001 11:52:03.945313  5182 solver.cpp:218] Iteration 75200 (19.0059 iter/s, 5.26151s/100 iters), loss = 0.0420962
I1001 11:52:03.945343  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0420959 (* 1 = 0.0420959 loss)
I1001 11:52:03.945358  5182 sgd_solver.cpp:105] Iteration 75200, lr = 0.01
I1001 11:52:09.206446  5182 solver.cpp:218] Iteration 75300 (19.0075 iter/s, 5.26107s/100 iters), loss = 0.0172174
I1001 11:52:09.206480  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0172171 (* 1 = 0.0172171 loss)
I1001 11:52:09.206485  5182 sgd_solver.cpp:105] Iteration 75300, lr = 0.01
I1001 11:52:14.458071  5182 solver.cpp:218] Iteration 75400 (19.0419 iter/s, 5.25157s/100 iters), loss = 0.0180946
I1001 11:52:14.458101  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0180944 (* 1 = 0.0180944 loss)
I1001 11:52:14.458117  5182 sgd_solver.cpp:105] Iteration 75400, lr = 0.01
I1001 11:52:19.453461  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:52:19.664305  5182 solver.cpp:330] Iteration 75500, Testing net (#0)
I1001 11:52:20.851496  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:52:20.902501  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8894
I1001 11:52:20.902539  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.412381 (* 1 = 0.412381 loss)
I1001 11:52:20.956243  5182 solver.cpp:218] Iteration 75500 (15.3891 iter/s, 6.49811s/100 iters), loss = 0.0345833
I1001 11:52:20.956284  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0345831 (* 1 = 0.0345831 loss)
I1001 11:52:20.956300  5182 sgd_solver.cpp:105] Iteration 75500, lr = 0.01
I1001 11:52:26.211745  5182 solver.cpp:218] Iteration 75600 (19.028 iter/s, 5.2554s/100 iters), loss = 0.0307217
I1001 11:52:26.211854  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0307214 (* 1 = 0.0307214 loss)
I1001 11:52:26.211871  5182 sgd_solver.cpp:105] Iteration 75600, lr = 0.01
I1001 11:52:31.464221  5182 solver.cpp:218] Iteration 75700 (19.0391 iter/s, 5.25234s/100 iters), loss = 0.0529722
I1001 11:52:31.464251  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0529719 (* 1 = 0.0529719 loss)
I1001 11:52:31.464257  5182 sgd_solver.cpp:105] Iteration 75700, lr = 0.01
I1001 11:52:36.719108  5182 solver.cpp:218] Iteration 75800 (19.0301 iter/s, 5.25484s/100 iters), loss = 0.0930385
I1001 11:52:36.719148  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0930382 (* 1 = 0.0930382 loss)
I1001 11:52:36.719154  5182 sgd_solver.cpp:105] Iteration 75800, lr = 0.01
I1001 11:52:41.966223  5182 solver.cpp:218] Iteration 75900 (19.0583 iter/s, 5.24705s/100 iters), loss = 0.00713862
I1001 11:52:41.966265  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00713832 (* 1 = 0.00713832 loss)
I1001 11:52:41.966272  5182 sgd_solver.cpp:105] Iteration 75900, lr = 0.01
I1001 11:52:46.955564  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:52:47.165767  5182 solver.cpp:330] Iteration 76000, Testing net (#0)
I1001 11:52:48.362329  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:52:48.412659  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8888
I1001 11:52:48.412684  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.468655 (* 1 = 0.468655 loss)
I1001 11:52:48.465286  5182 solver.cpp:218] Iteration 76000 (15.3871 iter/s, 6.49897s/100 iters), loss = 0.0440232
I1001 11:52:48.465317  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0440229 (* 1 = 0.0440229 loss)
I1001 11:52:48.465323  5182 sgd_solver.cpp:105] Iteration 76000, lr = 0.01
I1001 11:52:53.715175  5182 solver.cpp:218] Iteration 76100 (19.0482 iter/s, 5.24983s/100 iters), loss = 0.0175147
I1001 11:52:53.715215  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175143 (* 1 = 0.0175143 loss)
I1001 11:52:53.715221  5182 sgd_solver.cpp:105] Iteration 76100, lr = 0.01
I1001 11:52:58.974503  5182 solver.cpp:218] Iteration 76200 (19.014 iter/s, 5.25927s/100 iters), loss = 0.0208831
I1001 11:52:58.974629  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0208828 (* 1 = 0.0208828 loss)
I1001 11:52:58.974647  5182 sgd_solver.cpp:105] Iteration 76200, lr = 0.01
I1001 11:53:04.226505  5182 solver.cpp:218] Iteration 76300 (19.0409 iter/s, 5.25186s/100 iters), loss = 0.0773965
I1001 11:53:04.226558  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0773962 (* 1 = 0.0773962 loss)
I1001 11:53:04.226565  5182 sgd_solver.cpp:105] Iteration 76300, lr = 0.01
I1001 11:53:09.486620  5182 solver.cpp:218] Iteration 76400 (19.0113 iter/s, 5.26004s/100 iters), loss = 0.0156261
I1001 11:53:09.486661  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0156258 (* 1 = 0.0156258 loss)
I1001 11:53:09.486667  5182 sgd_solver.cpp:105] Iteration 76400, lr = 0.01
I1001 11:53:14.472429  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:53:14.683168  5182 solver.cpp:330] Iteration 76500, Testing net (#0)
I1001 11:53:15.880288  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:53:15.930438  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8948
I1001 11:53:15.930471  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.414138 (* 1 = 0.414138 loss)
I1001 11:53:15.983078  5182 solver.cpp:218] Iteration 76500 (15.3931 iter/s, 6.4964s/100 iters), loss = 0.0239153
I1001 11:53:15.983105  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.023915 (* 1 = 0.023915 loss)
I1001 11:53:15.983113  5182 sgd_solver.cpp:105] Iteration 76500, lr = 0.01
I1001 11:53:21.232630  5182 solver.cpp:218] Iteration 76600 (19.0494 iter/s, 5.2495s/100 iters), loss = 0.0184843
I1001 11:53:21.232658  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.018484 (* 1 = 0.018484 loss)
I1001 11:53:21.232666  5182 sgd_solver.cpp:105] Iteration 76600, lr = 0.01
I1001 11:53:26.484887  5182 solver.cpp:218] Iteration 76700 (19.0396 iter/s, 5.25221s/100 iters), loss = 0.0334052
I1001 11:53:26.484926  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0334049 (* 1 = 0.0334049 loss)
I1001 11:53:26.484932  5182 sgd_solver.cpp:105] Iteration 76700, lr = 0.01
I1001 11:53:31.737769  5182 solver.cpp:218] Iteration 76800 (19.0374 iter/s, 5.25282s/100 iters), loss = 0.0269595
I1001 11:53:31.737884  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0269592 (* 1 = 0.0269592 loss)
I1001 11:53:31.737890  5182 sgd_solver.cpp:105] Iteration 76800, lr = 0.01
I1001 11:53:36.995875  5182 solver.cpp:218] Iteration 76900 (19.0187 iter/s, 5.25798s/100 iters), loss = 0.00340145
I1001 11:53:36.995904  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00340117 (* 1 = 0.00340117 loss)
I1001 11:53:36.995910  5182 sgd_solver.cpp:105] Iteration 76900, lr = 0.01
I1001 11:53:41.986026  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:53:42.201228  5182 solver.cpp:330] Iteration 77000, Testing net (#0)
I1001 11:53:43.392196  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:53:43.442559  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8913
I1001 11:53:43.442584  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.411189 (* 1 = 0.411189 loss)
I1001 11:53:43.494711  5182 solver.cpp:218] Iteration 77000 (15.3875 iter/s, 6.49878s/100 iters), loss = 0.0123631
I1001 11:53:43.494741  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123628 (* 1 = 0.0123628 loss)
I1001 11:53:43.494751  5182 sgd_solver.cpp:105] Iteration 77000, lr = 0.01
I1001 11:53:48.752694  5182 solver.cpp:218] Iteration 77100 (19.0189 iter/s, 5.25793s/100 iters), loss = 0.0330462
I1001 11:53:48.752727  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.033046 (* 1 = 0.033046 loss)
I1001 11:53:48.752735  5182 sgd_solver.cpp:105] Iteration 77100, lr = 0.01
I1001 11:53:54.003540  5182 solver.cpp:218] Iteration 77200 (19.0448 iter/s, 5.25079s/100 iters), loss = 0.0100312
I1001 11:53:54.003578  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.010031 (* 1 = 0.010031 loss)
I1001 11:53:54.003589  5182 sgd_solver.cpp:105] Iteration 77200, lr = 0.01
I1001 11:53:59.255962  5182 solver.cpp:218] Iteration 77300 (19.0391 iter/s, 5.25236s/100 iters), loss = 0.0127685
I1001 11:53:59.255995  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0127682 (* 1 = 0.0127682 loss)
I1001 11:53:59.256014  5182 sgd_solver.cpp:105] Iteration 77300, lr = 0.01
I1001 11:54:04.511979  5182 solver.cpp:218] Iteration 77400 (19.026 iter/s, 5.25596s/100 iters), loss = 0.0304156
I1001 11:54:04.512076  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0304154 (* 1 = 0.0304154 loss)
I1001 11:54:04.512085  5182 sgd_solver.cpp:105] Iteration 77400, lr = 0.01
I1001 11:54:09.504312  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:54:09.713956  5182 solver.cpp:330] Iteration 77500, Testing net (#0)
I1001 11:54:10.902748  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:54:10.952898  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8949
I1001 11:54:10.952934  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.423352 (* 1 = 0.423352 loss)
I1001 11:54:11.005725  5182 solver.cpp:218] Iteration 77500 (15.3997 iter/s, 6.49363s/100 iters), loss = 0.0368351
I1001 11:54:11.005760  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0368348 (* 1 = 0.0368348 loss)
I1001 11:54:11.005767  5182 sgd_solver.cpp:105] Iteration 77500, lr = 0.01
I1001 11:54:16.262387  5182 solver.cpp:218] Iteration 77600 (19.0237 iter/s, 5.25661s/100 iters), loss = 0.0164189
I1001 11:54:16.262428  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0164186 (* 1 = 0.0164186 loss)
I1001 11:54:16.262434  5182 sgd_solver.cpp:105] Iteration 77600, lr = 0.01
I1001 11:54:21.521857  5182 solver.cpp:218] Iteration 77700 (19.0135 iter/s, 5.25941s/100 iters), loss = 0.0047972
I1001 11:54:21.521890  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00479692 (* 1 = 0.00479692 loss)
I1001 11:54:21.521898  5182 sgd_solver.cpp:105] Iteration 77700, lr = 0.01
I1001 11:54:26.769116  5182 solver.cpp:218] Iteration 77800 (19.0578 iter/s, 5.24721s/100 iters), loss = 0.020951
I1001 11:54:26.769145  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0209507 (* 1 = 0.0209507 loss)
I1001 11:54:26.769150  5182 sgd_solver.cpp:105] Iteration 77800, lr = 0.01
I1001 11:54:32.024701  5182 solver.cpp:218] Iteration 77900 (19.0276 iter/s, 5.25554s/100 iters), loss = 0.0174471
I1001 11:54:32.024729  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0174468 (* 1 = 0.0174468 loss)
I1001 11:54:32.024735  5182 sgd_solver.cpp:105] Iteration 77900, lr = 0.01
I1001 11:54:37.020757  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:54:37.230901  5182 solver.cpp:330] Iteration 78000, Testing net (#0)
I1001 11:54:38.419144  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:54:38.469532  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8866
I1001 11:54:38.469558  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.4551 (* 1 = 0.4551 loss)
I1001 11:54:38.522219  5182 solver.cpp:218] Iteration 78000 (15.3906 iter/s, 6.49747s/100 iters), loss = 0.00770149
I1001 11:54:38.522261  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00770121 (* 1 = 0.00770121 loss)
I1001 11:54:38.522269  5182 sgd_solver.cpp:105] Iteration 78000, lr = 0.01
I1001 11:54:43.785914  5182 solver.cpp:218] Iteration 78100 (18.9983 iter/s, 5.26363s/100 iters), loss = 0.023706
I1001 11:54:43.785954  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0237057 (* 1 = 0.0237057 loss)
I1001 11:54:43.785960  5182 sgd_solver.cpp:105] Iteration 78100, lr = 0.01
I1001 11:54:49.043231  5182 solver.cpp:218] Iteration 78200 (19.0213 iter/s, 5.25726s/100 iters), loss = 0.00939277
I1001 11:54:49.043262  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00939248 (* 1 = 0.00939248 loss)
I1001 11:54:49.043268  5182 sgd_solver.cpp:105] Iteration 78200, lr = 0.01
I1001 11:54:54.297497  5182 solver.cpp:218] Iteration 78300 (19.0323 iter/s, 5.25421s/100 iters), loss = 0.0940481
I1001 11:54:54.297525  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0940478 (* 1 = 0.0940478 loss)
I1001 11:54:54.297533  5182 sgd_solver.cpp:105] Iteration 78300, lr = 0.01
I1001 11:54:59.545971  5182 solver.cpp:218] Iteration 78400 (19.0533 iter/s, 5.24842s/100 iters), loss = 0.0102231
I1001 11:54:59.546000  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0102228 (* 1 = 0.0102228 loss)
I1001 11:54:59.546017  5182 sgd_solver.cpp:105] Iteration 78400, lr = 0.01
I1001 11:55:04.537343  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:55:04.747473  5182 solver.cpp:330] Iteration 78500, Testing net (#0)
I1001 11:55:05.937062  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:55:05.988488  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8785
I1001 11:55:05.988512  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.494091 (* 1 = 0.494091 loss)
I1001 11:55:06.042520  5182 solver.cpp:218] Iteration 78500 (15.3929 iter/s, 6.4965s/100 iters), loss = 0.00851422
I1001 11:55:06.042572  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00851391 (* 1 = 0.00851391 loss)
I1001 11:55:06.042594  5182 sgd_solver.cpp:105] Iteration 78500, lr = 0.01
I1001 11:55:11.297116  5182 solver.cpp:218] Iteration 78600 (19.0312 iter/s, 5.25452s/100 iters), loss = 0.00889807
I1001 11:55:11.297235  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00889776 (* 1 = 0.00889776 loss)
I1001 11:55:11.297253  5182 sgd_solver.cpp:105] Iteration 78600, lr = 0.01
I1001 11:55:16.554757  5182 solver.cpp:218] Iteration 78700 (19.0204 iter/s, 5.25751s/100 iters), loss = 0.0213673
I1001 11:55:16.554787  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.021367 (* 1 = 0.021367 loss)
I1001 11:55:16.554806  5182 sgd_solver.cpp:105] Iteration 78700, lr = 0.01
I1001 11:55:21.817811  5182 solver.cpp:218] Iteration 78800 (19.0009 iter/s, 5.26291s/100 iters), loss = 0.0969766
I1001 11:55:21.817848  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0969763 (* 1 = 0.0969763 loss)
I1001 11:55:21.817854  5182 sgd_solver.cpp:105] Iteration 78800, lr = 0.01
I1001 11:55:27.077469  5182 solver.cpp:218] Iteration 78900 (19.0129 iter/s, 5.2596s/100 iters), loss = 0.0135425
I1001 11:55:27.077505  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0135422 (* 1 = 0.0135422 loss)
I1001 11:55:27.077512  5182 sgd_solver.cpp:105] Iteration 78900, lr = 0.01
I1001 11:55:32.068522  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:55:32.278503  5182 solver.cpp:330] Iteration 79000, Testing net (#0)
I1001 11:55:33.474931  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:55:33.525140  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9008
I1001 11:55:33.525174  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.384246 (* 1 = 0.384246 loss)
I1001 11:55:33.577944  5182 solver.cpp:218] Iteration 79000 (15.3836 iter/s, 6.50042s/100 iters), loss = 0.0350608
I1001 11:55:33.577980  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0350605 (* 1 = 0.0350605 loss)
I1001 11:55:33.577986  5182 sgd_solver.cpp:105] Iteration 79000, lr = 0.01
I1001 11:55:38.827349  5182 solver.cpp:218] Iteration 79100 (19.05 iter/s, 5.24935s/100 iters), loss = 0.0192294
I1001 11:55:38.827399  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0192291 (* 1 = 0.0192291 loss)
I1001 11:55:38.827405  5182 sgd_solver.cpp:105] Iteration 79100, lr = 0.01
I1001 11:55:44.084725  5182 solver.cpp:218] Iteration 79200 (19.0212 iter/s, 5.25731s/100 iters), loss = 0.0226796
I1001 11:55:44.084872  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0226793 (* 1 = 0.0226793 loss)
I1001 11:55:44.084879  5182 sgd_solver.cpp:105] Iteration 79200, lr = 0.01
I1001 11:55:49.344971  5182 solver.cpp:218] Iteration 79300 (19.0111 iter/s, 5.26009s/100 iters), loss = 0.0446413
I1001 11:55:49.345012  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.044641 (* 1 = 0.044641 loss)
I1001 11:55:49.345018  5182 sgd_solver.cpp:105] Iteration 79300, lr = 0.01
I1001 11:55:54.602494  5182 solver.cpp:218] Iteration 79400 (19.0206 iter/s, 5.25746s/100 iters), loss = 0.0115968
I1001 11:55:54.602525  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0115965 (* 1 = 0.0115965 loss)
I1001 11:55:54.602533  5182 sgd_solver.cpp:105] Iteration 79400, lr = 0.01
I1001 11:55:59.590860  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:55:59.801067  5182 solver.cpp:330] Iteration 79500, Testing net (#0)
I1001 11:56:01.000180  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:56:01.050552  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8939
I1001 11:56:01.050586  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.418548 (* 1 = 0.418548 loss)
I1001 11:56:01.103341  5182 solver.cpp:218] Iteration 79500 (15.3827 iter/s, 6.5008s/100 iters), loss = 0.00825464
I1001 11:56:01.103368  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00825433 (* 1 = 0.00825433 loss)
I1001 11:56:01.103385  5182 sgd_solver.cpp:105] Iteration 79500, lr = 0.01
I1001 11:56:06.363548  5182 solver.cpp:218] Iteration 79600 (19.0109 iter/s, 5.26015s/100 iters), loss = 0.00696765
I1001 11:56:06.363579  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00696732 (* 1 = 0.00696732 loss)
I1001 11:56:06.363584  5182 sgd_solver.cpp:105] Iteration 79600, lr = 0.01
I1001 11:56:11.613507  5182 solver.cpp:218] Iteration 79700 (19.048 iter/s, 5.24991s/100 iters), loss = 0.0279139
I1001 11:56:11.613534  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0279136 (* 1 = 0.0279136 loss)
I1001 11:56:11.613540  5182 sgd_solver.cpp:105] Iteration 79700, lr = 0.01
I1001 11:56:16.873473  5182 solver.cpp:218] Iteration 79800 (19.0117 iter/s, 5.25992s/100 iters), loss = 0.00811502
I1001 11:56:16.873605  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00811472 (* 1 = 0.00811472 loss)
I1001 11:56:16.873627  5182 sgd_solver.cpp:105] Iteration 79800, lr = 0.01
I1001 11:56:22.134016  5182 solver.cpp:218] Iteration 79900 (19.01 iter/s, 5.2604s/100 iters), loss = 0.0368912
I1001 11:56:22.134044  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0368909 (* 1 = 0.0368909 loss)
I1001 11:56:22.134050  5182 sgd_solver.cpp:105] Iteration 79900, lr = 0.01
I1001 11:56:27.127785  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:56:27.339104  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_80000.caffemodel
I1001 11:56:27.344487  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_80000.solverstate
I1001 11:56:27.345916  5182 solver.cpp:330] Iteration 80000, Testing net (#0)
I1001 11:56:28.535064  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:56:28.585575  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9002
I1001 11:56:28.585613  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.398602 (* 1 = 0.398602 loss)
I1001 11:56:28.637913  5182 solver.cpp:218] Iteration 80000 (15.3755 iter/s, 6.50385s/100 iters), loss = 0.0165624
I1001 11:56:28.637939  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0165621 (* 1 = 0.0165621 loss)
I1001 11:56:28.637945  5182 sgd_solver.cpp:46] MultiStep Status: Iteration 80000, step = 2
I1001 11:56:28.637948  5182 sgd_solver.cpp:105] Iteration 80000, lr = 0.001
I1001 11:56:33.893077  5182 solver.cpp:218] Iteration 80100 (19.0291 iter/s, 5.25512s/100 iters), loss = 0.00680078
I1001 11:56:33.893107  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00680045 (* 1 = 0.00680045 loss)
I1001 11:56:33.893113  5182 sgd_solver.cpp:105] Iteration 80100, lr = 0.001
I1001 11:56:39.150554  5182 solver.cpp:218] Iteration 80200 (19.0207 iter/s, 5.25743s/100 iters), loss = 0.043243
I1001 11:56:39.150607  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0432426 (* 1 = 0.0432426 loss)
I1001 11:56:39.150614  5182 sgd_solver.cpp:105] Iteration 80200, lr = 0.001
I1001 11:56:44.402387  5182 solver.cpp:218] Iteration 80300 (19.0412 iter/s, 5.25176s/100 iters), loss = 0.037896
I1001 11:56:44.402426  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0378956 (* 1 = 0.0378956 loss)
I1001 11:56:44.402432  5182 sgd_solver.cpp:105] Iteration 80300, lr = 0.001
I1001 11:56:49.662101  5182 solver.cpp:218] Iteration 80400 (19.0126 iter/s, 5.25966s/100 iters), loss = 0.018175
I1001 11:56:49.662293  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0181747 (* 1 = 0.0181747 loss)
I1001 11:56:49.662313  5182 sgd_solver.cpp:105] Iteration 80400, lr = 0.001
I1001 11:56:54.659312  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:56:54.869114  5182 solver.cpp:330] Iteration 80500, Testing net (#0)
I1001 11:56:56.059392  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:56:56.109498  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9133
I1001 11:56:56.109525  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332637 (* 1 = 0.332637 loss)
I1001 11:56:56.162593  5182 solver.cpp:218] Iteration 80500 (15.3839 iter/s, 6.5003s/100 iters), loss = 0.00489754
I1001 11:56:56.162626  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00489721 (* 1 = 0.00489721 loss)
I1001 11:56:56.162634  5182 sgd_solver.cpp:105] Iteration 80500, lr = 0.001
I1001 11:57:01.420864  5182 solver.cpp:218] Iteration 80600 (19.0178 iter/s, 5.25822s/100 iters), loss = 0.0187916
I1001 11:57:01.420897  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0187912 (* 1 = 0.0187912 loss)
I1001 11:57:01.420903  5182 sgd_solver.cpp:105] Iteration 80600, lr = 0.001
I1001 11:57:06.673218  5182 solver.cpp:218] Iteration 80700 (19.0393 iter/s, 5.25231s/100 iters), loss = 0.0274952
I1001 11:57:06.673249  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0274949 (* 1 = 0.0274949 loss)
I1001 11:57:06.673267  5182 sgd_solver.cpp:105] Iteration 80700, lr = 0.001
I1001 11:57:11.925052  5182 solver.cpp:218] Iteration 80800 (19.0411 iter/s, 5.25179s/100 iters), loss = 0.0081941
I1001 11:57:11.925094  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00819378 (* 1 = 0.00819378 loss)
I1001 11:57:11.925101  5182 sgd_solver.cpp:105] Iteration 80800, lr = 0.001
I1001 11:57:17.178722  5182 solver.cpp:218] Iteration 80900 (19.0346 iter/s, 5.25359s/100 iters), loss = 0.00362898
I1001 11:57:17.178750  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00362866 (* 1 = 0.00362866 loss)
I1001 11:57:17.178756  5182 sgd_solver.cpp:105] Iteration 80900, lr = 0.001
I1001 11:57:22.170197  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:57:22.380602  5182 solver.cpp:330] Iteration 81000, Testing net (#0)
I1001 11:57:23.569548  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:57:23.619601  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916
I1001 11:57:23.619635  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.328258 (* 1 = 0.328258 loss)
I1001 11:57:23.672078  5182 solver.cpp:218] Iteration 81000 (15.4005 iter/s, 6.49331s/100 iters), loss = 0.00282451
I1001 11:57:23.672104  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00282418 (* 1 = 0.00282418 loss)
I1001 11:57:23.672111  5182 sgd_solver.cpp:105] Iteration 81000, lr = 0.001
I1001 11:57:28.913287  5182 solver.cpp:218] Iteration 81100 (19.0797 iter/s, 5.24116s/100 iters), loss = 0.0198462
I1001 11:57:28.913318  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0198459 (* 1 = 0.0198459 loss)
I1001 11:57:28.913326  5182 sgd_solver.cpp:105] Iteration 81100, lr = 0.001
I1001 11:57:34.166800  5182 solver.cpp:218] Iteration 81200 (19.0351 iter/s, 5.25347s/100 iters), loss = 0.00454939
I1001 11:57:34.166831  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00454906 (* 1 = 0.00454906 loss)
I1001 11:57:34.166839  5182 sgd_solver.cpp:105] Iteration 81200, lr = 0.001
I1001 11:57:39.420166  5182 solver.cpp:218] Iteration 81300 (19.0356 iter/s, 5.25332s/100 iters), loss = 0.0117359
I1001 11:57:39.420195  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117355 (* 1 = 0.0117355 loss)
I1001 11:57:39.420202  5182 sgd_solver.cpp:105] Iteration 81300, lr = 0.001
I1001 11:57:44.671869  5182 solver.cpp:218] Iteration 81400 (19.0416 iter/s, 5.25165s/100 iters), loss = 0.00960999
I1001 11:57:44.671910  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00960966 (* 1 = 0.00960966 loss)
I1001 11:57:44.671916  5182 sgd_solver.cpp:105] Iteration 81400, lr = 0.001
I1001 11:57:49.674538  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:57:49.884753  5182 solver.cpp:330] Iteration 81500, Testing net (#0)
I1001 11:57:51.079936  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:57:51.130803  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9171
I1001 11:57:51.130838  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329012 (* 1 = 0.329012 loss)
I1001 11:57:51.184289  5182 solver.cpp:218] Iteration 81500 (15.3554 iter/s, 6.51236s/100 iters), loss = 0.00492677
I1001 11:57:51.184322  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00492644 (* 1 = 0.00492644 loss)
I1001 11:57:51.184329  5182 sgd_solver.cpp:105] Iteration 81500, lr = 0.001
I1001 11:57:56.428459  5182 solver.cpp:218] Iteration 81600 (19.069 iter/s, 5.24412s/100 iters), loss = 0.00582471
I1001 11:57:56.428586  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00582438 (* 1 = 0.00582438 loss)
I1001 11:57:56.428603  5182 sgd_solver.cpp:105] Iteration 81600, lr = 0.001
I1001 11:58:01.689061  5182 solver.cpp:218] Iteration 81700 (19.0097 iter/s, 5.26047s/100 iters), loss = 0.0134901
I1001 11:58:01.689092  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0134898 (* 1 = 0.0134898 loss)
I1001 11:58:01.689098  5182 sgd_solver.cpp:105] Iteration 81700, lr = 0.001
I1001 11:58:06.945289  5182 solver.cpp:218] Iteration 81800 (19.0252 iter/s, 5.25618s/100 iters), loss = 0.0212531
I1001 11:58:06.945329  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212527 (* 1 = 0.0212527 loss)
I1001 11:58:06.945336  5182 sgd_solver.cpp:105] Iteration 81800, lr = 0.001
I1001 11:58:12.200919  5182 solver.cpp:218] Iteration 81900 (19.0275 iter/s, 5.25556s/100 iters), loss = 0.00471326
I1001 11:58:12.200956  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00471293 (* 1 = 0.00471293 loss)
I1001 11:58:12.200975  5182 sgd_solver.cpp:105] Iteration 81900, lr = 0.001
I1001 11:58:17.184427  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:58:17.395318  5182 solver.cpp:330] Iteration 82000, Testing net (#0)
I1001 11:58:18.593385  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:58:18.643519  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9177
I1001 11:58:18.643553  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326061 (* 1 = 0.326061 loss)
I1001 11:58:18.696405  5182 solver.cpp:218] Iteration 82000 (15.3955 iter/s, 6.49539s/100 iters), loss = 0.00806044
I1001 11:58:18.696434  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00806011 (* 1 = 0.00806011 loss)
I1001 11:58:18.696440  5182 sgd_solver.cpp:105] Iteration 82000, lr = 0.001
I1001 11:58:23.946555  5182 solver.cpp:218] Iteration 82100 (19.0473 iter/s, 5.2501s/100 iters), loss = 0.00714003
I1001 11:58:23.946602  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0071397 (* 1 = 0.0071397 loss)
I1001 11:58:23.946609  5182 sgd_solver.cpp:105] Iteration 82100, lr = 0.001
I1001 11:58:29.210631  5182 solver.cpp:218] Iteration 82200 (18.9969 iter/s, 5.26401s/100 iters), loss = 0.0328241
I1001 11:58:29.210769  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0328238 (* 1 = 0.0328238 loss)
I1001 11:58:29.210777  5182 sgd_solver.cpp:105] Iteration 82200, lr = 0.001
I1001 11:58:34.474330  5182 solver.cpp:218] Iteration 82300 (18.9986 iter/s, 5.26355s/100 iters), loss = 0.005402
I1001 11:58:34.474371  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00540167 (* 1 = 0.00540167 loss)
I1001 11:58:34.474377  5182 sgd_solver.cpp:105] Iteration 82300, lr = 0.001
I1001 11:58:39.738137  5182 solver.cpp:218] Iteration 82400 (18.9979 iter/s, 5.26374s/100 iters), loss = 0.0115074
I1001 11:58:39.738175  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0115071 (* 1 = 0.0115071 loss)
I1001 11:58:39.738181  5182 sgd_solver.cpp:105] Iteration 82400, lr = 0.001
I1001 11:58:44.727394  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:58:44.939141  5182 solver.cpp:330] Iteration 82500, Testing net (#0)
I1001 11:58:46.137230  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:58:46.187567  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9171
I1001 11:58:46.187590  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326492 (* 1 = 0.326492 loss)
I1001 11:58:46.240062  5182 solver.cpp:218] Iteration 82500 (15.3802 iter/s, 6.50187s/100 iters), loss = 0.00835313
I1001 11:58:46.240087  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00835279 (* 1 = 0.00835279 loss)
I1001 11:58:46.240093  5182 sgd_solver.cpp:105] Iteration 82500, lr = 0.001
I1001 11:58:51.493436  5182 solver.cpp:218] Iteration 82600 (19.0355 iter/s, 5.25333s/100 iters), loss = 0.00888318
I1001 11:58:51.493466  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00888284 (* 1 = 0.00888284 loss)
I1001 11:58:51.493472  5182 sgd_solver.cpp:105] Iteration 82600, lr = 0.001
I1001 11:58:56.741065  5182 solver.cpp:218] Iteration 82700 (19.0564 iter/s, 5.24758s/100 iters), loss = 0.00642079
I1001 11:58:56.741093  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00642046 (* 1 = 0.00642046 loss)
I1001 11:58:56.741109  5182 sgd_solver.cpp:105] Iteration 82700, lr = 0.001
I1001 11:59:02.000497  5182 solver.cpp:218] Iteration 82800 (19.0136 iter/s, 5.25938s/100 iters), loss = 0.0238234
I1001 11:59:02.000641  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0238231 (* 1 = 0.0238231 loss)
I1001 11:59:02.000650  5182 sgd_solver.cpp:105] Iteration 82800, lr = 0.001
I1001 11:59:07.255813  5182 solver.cpp:218] Iteration 82900 (19.0289 iter/s, 5.25515s/100 iters), loss = 0.00688173
I1001 11:59:07.255853  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0068814 (* 1 = 0.0068814 loss)
I1001 11:59:07.255861  5182 sgd_solver.cpp:105] Iteration 82900, lr = 0.001
I1001 11:59:12.248926  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:59:12.459194  5182 solver.cpp:330] Iteration 83000, Testing net (#0)
I1001 11:59:13.648943  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:59:13.698953  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9172
I1001 11:59:13.698978  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.327882 (* 1 = 0.327882 loss)
I1001 11:59:13.751595  5182 solver.cpp:218] Iteration 83000 (15.3947 iter/s, 6.49572s/100 iters), loss = 0.0088456
I1001 11:59:13.751619  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00884526 (* 1 = 0.00884526 loss)
I1001 11:59:13.751626  5182 sgd_solver.cpp:105] Iteration 83000, lr = 0.001
I1001 11:59:19.013747  5182 solver.cpp:218] Iteration 83100 (19.0038 iter/s, 5.26211s/100 iters), loss = 0.00841076
I1001 11:59:19.013787  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00841043 (* 1 = 0.00841043 loss)
I1001 11:59:19.013793  5182 sgd_solver.cpp:105] Iteration 83100, lr = 0.001
I1001 11:59:24.276154  5182 solver.cpp:218] Iteration 83200 (19.0029 iter/s, 5.26235s/100 iters), loss = 0.00509744
I1001 11:59:24.276186  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00509711 (* 1 = 0.00509711 loss)
I1001 11:59:24.276193  5182 sgd_solver.cpp:105] Iteration 83200, lr = 0.001
I1001 11:59:29.527935  5182 solver.cpp:218] Iteration 83300 (19.0413 iter/s, 5.25173s/100 iters), loss = 0.0163243
I1001 11:59:29.527973  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0163239 (* 1 = 0.0163239 loss)
I1001 11:59:29.527979  5182 sgd_solver.cpp:105] Iteration 83300, lr = 0.001
I1001 11:59:34.792197  5182 solver.cpp:218] Iteration 83400 (18.9962 iter/s, 5.2642s/100 iters), loss = 0.00547459
I1001 11:59:34.792361  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00547426 (* 1 = 0.00547426 loss)
I1001 11:59:34.792369  5182 sgd_solver.cpp:105] Iteration 83400, lr = 0.001
I1001 11:59:39.784039  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:59:39.995085  5182 solver.cpp:330] Iteration 83500, Testing net (#0)
I1001 11:59:41.184224  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 11:59:41.234468  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9171
I1001 11:59:41.234493  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.327805 (* 1 = 0.327805 loss)
I1001 11:59:41.286787  5182 solver.cpp:218] Iteration 83500 (15.3978 iter/s, 6.49442s/100 iters), loss = 0.00962165
I1001 11:59:41.286813  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00962132 (* 1 = 0.00962132 loss)
I1001 11:59:41.286819  5182 sgd_solver.cpp:105] Iteration 83500, lr = 0.001
I1001 11:59:46.539594  5182 solver.cpp:218] Iteration 83600 (19.0376 iter/s, 5.25276s/100 iters), loss = 0.00601463
I1001 11:59:46.539629  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0060143 (* 1 = 0.0060143 loss)
I1001 11:59:46.539635  5182 sgd_solver.cpp:105] Iteration 83600, lr = 0.001
I1001 11:59:51.794126  5182 solver.cpp:218] Iteration 83700 (19.0314 iter/s, 5.25448s/100 iters), loss = 0.00391845
I1001 11:59:51.794154  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00391812 (* 1 = 0.00391812 loss)
I1001 11:59:51.794160  5182 sgd_solver.cpp:105] Iteration 83700, lr = 0.001
I1001 11:59:57.043449  5182 solver.cpp:218] Iteration 83800 (19.0503 iter/s, 5.24927s/100 iters), loss = 0.0114665
I1001 11:59:57.043485  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114662 (* 1 = 0.0114662 loss)
I1001 11:59:57.043493  5182 sgd_solver.cpp:105] Iteration 83800, lr = 0.001
I1001 12:00:02.297564  5182 solver.cpp:218] Iteration 83900 (19.0329 iter/s, 5.25406s/100 iters), loss = 0.00164221
I1001 12:00:02.297592  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00164188 (* 1 = 0.00164188 loss)
I1001 12:00:02.297598  5182 sgd_solver.cpp:105] Iteration 83900, lr = 0.001
I1001 12:00:07.293362  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:00:07.503636  5182 solver.cpp:330] Iteration 84000, Testing net (#0)
I1001 12:00:08.693449  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:00:08.743532  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9164
I1001 12:00:08.743558  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.330056 (* 1 = 0.330056 loss)
I1001 12:00:08.796360  5182 solver.cpp:218] Iteration 84000 (15.3876 iter/s, 6.49875s/100 iters), loss = 0.00180713
I1001 12:00:08.796388  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00180679 (* 1 = 0.00180679 loss)
I1001 12:00:08.796398  5182 sgd_solver.cpp:105] Iteration 84000, lr = 0.001
I1001 12:00:14.055832  5182 solver.cpp:218] Iteration 84100 (19.0135 iter/s, 5.25943s/100 iters), loss = 0.00578451
I1001 12:00:14.055860  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00578418 (* 1 = 0.00578418 loss)
I1001 12:00:14.055876  5182 sgd_solver.cpp:105] Iteration 84100, lr = 0.001
I1001 12:00:19.314136  5182 solver.cpp:218] Iteration 84200 (19.0177 iter/s, 5.25825s/100 iters), loss = 0.0139667
I1001 12:00:19.314174  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0139663 (* 1 = 0.0139663 loss)
I1001 12:00:19.314180  5182 sgd_solver.cpp:105] Iteration 84200, lr = 0.001
I1001 12:00:24.571179  5182 solver.cpp:218] Iteration 84300 (19.0223 iter/s, 5.25698s/100 iters), loss = 0.0144421
I1001 12:00:24.571209  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144417 (* 1 = 0.0144417 loss)
I1001 12:00:24.571215  5182 sgd_solver.cpp:105] Iteration 84300, lr = 0.001
I1001 12:00:29.817843  5182 solver.cpp:218] Iteration 84400 (19.0599 iter/s, 5.24661s/100 iters), loss = 0.00354605
I1001 12:00:29.817883  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00354571 (* 1 = 0.00354571 loss)
I1001 12:00:29.817889  5182 sgd_solver.cpp:105] Iteration 84400, lr = 0.001
I1001 12:00:34.810556  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:00:35.020952  5182 solver.cpp:330] Iteration 84500, Testing net (#0)
I1001 12:00:36.216462  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:00:36.267516  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9165
I1001 12:00:36.267550  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.32688 (* 1 = 0.32688 loss)
I1001 12:00:36.320261  5182 solver.cpp:218] Iteration 84500 (15.379 iter/s, 6.50236s/100 iters), loss = 0.00392362
I1001 12:00:36.320308  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00392328 (* 1 = 0.00392328 loss)
I1001 12:00:36.320327  5182 sgd_solver.cpp:105] Iteration 84500, lr = 0.001
I1001 12:00:41.570624  5182 solver.cpp:218] Iteration 84600 (19.0465 iter/s, 5.2503s/100 iters), loss = 0.00657895
I1001 12:00:41.570740  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00657861 (* 1 = 0.00657861 loss)
I1001 12:00:41.570747  5182 sgd_solver.cpp:105] Iteration 84600, lr = 0.001
I1001 12:00:46.826182  5182 solver.cpp:218] Iteration 84700 (19.0279 iter/s, 5.25543s/100 iters), loss = 0.0251635
I1001 12:00:46.826217  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0251631 (* 1 = 0.0251631 loss)
I1001 12:00:46.826236  5182 sgd_solver.cpp:105] Iteration 84700, lr = 0.001
I1001 12:00:52.086599  5182 solver.cpp:218] Iteration 84800 (19.0101 iter/s, 5.26037s/100 iters), loss = 0.0140405
I1001 12:00:52.086632  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140402 (* 1 = 0.0140402 loss)
I1001 12:00:52.086642  5182 sgd_solver.cpp:105] Iteration 84800, lr = 0.001
I1001 12:00:57.343446  5182 solver.cpp:218] Iteration 84900 (19.023 iter/s, 5.25679s/100 iters), loss = 0.00715888
I1001 12:00:57.343479  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00715855 (* 1 = 0.00715855 loss)
I1001 12:00:57.343498  5182 sgd_solver.cpp:105] Iteration 84900, lr = 0.001
I1001 12:01:02.327225  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:01:02.536860  5182 solver.cpp:330] Iteration 85000, Testing net (#0)
I1001 12:01:03.733963  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:01:03.784104  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9185
I1001 12:01:03.784131  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.325186 (* 1 = 0.325186 loss)
I1001 12:01:03.836607  5182 solver.cpp:218] Iteration 85000 (15.4009 iter/s, 6.49311s/100 iters), loss = 0.0072096
I1001 12:01:03.836633  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00720927 (* 1 = 0.00720927 loss)
I1001 12:01:03.836643  5182 sgd_solver.cpp:105] Iteration 85000, lr = 0.001
I1001 12:01:09.086508  5182 solver.cpp:218] Iteration 85100 (19.0482 iter/s, 5.24985s/100 iters), loss = 0.0234101
I1001 12:01:09.086546  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0234098 (* 1 = 0.0234098 loss)
I1001 12:01:09.086556  5182 sgd_solver.cpp:105] Iteration 85100, lr = 0.001
I1001 12:01:14.334280  5182 solver.cpp:218] Iteration 85200 (19.0559 iter/s, 5.24771s/100 iters), loss = 0.0143501
I1001 12:01:14.334467  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0143498 (* 1 = 0.0143498 loss)
I1001 12:01:14.334487  5182 sgd_solver.cpp:105] Iteration 85200, lr = 0.001
I1001 12:01:19.589645  5182 solver.cpp:218] Iteration 85300 (19.0288 iter/s, 5.2552s/100 iters), loss = 0.0109475
I1001 12:01:19.589675  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109472 (* 1 = 0.0109472 loss)
I1001 12:01:19.589681  5182 sgd_solver.cpp:105] Iteration 85300, lr = 0.001
I1001 12:01:24.845697  5182 solver.cpp:218] Iteration 85400 (19.0259 iter/s, 5.256s/100 iters), loss = 0.00804321
I1001 12:01:24.845728  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00804288 (* 1 = 0.00804288 loss)
I1001 12:01:24.845736  5182 sgd_solver.cpp:105] Iteration 85400, lr = 0.001
I1001 12:01:29.829879  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:01:30.042629  5182 solver.cpp:330] Iteration 85500, Testing net (#0)
I1001 12:01:31.236009  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:01:31.285858  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9169
I1001 12:01:31.285884  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326878 (* 1 = 0.326878 loss)
I1001 12:01:31.338598  5182 solver.cpp:218] Iteration 85500 (15.4016 iter/s, 6.49285s/100 iters), loss = 0.00409266
I1001 12:01:31.338627  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00409232 (* 1 = 0.00409232 loss)
I1001 12:01:31.338636  5182 sgd_solver.cpp:105] Iteration 85500, lr = 0.001
I1001 12:01:36.598902  5182 solver.cpp:218] Iteration 85600 (19.0105 iter/s, 5.26026s/100 iters), loss = 0.0295299
I1001 12:01:36.598932  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0295296 (* 1 = 0.0295296 loss)
I1001 12:01:36.598938  5182 sgd_solver.cpp:105] Iteration 85600, lr = 0.001
I1001 12:01:41.841208  5182 solver.cpp:218] Iteration 85700 (19.0758 iter/s, 5.24226s/100 iters), loss = 0.00607961
I1001 12:01:41.841248  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00607926 (* 1 = 0.00607926 loss)
I1001 12:01:41.841254  5182 sgd_solver.cpp:105] Iteration 85700, lr = 0.001
I1001 12:01:47.099545  5182 solver.cpp:218] Iteration 85800 (19.0176 iter/s, 5.25828s/100 iters), loss = 0.0127882
I1001 12:01:47.099694  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0127878 (* 1 = 0.0127878 loss)
I1001 12:01:47.099714  5182 sgd_solver.cpp:105] Iteration 85800, lr = 0.001
I1001 12:01:52.353551  5182 solver.cpp:218] Iteration 85900 (19.0337 iter/s, 5.25384s/100 iters), loss = 0.00217909
I1001 12:01:52.353580  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00217874 (* 1 = 0.00217874 loss)
I1001 12:01:52.353587  5182 sgd_solver.cpp:105] Iteration 85900, lr = 0.001
I1001 12:01:57.350953  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:01:57.560966  5182 solver.cpp:330] Iteration 86000, Testing net (#0)
I1001 12:01:58.751642  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:01:58.801745  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9172
I1001 12:01:58.801770  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.324963 (* 1 = 0.324963 loss)
I1001 12:01:58.854475  5182 solver.cpp:218] Iteration 86000 (15.3825 iter/s, 6.50087s/100 iters), loss = 0.0194272
I1001 12:01:58.854499  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194269 (* 1 = 0.0194269 loss)
I1001 12:01:58.854506  5182 sgd_solver.cpp:105] Iteration 86000, lr = 0.001
I1001 12:02:04.112206  5182 solver.cpp:218] Iteration 86100 (19.0198 iter/s, 5.25769s/100 iters), loss = 0.0327493
I1001 12:02:04.112237  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.032749 (* 1 = 0.032749 loss)
I1001 12:02:04.112253  5182 sgd_solver.cpp:105] Iteration 86100, lr = 0.001
I1001 12:02:09.364998  5182 solver.cpp:218] Iteration 86200 (19.0377 iter/s, 5.25274s/100 iters), loss = 0.00539854
I1001 12:02:09.365028  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0053982 (* 1 = 0.0053982 loss)
I1001 12:02:09.365036  5182 sgd_solver.cpp:105] Iteration 86200, lr = 0.001
I1001 12:02:14.610720  5182 solver.cpp:218] Iteration 86300 (19.0634 iter/s, 5.24567s/100 iters), loss = 0.00710899
I1001 12:02:14.610749  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00710864 (* 1 = 0.00710864 loss)
I1001 12:02:14.610755  5182 sgd_solver.cpp:105] Iteration 86300, lr = 0.001
I1001 12:02:19.863607  5182 solver.cpp:218] Iteration 86400 (19.0373 iter/s, 5.25284s/100 iters), loss = 0.00274557
I1001 12:02:19.863766  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00274522 (* 1 = 0.00274522 loss)
I1001 12:02:19.863775  5182 sgd_solver.cpp:105] Iteration 86400, lr = 0.001
I1001 12:02:24.860451  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:02:25.072021  5182 solver.cpp:330] Iteration 86500, Testing net (#0)
I1001 12:02:26.261065  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:02:26.311324  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9182
I1001 12:02:26.311358  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.325006 (* 1 = 0.325006 loss)
I1001 12:02:26.364138  5182 solver.cpp:218] Iteration 86500 (15.3838 iter/s, 6.50036s/100 iters), loss = 0.00487366
I1001 12:02:26.364179  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0048733 (* 1 = 0.0048733 loss)
I1001 12:02:26.364187  5182 sgd_solver.cpp:105] Iteration 86500, lr = 0.001
I1001 12:02:31.618489  5182 solver.cpp:218] Iteration 86600 (19.0321 iter/s, 5.25429s/100 iters), loss = 0.0142076
I1001 12:02:31.618531  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142072 (* 1 = 0.0142072 loss)
I1001 12:02:31.618537  5182 sgd_solver.cpp:105] Iteration 86600, lr = 0.001
I1001 12:02:36.874397  5182 solver.cpp:218] Iteration 86700 (19.0265 iter/s, 5.25584s/100 iters), loss = 0.0147856
I1001 12:02:36.874439  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0147853 (* 1 = 0.0147853 loss)
I1001 12:02:36.874444  5182 sgd_solver.cpp:105] Iteration 86700, lr = 0.001
I1001 12:02:42.129827  5182 solver.cpp:218] Iteration 86800 (19.0282 iter/s, 5.25536s/100 iters), loss = 0.00799249
I1001 12:02:42.129878  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00799214 (* 1 = 0.00799214 loss)
I1001 12:02:42.129899  5182 sgd_solver.cpp:105] Iteration 86800, lr = 0.001
I1001 12:02:47.382345  5182 solver.cpp:218] Iteration 86900 (19.0388 iter/s, 5.25242s/100 iters), loss = 0.00195945
I1001 12:02:47.382375  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00195911 (* 1 = 0.00195911 loss)
I1001 12:02:47.382381  5182 sgd_solver.cpp:105] Iteration 86900, lr = 0.001
I1001 12:02:52.371969  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:02:52.580958  5182 solver.cpp:330] Iteration 87000, Testing net (#0)
I1001 12:02:53.770751  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:02:53.821264  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9179
I1001 12:02:53.821300  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326374 (* 1 = 0.326374 loss)
I1001 12:02:53.874212  5182 solver.cpp:218] Iteration 87000 (15.404 iter/s, 6.49182s/100 iters), loss = 0.0149742
I1001 12:02:53.874248  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0149739 (* 1 = 0.0149739 loss)
I1001 12:02:53.874254  5182 sgd_solver.cpp:105] Iteration 87000, lr = 0.001
I1001 12:02:59.133644  5182 solver.cpp:218] Iteration 87100 (19.0137 iter/s, 5.25937s/100 iters), loss = 0.0108637
I1001 12:02:59.133673  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0108634 (* 1 = 0.0108634 loss)
I1001 12:02:59.133680  5182 sgd_solver.cpp:105] Iteration 87100, lr = 0.001
I1001 12:03:04.390727  5182 solver.cpp:218] Iteration 87200 (19.0221 iter/s, 5.25703s/100 iters), loss = 0.00402066
I1001 12:03:04.390766  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00402033 (* 1 = 0.00402033 loss)
I1001 12:03:04.390772  5182 sgd_solver.cpp:105] Iteration 87200, lr = 0.001
I1001 12:03:09.649405  5182 solver.cpp:218] Iteration 87300 (19.0164 iter/s, 5.25862s/100 iters), loss = 0.0159243
I1001 12:03:09.649435  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015924 (* 1 = 0.015924 loss)
I1001 12:03:09.649441  5182 sgd_solver.cpp:105] Iteration 87300, lr = 0.001
I1001 12:03:14.902987  5182 solver.cpp:218] Iteration 87400 (19.0348 iter/s, 5.25353s/100 iters), loss = 0.00668615
I1001 12:03:14.903028  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00668582 (* 1 = 0.00668582 loss)
I1001 12:03:14.903034  5182 sgd_solver.cpp:105] Iteration 87400, lr = 0.001
I1001 12:03:19.897117  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:03:20.107587  5182 solver.cpp:330] Iteration 87500, Testing net (#0)
I1001 12:03:21.306632  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:03:21.357013  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9184
I1001 12:03:21.357049  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.327529 (* 1 = 0.327529 loss)
I1001 12:03:21.409994  5182 solver.cpp:218] Iteration 87500 (15.3682 iter/s, 6.50694s/100 iters), loss = 0.00445737
I1001 12:03:21.410024  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00445704 (* 1 = 0.00445704 loss)
I1001 12:03:21.410030  5182 sgd_solver.cpp:105] Iteration 87500, lr = 0.001
I1001 12:03:26.658964  5182 solver.cpp:218] Iteration 87600 (19.0515 iter/s, 5.24892s/100 iters), loss = 0.00671912
I1001 12:03:26.659087  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00671878 (* 1 = 0.00671878 loss)
I1001 12:03:26.659106  5182 sgd_solver.cpp:105] Iteration 87600, lr = 0.001
I1001 12:03:31.913843  5182 solver.cpp:218] Iteration 87700 (19.0304 iter/s, 5.25474s/100 iters), loss = 0.00482831
I1001 12:03:31.913872  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00482798 (* 1 = 0.00482798 loss)
I1001 12:03:31.913878  5182 sgd_solver.cpp:105] Iteration 87700, lr = 0.001
I1001 12:03:37.170898  5182 solver.cpp:218] Iteration 87800 (19.0222 iter/s, 5.257s/100 iters), loss = 0.0199185
I1001 12:03:37.170928  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0199182 (* 1 = 0.0199182 loss)
I1001 12:03:37.170944  5182 sgd_solver.cpp:105] Iteration 87800, lr = 0.001
I1001 12:03:42.428993  5182 solver.cpp:218] Iteration 87900 (19.0185 iter/s, 5.25804s/100 iters), loss = 0.00364114
I1001 12:03:42.429023  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00364081 (* 1 = 0.00364081 loss)
I1001 12:03:42.429029  5182 sgd_solver.cpp:105] Iteration 87900, lr = 0.001
I1001 12:03:47.412883  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:03:47.622316  5182 solver.cpp:330] Iteration 88000, Testing net (#0)
I1001 12:03:48.818943  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:03:48.869227  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9188
I1001 12:03:48.869262  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326166 (* 1 = 0.326166 loss)
I1001 12:03:48.921761  5182 solver.cpp:218] Iteration 88000 (15.4019 iter/s, 6.49272s/100 iters), loss = 0.00543989
I1001 12:03:48.921784  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00543956 (* 1 = 0.00543956 loss)
I1001 12:03:48.921792  5182 sgd_solver.cpp:105] Iteration 88000, lr = 0.001
I1001 12:03:54.173656  5182 solver.cpp:218] Iteration 88100 (19.0409 iter/s, 5.25184s/100 iters), loss = 0.00908826
I1001 12:03:54.173698  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00908793 (* 1 = 0.00908793 loss)
I1001 12:03:54.173718  5182 sgd_solver.cpp:105] Iteration 88100, lr = 0.001
I1001 12:03:59.423943  5182 solver.cpp:218] Iteration 88200 (19.0469 iter/s, 5.25019s/100 iters), loss = 0.00631898
I1001 12:03:59.424065  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00631865 (* 1 = 0.00631865 loss)
I1001 12:03:59.424073  5182 sgd_solver.cpp:105] Iteration 88200, lr = 0.001
I1001 12:04:04.683481  5182 solver.cpp:218] Iteration 88300 (19.0136 iter/s, 5.2594s/100 iters), loss = 0.00813903
I1001 12:04:04.683517  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0081387 (* 1 = 0.0081387 loss)
I1001 12:04:04.683524  5182 sgd_solver.cpp:105] Iteration 88300, lr = 0.001
I1001 12:04:09.941884  5182 solver.cpp:218] Iteration 88400 (19.0174 iter/s, 5.25835s/100 iters), loss = 0.0175793
I1001 12:04:09.941913  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175789 (* 1 = 0.0175789 loss)
I1001 12:04:09.941929  5182 sgd_solver.cpp:105] Iteration 88400, lr = 0.001
I1001 12:04:14.927961  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:04:15.144738  5182 solver.cpp:330] Iteration 88500, Testing net (#0)
I1001 12:04:16.335887  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:04:16.386114  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9188
I1001 12:04:16.386147  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.325741 (* 1 = 0.325741 loss)
I1001 12:04:16.438715  5182 solver.cpp:218] Iteration 88500 (15.3922 iter/s, 6.49678s/100 iters), loss = 0.0145035
I1001 12:04:16.438740  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0145032 (* 1 = 0.0145032 loss)
I1001 12:04:16.438746  5182 sgd_solver.cpp:105] Iteration 88500, lr = 0.001
I1001 12:04:21.695888  5182 solver.cpp:218] Iteration 88600 (19.0218 iter/s, 5.25713s/100 iters), loss = 0.0178791
I1001 12:04:21.695919  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0178787 (* 1 = 0.0178787 loss)
I1001 12:04:21.695924  5182 sgd_solver.cpp:105] Iteration 88600, lr = 0.001
I1001 12:04:26.944466  5182 solver.cpp:218] Iteration 88700 (19.053 iter/s, 5.24852s/100 iters), loss = 0.00618325
I1001 12:04:26.944499  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00618292 (* 1 = 0.00618292 loss)
I1001 12:04:26.944504  5182 sgd_solver.cpp:105] Iteration 88700, lr = 0.001
I1001 12:04:32.200557  5182 solver.cpp:218] Iteration 88800 (19.0257 iter/s, 5.25604s/100 iters), loss = 0.00525619
I1001 12:04:32.200693  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00525585 (* 1 = 0.00525585 loss)
I1001 12:04:32.200712  5182 sgd_solver.cpp:105] Iteration 88800, lr = 0.001
I1001 12:04:37.456640  5182 solver.cpp:218] Iteration 88900 (19.0261 iter/s, 5.25594s/100 iters), loss = 0.00235823
I1001 12:04:37.456670  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00235788 (* 1 = 0.00235788 loss)
I1001 12:04:37.456676  5182 sgd_solver.cpp:105] Iteration 88900, lr = 0.001
I1001 12:04:42.451501  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:04:42.661389  5182 solver.cpp:330] Iteration 89000, Testing net (#0)
I1001 12:04:43.848920  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:04:43.899157  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9166
I1001 12:04:43.899180  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326663 (* 1 = 0.326663 loss)
I1001 12:04:43.951879  5182 solver.cpp:218] Iteration 89000 (15.396 iter/s, 6.49519s/100 iters), loss = 0.00976899
I1001 12:04:43.951905  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00976865 (* 1 = 0.00976865 loss)
I1001 12:04:43.951910  5182 sgd_solver.cpp:105] Iteration 89000, lr = 0.001
I1001 12:04:49.210048  5182 solver.cpp:218] Iteration 89100 (19.0182 iter/s, 5.25812s/100 iters), loss = 0.00390544
I1001 12:04:49.210089  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00390509 (* 1 = 0.00390509 loss)
I1001 12:04:49.210095  5182 sgd_solver.cpp:105] Iteration 89100, lr = 0.001
I1001 12:04:54.466255  5182 solver.cpp:218] Iteration 89200 (19.0254 iter/s, 5.25615s/100 iters), loss = 0.0138461
I1001 12:04:54.466297  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0138458 (* 1 = 0.0138458 loss)
I1001 12:04:54.466305  5182 sgd_solver.cpp:105] Iteration 89200, lr = 0.001
I1001 12:04:59.720525  5182 solver.cpp:218] Iteration 89300 (19.0324 iter/s, 5.25421s/100 iters), loss = 0.00837447
I1001 12:04:59.720566  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00837413 (* 1 = 0.00837413 loss)
I1001 12:04:59.720571  5182 sgd_solver.cpp:105] Iteration 89300, lr = 0.001
I1001 12:05:04.981956  5182 solver.cpp:218] Iteration 89400 (19.0065 iter/s, 5.26137s/100 iters), loss = 0.0195994
I1001 12:05:04.982132  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.019599 (* 1 = 0.019599 loss)
I1001 12:05:04.982143  5182 sgd_solver.cpp:105] Iteration 89400, lr = 0.001
I1001 12:05:09.980042  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:05:10.190902  5182 solver.cpp:330] Iteration 89500, Testing net (#0)
I1001 12:05:11.379665  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:05:11.429761  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9193
I1001 12:05:11.429787  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326352 (* 1 = 0.326352 loss)
I1001 12:05:11.482461  5182 solver.cpp:218] Iteration 89500 (15.3838 iter/s, 6.50034s/100 iters), loss = 0.00443284
I1001 12:05:11.482493  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00443251 (* 1 = 0.00443251 loss)
I1001 12:05:11.482503  5182 sgd_solver.cpp:105] Iteration 89500, lr = 0.001
I1001 12:05:16.739692  5182 solver.cpp:218] Iteration 89600 (19.0216 iter/s, 5.25718s/100 iters), loss = 0.0110745
I1001 12:05:16.739724  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110742 (* 1 = 0.0110742 loss)
I1001 12:05:16.739732  5182 sgd_solver.cpp:105] Iteration 89600, lr = 0.001
I1001 12:05:21.992391  5182 solver.cpp:218] Iteration 89700 (19.038 iter/s, 5.25265s/100 iters), loss = 0.00857886
I1001 12:05:21.992424  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00857853 (* 1 = 0.00857853 loss)
I1001 12:05:21.992432  5182 sgd_solver.cpp:105] Iteration 89700, lr = 0.001
I1001 12:05:27.245523  5182 solver.cpp:218] Iteration 89800 (19.0365 iter/s, 5.25308s/100 iters), loss = 0.0105461
I1001 12:05:27.245558  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0105458 (* 1 = 0.0105458 loss)
I1001 12:05:27.245579  5182 sgd_solver.cpp:105] Iteration 89800, lr = 0.001
I1001 12:05:32.492497  5182 solver.cpp:218] Iteration 89900 (19.0589 iter/s, 5.24688s/100 iters), loss = 0.00484094
I1001 12:05:32.492534  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00484062 (* 1 = 0.00484062 loss)
I1001 12:05:32.492544  5182 sgd_solver.cpp:105] Iteration 89900, lr = 0.001
I1001 12:05:37.482511  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:05:37.692322  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_90000.caffemodel
I1001 12:05:37.697243  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_90000.solverstate
I1001 12:05:37.698596  5182 solver.cpp:330] Iteration 90000, Testing net (#0)
I1001 12:05:38.887246  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:05:38.937779  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917
I1001 12:05:38.937805  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.330137 (* 1 = 0.330137 loss)
I1001 12:05:38.991868  5182 solver.cpp:218] Iteration 90000 (15.3862 iter/s, 6.49931s/100 iters), loss = 0.00563776
I1001 12:05:38.991909  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00563744 (* 1 = 0.00563744 loss)
I1001 12:05:38.991928  5182 sgd_solver.cpp:105] Iteration 90000, lr = 0.001
I1001 12:05:44.250164  5182 solver.cpp:218] Iteration 90100 (19.0178 iter/s, 5.25824s/100 iters), loss = 0.0467327
I1001 12:05:44.250200  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0467324 (* 1 = 0.0467324 loss)
I1001 12:05:44.250207  5182 sgd_solver.cpp:105] Iteration 90100, lr = 0.001
I1001 12:05:49.511301  5182 solver.cpp:218] Iteration 90200 (19.0075 iter/s, 5.26109s/100 iters), loss = 0.00608186
I1001 12:05:49.511332  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00608153 (* 1 = 0.00608153 loss)
I1001 12:05:49.511338  5182 sgd_solver.cpp:105] Iteration 90200, lr = 0.001
I1001 12:05:54.771908  5182 solver.cpp:218] Iteration 90300 (19.0094 iter/s, 5.26056s/100 iters), loss = 0.0101087
I1001 12:05:54.771939  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0101084 (* 1 = 0.0101084 loss)
I1001 12:05:54.771948  5182 sgd_solver.cpp:105] Iteration 90300, lr = 0.001
I1001 12:06:00.025068  5182 solver.cpp:218] Iteration 90400 (19.0364 iter/s, 5.25311s/100 iters), loss = 0.030739
I1001 12:06:00.025110  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0307386 (* 1 = 0.0307386 loss)
I1001 12:06:00.025120  5182 sgd_solver.cpp:105] Iteration 90400, lr = 0.001
I1001 12:06:05.019882  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:06:05.230135  5182 solver.cpp:330] Iteration 90500, Testing net (#0)
I1001 12:06:06.427072  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:06:06.477496  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9175
I1001 12:06:06.477521  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.33093 (* 1 = 0.33093 loss)
I1001 12:06:06.530076  5182 solver.cpp:218] Iteration 90500 (15.3729 iter/s, 6.50495s/100 iters), loss = 0.00319506
I1001 12:06:06.530107  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00319474 (* 1 = 0.00319474 loss)
I1001 12:06:06.530117  5182 sgd_solver.cpp:105] Iteration 90500, lr = 0.001
I1001 12:06:11.777732  5182 solver.cpp:218] Iteration 90600 (19.0563 iter/s, 5.24761s/100 iters), loss = 0.00484842
I1001 12:06:11.777871  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00484809 (* 1 = 0.00484809 loss)
I1001 12:06:11.777920  5182 sgd_solver.cpp:105] Iteration 90600, lr = 0.001
I1001 12:06:17.027902  5182 solver.cpp:218] Iteration 90700 (19.0476 iter/s, 5.25002s/100 iters), loss = 0.00280842
I1001 12:06:17.027935  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00280809 (* 1 = 0.00280809 loss)
I1001 12:06:17.027942  5182 sgd_solver.cpp:105] Iteration 90700, lr = 0.001
I1001 12:06:22.280035  5182 solver.cpp:218] Iteration 90800 (19.0401 iter/s, 5.25208s/100 iters), loss = 0.012942
I1001 12:06:22.280076  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129417 (* 1 = 0.0129417 loss)
I1001 12:06:22.280084  5182 sgd_solver.cpp:105] Iteration 90800, lr = 0.001
I1001 12:06:27.538792  5182 solver.cpp:218] Iteration 90900 (19.0161 iter/s, 5.2587s/100 iters), loss = 0.00115503
I1001 12:06:27.538828  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00115469 (* 1 = 0.00115469 loss)
I1001 12:06:27.538875  5182 sgd_solver.cpp:105] Iteration 90900, lr = 0.001
I1001 12:06:32.522382  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:06:32.733521  5182 solver.cpp:330] Iteration 91000, Testing net (#0)
I1001 12:06:33.931433  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:06:33.981752  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9173
I1001 12:06:33.981777  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326758 (* 1 = 0.326758 loss)
I1001 12:06:34.034080  5182 solver.cpp:218] Iteration 91000 (15.3962 iter/s, 6.49512s/100 iters), loss = 0.0129742
I1001 12:06:34.034113  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129738 (* 1 = 0.0129738 loss)
I1001 12:06:34.034122  5182 sgd_solver.cpp:105] Iteration 91000, lr = 0.001
I1001 12:06:39.293174  5182 solver.cpp:218] Iteration 91100 (19.0149 iter/s, 5.25904s/100 iters), loss = 0.00803146
I1001 12:06:39.293210  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00803112 (* 1 = 0.00803112 loss)
I1001 12:06:39.293228  5182 sgd_solver.cpp:105] Iteration 91100, lr = 0.001
I1001 12:06:44.541170  5182 solver.cpp:218] Iteration 91200 (19.0551 iter/s, 5.24794s/100 iters), loss = 0.0040892
I1001 12:06:44.541311  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00408886 (* 1 = 0.00408886 loss)
I1001 12:06:44.541363  5182 sgd_solver.cpp:105] Iteration 91200, lr = 0.001
I1001 12:06:49.803401  5182 solver.cpp:218] Iteration 91300 (19.0039 iter/s, 5.26207s/100 iters), loss = 0.0121749
I1001 12:06:49.803433  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121746 (* 1 = 0.0121746 loss)
I1001 12:06:49.803452  5182 sgd_solver.cpp:105] Iteration 91300, lr = 0.001
I1001 12:06:55.056848  5182 solver.cpp:218] Iteration 91400 (19.0353 iter/s, 5.25339s/100 iters), loss = 0.00234936
I1001 12:06:55.056877  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00234903 (* 1 = 0.00234903 loss)
I1001 12:06:55.056885  5182 sgd_solver.cpp:105] Iteration 91400, lr = 0.001
I1001 12:07:00.040441  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:07:00.255635  5182 solver.cpp:330] Iteration 91500, Testing net (#0)
I1001 12:07:01.446964  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:07:01.497411  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9174
I1001 12:07:01.497447  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.327475 (* 1 = 0.327475 loss)
I1001 12:07:01.549968  5182 solver.cpp:218] Iteration 91500 (15.401 iter/s, 6.49307s/100 iters), loss = 0.0109606
I1001 12:07:01.549998  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109603 (* 1 = 0.0109603 loss)
I1001 12:07:01.550004  5182 sgd_solver.cpp:105] Iteration 91500, lr = 0.001
I1001 12:07:06.801986  5182 solver.cpp:218] Iteration 91600 (19.0405 iter/s, 5.25197s/100 iters), loss = 0.00657968
I1001 12:07:06.802026  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00657935 (* 1 = 0.00657935 loss)
I1001 12:07:06.802031  5182 sgd_solver.cpp:105] Iteration 91600, lr = 0.001
I1001 12:07:12.051051  5182 solver.cpp:218] Iteration 91700 (19.0512 iter/s, 5.249s/100 iters), loss = 0.0268511
I1001 12:07:12.051087  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0268507 (* 1 = 0.0268507 loss)
I1001 12:07:12.051105  5182 sgd_solver.cpp:105] Iteration 91700, lr = 0.001
I1001 12:07:17.300567  5182 solver.cpp:218] Iteration 91800 (19.0497 iter/s, 5.24943s/100 iters), loss = 0.0158484
I1001 12:07:17.300686  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158481 (* 1 = 0.0158481 loss)
I1001 12:07:17.300706  5182 sgd_solver.cpp:105] Iteration 91800, lr = 0.001
I1001 12:07:22.555418  5182 solver.cpp:218] Iteration 91900 (19.0305 iter/s, 5.25472s/100 iters), loss = 0.00642205
I1001 12:07:22.555446  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00642172 (* 1 = 0.00642172 loss)
I1001 12:07:22.555452  5182 sgd_solver.cpp:105] Iteration 91900, lr = 0.001
I1001 12:07:27.549723  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:07:27.760886  5182 solver.cpp:330] Iteration 92000, Testing net (#0)
I1001 12:07:28.947901  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:07:28.998144  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9168
I1001 12:07:28.998168  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329719 (* 1 = 0.329719 loss)
I1001 12:07:29.050734  5182 solver.cpp:218] Iteration 92000 (15.3958 iter/s, 6.49527s/100 iters), loss = 0.0224394
I1001 12:07:29.050770  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0224391 (* 1 = 0.0224391 loss)
I1001 12:07:29.050776  5182 sgd_solver.cpp:105] Iteration 92000, lr = 0.001
I1001 12:07:34.309188  5182 solver.cpp:218] Iteration 92100 (19.0172 iter/s, 5.2584s/100 iters), loss = 0.00563273
I1001 12:07:34.309218  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0056324 (* 1 = 0.0056324 loss)
I1001 12:07:34.309224  5182 sgd_solver.cpp:105] Iteration 92100, lr = 0.001
I1001 12:07:39.563894  5182 solver.cpp:218] Iteration 92200 (19.0307 iter/s, 5.25466s/100 iters), loss = 0.00783972
I1001 12:07:39.563925  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00783939 (* 1 = 0.00783939 loss)
I1001 12:07:39.563941  5182 sgd_solver.cpp:105] Iteration 92200, lr = 0.001
I1001 12:07:44.807243  5182 solver.cpp:218] Iteration 92300 (19.072 iter/s, 5.2433s/100 iters), loss = 0.00385178
I1001 12:07:44.807272  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00385145 (* 1 = 0.00385145 loss)
I1001 12:07:44.807288  5182 sgd_solver.cpp:105] Iteration 92300, lr = 0.001
I1001 12:07:50.060217  5182 solver.cpp:218] Iteration 92400 (19.037 iter/s, 5.25292s/100 iters), loss = 0.00689296
I1001 12:07:50.060401  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00689263 (* 1 = 0.00689263 loss)
I1001 12:07:50.060410  5182 sgd_solver.cpp:105] Iteration 92400, lr = 0.001
I1001 12:07:55.047721  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:07:55.257285  5182 solver.cpp:330] Iteration 92500, Testing net (#0)
I1001 12:07:56.446346  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:07:56.496565  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9187
I1001 12:07:56.496589  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.328997 (* 1 = 0.328997 loss)
I1001 12:07:56.549264  5182 solver.cpp:218] Iteration 92500 (15.4111 iter/s, 6.48884s/100 iters), loss = 0.0142505
I1001 12:07:56.549294  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142502 (* 1 = 0.0142502 loss)
I1001 12:07:56.549301  5182 sgd_solver.cpp:105] Iteration 92500, lr = 0.001
I1001 12:08:01.797135  5182 solver.cpp:218] Iteration 92600 (19.0555 iter/s, 5.24782s/100 iters), loss = 0.00747904
I1001 12:08:01.797164  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00747869 (* 1 = 0.00747869 loss)
I1001 12:08:01.797171  5182 sgd_solver.cpp:105] Iteration 92600, lr = 0.001
I1001 12:08:07.052525  5182 solver.cpp:218] Iteration 92700 (19.0283 iter/s, 5.25534s/100 iters), loss = 0.00427472
I1001 12:08:07.052564  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00427438 (* 1 = 0.00427438 loss)
I1001 12:08:07.052570  5182 sgd_solver.cpp:105] Iteration 92700, lr = 0.001
I1001 12:08:12.308872  5182 solver.cpp:218] Iteration 92800 (19.0249 iter/s, 5.25628s/100 iters), loss = 0.0123097
I1001 12:08:12.308914  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123093 (* 1 = 0.0123093 loss)
I1001 12:08:12.308921  5182 sgd_solver.cpp:105] Iteration 92800, lr = 0.001
I1001 12:08:17.555239  5182 solver.cpp:218] Iteration 92900 (19.061 iter/s, 5.24631s/100 iters), loss = 0.00547169
I1001 12:08:17.555269  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00547135 (* 1 = 0.00547135 loss)
I1001 12:08:17.555284  5182 sgd_solver.cpp:105] Iteration 92900, lr = 0.001
I1001 12:08:22.550073  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:08:22.761911  5182 solver.cpp:330] Iteration 93000, Testing net (#0)
I1001 12:08:23.951197  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:08:24.001932  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9168
I1001 12:08:24.001957  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329868 (* 1 = 0.329868 loss)
I1001 12:08:24.055786  5182 solver.cpp:218] Iteration 93000 (15.3835 iter/s, 6.50049s/100 iters), loss = 0.0124084
I1001 12:08:24.055856  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.012408 (* 1 = 0.012408 loss)
I1001 12:08:24.055881  5182 sgd_solver.cpp:105] Iteration 93000, lr = 0.001
I1001 12:08:29.310869  5182 solver.cpp:218] Iteration 93100 (19.0296 iter/s, 5.25497s/100 iters), loss = 0.00370453
I1001 12:08:29.310899  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00370419 (* 1 = 0.00370419 loss)
I1001 12:08:29.310904  5182 sgd_solver.cpp:105] Iteration 93100, lr = 0.001
I1001 12:08:34.562314  5182 solver.cpp:218] Iteration 93200 (19.0426 iter/s, 5.25139s/100 iters), loss = 0.0195196
I1001 12:08:34.562345  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0195193 (* 1 = 0.0195193 loss)
I1001 12:08:34.562350  5182 sgd_solver.cpp:105] Iteration 93200, lr = 0.001
I1001 12:08:39.806953  5182 solver.cpp:218] Iteration 93300 (19.0673 iter/s, 5.24459s/100 iters), loss = 0.00645231
I1001 12:08:39.806983  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00645196 (* 1 = 0.00645196 loss)
I1001 12:08:39.806990  5182 sgd_solver.cpp:105] Iteration 93300, lr = 0.001
I1001 12:08:45.055019  5182 solver.cpp:218] Iteration 93400 (19.0548 iter/s, 5.24801s/100 iters), loss = 0.0108639
I1001 12:08:45.055057  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0108635 (* 1 = 0.0108635 loss)
I1001 12:08:45.055063  5182 sgd_solver.cpp:105] Iteration 93400, lr = 0.001
I1001 12:08:50.039777  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:08:50.249739  5182 solver.cpp:330] Iteration 93500, Testing net (#0)
I1001 12:08:51.448071  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:08:51.498482  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917
I1001 12:08:51.498517  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.328362 (* 1 = 0.328362 loss)
I1001 12:08:51.550758  5182 solver.cpp:218] Iteration 93500 (15.3949 iter/s, 6.49568s/100 iters), loss = 0.00927496
I1001 12:08:51.550789  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0092746 (* 1 = 0.0092746 loss)
I1001 12:08:51.550796  5182 sgd_solver.cpp:105] Iteration 93500, lr = 0.001
I1001 12:08:56.801226  5182 solver.cpp:218] Iteration 93600 (19.0461 iter/s, 5.25041s/100 iters), loss = 0.0169218
I1001 12:08:56.801327  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0169215 (* 1 = 0.0169215 loss)
I1001 12:08:56.801334  5182 sgd_solver.cpp:105] Iteration 93600, lr = 0.001
I1001 12:09:02.064229  5182 solver.cpp:218] Iteration 93700 (19.001 iter/s, 5.26288s/100 iters), loss = 0.00349252
I1001 12:09:02.064258  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00349217 (* 1 = 0.00349217 loss)
I1001 12:09:02.064265  5182 sgd_solver.cpp:105] Iteration 93700, lr = 0.001
I1001 12:09:07.324940  5182 solver.cpp:218] Iteration 93800 (19.009 iter/s, 5.26066s/100 iters), loss = 0.00409962
I1001 12:09:07.324970  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00409926 (* 1 = 0.00409926 loss)
I1001 12:09:07.324975  5182 sgd_solver.cpp:105] Iteration 93800, lr = 0.001
I1001 12:09:12.581162  5182 solver.cpp:218] Iteration 93900 (19.0253 iter/s, 5.25617s/100 iters), loss = 0.00616254
I1001 12:09:12.581194  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00616218 (* 1 = 0.00616218 loss)
I1001 12:09:12.581202  5182 sgd_solver.cpp:105] Iteration 93900, lr = 0.001
I1001 12:09:17.565397  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:09:17.776389  5182 solver.cpp:330] Iteration 94000, Testing net (#0)
I1001 12:09:18.974220  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:09:19.024730  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9175
I1001 12:09:19.024755  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326532 (* 1 = 0.326532 loss)
I1001 12:09:19.077507  5182 solver.cpp:218] Iteration 94000 (15.3934 iter/s, 6.49629s/100 iters), loss = 0.00521362
I1001 12:09:19.077535  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00521327 (* 1 = 0.00521327 loss)
I1001 12:09:19.077543  5182 sgd_solver.cpp:105] Iteration 94000, lr = 0.001
I1001 12:09:24.328058  5182 solver.cpp:218] Iteration 94100 (19.0458 iter/s, 5.2505s/100 iters), loss = 0.00707273
I1001 12:09:24.328090  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00707237 (* 1 = 0.00707237 loss)
I1001 12:09:24.328096  5182 sgd_solver.cpp:105] Iteration 94100, lr = 0.001
I1001 12:09:29.573088  5182 solver.cpp:218] Iteration 94200 (19.0659 iter/s, 5.24498s/100 iters), loss = 0.00371994
I1001 12:09:29.573221  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00371957 (* 1 = 0.00371957 loss)
I1001 12:09:29.573267  5182 sgd_solver.cpp:105] Iteration 94200, lr = 0.001
I1001 12:09:34.827173  5182 solver.cpp:218] Iteration 94300 (19.0333 iter/s, 5.25394s/100 iters), loss = 0.00884168
I1001 12:09:34.827204  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00884131 (* 1 = 0.00884131 loss)
I1001 12:09:34.827210  5182 sgd_solver.cpp:105] Iteration 94300, lr = 0.001
I1001 12:09:40.083992  5182 solver.cpp:218] Iteration 94400 (19.0231 iter/s, 5.25676s/100 iters), loss = 0.015073
I1001 12:09:40.084033  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0150726 (* 1 = 0.0150726 loss)
I1001 12:09:40.084038  5182 sgd_solver.cpp:105] Iteration 94400, lr = 0.001
I1001 12:09:45.070191  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:09:45.286506  5182 solver.cpp:330] Iteration 94500, Testing net (#0)
I1001 12:09:46.477321  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:09:46.527575  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9165
I1001 12:09:46.527609  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329238 (* 1 = 0.329238 loss)
I1001 12:09:46.580207  5182 solver.cpp:218] Iteration 94500 (15.3937 iter/s, 6.49615s/100 iters), loss = 0.00400761
I1001 12:09:46.580236  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00400724 (* 1 = 0.00400724 loss)
I1001 12:09:46.580243  5182 sgd_solver.cpp:105] Iteration 94500, lr = 0.001
I1001 12:09:51.835758  5182 solver.cpp:218] Iteration 94600 (19.0277 iter/s, 5.2555s/100 iters), loss = 0.00195209
I1001 12:09:51.835793  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00195171 (* 1 = 0.00195171 loss)
I1001 12:09:51.835799  5182 sgd_solver.cpp:105] Iteration 94600, lr = 0.001
I1001 12:09:57.090565  5182 solver.cpp:218] Iteration 94700 (19.0304 iter/s, 5.25475s/100 iters), loss = 0.00738775
I1001 12:09:57.090600  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00738737 (* 1 = 0.00738737 loss)
I1001 12:09:57.090607  5182 sgd_solver.cpp:105] Iteration 94700, lr = 0.001
I1001 12:10:02.344772  5182 solver.cpp:218] Iteration 94800 (19.0327 iter/s, 5.25411s/100 iters), loss = 0.0296424
I1001 12:10:02.344892  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.029642 (* 1 = 0.029642 loss)
I1001 12:10:02.344899  5182 sgd_solver.cpp:105] Iteration 94800, lr = 0.001
I1001 12:10:07.603715  5182 solver.cpp:218] Iteration 94900 (19.0157 iter/s, 5.25881s/100 iters), loss = 0.0159054
I1001 12:10:07.603745  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015905 (* 1 = 0.015905 loss)
I1001 12:10:07.603751  5182 sgd_solver.cpp:105] Iteration 94900, lr = 0.001
I1001 12:10:12.599257  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:10:12.809000  5182 solver.cpp:330] Iteration 95000, Testing net (#0)
I1001 12:10:13.999538  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:10:14.049881  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9186
I1001 12:10:14.049916  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329609 (* 1 = 0.329609 loss)
I1001 12:10:14.102325  5182 solver.cpp:218] Iteration 95000 (15.388 iter/s, 6.49856s/100 iters), loss = 0.00400404
I1001 12:10:14.102356  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00400366 (* 1 = 0.00400366 loss)
I1001 12:10:14.102363  5182 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1001 12:10:19.362002  5182 solver.cpp:218] Iteration 95100 (19.0128 iter/s, 5.25963s/100 iters), loss = 0.0103511
I1001 12:10:19.362032  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0103507 (* 1 = 0.0103507 loss)
I1001 12:10:19.362037  5182 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1001 12:10:24.618605  5182 solver.cpp:218] Iteration 95200 (19.0239 iter/s, 5.25655s/100 iters), loss = 0.00971806
I1001 12:10:24.618636  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00971769 (* 1 = 0.00971769 loss)
I1001 12:10:24.618643  5182 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1001 12:10:29.869109  5182 solver.cpp:218] Iteration 95300 (19.046 iter/s, 5.25045s/100 iters), loss = 0.00320197
I1001 12:10:29.869148  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0032016 (* 1 = 0.0032016 loss)
I1001 12:10:29.869155  5182 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1001 12:10:35.130625  5182 solver.cpp:218] Iteration 95400 (19.0061 iter/s, 5.26146s/100 iters), loss = 0.011435
I1001 12:10:35.130749  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114347 (* 1 = 0.0114347 loss)
I1001 12:10:35.130758  5182 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1001 12:10:40.124649  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:10:40.334558  5182 solver.cpp:330] Iteration 95500, Testing net (#0)
I1001 12:10:41.524376  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:10:41.574635  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9183
I1001 12:10:41.574661  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.33034 (* 1 = 0.33034 loss)
I1001 12:10:41.627005  5182 solver.cpp:218] Iteration 95500 (15.3935 iter/s, 6.49624s/100 iters), loss = 0.00284631
I1001 12:10:41.627033  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00284594 (* 1 = 0.00284594 loss)
I1001 12:10:41.627043  5182 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1001 12:10:46.881994  5182 solver.cpp:218] Iteration 95600 (19.0297 iter/s, 5.25494s/100 iters), loss = 0.00888107
I1001 12:10:46.882025  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0088807 (* 1 = 0.0088807 loss)
I1001 12:10:46.882035  5182 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1001 12:10:52.136653  5182 solver.cpp:218] Iteration 95700 (19.0309 iter/s, 5.25461s/100 iters), loss = 0.0247369
I1001 12:10:52.136682  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0247365 (* 1 = 0.0247365 loss)
I1001 12:10:52.136688  5182 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1001 12:10:57.387271  5182 solver.cpp:218] Iteration 95800 (19.0456 iter/s, 5.25057s/100 iters), loss = 0.00825941
I1001 12:10:57.387312  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00825903 (* 1 = 0.00825903 loss)
I1001 12:10:57.387320  5182 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1001 12:11:02.633776  5182 solver.cpp:218] Iteration 95900 (19.0605 iter/s, 5.24645s/100 iters), loss = 0.00565707
I1001 12:11:02.633803  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00565669 (* 1 = 0.00565669 loss)
I1001 12:11:02.633810  5182 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1001 12:11:07.624836  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:11:07.834676  5182 solver.cpp:330] Iteration 96000, Testing net (#0)
I1001 12:11:09.024966  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:11:09.075752  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9181
I1001 12:11:09.075795  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329544 (* 1 = 0.329544 loss)
I1001 12:11:09.129434  5182 solver.cpp:218] Iteration 96000 (15.395 iter/s, 6.49561s/100 iters), loss = 0.00558896
I1001 12:11:09.129489  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00558859 (* 1 = 0.00558859 loss)
I1001 12:11:09.129509  5182 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1001 12:11:14.385098  5182 solver.cpp:218] Iteration 96100 (19.0274 iter/s, 5.25557s/100 iters), loss = 0.00457315
I1001 12:11:14.385129  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00457278 (* 1 = 0.00457278 loss)
I1001 12:11:14.385148  5182 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1001 12:11:19.643643  5182 solver.cpp:218] Iteration 96200 (19.0169 iter/s, 5.25849s/100 iters), loss = 0.0136762
I1001 12:11:19.643674  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0136758 (* 1 = 0.0136758 loss)
I1001 12:11:19.643692  5182 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1001 12:11:24.905794  5182 solver.cpp:218] Iteration 96300 (19.0038 iter/s, 5.2621s/100 iters), loss = 0.0251594
I1001 12:11:24.905835  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.025159 (* 1 = 0.025159 loss)
I1001 12:11:24.905841  5182 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1001 12:11:30.159370  5182 solver.cpp:218] Iteration 96400 (19.0349 iter/s, 5.25351s/100 iters), loss = 0.00536796
I1001 12:11:30.159411  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00536758 (* 1 = 0.00536758 loss)
I1001 12:11:30.159420  5182 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1001 12:11:35.143467  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:11:35.354007  5182 solver.cpp:330] Iteration 96500, Testing net (#0)
I1001 12:11:36.552479  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:11:36.602455  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9176
I1001 12:11:36.602490  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.330631 (* 1 = 0.330631 loss)
I1001 12:11:36.655252  5182 solver.cpp:218] Iteration 96500 (15.3945 iter/s, 6.49582s/100 iters), loss = 0.00107071
I1001 12:11:36.655280  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00107033 (* 1 = 0.00107033 loss)
I1001 12:11:36.655287  5182 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1001 12:11:41.901343  5182 solver.cpp:218] Iteration 96600 (19.062 iter/s, 5.24604s/100 iters), loss = 0.00377809
I1001 12:11:41.901497  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00377771 (* 1 = 0.00377771 loss)
I1001 12:11:41.901504  5182 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1001 12:11:47.156978  5182 solver.cpp:218] Iteration 96700 (19.0278 iter/s, 5.25546s/100 iters), loss = 0.00194014
I1001 12:11:47.157017  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00193976 (* 1 = 0.00193976 loss)
I1001 12:11:47.157024  5182 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1001 12:11:52.403913  5182 solver.cpp:218] Iteration 96800 (19.0589 iter/s, 5.24688s/100 iters), loss = 0.0119301
I1001 12:11:52.403954  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119297 (* 1 = 0.0119297 loss)
I1001 12:11:52.403959  5182 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1001 12:11:57.665256  5182 solver.cpp:218] Iteration 96900 (19.0068 iter/s, 5.26128s/100 iters), loss = 0.00404683
I1001 12:11:57.665287  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00404645 (* 1 = 0.00404645 loss)
I1001 12:11:57.665292  5182 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1001 12:12:02.656517  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:12:02.866981  5182 solver.cpp:330] Iteration 97000, Testing net (#0)
I1001 12:12:04.066699  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:12:04.116670  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9164
I1001 12:12:04.116695  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.331816 (* 1 = 0.331816 loss)
I1001 12:12:04.169244  5182 solver.cpp:218] Iteration 97000 (15.3753 iter/s, 6.50394s/100 iters), loss = 0.0189841
I1001 12:12:04.169270  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0189837 (* 1 = 0.0189837 loss)
I1001 12:12:04.169276  5182 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1001 12:12:09.429188  5182 solver.cpp:218] Iteration 97100 (19.0118 iter/s, 5.25989s/100 iters), loss = 0.00281711
I1001 12:12:09.429229  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00281673 (* 1 = 0.00281673 loss)
I1001 12:12:09.429236  5182 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1001 12:12:14.673982  5182 solver.cpp:218] Iteration 97200 (19.0667 iter/s, 5.24473s/100 iters), loss = 0.0102038
I1001 12:12:14.674118  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0102035 (* 1 = 0.0102035 loss)
I1001 12:12:14.674124  5182 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1001 12:12:19.932446  5182 solver.cpp:218] Iteration 97300 (19.0175 iter/s, 5.25831s/100 iters), loss = 0.00562996
I1001 12:12:19.932476  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00562958 (* 1 = 0.00562958 loss)
I1001 12:12:19.932482  5182 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1001 12:12:25.193249  5182 solver.cpp:218] Iteration 97400 (19.0087 iter/s, 5.26075s/100 iters), loss = 0.0169623
I1001 12:12:25.193289  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0169619 (* 1 = 0.0169619 loss)
I1001 12:12:25.193295  5182 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1001 12:12:30.182281  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:12:30.395010  5182 solver.cpp:330] Iteration 97500, Testing net (#0)
I1001 12:12:31.584012  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:12:31.634188  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9179
I1001 12:12:31.634222  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332236 (* 1 = 0.332236 loss)
I1001 12:12:31.686795  5182 solver.cpp:218] Iteration 97500 (15.4 iter/s, 6.49349s/100 iters), loss = 0.00258496
I1001 12:12:31.686822  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00258457 (* 1 = 0.00258457 loss)
I1001 12:12:31.686828  5182 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1001 12:12:36.938462  5182 solver.cpp:218] Iteration 97600 (19.0417 iter/s, 5.25162s/100 iters), loss = 0.00458321
I1001 12:12:36.938495  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00458282 (* 1 = 0.00458282 loss)
I1001 12:12:36.938503  5182 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1001 12:12:42.190639  5182 solver.cpp:218] Iteration 97700 (19.0399 iter/s, 5.25212s/100 iters), loss = 0.0107274
I1001 12:12:42.190675  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107271 (* 1 = 0.0107271 loss)
I1001 12:12:42.190695  5182 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1001 12:12:47.439080  5182 solver.cpp:218] Iteration 97800 (19.0536 iter/s, 5.24835s/100 iters), loss = 0.00391245
I1001 12:12:47.439209  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00391206 (* 1 = 0.00391206 loss)
I1001 12:12:47.439229  5182 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1001 12:12:52.699667  5182 solver.cpp:218] Iteration 97900 (19.0098 iter/s, 5.26045s/100 iters), loss = 0.00737628
I1001 12:12:52.699697  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0073759 (* 1 = 0.0073759 loss)
I1001 12:12:52.699702  5182 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1001 12:12:57.699491  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:12:57.910290  5182 solver.cpp:330] Iteration 98000, Testing net (#0)
I1001 12:12:59.102263  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:12:59.152284  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9177
I1001 12:12:59.152307  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.33257 (* 1 = 0.33257 loss)
I1001 12:12:59.204814  5182 solver.cpp:218] Iteration 98000 (15.3726 iter/s, 6.5051s/100 iters), loss = 0.00814903
I1001 12:12:59.204843  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00814865 (* 1 = 0.00814865 loss)
I1001 12:12:59.204849  5182 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1001 12:13:04.464051  5182 solver.cpp:218] Iteration 98100 (19.0143 iter/s, 5.25919s/100 iters), loss = 0.0355614
I1001 12:13:04.464092  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.035561 (* 1 = 0.035561 loss)
I1001 12:13:04.464099  5182 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1001 12:13:09.721577  5182 solver.cpp:218] Iteration 98200 (19.0206 iter/s, 5.25746s/100 iters), loss = 0.00973035
I1001 12:13:09.721614  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00972996 (* 1 = 0.00972996 loss)
I1001 12:13:09.721621  5182 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1001 12:13:14.966648  5182 solver.cpp:218] Iteration 98300 (19.0657 iter/s, 5.24501s/100 iters), loss = 0.00301466
I1001 12:13:14.966676  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00301428 (* 1 = 0.00301428 loss)
I1001 12:13:14.966683  5182 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1001 12:13:20.223021  5182 solver.cpp:218] Iteration 98400 (19.0247 iter/s, 5.25632s/100 iters), loss = 0.01393
I1001 12:13:20.223157  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0139296 (* 1 = 0.0139296 loss)
I1001 12:13:20.223166  5182 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1001 12:13:25.218061  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:13:25.428856  5182 solver.cpp:330] Iteration 98500, Testing net (#0)
I1001 12:13:26.617893  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:13:26.668246  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9178
I1001 12:13:26.668269  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332319 (* 1 = 0.332319 loss)
I1001 12:13:26.720875  5182 solver.cpp:218] Iteration 98500 (15.39 iter/s, 6.49771s/100 iters), loss = 0.00827606
I1001 12:13:26.720899  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00827567 (* 1 = 0.00827567 loss)
I1001 12:13:26.720906  5182 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1001 12:13:31.975338  5182 solver.cpp:218] Iteration 98600 (19.0316 iter/s, 5.25442s/100 iters), loss = 0.00607949
I1001 12:13:31.975369  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0060791 (* 1 = 0.0060791 loss)
I1001 12:13:31.975378  5182 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1001 12:13:37.235512  5182 solver.cpp:218] Iteration 98700 (19.011 iter/s, 5.26012s/100 iters), loss = 0.0267611
I1001 12:13:37.235551  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0267607 (* 1 = 0.0267607 loss)
I1001 12:13:37.235556  5182 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1001 12:13:42.495719  5182 solver.cpp:218] Iteration 98800 (19.0109 iter/s, 5.26015s/100 iters), loss = 0.024015
I1001 12:13:42.495751  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0240146 (* 1 = 0.0240146 loss)
I1001 12:13:42.495757  5182 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1001 12:13:47.742485  5182 solver.cpp:218] Iteration 98900 (19.0596 iter/s, 5.24671s/100 iters), loss = 0.00239343
I1001 12:13:47.742514  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00239304 (* 1 = 0.00239304 loss)
I1001 12:13:47.742532  5182 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1001 12:13:52.739181  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:13:52.949136  5182 solver.cpp:330] Iteration 99000, Testing net (#0)
I1001 12:13:54.143335  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:13:54.195269  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9173
I1001 12:13:54.195314  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332198 (* 1 = 0.332198 loss)
I1001 12:13:54.248181  5182 solver.cpp:218] Iteration 99000 (15.3713 iter/s, 6.50564s/100 iters), loss = 0.00396328
I1001 12:13:54.248229  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00396289 (* 1 = 0.00396289 loss)
I1001 12:13:54.248237  5182 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1001 12:13:59.503288  5182 solver.cpp:218] Iteration 99100 (19.0295 iter/s, 5.25501s/100 iters), loss = 0.010438
I1001 12:13:59.503316  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0104376 (* 1 = 0.0104376 loss)
I1001 12:13:59.503322  5182 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1001 12:14:04.755316  5182 solver.cpp:218] Iteration 99200 (19.0404 iter/s, 5.25198s/100 iters), loss = 0.0100771
I1001 12:14:04.755345  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0100767 (* 1 = 0.0100767 loss)
I1001 12:14:04.755352  5182 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1001 12:14:10.009181  5182 solver.cpp:218] Iteration 99300 (19.0338 iter/s, 5.25381s/100 iters), loss = 0.00765818
I1001 12:14:10.009222  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00765778 (* 1 = 0.00765778 loss)
I1001 12:14:10.009228  5182 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1001 12:14:15.264470  5182 solver.cpp:218] Iteration 99400 (19.0287 iter/s, 5.25523s/100 iters), loss = 0.00452335
I1001 12:14:15.264505  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00452295 (* 1 = 0.00452295 loss)
I1001 12:14:15.264513  5182 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1001 12:14:20.249712  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:14:20.459923  5182 solver.cpp:330] Iteration 99500, Testing net (#0)
I1001 12:14:21.658077  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:14:21.708041  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9176
I1001 12:14:21.708076  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.332776 (* 1 = 0.332776 loss)
I1001 12:14:21.760929  5182 solver.cpp:218] Iteration 99500 (15.3931 iter/s, 6.4964s/100 iters), loss = 0.00148859
I1001 12:14:21.760958  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00148819 (* 1 = 0.00148819 loss)
I1001 12:14:21.760965  5182 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1001 12:14:27.009485  5182 solver.cpp:218] Iteration 99600 (19.053 iter/s, 5.24851s/100 iters), loss = 0.00669272
I1001 12:14:27.009662  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00669232 (* 1 = 0.00669232 loss)
I1001 12:14:27.009680  5182 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1001 12:14:32.266774  5182 solver.cpp:218] Iteration 99700 (19.0219 iter/s, 5.2571s/100 iters), loss = 0.00728899
I1001 12:14:32.266815  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00728859 (* 1 = 0.00728859 loss)
I1001 12:14:32.266822  5182 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1001 12:14:37.524467  5182 solver.cpp:218] Iteration 99800 (19.02 iter/s, 5.25763s/100 iters), loss = 0.00504146
I1001 12:14:37.524507  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00504107 (* 1 = 0.00504107 loss)
I1001 12:14:37.524514  5182 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1001 12:14:42.783612  5182 solver.cpp:218] Iteration 99900 (19.0147 iter/s, 5.25908s/100 iters), loss = 0.00852259
I1001 12:14:42.783640  5182 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00852219 (* 1 = 0.00852219 loss)
I1001 12:14:42.783656  5182 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1001 12:14:47.771308  5191 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:14:47.980526  5182 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_100000.caffemodel
I1001 12:14:47.985654  5182 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha0.25_beta1_etanostudy_2study_2decay_gauss_iter_100000.solverstate
I1001 12:14:47.999809  5182 solver.cpp:310] Iteration 100000, loss = 0.00208963
I1001 12:14:47.999828  5182 solver.cpp:330] Iteration 100000, Testing net (#0)
I1001 12:14:49.197330  5192 data_layer.cpp:73] Restarting data prefetching from start.
I1001 12:14:49.247047  5182 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9172
I1001 12:14:49.247081  5182 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.336062 (* 1 = 0.336062 loss)
I1001 12:14:49.247087  5182 solver.cpp:315] Optimization Done.
I1001 12:14:49.247088  5182 caffe.cpp:259] Optimization Done.
