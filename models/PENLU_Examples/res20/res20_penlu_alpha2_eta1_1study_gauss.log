I0930 19:59:22.393821  4033 caffe.cpp:218] Using GPUs 0
I0930 19:59:22.420073  4033 caffe.cpp:223] GPU 0: GeForce GTX 1080
I0930 19:59:22.661905  4033 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "xn/PENLU/snapshot/resnet/res20/res20_penlu_aipha2_eta1_1study_gauss"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 40000
stepvalue: 80000
I0930 19:59:22.662051  4033 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I0930 19:59:22.663636  4033 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I0930 19:59:22.663647  4033 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0930 19:59:22.663774  4033 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I0930 19:59:22.663836  4033 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I0930 19:59:22.664319  4033 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I0930 19:59:22.664669  4033 layer_factory.hpp:77] Creating layer Data1
I0930 19:59:22.664746  4033 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb
I0930 19:59:22.664770  4033 net.cpp:84] Creating Layer Data1
I0930 19:59:22.664777  4033 net.cpp:380] Data1 -> Data1
I0930 19:59:22.664793  4033 net.cpp:380] Data1 -> Data2
I0930 19:59:22.664800  4033 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I0930 19:59:22.666236  4033 data_layer.cpp:45] output data size: 100,3,28,28
I0930 19:59:22.668531  4033 net.cpp:122] Setting up Data1
I0930 19:59:22.668545  4033 net.cpp:129] Top shape: 100 3 28 28 (235200)
I0930 19:59:22.668548  4033 net.cpp:129] Top shape: 100 (100)
I0930 19:59:22.668551  4033 net.cpp:137] Memory required for data: 941200
I0930 19:59:22.668557  4033 layer_factory.hpp:77] Creating layer Convolution1
I0930 19:59:22.668576  4033 net.cpp:84] Creating Layer Convolution1
I0930 19:59:22.668581  4033 net.cpp:406] Convolution1 <- Data1
I0930 19:59:22.668589  4033 net.cpp:380] Convolution1 -> Convolution1
I0930 19:59:22.815460  4033 net.cpp:122] Setting up Convolution1
I0930 19:59:22.815485  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.815487  4033 net.cpp:137] Memory required for data: 5958800
I0930 19:59:22.815502  4033 layer_factory.hpp:77] Creating layer BatchNorm1
I0930 19:59:22.815524  4033 net.cpp:84] Creating Layer BatchNorm1
I0930 19:59:22.815527  4033 net.cpp:406] BatchNorm1 <- Convolution1
I0930 19:59:22.815542  4033 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0930 19:59:22.815683  4033 net.cpp:122] Setting up BatchNorm1
I0930 19:59:22.815690  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.815691  4033 net.cpp:137] Memory required for data: 10976400
I0930 19:59:22.815698  4033 layer_factory.hpp:77] Creating layer Scale1
I0930 19:59:22.815718  4033 net.cpp:84] Creating Layer Scale1
I0930 19:59:22.815721  4033 net.cpp:406] Scale1 <- Convolution1
I0930 19:59:22.815726  4033 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0930 19:59:22.815765  4033 layer_factory.hpp:77] Creating layer Scale1
I0930 19:59:22.815860  4033 net.cpp:122] Setting up Scale1
I0930 19:59:22.815865  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.815867  4033 net.cpp:137] Memory required for data: 15994000
I0930 19:59:22.815871  4033 layer_factory.hpp:77] Creating layer penlu1
I0930 19:59:22.815881  4033 net.cpp:84] Creating Layer penlu1
I0930 19:59:22.815883  4033 net.cpp:406] penlu1 <- Convolution1
I0930 19:59:22.815897  4033 net.cpp:367] penlu1 -> Convolution1 (in-place)
I0930 19:59:22.816509  4033 net.cpp:122] Setting up penlu1
I0930 19:59:22.816519  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.816520  4033 net.cpp:137] Memory required for data: 21011600
I0930 19:59:22.816527  4033 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I0930 19:59:22.816532  4033 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I0930 19:59:22.816534  4033 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I0930 19:59:22.816548  4033 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I0930 19:59:22.816555  4033 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I0930 19:59:22.816586  4033 net.cpp:122] Setting up Convolution1_penlu1_0_split
I0930 19:59:22.816591  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.816606  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.816607  4033 net.cpp:137] Memory required for data: 31046800
I0930 19:59:22.816609  4033 layer_factory.hpp:77] Creating layer Convolution2
I0930 19:59:22.816625  4033 net.cpp:84] Creating Layer Convolution2
I0930 19:59:22.816628  4033 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I0930 19:59:22.816643  4033 net.cpp:380] Convolution2 -> Convolution2
I0930 19:59:22.817481  4033 net.cpp:122] Setting up Convolution2
I0930 19:59:22.817492  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.817494  4033 net.cpp:137] Memory required for data: 36064400
I0930 19:59:22.817498  4033 layer_factory.hpp:77] Creating layer BatchNorm2
I0930 19:59:22.817524  4033 net.cpp:84] Creating Layer BatchNorm2
I0930 19:59:22.817529  4033 net.cpp:406] BatchNorm2 <- Convolution2
I0930 19:59:22.817533  4033 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0930 19:59:22.817663  4033 net.cpp:122] Setting up BatchNorm2
I0930 19:59:22.817668  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.817672  4033 net.cpp:137] Memory required for data: 41082000
I0930 19:59:22.817675  4033 layer_factory.hpp:77] Creating layer Scale2
I0930 19:59:22.817680  4033 net.cpp:84] Creating Layer Scale2
I0930 19:59:22.817683  4033 net.cpp:406] Scale2 <- Convolution2
I0930 19:59:22.817687  4033 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0930 19:59:22.817730  4033 layer_factory.hpp:77] Creating layer Scale2
I0930 19:59:22.817844  4033 net.cpp:122] Setting up Scale2
I0930 19:59:22.817849  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.817852  4033 net.cpp:137] Memory required for data: 46099600
I0930 19:59:22.817857  4033 layer_factory.hpp:77] Creating layer penlu2
I0930 19:59:22.817862  4033 net.cpp:84] Creating Layer penlu2
I0930 19:59:22.817865  4033 net.cpp:406] penlu2 <- Convolution2
I0930 19:59:22.817870  4033 net.cpp:367] penlu2 -> Convolution2 (in-place)
I0930 19:59:22.817982  4033 net.cpp:122] Setting up penlu2
I0930 19:59:22.817987  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.817989  4033 net.cpp:137] Memory required for data: 51117200
I0930 19:59:22.817993  4033 layer_factory.hpp:77] Creating layer Convolution3
I0930 19:59:22.818001  4033 net.cpp:84] Creating Layer Convolution3
I0930 19:59:22.818002  4033 net.cpp:406] Convolution3 <- Convolution2
I0930 19:59:22.818006  4033 net.cpp:380] Convolution3 -> Convolution3
I0930 19:59:22.818883  4033 net.cpp:122] Setting up Convolution3
I0930 19:59:22.818894  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.818897  4033 net.cpp:137] Memory required for data: 56134800
I0930 19:59:22.818902  4033 layer_factory.hpp:77] Creating layer BatchNorm3
I0930 19:59:22.818907  4033 net.cpp:84] Creating Layer BatchNorm3
I0930 19:59:22.818909  4033 net.cpp:406] BatchNorm3 <- Convolution3
I0930 19:59:22.818913  4033 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0930 19:59:22.819032  4033 net.cpp:122] Setting up BatchNorm3
I0930 19:59:22.819037  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.819039  4033 net.cpp:137] Memory required for data: 61152400
I0930 19:59:22.819044  4033 layer_factory.hpp:77] Creating layer Scale3
I0930 19:59:22.819048  4033 net.cpp:84] Creating Layer Scale3
I0930 19:59:22.819051  4033 net.cpp:406] Scale3 <- Convolution3
I0930 19:59:22.819053  4033 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0930 19:59:22.819078  4033 layer_factory.hpp:77] Creating layer Scale3
I0930 19:59:22.819149  4033 net.cpp:122] Setting up Scale3
I0930 19:59:22.819154  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.819157  4033 net.cpp:137] Memory required for data: 66170000
I0930 19:59:22.819160  4033 layer_factory.hpp:77] Creating layer Eltwise1
I0930 19:59:22.819165  4033 net.cpp:84] Creating Layer Eltwise1
I0930 19:59:22.819167  4033 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I0930 19:59:22.819170  4033 net.cpp:406] Eltwise1 <- Convolution3
I0930 19:59:22.819173  4033 net.cpp:380] Eltwise1 -> Eltwise1
I0930 19:59:22.819190  4033 net.cpp:122] Setting up Eltwise1
I0930 19:59:22.819193  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.819195  4033 net.cpp:137] Memory required for data: 71187600
I0930 19:59:22.819197  4033 layer_factory.hpp:77] Creating layer penlu3
I0930 19:59:22.819202  4033 net.cpp:84] Creating Layer penlu3
I0930 19:59:22.819205  4033 net.cpp:406] penlu3 <- Eltwise1
I0930 19:59:22.819209  4033 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I0930 19:59:22.819324  4033 net.cpp:122] Setting up penlu3
I0930 19:59:22.819329  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.819331  4033 net.cpp:137] Memory required for data: 76205200
I0930 19:59:22.819355  4033 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I0930 19:59:22.819360  4033 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I0930 19:59:22.819362  4033 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I0930 19:59:22.819365  4033 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I0930 19:59:22.819370  4033 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I0930 19:59:22.819402  4033 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I0930 19:59:22.819407  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.819422  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.819424  4033 net.cpp:137] Memory required for data: 86240400
I0930 19:59:22.819427  4033 layer_factory.hpp:77] Creating layer Convolution4
I0930 19:59:22.819433  4033 net.cpp:84] Creating Layer Convolution4
I0930 19:59:22.819437  4033 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I0930 19:59:22.819450  4033 net.cpp:380] Convolution4 -> Convolution4
I0930 19:59:22.820323  4033 net.cpp:122] Setting up Convolution4
I0930 19:59:22.820333  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.820338  4033 net.cpp:137] Memory required for data: 91258000
I0930 19:59:22.820341  4033 layer_factory.hpp:77] Creating layer BatchNorm4
I0930 19:59:22.820349  4033 net.cpp:84] Creating Layer BatchNorm4
I0930 19:59:22.820353  4033 net.cpp:406] BatchNorm4 <- Convolution4
I0930 19:59:22.820356  4033 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0930 19:59:22.820477  4033 net.cpp:122] Setting up BatchNorm4
I0930 19:59:22.820482  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.820485  4033 net.cpp:137] Memory required for data: 96275600
I0930 19:59:22.820493  4033 layer_factory.hpp:77] Creating layer Scale4
I0930 19:59:22.820499  4033 net.cpp:84] Creating Layer Scale4
I0930 19:59:22.820502  4033 net.cpp:406] Scale4 <- Convolution4
I0930 19:59:22.820505  4033 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0930 19:59:22.820531  4033 layer_factory.hpp:77] Creating layer Scale4
I0930 19:59:22.820605  4033 net.cpp:122] Setting up Scale4
I0930 19:59:22.820610  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.820612  4033 net.cpp:137] Memory required for data: 101293200
I0930 19:59:22.820616  4033 layer_factory.hpp:77] Creating layer penlu4
I0930 19:59:22.820622  4033 net.cpp:84] Creating Layer penlu4
I0930 19:59:22.820626  4033 net.cpp:406] penlu4 <- Convolution4
I0930 19:59:22.820629  4033 net.cpp:367] penlu4 -> Convolution4 (in-place)
I0930 19:59:22.820724  4033 net.cpp:122] Setting up penlu4
I0930 19:59:22.820729  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.820732  4033 net.cpp:137] Memory required for data: 106310800
I0930 19:59:22.820737  4033 layer_factory.hpp:77] Creating layer Convolution5
I0930 19:59:22.820744  4033 net.cpp:84] Creating Layer Convolution5
I0930 19:59:22.820749  4033 net.cpp:406] Convolution5 <- Convolution4
I0930 19:59:22.820751  4033 net.cpp:380] Convolution5 -> Convolution5
I0930 19:59:22.821614  4033 net.cpp:122] Setting up Convolution5
I0930 19:59:22.821624  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.821629  4033 net.cpp:137] Memory required for data: 111328400
I0930 19:59:22.821632  4033 layer_factory.hpp:77] Creating layer BatchNorm5
I0930 19:59:22.821638  4033 net.cpp:84] Creating Layer BatchNorm5
I0930 19:59:22.821642  4033 net.cpp:406] BatchNorm5 <- Convolution5
I0930 19:59:22.821646  4033 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0930 19:59:22.821771  4033 net.cpp:122] Setting up BatchNorm5
I0930 19:59:22.821777  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.821780  4033 net.cpp:137] Memory required for data: 116346000
I0930 19:59:22.821785  4033 layer_factory.hpp:77] Creating layer Scale5
I0930 19:59:22.821790  4033 net.cpp:84] Creating Layer Scale5
I0930 19:59:22.821794  4033 net.cpp:406] Scale5 <- Convolution5
I0930 19:59:22.821799  4033 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0930 19:59:22.821822  4033 layer_factory.hpp:77] Creating layer Scale5
I0930 19:59:22.821907  4033 net.cpp:122] Setting up Scale5
I0930 19:59:22.821913  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.821915  4033 net.cpp:137] Memory required for data: 121363600
I0930 19:59:22.821919  4033 layer_factory.hpp:77] Creating layer Eltwise2
I0930 19:59:22.821924  4033 net.cpp:84] Creating Layer Eltwise2
I0930 19:59:22.821928  4033 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I0930 19:59:22.821930  4033 net.cpp:406] Eltwise2 <- Convolution5
I0930 19:59:22.821935  4033 net.cpp:380] Eltwise2 -> Eltwise2
I0930 19:59:22.821951  4033 net.cpp:122] Setting up Eltwise2
I0930 19:59:22.821956  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.821959  4033 net.cpp:137] Memory required for data: 126381200
I0930 19:59:22.821961  4033 layer_factory.hpp:77] Creating layer penlu5
I0930 19:59:22.821966  4033 net.cpp:84] Creating Layer penlu5
I0930 19:59:22.821969  4033 net.cpp:406] penlu5 <- Eltwise2
I0930 19:59:22.821974  4033 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I0930 19:59:22.822077  4033 net.cpp:122] Setting up penlu5
I0930 19:59:22.822082  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.822084  4033 net.cpp:137] Memory required for data: 131398800
I0930 19:59:22.822088  4033 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I0930 19:59:22.822093  4033 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I0930 19:59:22.822096  4033 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I0930 19:59:22.822099  4033 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I0930 19:59:22.822104  4033 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I0930 19:59:22.822126  4033 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I0930 19:59:22.822131  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.822134  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.822137  4033 net.cpp:137] Memory required for data: 141434000
I0930 19:59:22.822140  4033 layer_factory.hpp:77] Creating layer Convolution6
I0930 19:59:22.822146  4033 net.cpp:84] Creating Layer Convolution6
I0930 19:59:22.822149  4033 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I0930 19:59:22.822154  4033 net.cpp:380] Convolution6 -> Convolution6
I0930 19:59:22.823021  4033 net.cpp:122] Setting up Convolution6
I0930 19:59:22.823032  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.823036  4033 net.cpp:137] Memory required for data: 146451600
I0930 19:59:22.823040  4033 layer_factory.hpp:77] Creating layer BatchNorm6
I0930 19:59:22.823046  4033 net.cpp:84] Creating Layer BatchNorm6
I0930 19:59:22.823050  4033 net.cpp:406] BatchNorm6 <- Convolution6
I0930 19:59:22.823055  4033 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0930 19:59:22.823181  4033 net.cpp:122] Setting up BatchNorm6
I0930 19:59:22.823187  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.823190  4033 net.cpp:137] Memory required for data: 151469200
I0930 19:59:22.823195  4033 layer_factory.hpp:77] Creating layer Scale6
I0930 19:59:22.823200  4033 net.cpp:84] Creating Layer Scale6
I0930 19:59:22.823204  4033 net.cpp:406] Scale6 <- Convolution6
I0930 19:59:22.823210  4033 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0930 19:59:22.823233  4033 layer_factory.hpp:77] Creating layer Scale6
I0930 19:59:22.823308  4033 net.cpp:122] Setting up Scale6
I0930 19:59:22.823313  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.823317  4033 net.cpp:137] Memory required for data: 156486800
I0930 19:59:22.823320  4033 layer_factory.hpp:77] Creating layer penlu6
I0930 19:59:22.823326  4033 net.cpp:84] Creating Layer penlu6
I0930 19:59:22.823329  4033 net.cpp:406] penlu6 <- Convolution6
I0930 19:59:22.823333  4033 net.cpp:367] penlu6 -> Convolution6 (in-place)
I0930 19:59:22.823436  4033 net.cpp:122] Setting up penlu6
I0930 19:59:22.823441  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.823444  4033 net.cpp:137] Memory required for data: 161504400
I0930 19:59:22.823448  4033 layer_factory.hpp:77] Creating layer Convolution7
I0930 19:59:22.823462  4033 net.cpp:84] Creating Layer Convolution7
I0930 19:59:22.823467  4033 net.cpp:406] Convolution7 <- Convolution6
I0930 19:59:22.823472  4033 net.cpp:380] Convolution7 -> Convolution7
I0930 19:59:22.824014  4033 net.cpp:122] Setting up Convolution7
I0930 19:59:22.824024  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.824028  4033 net.cpp:137] Memory required for data: 166522000
I0930 19:59:22.824033  4033 layer_factory.hpp:77] Creating layer BatchNorm7
I0930 19:59:22.824038  4033 net.cpp:84] Creating Layer BatchNorm7
I0930 19:59:22.824043  4033 net.cpp:406] BatchNorm7 <- Convolution7
I0930 19:59:22.824046  4033 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0930 19:59:22.824172  4033 net.cpp:122] Setting up BatchNorm7
I0930 19:59:22.824177  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.824180  4033 net.cpp:137] Memory required for data: 171539600
I0930 19:59:22.824190  4033 layer_factory.hpp:77] Creating layer Scale7
I0930 19:59:22.824198  4033 net.cpp:84] Creating Layer Scale7
I0930 19:59:22.824200  4033 net.cpp:406] Scale7 <- Convolution7
I0930 19:59:22.824204  4033 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0930 19:59:22.824231  4033 layer_factory.hpp:77] Creating layer Scale7
I0930 19:59:22.824306  4033 net.cpp:122] Setting up Scale7
I0930 19:59:22.824311  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.824314  4033 net.cpp:137] Memory required for data: 176557200
I0930 19:59:22.824318  4033 layer_factory.hpp:77] Creating layer Eltwise3
I0930 19:59:22.824323  4033 net.cpp:84] Creating Layer Eltwise3
I0930 19:59:22.824326  4033 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I0930 19:59:22.824329  4033 net.cpp:406] Eltwise3 <- Convolution7
I0930 19:59:22.824333  4033 net.cpp:380] Eltwise3 -> Eltwise3
I0930 19:59:22.824348  4033 net.cpp:122] Setting up Eltwise3
I0930 19:59:22.824352  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.824355  4033 net.cpp:137] Memory required for data: 181574800
I0930 19:59:22.824357  4033 layer_factory.hpp:77] Creating layer penlu7
I0930 19:59:22.824363  4033 net.cpp:84] Creating Layer penlu7
I0930 19:59:22.824368  4033 net.cpp:406] penlu7 <- Eltwise3
I0930 19:59:22.824370  4033 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I0930 19:59:22.824473  4033 net.cpp:122] Setting up penlu7
I0930 19:59:22.824478  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.824481  4033 net.cpp:137] Memory required for data: 186592400
I0930 19:59:22.824486  4033 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I0930 19:59:22.824491  4033 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I0930 19:59:22.824493  4033 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I0930 19:59:22.824497  4033 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I0930 19:59:22.824502  4033 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I0930 19:59:22.824523  4033 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I0930 19:59:22.824528  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.824532  4033 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0930 19:59:22.824534  4033 net.cpp:137] Memory required for data: 196627600
I0930 19:59:22.824537  4033 layer_factory.hpp:77] Creating layer Convolution8
I0930 19:59:22.824543  4033 net.cpp:84] Creating Layer Convolution8
I0930 19:59:22.824546  4033 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I0930 19:59:22.824550  4033 net.cpp:380] Convolution8 -> Convolution8
I0930 19:59:22.825726  4033 net.cpp:122] Setting up Convolution8
I0930 19:59:22.825736  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.825739  4033 net.cpp:137] Memory required for data: 199136400
I0930 19:59:22.825754  4033 layer_factory.hpp:77] Creating layer BatchNorm8
I0930 19:59:22.825760  4033 net.cpp:84] Creating Layer BatchNorm8
I0930 19:59:22.825764  4033 net.cpp:406] BatchNorm8 <- Convolution8
I0930 19:59:22.825768  4033 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0930 19:59:22.825918  4033 net.cpp:122] Setting up BatchNorm8
I0930 19:59:22.825924  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.825927  4033 net.cpp:137] Memory required for data: 201645200
I0930 19:59:22.825932  4033 layer_factory.hpp:77] Creating layer Scale8
I0930 19:59:22.825938  4033 net.cpp:84] Creating Layer Scale8
I0930 19:59:22.825942  4033 net.cpp:406] Scale8 <- Convolution8
I0930 19:59:22.825944  4033 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0930 19:59:22.825971  4033 layer_factory.hpp:77] Creating layer Scale8
I0930 19:59:22.826043  4033 net.cpp:122] Setting up Scale8
I0930 19:59:22.826048  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.826051  4033 net.cpp:137] Memory required for data: 204154000
I0930 19:59:22.826056  4033 layer_factory.hpp:77] Creating layer Convolution9
I0930 19:59:22.826063  4033 net.cpp:84] Creating Layer Convolution9
I0930 19:59:22.826066  4033 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I0930 19:59:22.826071  4033 net.cpp:380] Convolution9 -> Convolution9
I0930 19:59:22.827358  4033 net.cpp:122] Setting up Convolution9
I0930 19:59:22.827368  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.827371  4033 net.cpp:137] Memory required for data: 206662800
I0930 19:59:22.827376  4033 layer_factory.hpp:77] Creating layer BatchNorm9
I0930 19:59:22.827383  4033 net.cpp:84] Creating Layer BatchNorm9
I0930 19:59:22.827385  4033 net.cpp:406] BatchNorm9 <- Convolution9
I0930 19:59:22.827390  4033 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0930 19:59:22.827527  4033 net.cpp:122] Setting up BatchNorm9
I0930 19:59:22.827533  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.827545  4033 net.cpp:137] Memory required for data: 209171600
I0930 19:59:22.827549  4033 layer_factory.hpp:77] Creating layer Scale9
I0930 19:59:22.827554  4033 net.cpp:84] Creating Layer Scale9
I0930 19:59:22.827556  4033 net.cpp:406] Scale9 <- Convolution9
I0930 19:59:22.827560  4033 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0930 19:59:22.827596  4033 layer_factory.hpp:77] Creating layer Scale9
I0930 19:59:22.827718  4033 net.cpp:122] Setting up Scale9
I0930 19:59:22.827723  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.827724  4033 net.cpp:137] Memory required for data: 211680400
I0930 19:59:22.827728  4033 layer_factory.hpp:77] Creating layer penlu8
I0930 19:59:22.827734  4033 net.cpp:84] Creating Layer penlu8
I0930 19:59:22.827738  4033 net.cpp:406] penlu8 <- Convolution9
I0930 19:59:22.827750  4033 net.cpp:367] penlu8 -> Convolution9 (in-place)
I0930 19:59:22.827862  4033 net.cpp:122] Setting up penlu8
I0930 19:59:22.827867  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.827869  4033 net.cpp:137] Memory required for data: 214189200
I0930 19:59:22.827874  4033 layer_factory.hpp:77] Creating layer Convolution10
I0930 19:59:22.827889  4033 net.cpp:84] Creating Layer Convolution10
I0930 19:59:22.827893  4033 net.cpp:406] Convolution10 <- Convolution9
I0930 19:59:22.827908  4033 net.cpp:380] Convolution10 -> Convolution10
I0930 19:59:22.829082  4033 net.cpp:122] Setting up Convolution10
I0930 19:59:22.829093  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.829107  4033 net.cpp:137] Memory required for data: 216698000
I0930 19:59:22.829111  4033 layer_factory.hpp:77] Creating layer BatchNorm10
I0930 19:59:22.829116  4033 net.cpp:84] Creating Layer BatchNorm10
I0930 19:59:22.829119  4033 net.cpp:406] BatchNorm10 <- Convolution10
I0930 19:59:22.829123  4033 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0930 19:59:22.829264  4033 net.cpp:122] Setting up BatchNorm10
I0930 19:59:22.829270  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.829272  4033 net.cpp:137] Memory required for data: 219206800
I0930 19:59:22.829288  4033 layer_factory.hpp:77] Creating layer Scale10
I0930 19:59:22.829291  4033 net.cpp:84] Creating Layer Scale10
I0930 19:59:22.829294  4033 net.cpp:406] Scale10 <- Convolution10
I0930 19:59:22.829298  4033 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0930 19:59:22.829354  4033 layer_factory.hpp:77] Creating layer Scale10
I0930 19:59:22.829447  4033 net.cpp:122] Setting up Scale10
I0930 19:59:22.829452  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.829464  4033 net.cpp:137] Memory required for data: 221715600
I0930 19:59:22.829468  4033 layer_factory.hpp:77] Creating layer Eltwise4
I0930 19:59:22.829473  4033 net.cpp:84] Creating Layer Eltwise4
I0930 19:59:22.829475  4033 net.cpp:406] Eltwise4 <- Convolution8
I0930 19:59:22.829478  4033 net.cpp:406] Eltwise4 <- Convolution10
I0930 19:59:22.829481  4033 net.cpp:380] Eltwise4 -> Eltwise4
I0930 19:59:22.829497  4033 net.cpp:122] Setting up Eltwise4
I0930 19:59:22.829501  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.829504  4033 net.cpp:137] Memory required for data: 224224400
I0930 19:59:22.829506  4033 layer_factory.hpp:77] Creating layer penlu9
I0930 19:59:22.829511  4033 net.cpp:84] Creating Layer penlu9
I0930 19:59:22.829514  4033 net.cpp:406] penlu9 <- Eltwise4
I0930 19:59:22.829519  4033 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I0930 19:59:22.829620  4033 net.cpp:122] Setting up penlu9
I0930 19:59:22.829627  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.829628  4033 net.cpp:137] Memory required for data: 226733200
I0930 19:59:22.829632  4033 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I0930 19:59:22.829637  4033 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I0930 19:59:22.829639  4033 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I0930 19:59:22.829643  4033 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I0930 19:59:22.829646  4033 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I0930 19:59:22.829668  4033 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I0930 19:59:22.829674  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.829676  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.829679  4033 net.cpp:137] Memory required for data: 231750800
I0930 19:59:22.829681  4033 layer_factory.hpp:77] Creating layer Convolution11
I0930 19:59:22.829686  4033 net.cpp:84] Creating Layer Convolution11
I0930 19:59:22.829690  4033 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I0930 19:59:22.829694  4033 net.cpp:380] Convolution11 -> Convolution11
I0930 19:59:22.830744  4033 net.cpp:122] Setting up Convolution11
I0930 19:59:22.830755  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.830760  4033 net.cpp:137] Memory required for data: 234259600
I0930 19:59:22.830763  4033 layer_factory.hpp:77] Creating layer BatchNorm11
I0930 19:59:22.830768  4033 net.cpp:84] Creating Layer BatchNorm11
I0930 19:59:22.830772  4033 net.cpp:406] BatchNorm11 <- Convolution11
I0930 19:59:22.830776  4033 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0930 19:59:22.830905  4033 net.cpp:122] Setting up BatchNorm11
I0930 19:59:22.830911  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.830914  4033 net.cpp:137] Memory required for data: 236768400
I0930 19:59:22.830919  4033 layer_factory.hpp:77] Creating layer Scale11
I0930 19:59:22.830924  4033 net.cpp:84] Creating Layer Scale11
I0930 19:59:22.830926  4033 net.cpp:406] Scale11 <- Convolution11
I0930 19:59:22.830929  4033 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0930 19:59:22.830955  4033 layer_factory.hpp:77] Creating layer Scale11
I0930 19:59:22.831030  4033 net.cpp:122] Setting up Scale11
I0930 19:59:22.831035  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.831038  4033 net.cpp:137] Memory required for data: 239277200
I0930 19:59:22.831043  4033 layer_factory.hpp:77] Creating layer penlu10
I0930 19:59:22.831048  4033 net.cpp:84] Creating Layer penlu10
I0930 19:59:22.831050  4033 net.cpp:406] penlu10 <- Convolution11
I0930 19:59:22.831054  4033 net.cpp:367] penlu10 -> Convolution11 (in-place)
I0930 19:59:22.831157  4033 net.cpp:122] Setting up penlu10
I0930 19:59:22.831162  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.831172  4033 net.cpp:137] Memory required for data: 241786000
I0930 19:59:22.831177  4033 layer_factory.hpp:77] Creating layer Convolution12
I0930 19:59:22.831184  4033 net.cpp:84] Creating Layer Convolution12
I0930 19:59:22.831187  4033 net.cpp:406] Convolution12 <- Convolution11
I0930 19:59:22.831192  4033 net.cpp:380] Convolution12 -> Convolution12
I0930 19:59:22.832203  4033 net.cpp:122] Setting up Convolution12
I0930 19:59:22.832213  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.832217  4033 net.cpp:137] Memory required for data: 244294800
I0930 19:59:22.832221  4033 layer_factory.hpp:77] Creating layer BatchNorm12
I0930 19:59:22.832227  4033 net.cpp:84] Creating Layer BatchNorm12
I0930 19:59:22.832231  4033 net.cpp:406] BatchNorm12 <- Convolution12
I0930 19:59:22.832234  4033 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0930 19:59:22.832362  4033 net.cpp:122] Setting up BatchNorm12
I0930 19:59:22.832367  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.832370  4033 net.cpp:137] Memory required for data: 246803600
I0930 19:59:22.832375  4033 layer_factory.hpp:77] Creating layer Scale12
I0930 19:59:22.832379  4033 net.cpp:84] Creating Layer Scale12
I0930 19:59:22.832382  4033 net.cpp:406] Scale12 <- Convolution12
I0930 19:59:22.832386  4033 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0930 19:59:22.832412  4033 layer_factory.hpp:77] Creating layer Scale12
I0930 19:59:22.832485  4033 net.cpp:122] Setting up Scale12
I0930 19:59:22.832490  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.832492  4033 net.cpp:137] Memory required for data: 249312400
I0930 19:59:22.832496  4033 layer_factory.hpp:77] Creating layer Eltwise5
I0930 19:59:22.832502  4033 net.cpp:84] Creating Layer Eltwise5
I0930 19:59:22.832505  4033 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I0930 19:59:22.832509  4033 net.cpp:406] Eltwise5 <- Convolution12
I0930 19:59:22.832511  4033 net.cpp:380] Eltwise5 -> Eltwise5
I0930 19:59:22.832527  4033 net.cpp:122] Setting up Eltwise5
I0930 19:59:22.832532  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.832535  4033 net.cpp:137] Memory required for data: 251821200
I0930 19:59:22.832536  4033 layer_factory.hpp:77] Creating layer penlu11
I0930 19:59:22.832541  4033 net.cpp:84] Creating Layer penlu11
I0930 19:59:22.832545  4033 net.cpp:406] penlu11 <- Eltwise5
I0930 19:59:22.832548  4033 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I0930 19:59:22.832649  4033 net.cpp:122] Setting up penlu11
I0930 19:59:22.832653  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.832656  4033 net.cpp:137] Memory required for data: 254330000
I0930 19:59:22.832660  4033 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I0930 19:59:22.832664  4033 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I0930 19:59:22.832666  4033 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I0930 19:59:22.832671  4033 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I0930 19:59:22.832676  4033 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I0930 19:59:22.832697  4033 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I0930 19:59:22.832702  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.832706  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.832708  4033 net.cpp:137] Memory required for data: 259347600
I0930 19:59:22.832710  4033 layer_factory.hpp:77] Creating layer Convolution13
I0930 19:59:22.832716  4033 net.cpp:84] Creating Layer Convolution13
I0930 19:59:22.832720  4033 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I0930 19:59:22.832723  4033 net.cpp:380] Convolution13 -> Convolution13
I0930 19:59:22.833734  4033 net.cpp:122] Setting up Convolution13
I0930 19:59:22.833744  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.833746  4033 net.cpp:137] Memory required for data: 261856400
I0930 19:59:22.833751  4033 layer_factory.hpp:77] Creating layer BatchNorm13
I0930 19:59:22.833757  4033 net.cpp:84] Creating Layer BatchNorm13
I0930 19:59:22.833766  4033 net.cpp:406] BatchNorm13 <- Convolution13
I0930 19:59:22.833771  4033 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0930 19:59:22.833899  4033 net.cpp:122] Setting up BatchNorm13
I0930 19:59:22.833905  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.833909  4033 net.cpp:137] Memory required for data: 264365200
I0930 19:59:22.833914  4033 layer_factory.hpp:77] Creating layer Scale13
I0930 19:59:22.833919  4033 net.cpp:84] Creating Layer Scale13
I0930 19:59:22.833921  4033 net.cpp:406] Scale13 <- Convolution13
I0930 19:59:22.833925  4033 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0930 19:59:22.833951  4033 layer_factory.hpp:77] Creating layer Scale13
I0930 19:59:22.834024  4033 net.cpp:122] Setting up Scale13
I0930 19:59:22.834029  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.834033  4033 net.cpp:137] Memory required for data: 266874000
I0930 19:59:22.834035  4033 layer_factory.hpp:77] Creating layer penlu12
I0930 19:59:22.834043  4033 net.cpp:84] Creating Layer penlu12
I0930 19:59:22.834045  4033 net.cpp:406] penlu12 <- Convolution13
I0930 19:59:22.834049  4033 net.cpp:367] penlu12 -> Convolution13 (in-place)
I0930 19:59:22.834153  4033 net.cpp:122] Setting up penlu12
I0930 19:59:22.834158  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.834161  4033 net.cpp:137] Memory required for data: 269382800
I0930 19:59:22.834166  4033 layer_factory.hpp:77] Creating layer Convolution14
I0930 19:59:22.834173  4033 net.cpp:84] Creating Layer Convolution14
I0930 19:59:22.834177  4033 net.cpp:406] Convolution14 <- Convolution13
I0930 19:59:22.834180  4033 net.cpp:380] Convolution14 -> Convolution14
I0930 19:59:22.835220  4033 net.cpp:122] Setting up Convolution14
I0930 19:59:22.835230  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.835234  4033 net.cpp:137] Memory required for data: 271891600
I0930 19:59:22.835250  4033 layer_factory.hpp:77] Creating layer BatchNorm14
I0930 19:59:22.835259  4033 net.cpp:84] Creating Layer BatchNorm14
I0930 19:59:22.835263  4033 net.cpp:406] BatchNorm14 <- Convolution14
I0930 19:59:22.835268  4033 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0930 19:59:22.835395  4033 net.cpp:122] Setting up BatchNorm14
I0930 19:59:22.835400  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.835404  4033 net.cpp:137] Memory required for data: 274400400
I0930 19:59:22.835409  4033 layer_factory.hpp:77] Creating layer Scale14
I0930 19:59:22.835413  4033 net.cpp:84] Creating Layer Scale14
I0930 19:59:22.835417  4033 net.cpp:406] Scale14 <- Convolution14
I0930 19:59:22.835420  4033 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0930 19:59:22.835448  4033 layer_factory.hpp:77] Creating layer Scale14
I0930 19:59:22.835522  4033 net.cpp:122] Setting up Scale14
I0930 19:59:22.835528  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.835531  4033 net.cpp:137] Memory required for data: 276909200
I0930 19:59:22.835536  4033 layer_factory.hpp:77] Creating layer Eltwise6
I0930 19:59:22.835539  4033 net.cpp:84] Creating Layer Eltwise6
I0930 19:59:22.835541  4033 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I0930 19:59:22.835544  4033 net.cpp:406] Eltwise6 <- Convolution14
I0930 19:59:22.835547  4033 net.cpp:380] Eltwise6 -> Eltwise6
I0930 19:59:22.835562  4033 net.cpp:122] Setting up Eltwise6
I0930 19:59:22.835566  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.835568  4033 net.cpp:137] Memory required for data: 279418000
I0930 19:59:22.835571  4033 layer_factory.hpp:77] Creating layer penlu13
I0930 19:59:22.835575  4033 net.cpp:84] Creating Layer penlu13
I0930 19:59:22.835577  4033 net.cpp:406] penlu13 <- Eltwise6
I0930 19:59:22.835582  4033 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I0930 19:59:22.835683  4033 net.cpp:122] Setting up penlu13
I0930 19:59:22.835688  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.835690  4033 net.cpp:137] Memory required for data: 281926800
I0930 19:59:22.835695  4033 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I0930 19:59:22.835705  4033 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I0930 19:59:22.835707  4033 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I0930 19:59:22.835711  4033 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I0930 19:59:22.835716  4033 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I0930 19:59:22.835737  4033 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I0930 19:59:22.835741  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.835744  4033 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0930 19:59:22.835747  4033 net.cpp:137] Memory required for data: 286944400
I0930 19:59:22.835748  4033 layer_factory.hpp:77] Creating layer Convolution15
I0930 19:59:22.835764  4033 net.cpp:84] Creating Layer Convolution15
I0930 19:59:22.835767  4033 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I0930 19:59:22.835772  4033 net.cpp:380] Convolution15 -> Convolution15
I0930 19:59:22.836678  4033 net.cpp:122] Setting up Convolution15
I0930 19:59:22.836686  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.836689  4033 net.cpp:137] Memory required for data: 288198800
I0930 19:59:22.836694  4033 layer_factory.hpp:77] Creating layer BatchNorm15
I0930 19:59:22.836699  4033 net.cpp:84] Creating Layer BatchNorm15
I0930 19:59:22.836702  4033 net.cpp:406] BatchNorm15 <- Convolution15
I0930 19:59:22.836706  4033 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0930 19:59:22.836836  4033 net.cpp:122] Setting up BatchNorm15
I0930 19:59:22.836840  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.836843  4033 net.cpp:137] Memory required for data: 289453200
I0930 19:59:22.836848  4033 layer_factory.hpp:77] Creating layer Scale15
I0930 19:59:22.836851  4033 net.cpp:84] Creating Layer Scale15
I0930 19:59:22.836854  4033 net.cpp:406] Scale15 <- Convolution15
I0930 19:59:22.836858  4033 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0930 19:59:22.836884  4033 layer_factory.hpp:77] Creating layer Scale15
I0930 19:59:22.836956  4033 net.cpp:122] Setting up Scale15
I0930 19:59:22.836961  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.836964  4033 net.cpp:137] Memory required for data: 290707600
I0930 19:59:22.836967  4033 layer_factory.hpp:77] Creating layer Convolution16
I0930 19:59:22.836974  4033 net.cpp:84] Creating Layer Convolution16
I0930 19:59:22.836977  4033 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I0930 19:59:22.836980  4033 net.cpp:380] Convolution16 -> Convolution16
I0930 19:59:22.838781  4033 net.cpp:122] Setting up Convolution16
I0930 19:59:22.838791  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.838794  4033 net.cpp:137] Memory required for data: 291962000
I0930 19:59:22.838798  4033 layer_factory.hpp:77] Creating layer BatchNorm16
I0930 19:59:22.838804  4033 net.cpp:84] Creating Layer BatchNorm16
I0930 19:59:22.838807  4033 net.cpp:406] BatchNorm16 <- Convolution16
I0930 19:59:22.838810  4033 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0930 19:59:22.838951  4033 net.cpp:122] Setting up BatchNorm16
I0930 19:59:22.838956  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.838958  4033 net.cpp:137] Memory required for data: 293216400
I0930 19:59:22.838963  4033 layer_factory.hpp:77] Creating layer Scale16
I0930 19:59:22.838968  4033 net.cpp:84] Creating Layer Scale16
I0930 19:59:22.838969  4033 net.cpp:406] Scale16 <- Convolution16
I0930 19:59:22.838973  4033 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0930 19:59:22.839001  4033 layer_factory.hpp:77] Creating layer Scale16
I0930 19:59:22.839076  4033 net.cpp:122] Setting up Scale16
I0930 19:59:22.839082  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.839084  4033 net.cpp:137] Memory required for data: 294470800
I0930 19:59:22.839087  4033 layer_factory.hpp:77] Creating layer penlu14
I0930 19:59:22.839092  4033 net.cpp:84] Creating Layer penlu14
I0930 19:59:22.839095  4033 net.cpp:406] penlu14 <- Convolution16
I0930 19:59:22.839099  4033 net.cpp:367] penlu14 -> Convolution16 (in-place)
I0930 19:59:22.839215  4033 net.cpp:122] Setting up penlu14
I0930 19:59:22.839221  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.839222  4033 net.cpp:137] Memory required for data: 295725200
I0930 19:59:22.839226  4033 layer_factory.hpp:77] Creating layer Convolution17
I0930 19:59:22.839233  4033 net.cpp:84] Creating Layer Convolution17
I0930 19:59:22.839236  4033 net.cpp:406] Convolution17 <- Convolution16
I0930 19:59:22.839241  4033 net.cpp:380] Convolution17 -> Convolution17
I0930 19:59:22.840966  4033 net.cpp:122] Setting up Convolution17
I0930 19:59:22.840975  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.840977  4033 net.cpp:137] Memory required for data: 296979600
I0930 19:59:22.840982  4033 layer_factory.hpp:77] Creating layer BatchNorm17
I0930 19:59:22.840987  4033 net.cpp:84] Creating Layer BatchNorm17
I0930 19:59:22.840991  4033 net.cpp:406] BatchNorm17 <- Convolution17
I0930 19:59:22.840994  4033 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0930 19:59:22.841125  4033 net.cpp:122] Setting up BatchNorm17
I0930 19:59:22.841130  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.841132  4033 net.cpp:137] Memory required for data: 298234000
I0930 19:59:22.841137  4033 layer_factory.hpp:77] Creating layer Scale17
I0930 19:59:22.841141  4033 net.cpp:84] Creating Layer Scale17
I0930 19:59:22.841143  4033 net.cpp:406] Scale17 <- Convolution17
I0930 19:59:22.841146  4033 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0930 19:59:22.841173  4033 layer_factory.hpp:77] Creating layer Scale17
I0930 19:59:22.841248  4033 net.cpp:122] Setting up Scale17
I0930 19:59:22.841253  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.841255  4033 net.cpp:137] Memory required for data: 299488400
I0930 19:59:22.841259  4033 layer_factory.hpp:77] Creating layer Eltwise7
I0930 19:59:22.841264  4033 net.cpp:84] Creating Layer Eltwise7
I0930 19:59:22.841266  4033 net.cpp:406] Eltwise7 <- Convolution15
I0930 19:59:22.841269  4033 net.cpp:406] Eltwise7 <- Convolution17
I0930 19:59:22.841272  4033 net.cpp:380] Eltwise7 -> Eltwise7
I0930 19:59:22.841298  4033 net.cpp:122] Setting up Eltwise7
I0930 19:59:22.841302  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.841305  4033 net.cpp:137] Memory required for data: 300742800
I0930 19:59:22.841306  4033 layer_factory.hpp:77] Creating layer penlu15
I0930 19:59:22.841312  4033 net.cpp:84] Creating Layer penlu15
I0930 19:59:22.841331  4033 net.cpp:406] penlu15 <- Eltwise7
I0930 19:59:22.841334  4033 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I0930 19:59:22.841464  4033 net.cpp:122] Setting up penlu15
I0930 19:59:22.841467  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.841470  4033 net.cpp:137] Memory required for data: 301997200
I0930 19:59:22.841475  4033 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I0930 19:59:22.841478  4033 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I0930 19:59:22.841481  4033 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I0930 19:59:22.841493  4033 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I0930 19:59:22.841497  4033 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I0930 19:59:22.841519  4033 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I0930 19:59:22.841523  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.841526  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.841528  4033 net.cpp:137] Memory required for data: 304506000
I0930 19:59:22.841531  4033 layer_factory.hpp:77] Creating layer Convolution18
I0930 19:59:22.841536  4033 net.cpp:84] Creating Layer Convolution18
I0930 19:59:22.841538  4033 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I0930 19:59:22.841542  4033 net.cpp:380] Convolution18 -> Convolution18
I0930 19:59:22.843348  4033 net.cpp:122] Setting up Convolution18
I0930 19:59:22.843356  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.843359  4033 net.cpp:137] Memory required for data: 305760400
I0930 19:59:22.843370  4033 layer_factory.hpp:77] Creating layer BatchNorm18
I0930 19:59:22.843376  4033 net.cpp:84] Creating Layer BatchNorm18
I0930 19:59:22.843379  4033 net.cpp:406] BatchNorm18 <- Convolution18
I0930 19:59:22.843384  4033 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0930 19:59:22.843516  4033 net.cpp:122] Setting up BatchNorm18
I0930 19:59:22.843520  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.843523  4033 net.cpp:137] Memory required for data: 307014800
I0930 19:59:22.843528  4033 layer_factory.hpp:77] Creating layer Scale18
I0930 19:59:22.843531  4033 net.cpp:84] Creating Layer Scale18
I0930 19:59:22.843534  4033 net.cpp:406] Scale18 <- Convolution18
I0930 19:59:22.843538  4033 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0930 19:59:22.843564  4033 layer_factory.hpp:77] Creating layer Scale18
I0930 19:59:22.843638  4033 net.cpp:122] Setting up Scale18
I0930 19:59:22.843643  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.843646  4033 net.cpp:137] Memory required for data: 308269200
I0930 19:59:22.843649  4033 layer_factory.hpp:77] Creating layer penlu16
I0930 19:59:22.843654  4033 net.cpp:84] Creating Layer penlu16
I0930 19:59:22.843657  4033 net.cpp:406] penlu16 <- Convolution18
I0930 19:59:22.843660  4033 net.cpp:367] penlu16 -> Convolution18 (in-place)
I0930 19:59:22.843768  4033 net.cpp:122] Setting up penlu16
I0930 19:59:22.843772  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.843775  4033 net.cpp:137] Memory required for data: 309523600
I0930 19:59:22.843778  4033 layer_factory.hpp:77] Creating layer Convolution19
I0930 19:59:22.843786  4033 net.cpp:84] Creating Layer Convolution19
I0930 19:59:22.843788  4033 net.cpp:406] Convolution19 <- Convolution18
I0930 19:59:22.843792  4033 net.cpp:380] Convolution19 -> Convolution19
I0930 19:59:22.846041  4033 net.cpp:122] Setting up Convolution19
I0930 19:59:22.846051  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.846055  4033 net.cpp:137] Memory required for data: 310778000
I0930 19:59:22.846060  4033 layer_factory.hpp:77] Creating layer BatchNorm19
I0930 19:59:22.846065  4033 net.cpp:84] Creating Layer BatchNorm19
I0930 19:59:22.846068  4033 net.cpp:406] BatchNorm19 <- Convolution19
I0930 19:59:22.846073  4033 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0930 19:59:22.846207  4033 net.cpp:122] Setting up BatchNorm19
I0930 19:59:22.846213  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.846216  4033 net.cpp:137] Memory required for data: 312032400
I0930 19:59:22.846221  4033 layer_factory.hpp:77] Creating layer Scale19
I0930 19:59:22.846227  4033 net.cpp:84] Creating Layer Scale19
I0930 19:59:22.846230  4033 net.cpp:406] Scale19 <- Convolution19
I0930 19:59:22.846233  4033 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0930 19:59:22.846259  4033 layer_factory.hpp:77] Creating layer Scale19
I0930 19:59:22.846338  4033 net.cpp:122] Setting up Scale19
I0930 19:59:22.846344  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.846346  4033 net.cpp:137] Memory required for data: 313286800
I0930 19:59:22.846350  4033 layer_factory.hpp:77] Creating layer Eltwise8
I0930 19:59:22.846354  4033 net.cpp:84] Creating Layer Eltwise8
I0930 19:59:22.846359  4033 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I0930 19:59:22.846360  4033 net.cpp:406] Eltwise8 <- Convolution19
I0930 19:59:22.846365  4033 net.cpp:380] Eltwise8 -> Eltwise8
I0930 19:59:22.846381  4033 net.cpp:122] Setting up Eltwise8
I0930 19:59:22.846385  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.846387  4033 net.cpp:137] Memory required for data: 314541200
I0930 19:59:22.846390  4033 layer_factory.hpp:77] Creating layer penlu17
I0930 19:59:22.846396  4033 net.cpp:84] Creating Layer penlu17
I0930 19:59:22.846400  4033 net.cpp:406] penlu17 <- Eltwise8
I0930 19:59:22.846402  4033 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I0930 19:59:22.846510  4033 net.cpp:122] Setting up penlu17
I0930 19:59:22.846515  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.846547  4033 net.cpp:137] Memory required for data: 315795600
I0930 19:59:22.846554  4033 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I0930 19:59:22.846568  4033 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I0930 19:59:22.846571  4033 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I0930 19:59:22.846575  4033 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I0930 19:59:22.846580  4033 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I0930 19:59:22.846606  4033 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I0930 19:59:22.846611  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.846613  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.846616  4033 net.cpp:137] Memory required for data: 318304400
I0930 19:59:22.846618  4033 layer_factory.hpp:77] Creating layer Convolution20
I0930 19:59:22.846626  4033 net.cpp:84] Creating Layer Convolution20
I0930 19:59:22.846628  4033 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I0930 19:59:22.846632  4033 net.cpp:380] Convolution20 -> Convolution20
I0930 19:59:22.848265  4033 net.cpp:122] Setting up Convolution20
I0930 19:59:22.848275  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.848279  4033 net.cpp:137] Memory required for data: 319558800
I0930 19:59:22.848284  4033 layer_factory.hpp:77] Creating layer BatchNorm20
I0930 19:59:22.848290  4033 net.cpp:84] Creating Layer BatchNorm20
I0930 19:59:22.848294  4033 net.cpp:406] BatchNorm20 <- Convolution20
I0930 19:59:22.848299  4033 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0930 19:59:22.848428  4033 net.cpp:122] Setting up BatchNorm20
I0930 19:59:22.848433  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.848435  4033 net.cpp:137] Memory required for data: 320813200
I0930 19:59:22.848440  4033 layer_factory.hpp:77] Creating layer Scale20
I0930 19:59:22.848448  4033 net.cpp:84] Creating Layer Scale20
I0930 19:59:22.848450  4033 net.cpp:406] Scale20 <- Convolution20
I0930 19:59:22.848453  4033 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0930 19:59:22.848480  4033 layer_factory.hpp:77] Creating layer Scale20
I0930 19:59:22.848556  4033 net.cpp:122] Setting up Scale20
I0930 19:59:22.848562  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.848563  4033 net.cpp:137] Memory required for data: 322067600
I0930 19:59:22.848567  4033 layer_factory.hpp:77] Creating layer penlu18
I0930 19:59:22.848573  4033 net.cpp:84] Creating Layer penlu18
I0930 19:59:22.848577  4033 net.cpp:406] penlu18 <- Convolution20
I0930 19:59:22.848580  4033 net.cpp:367] penlu18 -> Convolution20 (in-place)
I0930 19:59:22.848685  4033 net.cpp:122] Setting up penlu18
I0930 19:59:22.848688  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.848691  4033 net.cpp:137] Memory required for data: 323322000
I0930 19:59:22.848695  4033 layer_factory.hpp:77] Creating layer Convolution21
I0930 19:59:22.848702  4033 net.cpp:84] Creating Layer Convolution21
I0930 19:59:22.848704  4033 net.cpp:406] Convolution21 <- Convolution20
I0930 19:59:22.848708  4033 net.cpp:380] Convolution21 -> Convolution21
I0930 19:59:22.850963  4033 net.cpp:122] Setting up Convolution21
I0930 19:59:22.850972  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.850975  4033 net.cpp:137] Memory required for data: 324576400
I0930 19:59:22.850980  4033 layer_factory.hpp:77] Creating layer BatchNorm21
I0930 19:59:22.850986  4033 net.cpp:84] Creating Layer BatchNorm21
I0930 19:59:22.850988  4033 net.cpp:406] BatchNorm21 <- Convolution21
I0930 19:59:22.850992  4033 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0930 19:59:22.851125  4033 net.cpp:122] Setting up BatchNorm21
I0930 19:59:22.851130  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.851132  4033 net.cpp:137] Memory required for data: 325830800
I0930 19:59:22.851137  4033 layer_factory.hpp:77] Creating layer Scale21
I0930 19:59:22.851141  4033 net.cpp:84] Creating Layer Scale21
I0930 19:59:22.851145  4033 net.cpp:406] Scale21 <- Convolution21
I0930 19:59:22.851153  4033 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0930 19:59:22.851182  4033 layer_factory.hpp:77] Creating layer Scale21
I0930 19:59:22.851258  4033 net.cpp:122] Setting up Scale21
I0930 19:59:22.851263  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.851264  4033 net.cpp:137] Memory required for data: 327085200
I0930 19:59:22.851269  4033 layer_factory.hpp:77] Creating layer Eltwise9
I0930 19:59:22.851272  4033 net.cpp:84] Creating Layer Eltwise9
I0930 19:59:22.851275  4033 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I0930 19:59:22.851279  4033 net.cpp:406] Eltwise9 <- Convolution21
I0930 19:59:22.851281  4033 net.cpp:380] Eltwise9 -> Eltwise9
I0930 19:59:22.851295  4033 net.cpp:122] Setting up Eltwise9
I0930 19:59:22.851300  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.851301  4033 net.cpp:137] Memory required for data: 328339600
I0930 19:59:22.851303  4033 layer_factory.hpp:77] Creating layer penlu19
I0930 19:59:22.851308  4033 net.cpp:84] Creating Layer penlu19
I0930 19:59:22.851311  4033 net.cpp:406] penlu19 <- Eltwise9
I0930 19:59:22.851315  4033 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I0930 19:59:22.851430  4033 net.cpp:122] Setting up penlu19
I0930 19:59:22.851435  4033 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0930 19:59:22.851438  4033 net.cpp:137] Memory required for data: 329594000
I0930 19:59:22.851442  4033 layer_factory.hpp:77] Creating layer Pooling1
I0930 19:59:22.851447  4033 net.cpp:84] Creating Layer Pooling1
I0930 19:59:22.851450  4033 net.cpp:406] Pooling1 <- Eltwise9
I0930 19:59:22.851454  4033 net.cpp:380] Pooling1 -> Pooling1
I0930 19:59:22.851608  4033 net.cpp:122] Setting up Pooling1
I0930 19:59:22.851614  4033 net.cpp:129] Top shape: 100 64 1 1 (6400)
I0930 19:59:22.851616  4033 net.cpp:137] Memory required for data: 329619600
I0930 19:59:22.851619  4033 layer_factory.hpp:77] Creating layer InnerProduct1
I0930 19:59:22.851629  4033 net.cpp:84] Creating Layer InnerProduct1
I0930 19:59:22.851630  4033 net.cpp:406] InnerProduct1 <- Pooling1
I0930 19:59:22.851635  4033 net.cpp:380] InnerProduct1 -> InnerProduct1
I0930 19:59:22.851728  4033 net.cpp:122] Setting up InnerProduct1
I0930 19:59:22.851732  4033 net.cpp:129] Top shape: 100 10 (1000)
I0930 19:59:22.851735  4033 net.cpp:137] Memory required for data: 329623600
I0930 19:59:22.851738  4033 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0930 19:59:22.851743  4033 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0930 19:59:22.851747  4033 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I0930 19:59:22.851749  4033 net.cpp:406] SoftmaxWithLoss1 <- Data2
I0930 19:59:22.851753  4033 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0930 19:59:22.851758  4033 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0930 19:59:22.852309  4033 net.cpp:122] Setting up SoftmaxWithLoss1
I0930 19:59:22.852318  4033 net.cpp:129] Top shape: (1)
I0930 19:59:22.852320  4033 net.cpp:132]     with loss weight 1
I0930 19:59:22.852332  4033 net.cpp:137] Memory required for data: 329623604
I0930 19:59:22.852335  4033 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0930 19:59:22.852339  4033 net.cpp:198] InnerProduct1 needs backward computation.
I0930 19:59:22.852340  4033 net.cpp:198] Pooling1 needs backward computation.
I0930 19:59:22.852342  4033 net.cpp:198] penlu19 needs backward computation.
I0930 19:59:22.852344  4033 net.cpp:198] Eltwise9 needs backward computation.
I0930 19:59:22.852347  4033 net.cpp:198] Scale21 needs backward computation.
I0930 19:59:22.852349  4033 net.cpp:198] BatchNorm21 needs backward computation.
I0930 19:59:22.852351  4033 net.cpp:198] Convolution21 needs backward computation.
I0930 19:59:22.852354  4033 net.cpp:198] penlu18 needs backward computation.
I0930 19:59:22.852355  4033 net.cpp:198] Scale20 needs backward computation.
I0930 19:59:22.852357  4033 net.cpp:198] BatchNorm20 needs backward computation.
I0930 19:59:22.852360  4033 net.cpp:198] Convolution20 needs backward computation.
I0930 19:59:22.852361  4033 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I0930 19:59:22.852370  4033 net.cpp:198] penlu17 needs backward computation.
I0930 19:59:22.852372  4033 net.cpp:198] Eltwise8 needs backward computation.
I0930 19:59:22.852375  4033 net.cpp:198] Scale19 needs backward computation.
I0930 19:59:22.852376  4033 net.cpp:198] BatchNorm19 needs backward computation.
I0930 19:59:22.852378  4033 net.cpp:198] Convolution19 needs backward computation.
I0930 19:59:22.852380  4033 net.cpp:198] penlu16 needs backward computation.
I0930 19:59:22.852382  4033 net.cpp:198] Scale18 needs backward computation.
I0930 19:59:22.852385  4033 net.cpp:198] BatchNorm18 needs backward computation.
I0930 19:59:22.852386  4033 net.cpp:198] Convolution18 needs backward computation.
I0930 19:59:22.852388  4033 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I0930 19:59:22.852391  4033 net.cpp:198] penlu15 needs backward computation.
I0930 19:59:22.852393  4033 net.cpp:198] Eltwise7 needs backward computation.
I0930 19:59:22.852396  4033 net.cpp:198] Scale17 needs backward computation.
I0930 19:59:22.852398  4033 net.cpp:198] BatchNorm17 needs backward computation.
I0930 19:59:22.852401  4033 net.cpp:198] Convolution17 needs backward computation.
I0930 19:59:22.852402  4033 net.cpp:198] penlu14 needs backward computation.
I0930 19:59:22.852404  4033 net.cpp:198] Scale16 needs backward computation.
I0930 19:59:22.852406  4033 net.cpp:198] BatchNorm16 needs backward computation.
I0930 19:59:22.852408  4033 net.cpp:198] Convolution16 needs backward computation.
I0930 19:59:22.852411  4033 net.cpp:198] Scale15 needs backward computation.
I0930 19:59:22.852413  4033 net.cpp:198] BatchNorm15 needs backward computation.
I0930 19:59:22.852416  4033 net.cpp:198] Convolution15 needs backward computation.
I0930 19:59:22.852418  4033 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I0930 19:59:22.852421  4033 net.cpp:198] penlu13 needs backward computation.
I0930 19:59:22.852423  4033 net.cpp:198] Eltwise6 needs backward computation.
I0930 19:59:22.852427  4033 net.cpp:198] Scale14 needs backward computation.
I0930 19:59:22.852428  4033 net.cpp:198] BatchNorm14 needs backward computation.
I0930 19:59:22.852432  4033 net.cpp:198] Convolution14 needs backward computation.
I0930 19:59:22.852433  4033 net.cpp:198] penlu12 needs backward computation.
I0930 19:59:22.852435  4033 net.cpp:198] Scale13 needs backward computation.
I0930 19:59:22.852437  4033 net.cpp:198] BatchNorm13 needs backward computation.
I0930 19:59:22.852439  4033 net.cpp:198] Convolution13 needs backward computation.
I0930 19:59:22.852442  4033 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I0930 19:59:22.852445  4033 net.cpp:198] penlu11 needs backward computation.
I0930 19:59:22.852447  4033 net.cpp:198] Eltwise5 needs backward computation.
I0930 19:59:22.852450  4033 net.cpp:198] Scale12 needs backward computation.
I0930 19:59:22.852452  4033 net.cpp:198] BatchNorm12 needs backward computation.
I0930 19:59:22.852454  4033 net.cpp:198] Convolution12 needs backward computation.
I0930 19:59:22.852457  4033 net.cpp:198] penlu10 needs backward computation.
I0930 19:59:22.852458  4033 net.cpp:198] Scale11 needs backward computation.
I0930 19:59:22.852461  4033 net.cpp:198] BatchNorm11 needs backward computation.
I0930 19:59:22.852463  4033 net.cpp:198] Convolution11 needs backward computation.
I0930 19:59:22.852465  4033 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I0930 19:59:22.852468  4033 net.cpp:198] penlu9 needs backward computation.
I0930 19:59:22.852470  4033 net.cpp:198] Eltwise4 needs backward computation.
I0930 19:59:22.852473  4033 net.cpp:198] Scale10 needs backward computation.
I0930 19:59:22.852475  4033 net.cpp:198] BatchNorm10 needs backward computation.
I0930 19:59:22.852478  4033 net.cpp:198] Convolution10 needs backward computation.
I0930 19:59:22.852480  4033 net.cpp:198] penlu8 needs backward computation.
I0930 19:59:22.852483  4033 net.cpp:198] Scale9 needs backward computation.
I0930 19:59:22.852488  4033 net.cpp:198] BatchNorm9 needs backward computation.
I0930 19:59:22.852490  4033 net.cpp:198] Convolution9 needs backward computation.
I0930 19:59:22.852494  4033 net.cpp:198] Scale8 needs backward computation.
I0930 19:59:22.852495  4033 net.cpp:198] BatchNorm8 needs backward computation.
I0930 19:59:22.852497  4033 net.cpp:198] Convolution8 needs backward computation.
I0930 19:59:22.852500  4033 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I0930 19:59:22.852502  4033 net.cpp:198] penlu7 needs backward computation.
I0930 19:59:22.852504  4033 net.cpp:198] Eltwise3 needs backward computation.
I0930 19:59:22.852507  4033 net.cpp:198] Scale7 needs backward computation.
I0930 19:59:22.852509  4033 net.cpp:198] BatchNorm7 needs backward computation.
I0930 19:59:22.852512  4033 net.cpp:198] Convolution7 needs backward computation.
I0930 19:59:22.852514  4033 net.cpp:198] penlu6 needs backward computation.
I0930 19:59:22.852516  4033 net.cpp:198] Scale6 needs backward computation.
I0930 19:59:22.852519  4033 net.cpp:198] BatchNorm6 needs backward computation.
I0930 19:59:22.852521  4033 net.cpp:198] Convolution6 needs backward computation.
I0930 19:59:22.852524  4033 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I0930 19:59:22.852525  4033 net.cpp:198] penlu5 needs backward computation.
I0930 19:59:22.852529  4033 net.cpp:198] Eltwise2 needs backward computation.
I0930 19:59:22.852530  4033 net.cpp:198] Scale5 needs backward computation.
I0930 19:59:22.852533  4033 net.cpp:198] BatchNorm5 needs backward computation.
I0930 19:59:22.852535  4033 net.cpp:198] Convolution5 needs backward computation.
I0930 19:59:22.852537  4033 net.cpp:198] penlu4 needs backward computation.
I0930 19:59:22.852540  4033 net.cpp:198] Scale4 needs backward computation.
I0930 19:59:22.852542  4033 net.cpp:198] BatchNorm4 needs backward computation.
I0930 19:59:22.852545  4033 net.cpp:198] Convolution4 needs backward computation.
I0930 19:59:22.852547  4033 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I0930 19:59:22.852550  4033 net.cpp:198] penlu3 needs backward computation.
I0930 19:59:22.852551  4033 net.cpp:198] Eltwise1 needs backward computation.
I0930 19:59:22.852555  4033 net.cpp:198] Scale3 needs backward computation.
I0930 19:59:22.852556  4033 net.cpp:198] BatchNorm3 needs backward computation.
I0930 19:59:22.852558  4033 net.cpp:198] Convolution3 needs backward computation.
I0930 19:59:22.852561  4033 net.cpp:198] penlu2 needs backward computation.
I0930 19:59:22.852563  4033 net.cpp:198] Scale2 needs backward computation.
I0930 19:59:22.852566  4033 net.cpp:198] BatchNorm2 needs backward computation.
I0930 19:59:22.852567  4033 net.cpp:198] Convolution2 needs backward computation.
I0930 19:59:22.852571  4033 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I0930 19:59:22.852573  4033 net.cpp:198] penlu1 needs backward computation.
I0930 19:59:22.852576  4033 net.cpp:198] Scale1 needs backward computation.
I0930 19:59:22.852577  4033 net.cpp:198] BatchNorm1 needs backward computation.
I0930 19:59:22.852579  4033 net.cpp:198] Convolution1 needs backward computation.
I0930 19:59:22.852582  4033 net.cpp:200] Data1 does not need backward computation.
I0930 19:59:22.852584  4033 net.cpp:242] This network produces output SoftmaxWithLoss1
I0930 19:59:22.852617  4033 net.cpp:255] Network initialization done.
I0930 19:59:22.854295  4033 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I0930 19:59:22.854302  4033 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0930 19:59:22.854306  4033 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I0930 19:59:22.854382  4033 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I0930 19:59:22.854882  4033 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I0930 19:59:22.855134  4033 layer_factory.hpp:77] Creating layer Data1
I0930 19:59:22.855170  4033 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb
I0930 19:59:22.855180  4033 net.cpp:84] Creating Layer Data1
I0930 19:59:22.855185  4033 net.cpp:380] Data1 -> Data1
I0930 19:59:22.855190  4033 net.cpp:380] Data1 -> Data2
I0930 19:59:22.855195  4033 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I0930 19:59:22.855303  4033 data_layer.cpp:45] output data size: 100,3,32,32
I0930 19:59:22.859071  4033 net.cpp:122] Setting up Data1
I0930 19:59:22.859091  4033 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0930 19:59:22.859094  4033 net.cpp:129] Top shape: 100 (100)
I0930 19:59:22.859097  4033 net.cpp:137] Memory required for data: 1229200
I0930 19:59:22.859100  4033 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0930 19:59:22.859109  4033 net.cpp:84] Creating Layer Data2_Data1_1_split
I0930 19:59:22.859112  4033 net.cpp:406] Data2_Data1_1_split <- Data2
I0930 19:59:22.859117  4033 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0930 19:59:22.859123  4033 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0930 19:59:22.859195  4033 net.cpp:122] Setting up Data2_Data1_1_split
I0930 19:59:22.859200  4033 net.cpp:129] Top shape: 100 (100)
I0930 19:59:22.859215  4033 net.cpp:129] Top shape: 100 (100)
I0930 19:59:22.859218  4033 net.cpp:137] Memory required for data: 1230000
I0930 19:59:22.859220  4033 layer_factory.hpp:77] Creating layer Convolution1
I0930 19:59:22.859230  4033 net.cpp:84] Creating Layer Convolution1
I0930 19:59:22.859233  4033 net.cpp:406] Convolution1 <- Data1
I0930 19:59:22.859237  4033 net.cpp:380] Convolution1 -> Convolution1
I0930 19:59:22.860364  4033 net.cpp:122] Setting up Convolution1
I0930 19:59:22.860374  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.860378  4033 net.cpp:137] Memory required for data: 7783600
I0930 19:59:22.860385  4033 layer_factory.hpp:77] Creating layer BatchNorm1
I0930 19:59:22.860391  4033 net.cpp:84] Creating Layer BatchNorm1
I0930 19:59:22.860395  4033 net.cpp:406] BatchNorm1 <- Convolution1
I0930 19:59:22.860399  4033 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0930 19:59:22.860538  4033 net.cpp:122] Setting up BatchNorm1
I0930 19:59:22.860545  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.860548  4033 net.cpp:137] Memory required for data: 14337200
I0930 19:59:22.860554  4033 layer_factory.hpp:77] Creating layer Scale1
I0930 19:59:22.860560  4033 net.cpp:84] Creating Layer Scale1
I0930 19:59:22.860563  4033 net.cpp:406] Scale1 <- Convolution1
I0930 19:59:22.860565  4033 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0930 19:59:22.860595  4033 layer_factory.hpp:77] Creating layer Scale1
I0930 19:59:22.860674  4033 net.cpp:122] Setting up Scale1
I0930 19:59:22.860679  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.860682  4033 net.cpp:137] Memory required for data: 20890800
I0930 19:59:22.860687  4033 layer_factory.hpp:77] Creating layer penlu1
I0930 19:59:22.860692  4033 net.cpp:84] Creating Layer penlu1
I0930 19:59:22.860694  4033 net.cpp:406] penlu1 <- Convolution1
I0930 19:59:22.860697  4033 net.cpp:367] penlu1 -> Convolution1 (in-place)
I0930 19:59:22.860816  4033 net.cpp:122] Setting up penlu1
I0930 19:59:22.860821  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.860824  4033 net.cpp:137] Memory required for data: 27444400
I0930 19:59:22.860831  4033 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I0930 19:59:22.860836  4033 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I0930 19:59:22.860838  4033 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I0930 19:59:22.860841  4033 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I0930 19:59:22.860846  4033 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I0930 19:59:22.860870  4033 net.cpp:122] Setting up Convolution1_penlu1_0_split
I0930 19:59:22.860875  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.860878  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.860880  4033 net.cpp:137] Memory required for data: 40551600
I0930 19:59:22.860882  4033 layer_factory.hpp:77] Creating layer Convolution2
I0930 19:59:22.860889  4033 net.cpp:84] Creating Layer Convolution2
I0930 19:59:22.860891  4033 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I0930 19:59:22.860896  4033 net.cpp:380] Convolution2 -> Convolution2
I0930 19:59:22.861497  4033 net.cpp:122] Setting up Convolution2
I0930 19:59:22.861505  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.861507  4033 net.cpp:137] Memory required for data: 47105200
I0930 19:59:22.861512  4033 layer_factory.hpp:77] Creating layer BatchNorm2
I0930 19:59:22.861518  4033 net.cpp:84] Creating Layer BatchNorm2
I0930 19:59:22.861521  4033 net.cpp:406] BatchNorm2 <- Convolution2
I0930 19:59:22.861526  4033 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0930 19:59:22.861678  4033 net.cpp:122] Setting up BatchNorm2
I0930 19:59:22.861685  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.861686  4033 net.cpp:137] Memory required for data: 53658800
I0930 19:59:22.861691  4033 layer_factory.hpp:77] Creating layer Scale2
I0930 19:59:22.861696  4033 net.cpp:84] Creating Layer Scale2
I0930 19:59:22.861704  4033 net.cpp:406] Scale2 <- Convolution2
I0930 19:59:22.861709  4033 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0930 19:59:22.861812  4033 layer_factory.hpp:77] Creating layer Scale2
I0930 19:59:22.861891  4033 net.cpp:122] Setting up Scale2
I0930 19:59:22.861897  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.861899  4033 net.cpp:137] Memory required for data: 60212400
I0930 19:59:22.861907  4033 layer_factory.hpp:77] Creating layer penlu2
I0930 19:59:22.861912  4033 net.cpp:84] Creating Layer penlu2
I0930 19:59:22.861913  4033 net.cpp:406] penlu2 <- Convolution2
I0930 19:59:22.861918  4033 net.cpp:367] penlu2 -> Convolution2 (in-place)
I0930 19:59:22.862036  4033 net.cpp:122] Setting up penlu2
I0930 19:59:22.862041  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.862043  4033 net.cpp:137] Memory required for data: 66766000
I0930 19:59:22.862048  4033 layer_factory.hpp:77] Creating layer Convolution3
I0930 19:59:22.862056  4033 net.cpp:84] Creating Layer Convolution3
I0930 19:59:22.862058  4033 net.cpp:406] Convolution3 <- Convolution2
I0930 19:59:22.862062  4033 net.cpp:380] Convolution3 -> Convolution3
I0930 19:59:22.863025  4033 net.cpp:122] Setting up Convolution3
I0930 19:59:22.863035  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.863039  4033 net.cpp:137] Memory required for data: 73319600
I0930 19:59:22.863042  4033 layer_factory.hpp:77] Creating layer BatchNorm3
I0930 19:59:22.863049  4033 net.cpp:84] Creating Layer BatchNorm3
I0930 19:59:22.863052  4033 net.cpp:406] BatchNorm3 <- Convolution3
I0930 19:59:22.863055  4033 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0930 19:59:22.863207  4033 net.cpp:122] Setting up BatchNorm3
I0930 19:59:22.863214  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.863215  4033 net.cpp:137] Memory required for data: 79873200
I0930 19:59:22.863220  4033 layer_factory.hpp:77] Creating layer Scale3
I0930 19:59:22.863224  4033 net.cpp:84] Creating Layer Scale3
I0930 19:59:22.863227  4033 net.cpp:406] Scale3 <- Convolution3
I0930 19:59:22.863230  4033 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0930 19:59:22.863348  4033 layer_factory.hpp:77] Creating layer Scale3
I0930 19:59:22.863426  4033 net.cpp:122] Setting up Scale3
I0930 19:59:22.863432  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.863435  4033 net.cpp:137] Memory required for data: 86426800
I0930 19:59:22.863438  4033 layer_factory.hpp:77] Creating layer Eltwise1
I0930 19:59:22.863445  4033 net.cpp:84] Creating Layer Eltwise1
I0930 19:59:22.863446  4033 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I0930 19:59:22.863450  4033 net.cpp:406] Eltwise1 <- Convolution3
I0930 19:59:22.863453  4033 net.cpp:380] Eltwise1 -> Eltwise1
I0930 19:59:22.863469  4033 net.cpp:122] Setting up Eltwise1
I0930 19:59:22.863473  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.863476  4033 net.cpp:137] Memory required for data: 92980400
I0930 19:59:22.863477  4033 layer_factory.hpp:77] Creating layer penlu3
I0930 19:59:22.863483  4033 net.cpp:84] Creating Layer penlu3
I0930 19:59:22.863487  4033 net.cpp:406] penlu3 <- Eltwise1
I0930 19:59:22.863492  4033 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I0930 19:59:22.863608  4033 net.cpp:122] Setting up penlu3
I0930 19:59:22.863612  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.863615  4033 net.cpp:137] Memory required for data: 99534000
I0930 19:59:22.863620  4033 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I0930 19:59:22.863623  4033 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I0930 19:59:22.863626  4033 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I0930 19:59:22.863629  4033 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I0930 19:59:22.863634  4033 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I0930 19:59:22.863656  4033 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I0930 19:59:22.863661  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.863670  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.863672  4033 net.cpp:137] Memory required for data: 112641200
I0930 19:59:22.863675  4033 layer_factory.hpp:77] Creating layer Convolution4
I0930 19:59:22.863682  4033 net.cpp:84] Creating Layer Convolution4
I0930 19:59:22.863684  4033 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I0930 19:59:22.863688  4033 net.cpp:380] Convolution4 -> Convolution4
I0930 19:59:22.864625  4033 net.cpp:122] Setting up Convolution4
I0930 19:59:22.864634  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.864636  4033 net.cpp:137] Memory required for data: 119194800
I0930 19:59:22.864641  4033 layer_factory.hpp:77] Creating layer BatchNorm4
I0930 19:59:22.864646  4033 net.cpp:84] Creating Layer BatchNorm4
I0930 19:59:22.864650  4033 net.cpp:406] BatchNorm4 <- Convolution4
I0930 19:59:22.864653  4033 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0930 19:59:22.864877  4033 net.cpp:122] Setting up BatchNorm4
I0930 19:59:22.864882  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.864886  4033 net.cpp:137] Memory required for data: 125748400
I0930 19:59:22.864894  4033 layer_factory.hpp:77] Creating layer Scale4
I0930 19:59:22.864898  4033 net.cpp:84] Creating Layer Scale4
I0930 19:59:22.864902  4033 net.cpp:406] Scale4 <- Convolution4
I0930 19:59:22.864905  4033 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0930 19:59:22.864934  4033 layer_factory.hpp:77] Creating layer Scale4
I0930 19:59:22.865011  4033 net.cpp:122] Setting up Scale4
I0930 19:59:22.865015  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.865018  4033 net.cpp:137] Memory required for data: 132302000
I0930 19:59:22.865022  4033 layer_factory.hpp:77] Creating layer penlu4
I0930 19:59:22.865027  4033 net.cpp:84] Creating Layer penlu4
I0930 19:59:22.865030  4033 net.cpp:406] penlu4 <- Convolution4
I0930 19:59:22.865033  4033 net.cpp:367] penlu4 -> Convolution4 (in-place)
I0930 19:59:22.865149  4033 net.cpp:122] Setting up penlu4
I0930 19:59:22.865154  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.865155  4033 net.cpp:137] Memory required for data: 138855600
I0930 19:59:22.865166  4033 layer_factory.hpp:77] Creating layer Convolution5
I0930 19:59:22.865172  4033 net.cpp:84] Creating Layer Convolution5
I0930 19:59:22.865175  4033 net.cpp:406] Convolution5 <- Convolution4
I0930 19:59:22.865180  4033 net.cpp:380] Convolution5 -> Convolution5
I0930 19:59:22.866441  4033 net.cpp:122] Setting up Convolution5
I0930 19:59:22.866451  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.866453  4033 net.cpp:137] Memory required for data: 145409200
I0930 19:59:22.866458  4033 layer_factory.hpp:77] Creating layer BatchNorm5
I0930 19:59:22.866464  4033 net.cpp:84] Creating Layer BatchNorm5
I0930 19:59:22.866467  4033 net.cpp:406] BatchNorm5 <- Convolution5
I0930 19:59:22.866470  4033 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0930 19:59:22.866621  4033 net.cpp:122] Setting up BatchNorm5
I0930 19:59:22.866627  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.866631  4033 net.cpp:137] Memory required for data: 151962800
I0930 19:59:22.866636  4033 layer_factory.hpp:77] Creating layer Scale5
I0930 19:59:22.866641  4033 net.cpp:84] Creating Layer Scale5
I0930 19:59:22.866642  4033 net.cpp:406] Scale5 <- Convolution5
I0930 19:59:22.866647  4033 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0930 19:59:22.866674  4033 layer_factory.hpp:77] Creating layer Scale5
I0930 19:59:22.866751  4033 net.cpp:122] Setting up Scale5
I0930 19:59:22.866756  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.866758  4033 net.cpp:137] Memory required for data: 158516400
I0930 19:59:22.866762  4033 layer_factory.hpp:77] Creating layer Eltwise2
I0930 19:59:22.866766  4033 net.cpp:84] Creating Layer Eltwise2
I0930 19:59:22.866768  4033 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I0930 19:59:22.866771  4033 net.cpp:406] Eltwise2 <- Convolution5
I0930 19:59:22.866776  4033 net.cpp:380] Eltwise2 -> Eltwise2
I0930 19:59:22.866801  4033 net.cpp:122] Setting up Eltwise2
I0930 19:59:22.866806  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.866807  4033 net.cpp:137] Memory required for data: 165070000
I0930 19:59:22.866811  4033 layer_factory.hpp:77] Creating layer penlu5
I0930 19:59:22.866816  4033 net.cpp:84] Creating Layer penlu5
I0930 19:59:22.866817  4033 net.cpp:406] penlu5 <- Eltwise2
I0930 19:59:22.866822  4033 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I0930 19:59:22.866937  4033 net.cpp:122] Setting up penlu5
I0930 19:59:22.866942  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.866945  4033 net.cpp:137] Memory required for data: 171623600
I0930 19:59:22.866950  4033 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I0930 19:59:22.866952  4033 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I0930 19:59:22.866955  4033 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I0930 19:59:22.866958  4033 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I0930 19:59:22.866961  4033 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I0930 19:59:22.866987  4033 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I0930 19:59:22.866991  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.866994  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.866997  4033 net.cpp:137] Memory required for data: 184730800
I0930 19:59:22.866999  4033 layer_factory.hpp:77] Creating layer Convolution6
I0930 19:59:22.867005  4033 net.cpp:84] Creating Layer Convolution6
I0930 19:59:22.867008  4033 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I0930 19:59:22.867012  4033 net.cpp:380] Convolution6 -> Convolution6
I0930 19:59:22.867940  4033 net.cpp:122] Setting up Convolution6
I0930 19:59:22.867949  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.867952  4033 net.cpp:137] Memory required for data: 191284400
I0930 19:59:22.867956  4033 layer_factory.hpp:77] Creating layer BatchNorm6
I0930 19:59:22.867961  4033 net.cpp:84] Creating Layer BatchNorm6
I0930 19:59:22.867964  4033 net.cpp:406] BatchNorm6 <- Convolution6
I0930 19:59:22.867969  4033 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0930 19:59:22.868108  4033 net.cpp:122] Setting up BatchNorm6
I0930 19:59:22.868113  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.868114  4033 net.cpp:137] Memory required for data: 197838000
I0930 19:59:22.868119  4033 layer_factory.hpp:77] Creating layer Scale6
I0930 19:59:22.868124  4033 net.cpp:84] Creating Layer Scale6
I0930 19:59:22.868125  4033 net.cpp:406] Scale6 <- Convolution6
I0930 19:59:22.868129  4033 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0930 19:59:22.868156  4033 layer_factory.hpp:77] Creating layer Scale6
I0930 19:59:22.868232  4033 net.cpp:122] Setting up Scale6
I0930 19:59:22.868237  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.868238  4033 net.cpp:137] Memory required for data: 204391600
I0930 19:59:22.868242  4033 layer_factory.hpp:77] Creating layer penlu6
I0930 19:59:22.868247  4033 net.cpp:84] Creating Layer penlu6
I0930 19:59:22.868250  4033 net.cpp:406] penlu6 <- Convolution6
I0930 19:59:22.868253  4033 net.cpp:367] penlu6 -> Convolution6 (in-place)
I0930 19:59:22.868371  4033 net.cpp:122] Setting up penlu6
I0930 19:59:22.868376  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.868377  4033 net.cpp:137] Memory required for data: 210945200
I0930 19:59:22.868381  4033 layer_factory.hpp:77] Creating layer Convolution7
I0930 19:59:22.868388  4033 net.cpp:84] Creating Layer Convolution7
I0930 19:59:22.868391  4033 net.cpp:406] Convolution7 <- Convolution6
I0930 19:59:22.868394  4033 net.cpp:380] Convolution7 -> Convolution7
I0930 19:59:22.869338  4033 net.cpp:122] Setting up Convolution7
I0930 19:59:22.869345  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.869349  4033 net.cpp:137] Memory required for data: 217498800
I0930 19:59:22.869354  4033 layer_factory.hpp:77] Creating layer BatchNorm7
I0930 19:59:22.869360  4033 net.cpp:84] Creating Layer BatchNorm7
I0930 19:59:22.869369  4033 net.cpp:406] BatchNorm7 <- Convolution7
I0930 19:59:22.869374  4033 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0930 19:59:22.869514  4033 net.cpp:122] Setting up BatchNorm7
I0930 19:59:22.869519  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.869521  4033 net.cpp:137] Memory required for data: 224052400
I0930 19:59:22.869531  4033 layer_factory.hpp:77] Creating layer Scale7
I0930 19:59:22.869536  4033 net.cpp:84] Creating Layer Scale7
I0930 19:59:22.869539  4033 net.cpp:406] Scale7 <- Convolution7
I0930 19:59:22.869541  4033 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0930 19:59:22.869571  4033 layer_factory.hpp:77] Creating layer Scale7
I0930 19:59:22.869649  4033 net.cpp:122] Setting up Scale7
I0930 19:59:22.869654  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.869657  4033 net.cpp:137] Memory required for data: 230606000
I0930 19:59:22.869660  4033 layer_factory.hpp:77] Creating layer Eltwise3
I0930 19:59:22.869664  4033 net.cpp:84] Creating Layer Eltwise3
I0930 19:59:22.869666  4033 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I0930 19:59:22.869669  4033 net.cpp:406] Eltwise3 <- Convolution7
I0930 19:59:22.869673  4033 net.cpp:380] Eltwise3 -> Eltwise3
I0930 19:59:22.869689  4033 net.cpp:122] Setting up Eltwise3
I0930 19:59:22.869693  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.869695  4033 net.cpp:137] Memory required for data: 237159600
I0930 19:59:22.869698  4033 layer_factory.hpp:77] Creating layer penlu7
I0930 19:59:22.869702  4033 net.cpp:84] Creating Layer penlu7
I0930 19:59:22.869704  4033 net.cpp:406] penlu7 <- Eltwise3
I0930 19:59:22.869709  4033 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I0930 19:59:22.869827  4033 net.cpp:122] Setting up penlu7
I0930 19:59:22.869832  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.869833  4033 net.cpp:137] Memory required for data: 243713200
I0930 19:59:22.869838  4033 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I0930 19:59:22.869841  4033 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I0930 19:59:22.869844  4033 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I0930 19:59:22.869848  4033 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I0930 19:59:22.869851  4033 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I0930 19:59:22.869874  4033 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I0930 19:59:22.869879  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.869881  4033 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0930 19:59:22.869884  4033 net.cpp:137] Memory required for data: 256820400
I0930 19:59:22.869885  4033 layer_factory.hpp:77] Creating layer Convolution8
I0930 19:59:22.869890  4033 net.cpp:84] Creating Layer Convolution8
I0930 19:59:22.869894  4033 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I0930 19:59:22.869899  4033 net.cpp:380] Convolution8 -> Convolution8
I0930 19:59:22.870800  4033 net.cpp:122] Setting up Convolution8
I0930 19:59:22.870808  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.870811  4033 net.cpp:137] Memory required for data: 260097200
I0930 19:59:22.870815  4033 layer_factory.hpp:77] Creating layer BatchNorm8
I0930 19:59:22.870820  4033 net.cpp:84] Creating Layer BatchNorm8
I0930 19:59:22.870823  4033 net.cpp:406] BatchNorm8 <- Convolution8
I0930 19:59:22.870827  4033 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0930 19:59:22.870970  4033 net.cpp:122] Setting up BatchNorm8
I0930 19:59:22.870975  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.870977  4033 net.cpp:137] Memory required for data: 263374000
I0930 19:59:22.870981  4033 layer_factory.hpp:77] Creating layer Scale8
I0930 19:59:22.870985  4033 net.cpp:84] Creating Layer Scale8
I0930 19:59:22.874030  4033 net.cpp:406] Scale8 <- Convolution8
I0930 19:59:22.874034  4033 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0930 19:59:22.874066  4033 layer_factory.hpp:77] Creating layer Scale8
I0930 19:59:22.874161  4033 net.cpp:122] Setting up Scale8
I0930 19:59:22.874166  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.874168  4033 net.cpp:137] Memory required for data: 266650800
I0930 19:59:22.874172  4033 layer_factory.hpp:77] Creating layer Convolution9
I0930 19:59:22.874179  4033 net.cpp:84] Creating Layer Convolution9
I0930 19:59:22.874182  4033 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I0930 19:59:22.874188  4033 net.cpp:380] Convolution9 -> Convolution9
I0930 19:59:22.875248  4033 net.cpp:122] Setting up Convolution9
I0930 19:59:22.875258  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.875262  4033 net.cpp:137] Memory required for data: 269927600
I0930 19:59:22.875267  4033 layer_factory.hpp:77] Creating layer BatchNorm9
I0930 19:59:22.875272  4033 net.cpp:84] Creating Layer BatchNorm9
I0930 19:59:22.875277  4033 net.cpp:406] BatchNorm9 <- Convolution9
I0930 19:59:22.875279  4033 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0930 19:59:22.875432  4033 net.cpp:122] Setting up BatchNorm9
I0930 19:59:22.875437  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.875438  4033 net.cpp:137] Memory required for data: 273204400
I0930 19:59:22.875443  4033 layer_factory.hpp:77] Creating layer Scale9
I0930 19:59:22.875448  4033 net.cpp:84] Creating Layer Scale9
I0930 19:59:22.875452  4033 net.cpp:406] Scale9 <- Convolution9
I0930 19:59:22.875454  4033 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0930 19:59:22.875483  4033 layer_factory.hpp:77] Creating layer Scale9
I0930 19:59:22.875562  4033 net.cpp:122] Setting up Scale9
I0930 19:59:22.875567  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.875569  4033 net.cpp:137] Memory required for data: 276481200
I0930 19:59:22.875573  4033 layer_factory.hpp:77] Creating layer penlu8
I0930 19:59:22.875579  4033 net.cpp:84] Creating Layer penlu8
I0930 19:59:22.875582  4033 net.cpp:406] penlu8 <- Convolution9
I0930 19:59:22.875586  4033 net.cpp:367] penlu8 -> Convolution9 (in-place)
I0930 19:59:22.875700  4033 net.cpp:122] Setting up penlu8
I0930 19:59:22.875705  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.875707  4033 net.cpp:137] Memory required for data: 279758000
I0930 19:59:22.875711  4033 layer_factory.hpp:77] Creating layer Convolution10
I0930 19:59:22.875718  4033 net.cpp:84] Creating Layer Convolution10
I0930 19:59:22.875721  4033 net.cpp:406] Convolution10 <- Convolution9
I0930 19:59:22.875725  4033 net.cpp:380] Convolution10 -> Convolution10
I0930 19:59:22.877188  4033 net.cpp:122] Setting up Convolution10
I0930 19:59:22.877197  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.877199  4033 net.cpp:137] Memory required for data: 283034800
I0930 19:59:22.877204  4033 layer_factory.hpp:77] Creating layer BatchNorm10
I0930 19:59:22.877209  4033 net.cpp:84] Creating Layer BatchNorm10
I0930 19:59:22.877212  4033 net.cpp:406] BatchNorm10 <- Convolution10
I0930 19:59:22.877216  4033 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0930 19:59:22.877352  4033 net.cpp:122] Setting up BatchNorm10
I0930 19:59:22.877357  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.877360  4033 net.cpp:137] Memory required for data: 286311600
I0930 19:59:22.877364  4033 layer_factory.hpp:77] Creating layer Scale10
I0930 19:59:22.877368  4033 net.cpp:84] Creating Layer Scale10
I0930 19:59:22.877372  4033 net.cpp:406] Scale10 <- Convolution10
I0930 19:59:22.877374  4033 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0930 19:59:22.877403  4033 layer_factory.hpp:77] Creating layer Scale10
I0930 19:59:22.877478  4033 net.cpp:122] Setting up Scale10
I0930 19:59:22.877483  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.877485  4033 net.cpp:137] Memory required for data: 289588400
I0930 19:59:22.877490  4033 layer_factory.hpp:77] Creating layer Eltwise4
I0930 19:59:22.877493  4033 net.cpp:84] Creating Layer Eltwise4
I0930 19:59:22.877497  4033 net.cpp:406] Eltwise4 <- Convolution8
I0930 19:59:22.877499  4033 net.cpp:406] Eltwise4 <- Convolution10
I0930 19:59:22.877511  4033 net.cpp:380] Eltwise4 -> Eltwise4
I0930 19:59:22.877526  4033 net.cpp:122] Setting up Eltwise4
I0930 19:59:22.877529  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.877532  4033 net.cpp:137] Memory required for data: 292865200
I0930 19:59:22.877533  4033 layer_factory.hpp:77] Creating layer penlu9
I0930 19:59:22.877539  4033 net.cpp:84] Creating Layer penlu9
I0930 19:59:22.877542  4033 net.cpp:406] penlu9 <- Eltwise4
I0930 19:59:22.877545  4033 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I0930 19:59:22.877684  4033 net.cpp:122] Setting up penlu9
I0930 19:59:22.877688  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.877691  4033 net.cpp:137] Memory required for data: 296142000
I0930 19:59:22.877696  4033 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I0930 19:59:22.877709  4033 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I0930 19:59:22.877712  4033 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I0930 19:59:22.877714  4033 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I0930 19:59:22.877719  4033 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I0930 19:59:22.877743  4033 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I0930 19:59:22.877748  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.877750  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.877753  4033 net.cpp:137] Memory required for data: 302695600
I0930 19:59:22.877754  4033 layer_factory.hpp:77] Creating layer Convolution11
I0930 19:59:22.877761  4033 net.cpp:84] Creating Layer Convolution11
I0930 19:59:22.877774  4033 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I0930 19:59:22.877776  4033 net.cpp:380] Convolution11 -> Convolution11
I0930 19:59:22.878908  4033 net.cpp:122] Setting up Convolution11
I0930 19:59:22.878917  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.878921  4033 net.cpp:137] Memory required for data: 305972400
I0930 19:59:22.878924  4033 layer_factory.hpp:77] Creating layer BatchNorm11
I0930 19:59:22.878929  4033 net.cpp:84] Creating Layer BatchNorm11
I0930 19:59:22.878932  4033 net.cpp:406] BatchNorm11 <- Convolution11
I0930 19:59:22.878937  4033 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0930 19:59:22.879070  4033 net.cpp:122] Setting up BatchNorm11
I0930 19:59:22.879075  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.879076  4033 net.cpp:137] Memory required for data: 309249200
I0930 19:59:22.879081  4033 layer_factory.hpp:77] Creating layer Scale11
I0930 19:59:22.879086  4033 net.cpp:84] Creating Layer Scale11
I0930 19:59:22.879087  4033 net.cpp:406] Scale11 <- Convolution11
I0930 19:59:22.879091  4033 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0930 19:59:22.879118  4033 layer_factory.hpp:77] Creating layer Scale11
I0930 19:59:22.879194  4033 net.cpp:122] Setting up Scale11
I0930 19:59:22.879199  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.879200  4033 net.cpp:137] Memory required for data: 312526000
I0930 19:59:22.879204  4033 layer_factory.hpp:77] Creating layer penlu10
I0930 19:59:22.879209  4033 net.cpp:84] Creating Layer penlu10
I0930 19:59:22.879212  4033 net.cpp:406] penlu10 <- Convolution11
I0930 19:59:22.879215  4033 net.cpp:367] penlu10 -> Convolution11 (in-place)
I0930 19:59:22.879321  4033 net.cpp:122] Setting up penlu10
I0930 19:59:22.879326  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.879328  4033 net.cpp:137] Memory required for data: 315802800
I0930 19:59:22.879333  4033 layer_factory.hpp:77] Creating layer Convolution12
I0930 19:59:22.879338  4033 net.cpp:84] Creating Layer Convolution12
I0930 19:59:22.879341  4033 net.cpp:406] Convolution12 <- Convolution11
I0930 19:59:22.879346  4033 net.cpp:380] Convolution12 -> Convolution12
I0930 19:59:22.880069  4033 net.cpp:122] Setting up Convolution12
I0930 19:59:22.880076  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.880079  4033 net.cpp:137] Memory required for data: 319079600
I0930 19:59:22.880090  4033 layer_factory.hpp:77] Creating layer BatchNorm12
I0930 19:59:22.880096  4033 net.cpp:84] Creating Layer BatchNorm12
I0930 19:59:22.880100  4033 net.cpp:406] BatchNorm12 <- Convolution12
I0930 19:59:22.880102  4033 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0930 19:59:22.880239  4033 net.cpp:122] Setting up BatchNorm12
I0930 19:59:22.880244  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.880246  4033 net.cpp:137] Memory required for data: 322356400
I0930 19:59:22.880250  4033 layer_factory.hpp:77] Creating layer Scale12
I0930 19:59:22.880254  4033 net.cpp:84] Creating Layer Scale12
I0930 19:59:22.880257  4033 net.cpp:406] Scale12 <- Convolution12
I0930 19:59:22.880260  4033 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0930 19:59:22.880287  4033 layer_factory.hpp:77] Creating layer Scale12
I0930 19:59:22.880363  4033 net.cpp:122] Setting up Scale12
I0930 19:59:22.880367  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.880369  4033 net.cpp:137] Memory required for data: 325633200
I0930 19:59:22.880373  4033 layer_factory.hpp:77] Creating layer Eltwise5
I0930 19:59:22.880378  4033 net.cpp:84] Creating Layer Eltwise5
I0930 19:59:22.880380  4033 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I0930 19:59:22.880383  4033 net.cpp:406] Eltwise5 <- Convolution12
I0930 19:59:22.880386  4033 net.cpp:380] Eltwise5 -> Eltwise5
I0930 19:59:22.880399  4033 net.cpp:122] Setting up Eltwise5
I0930 19:59:22.880403  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.880404  4033 net.cpp:137] Memory required for data: 328910000
I0930 19:59:22.880406  4033 layer_factory.hpp:77] Creating layer penlu11
I0930 19:59:22.880412  4033 net.cpp:84] Creating Layer penlu11
I0930 19:59:22.880415  4033 net.cpp:406] penlu11 <- Eltwise5
I0930 19:59:22.880419  4033 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I0930 19:59:22.880530  4033 net.cpp:122] Setting up penlu11
I0930 19:59:22.880534  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.880537  4033 net.cpp:137] Memory required for data: 332186800
I0930 19:59:22.880540  4033 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I0930 19:59:22.880544  4033 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I0930 19:59:22.880547  4033 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I0930 19:59:22.880550  4033 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I0930 19:59:22.880554  4033 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I0930 19:59:22.880580  4033 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I0930 19:59:22.880584  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.880587  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.880589  4033 net.cpp:137] Memory required for data: 338740400
I0930 19:59:22.880591  4033 layer_factory.hpp:77] Creating layer Convolution13
I0930 19:59:22.880597  4033 net.cpp:84] Creating Layer Convolution13
I0930 19:59:22.880599  4033 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I0930 19:59:22.880604  4033 net.cpp:380] Convolution13 -> Convolution13
I0930 19:59:22.881651  4033 net.cpp:122] Setting up Convolution13
I0930 19:59:22.881659  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.881662  4033 net.cpp:137] Memory required for data: 342017200
I0930 19:59:22.881666  4033 layer_factory.hpp:77] Creating layer BatchNorm13
I0930 19:59:22.881671  4033 net.cpp:84] Creating Layer BatchNorm13
I0930 19:59:22.881675  4033 net.cpp:406] BatchNorm13 <- Convolution13
I0930 19:59:22.881678  4033 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0930 19:59:22.881811  4033 net.cpp:122] Setting up BatchNorm13
I0930 19:59:22.881816  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.881819  4033 net.cpp:137] Memory required for data: 345294000
I0930 19:59:22.881824  4033 layer_factory.hpp:77] Creating layer Scale13
I0930 19:59:22.881829  4033 net.cpp:84] Creating Layer Scale13
I0930 19:59:22.881830  4033 net.cpp:406] Scale13 <- Convolution13
I0930 19:59:22.881839  4033 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0930 19:59:22.881868  4033 layer_factory.hpp:77] Creating layer Scale13
I0930 19:59:22.881947  4033 net.cpp:122] Setting up Scale13
I0930 19:59:22.881950  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.881953  4033 net.cpp:137] Memory required for data: 348570800
I0930 19:59:22.881956  4033 layer_factory.hpp:77] Creating layer penlu12
I0930 19:59:22.881963  4033 net.cpp:84] Creating Layer penlu12
I0930 19:59:22.881964  4033 net.cpp:406] penlu12 <- Convolution13
I0930 19:59:22.881968  4033 net.cpp:367] penlu12 -> Convolution13 (in-place)
I0930 19:59:22.882076  4033 net.cpp:122] Setting up penlu12
I0930 19:59:22.882079  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.882082  4033 net.cpp:137] Memory required for data: 351847600
I0930 19:59:22.882086  4033 layer_factory.hpp:77] Creating layer Convolution14
I0930 19:59:22.882097  4033 net.cpp:84] Creating Layer Convolution14
I0930 19:59:22.882099  4033 net.cpp:406] Convolution14 <- Convolution13
I0930 19:59:22.882103  4033 net.cpp:380] Convolution14 -> Convolution14
I0930 19:59:22.883234  4033 net.cpp:122] Setting up Convolution14
I0930 19:59:22.883241  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.883244  4033 net.cpp:137] Memory required for data: 355124400
I0930 19:59:22.883260  4033 layer_factory.hpp:77] Creating layer BatchNorm14
I0930 19:59:22.883265  4033 net.cpp:84] Creating Layer BatchNorm14
I0930 19:59:22.883268  4033 net.cpp:406] BatchNorm14 <- Convolution14
I0930 19:59:22.883271  4033 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0930 19:59:22.883405  4033 net.cpp:122] Setting up BatchNorm14
I0930 19:59:22.883410  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.883412  4033 net.cpp:137] Memory required for data: 358401200
I0930 19:59:22.883419  4033 layer_factory.hpp:77] Creating layer Scale14
I0930 19:59:22.883422  4033 net.cpp:84] Creating Layer Scale14
I0930 19:59:22.883424  4033 net.cpp:406] Scale14 <- Convolution14
I0930 19:59:22.883427  4033 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0930 19:59:22.883455  4033 layer_factory.hpp:77] Creating layer Scale14
I0930 19:59:22.883533  4033 net.cpp:122] Setting up Scale14
I0930 19:59:22.883536  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.883538  4033 net.cpp:137] Memory required for data: 361678000
I0930 19:59:22.883543  4033 layer_factory.hpp:77] Creating layer Eltwise6
I0930 19:59:22.883546  4033 net.cpp:84] Creating Layer Eltwise6
I0930 19:59:22.883549  4033 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I0930 19:59:22.883551  4033 net.cpp:406] Eltwise6 <- Convolution14
I0930 19:59:22.883554  4033 net.cpp:380] Eltwise6 -> Eltwise6
I0930 19:59:22.883568  4033 net.cpp:122] Setting up Eltwise6
I0930 19:59:22.883570  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.883572  4033 net.cpp:137] Memory required for data: 364954800
I0930 19:59:22.883574  4033 layer_factory.hpp:77] Creating layer penlu13
I0930 19:59:22.883581  4033 net.cpp:84] Creating Layer penlu13
I0930 19:59:22.883584  4033 net.cpp:406] penlu13 <- Eltwise6
I0930 19:59:22.883587  4033 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I0930 19:59:22.883702  4033 net.cpp:122] Setting up penlu13
I0930 19:59:22.883705  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.883708  4033 net.cpp:137] Memory required for data: 368231600
I0930 19:59:22.883713  4033 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I0930 19:59:22.883715  4033 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I0930 19:59:22.883718  4033 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I0930 19:59:22.883721  4033 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I0930 19:59:22.904804  4033 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I0930 19:59:22.904860  4033 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I0930 19:59:22.904870  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.904875  4033 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0930 19:59:22.904891  4033 net.cpp:137] Memory required for data: 374785200
I0930 19:59:22.904896  4033 layer_factory.hpp:77] Creating layer Convolution15
I0930 19:59:22.904907  4033 net.cpp:84] Creating Layer Convolution15
I0930 19:59:22.904912  4033 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I0930 19:59:22.904920  4033 net.cpp:380] Convolution15 -> Convolution15
I0930 19:59:22.906029  4033 net.cpp:122] Setting up Convolution15
I0930 19:59:22.906038  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.906041  4033 net.cpp:137] Memory required for data: 376423600
I0930 19:59:22.906046  4033 layer_factory.hpp:77] Creating layer BatchNorm15
I0930 19:59:22.906052  4033 net.cpp:84] Creating Layer BatchNorm15
I0930 19:59:22.906054  4033 net.cpp:406] BatchNorm15 <- Convolution15
I0930 19:59:22.906059  4033 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0930 19:59:22.906214  4033 net.cpp:122] Setting up BatchNorm15
I0930 19:59:22.906220  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.906222  4033 net.cpp:137] Memory required for data: 378062000
I0930 19:59:22.906227  4033 layer_factory.hpp:77] Creating layer Scale15
I0930 19:59:22.906231  4033 net.cpp:84] Creating Layer Scale15
I0930 19:59:22.906234  4033 net.cpp:406] Scale15 <- Convolution15
I0930 19:59:22.906237  4033 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0930 19:59:22.906270  4033 layer_factory.hpp:77] Creating layer Scale15
I0930 19:59:22.906370  4033 net.cpp:122] Setting up Scale15
I0930 19:59:22.906375  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.906378  4033 net.cpp:137] Memory required for data: 379700400
I0930 19:59:22.906381  4033 layer_factory.hpp:77] Creating layer Convolution16
I0930 19:59:22.906388  4033 net.cpp:84] Creating Layer Convolution16
I0930 19:59:22.906391  4033 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I0930 19:59:22.906396  4033 net.cpp:380] Convolution16 -> Convolution16
I0930 19:59:22.907832  4033 net.cpp:122] Setting up Convolution16
I0930 19:59:22.907841  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.907845  4033 net.cpp:137] Memory required for data: 381338800
I0930 19:59:22.907850  4033 layer_factory.hpp:77] Creating layer BatchNorm16
I0930 19:59:22.907853  4033 net.cpp:84] Creating Layer BatchNorm16
I0930 19:59:22.907856  4033 net.cpp:406] BatchNorm16 <- Convolution16
I0930 19:59:22.907861  4033 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0930 19:59:22.907997  4033 net.cpp:122] Setting up BatchNorm16
I0930 19:59:22.908001  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.908004  4033 net.cpp:137] Memory required for data: 382977200
I0930 19:59:22.908008  4033 layer_factory.hpp:77] Creating layer Scale16
I0930 19:59:22.908012  4033 net.cpp:84] Creating Layer Scale16
I0930 19:59:22.908015  4033 net.cpp:406] Scale16 <- Convolution16
I0930 19:59:22.908018  4033 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0930 19:59:22.908046  4033 layer_factory.hpp:77] Creating layer Scale16
I0930 19:59:22.908123  4033 net.cpp:122] Setting up Scale16
I0930 19:59:22.908128  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.908129  4033 net.cpp:137] Memory required for data: 384615600
I0930 19:59:22.908133  4033 layer_factory.hpp:77] Creating layer penlu14
I0930 19:59:22.908138  4033 net.cpp:84] Creating Layer penlu14
I0930 19:59:22.908141  4033 net.cpp:406] penlu14 <- Convolution16
I0930 19:59:22.908144  4033 net.cpp:367] penlu14 -> Convolution16 (in-place)
I0930 19:59:22.908253  4033 net.cpp:122] Setting up penlu14
I0930 19:59:22.908259  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.908262  4033 net.cpp:137] Memory required for data: 386254000
I0930 19:59:22.908267  4033 layer_factory.hpp:77] Creating layer Convolution17
I0930 19:59:22.908272  4033 net.cpp:84] Creating Layer Convolution17
I0930 19:59:22.908274  4033 net.cpp:406] Convolution17 <- Convolution16
I0930 19:59:22.908278  4033 net.cpp:380] Convolution17 -> Convolution17
I0930 19:59:22.910187  4033 net.cpp:122] Setting up Convolution17
I0930 19:59:22.910202  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.910205  4033 net.cpp:137] Memory required for data: 387892400
I0930 19:59:22.910209  4033 layer_factory.hpp:77] Creating layer BatchNorm17
I0930 19:59:22.910215  4033 net.cpp:84] Creating Layer BatchNorm17
I0930 19:59:22.910218  4033 net.cpp:406] BatchNorm17 <- Convolution17
I0930 19:59:22.910221  4033 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0930 19:59:22.910362  4033 net.cpp:122] Setting up BatchNorm17
I0930 19:59:22.910367  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.910369  4033 net.cpp:137] Memory required for data: 389530800
I0930 19:59:22.910374  4033 layer_factory.hpp:77] Creating layer Scale17
I0930 19:59:22.910378  4033 net.cpp:84] Creating Layer Scale17
I0930 19:59:22.910380  4033 net.cpp:406] Scale17 <- Convolution17
I0930 19:59:22.910384  4033 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0930 19:59:22.910413  4033 layer_factory.hpp:77] Creating layer Scale17
I0930 19:59:22.910492  4033 net.cpp:122] Setting up Scale17
I0930 19:59:22.910497  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.910500  4033 net.cpp:137] Memory required for data: 391169200
I0930 19:59:22.910503  4033 layer_factory.hpp:77] Creating layer Eltwise7
I0930 19:59:22.910507  4033 net.cpp:84] Creating Layer Eltwise7
I0930 19:59:22.910509  4033 net.cpp:406] Eltwise7 <- Convolution15
I0930 19:59:22.910512  4033 net.cpp:406] Eltwise7 <- Convolution17
I0930 19:59:22.910516  4033 net.cpp:380] Eltwise7 -> Eltwise7
I0930 19:59:22.910549  4033 net.cpp:122] Setting up Eltwise7
I0930 19:59:22.910553  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.910557  4033 net.cpp:137] Memory required for data: 392807600
I0930 19:59:22.910567  4033 layer_factory.hpp:77] Creating layer penlu15
I0930 19:59:22.910573  4033 net.cpp:84] Creating Layer penlu15
I0930 19:59:22.910575  4033 net.cpp:406] penlu15 <- Eltwise7
I0930 19:59:22.910578  4033 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I0930 19:59:22.910691  4033 net.cpp:122] Setting up penlu15
I0930 19:59:22.910694  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.910696  4033 net.cpp:137] Memory required for data: 394446000
I0930 19:59:22.910701  4033 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I0930 19:59:22.910706  4033 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I0930 19:59:22.910707  4033 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I0930 19:59:22.910712  4033 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I0930 19:59:22.910715  4033 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I0930 19:59:22.910738  4033 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I0930 19:59:22.910742  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.910745  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.910748  4033 net.cpp:137] Memory required for data: 397722800
I0930 19:59:22.910749  4033 layer_factory.hpp:77] Creating layer Convolution18
I0930 19:59:22.910754  4033 net.cpp:84] Creating Layer Convolution18
I0930 19:59:22.910758  4033 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I0930 19:59:22.910761  4033 net.cpp:380] Convolution18 -> Convolution18
I0930 19:59:22.912891  4033 net.cpp:122] Setting up Convolution18
I0930 19:59:22.912900  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.912904  4033 net.cpp:137] Memory required for data: 399361200
I0930 19:59:22.912907  4033 layer_factory.hpp:77] Creating layer BatchNorm18
I0930 19:59:22.912914  4033 net.cpp:84] Creating Layer BatchNorm18
I0930 19:59:22.912916  4033 net.cpp:406] BatchNorm18 <- Convolution18
I0930 19:59:22.912920  4033 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0930 19:59:22.913058  4033 net.cpp:122] Setting up BatchNorm18
I0930 19:59:22.913064  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.913065  4033 net.cpp:137] Memory required for data: 400999600
I0930 19:59:22.913070  4033 layer_factory.hpp:77] Creating layer Scale18
I0930 19:59:22.913081  4033 net.cpp:84] Creating Layer Scale18
I0930 19:59:22.913084  4033 net.cpp:406] Scale18 <- Convolution18
I0930 19:59:22.913087  4033 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0930 19:59:22.913118  4033 layer_factory.hpp:77] Creating layer Scale18
I0930 19:59:22.913198  4033 net.cpp:122] Setting up Scale18
I0930 19:59:22.913203  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.913205  4033 net.cpp:137] Memory required for data: 402638000
I0930 19:59:22.913209  4033 layer_factory.hpp:77] Creating layer penlu16
I0930 19:59:22.913213  4033 net.cpp:84] Creating Layer penlu16
I0930 19:59:22.913216  4033 net.cpp:406] penlu16 <- Convolution18
I0930 19:59:22.913220  4033 net.cpp:367] penlu16 -> Convolution18 (in-place)
I0930 19:59:22.913332  4033 net.cpp:122] Setting up penlu16
I0930 19:59:22.913336  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.913339  4033 net.cpp:137] Memory required for data: 404276400
I0930 19:59:22.913343  4033 layer_factory.hpp:77] Creating layer Convolution19
I0930 19:59:22.913350  4033 net.cpp:84] Creating Layer Convolution19
I0930 19:59:22.913352  4033 net.cpp:406] Convolution19 <- Convolution18
I0930 19:59:22.913357  4033 net.cpp:380] Convolution19 -> Convolution19
I0930 19:59:22.915029  4033 net.cpp:122] Setting up Convolution19
I0930 19:59:22.915037  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.915040  4033 net.cpp:137] Memory required for data: 405914800
I0930 19:59:22.915045  4033 layer_factory.hpp:77] Creating layer BatchNorm19
I0930 19:59:22.915050  4033 net.cpp:84] Creating Layer BatchNorm19
I0930 19:59:22.915052  4033 net.cpp:406] BatchNorm19 <- Convolution19
I0930 19:59:22.915056  4033 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0930 19:59:22.915196  4033 net.cpp:122] Setting up BatchNorm19
I0930 19:59:22.915201  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.915204  4033 net.cpp:137] Memory required for data: 407553200
I0930 19:59:22.915208  4033 layer_factory.hpp:77] Creating layer Scale19
I0930 19:59:22.915212  4033 net.cpp:84] Creating Layer Scale19
I0930 19:59:22.915215  4033 net.cpp:406] Scale19 <- Convolution19
I0930 19:59:22.915217  4033 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0930 19:59:22.915246  4033 layer_factory.hpp:77] Creating layer Scale19
I0930 19:59:22.915325  4033 net.cpp:122] Setting up Scale19
I0930 19:59:22.915330  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.915333  4033 net.cpp:137] Memory required for data: 409191600
I0930 19:59:22.915336  4033 layer_factory.hpp:77] Creating layer Eltwise8
I0930 19:59:22.915340  4033 net.cpp:84] Creating Layer Eltwise8
I0930 19:59:22.915343  4033 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I0930 19:59:22.915346  4033 net.cpp:406] Eltwise8 <- Convolution19
I0930 19:59:22.915349  4033 net.cpp:380] Eltwise8 -> Eltwise8
I0930 19:59:22.915366  4033 net.cpp:122] Setting up Eltwise8
I0930 19:59:22.915369  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.915371  4033 net.cpp:137] Memory required for data: 410830000
I0930 19:59:22.915374  4033 layer_factory.hpp:77] Creating layer penlu17
I0930 19:59:22.915380  4033 net.cpp:84] Creating Layer penlu17
I0930 19:59:22.915381  4033 net.cpp:406] penlu17 <- Eltwise8
I0930 19:59:22.915385  4033 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I0930 19:59:22.915496  4033 net.cpp:122] Setting up penlu17
I0930 19:59:22.915500  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.915503  4033 net.cpp:137] Memory required for data: 412468400
I0930 19:59:22.915508  4033 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I0930 19:59:22.915511  4033 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I0930 19:59:22.915513  4033 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I0930 19:59:22.915518  4033 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I0930 19:59:22.915520  4033 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I0930 19:59:22.915544  4033 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I0930 19:59:22.915555  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.915558  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.915560  4033 net.cpp:137] Memory required for data: 415745200
I0930 19:59:22.915562  4033 layer_factory.hpp:77] Creating layer Convolution20
I0930 19:59:22.915567  4033 net.cpp:84] Creating Layer Convolution20
I0930 19:59:22.915570  4033 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I0930 19:59:22.915575  4033 net.cpp:380] Convolution20 -> Convolution20
I0930 19:59:22.917536  4033 net.cpp:122] Setting up Convolution20
I0930 19:59:22.917544  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.917547  4033 net.cpp:137] Memory required for data: 417383600
I0930 19:59:22.917552  4033 layer_factory.hpp:77] Creating layer BatchNorm20
I0930 19:59:22.917557  4033 net.cpp:84] Creating Layer BatchNorm20
I0930 19:59:22.917559  4033 net.cpp:406] BatchNorm20 <- Convolution20
I0930 19:59:22.917563  4033 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0930 19:59:22.917706  4033 net.cpp:122] Setting up BatchNorm20
I0930 19:59:22.917709  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.917711  4033 net.cpp:137] Memory required for data: 419022000
I0930 19:59:22.917716  4033 layer_factory.hpp:77] Creating layer Scale20
I0930 19:59:22.917721  4033 net.cpp:84] Creating Layer Scale20
I0930 19:59:22.917723  4033 net.cpp:406] Scale20 <- Convolution20
I0930 19:59:22.917728  4033 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0930 19:59:22.917757  4033 layer_factory.hpp:77] Creating layer Scale20
I0930 19:59:22.917837  4033 net.cpp:122] Setting up Scale20
I0930 19:59:22.917841  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.917843  4033 net.cpp:137] Memory required for data: 420660400
I0930 19:59:22.917847  4033 layer_factory.hpp:77] Creating layer penlu18
I0930 19:59:22.917852  4033 net.cpp:84] Creating Layer penlu18
I0930 19:59:22.917855  4033 net.cpp:406] penlu18 <- Convolution20
I0930 19:59:22.917858  4033 net.cpp:367] penlu18 -> Convolution20 (in-place)
I0930 19:59:22.917970  4033 net.cpp:122] Setting up penlu18
I0930 19:59:22.917974  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.917976  4033 net.cpp:137] Memory required for data: 422298800
I0930 19:59:22.917980  4033 layer_factory.hpp:77] Creating layer Convolution21
I0930 19:59:22.917987  4033 net.cpp:84] Creating Layer Convolution21
I0930 19:59:22.917989  4033 net.cpp:406] Convolution21 <- Convolution20
I0930 19:59:22.917994  4033 net.cpp:380] Convolution21 -> Convolution21
I0930 19:59:22.920032  4033 net.cpp:122] Setting up Convolution21
I0930 19:59:22.920042  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.920044  4033 net.cpp:137] Memory required for data: 423937200
I0930 19:59:22.920049  4033 layer_factory.hpp:77] Creating layer BatchNorm21
I0930 19:59:22.920054  4033 net.cpp:84] Creating Layer BatchNorm21
I0930 19:59:22.920058  4033 net.cpp:406] BatchNorm21 <- Convolution21
I0930 19:59:22.920061  4033 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0930 19:59:22.920200  4033 net.cpp:122] Setting up BatchNorm21
I0930 19:59:22.920204  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.920207  4033 net.cpp:137] Memory required for data: 425575600
I0930 19:59:22.920212  4033 layer_factory.hpp:77] Creating layer Scale21
I0930 19:59:22.920217  4033 net.cpp:84] Creating Layer Scale21
I0930 19:59:22.920218  4033 net.cpp:406] Scale21 <- Convolution21
I0930 19:59:22.920222  4033 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0930 19:59:22.935585  4033 layer_factory.hpp:77] Creating layer Scale21
I0930 19:59:22.935722  4033 net.cpp:122] Setting up Scale21
I0930 19:59:22.935731  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.935735  4033 net.cpp:137] Memory required for data: 427214000
I0930 19:59:22.935742  4033 layer_factory.hpp:77] Creating layer Eltwise9
I0930 19:59:22.935750  4033 net.cpp:84] Creating Layer Eltwise9
I0930 19:59:22.935755  4033 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I0930 19:59:22.935770  4033 net.cpp:406] Eltwise9 <- Convolution21
I0930 19:59:22.935778  4033 net.cpp:380] Eltwise9 -> Eltwise9
I0930 19:59:22.935803  4033 net.cpp:122] Setting up Eltwise9
I0930 19:59:22.935808  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.935811  4033 net.cpp:137] Memory required for data: 428852400
I0930 19:59:22.935813  4033 layer_factory.hpp:77] Creating layer penlu19
I0930 19:59:22.935819  4033 net.cpp:84] Creating Layer penlu19
I0930 19:59:22.935822  4033 net.cpp:406] penlu19 <- Eltwise9
I0930 19:59:22.935827  4033 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I0930 19:59:22.935956  4033 net.cpp:122] Setting up penlu19
I0930 19:59:22.935961  4033 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0930 19:59:22.935963  4033 net.cpp:137] Memory required for data: 430490800
I0930 19:59:22.935968  4033 layer_factory.hpp:77] Creating layer Pooling1
I0930 19:59:22.935973  4033 net.cpp:84] Creating Layer Pooling1
I0930 19:59:22.935976  4033 net.cpp:406] Pooling1 <- Eltwise9
I0930 19:59:22.935981  4033 net.cpp:380] Pooling1 -> Pooling1
I0930 19:59:22.936133  4033 net.cpp:122] Setting up Pooling1
I0930 19:59:22.936141  4033 net.cpp:129] Top shape: 100 64 1 1 (6400)
I0930 19:59:22.936143  4033 net.cpp:137] Memory required for data: 430516400
I0930 19:59:22.936146  4033 layer_factory.hpp:77] Creating layer InnerProduct1
I0930 19:59:22.936152  4033 net.cpp:84] Creating Layer InnerProduct1
I0930 19:59:22.936154  4033 net.cpp:406] InnerProduct1 <- Pooling1
I0930 19:59:22.936158  4033 net.cpp:380] InnerProduct1 -> InnerProduct1
I0930 19:59:22.936269  4033 net.cpp:122] Setting up InnerProduct1
I0930 19:59:22.936273  4033 net.cpp:129] Top shape: 100 10 (1000)
I0930 19:59:22.936275  4033 net.cpp:137] Memory required for data: 430520400
I0930 19:59:22.936280  4033 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0930 19:59:22.936285  4033 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I0930 19:59:22.936286  4033 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0930 19:59:22.936291  4033 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0930 19:59:22.936296  4033 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0930 19:59:22.936324  4033 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I0930 19:59:22.936328  4033 net.cpp:129] Top shape: 100 10 (1000)
I0930 19:59:22.936332  4033 net.cpp:129] Top shape: 100 10 (1000)
I0930 19:59:22.936334  4033 net.cpp:137] Memory required for data: 430528400
I0930 19:59:22.936336  4033 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0930 19:59:22.936350  4033 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0930 19:59:22.936352  4033 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0930 19:59:22.936355  4033 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0930 19:59:22.936359  4033 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0930 19:59:22.936363  4033 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0930 19:59:22.936552  4033 net.cpp:122] Setting up SoftmaxWithLoss1
I0930 19:59:22.936558  4033 net.cpp:129] Top shape: (1)
I0930 19:59:22.936559  4033 net.cpp:132]     with loss weight 1
I0930 19:59:22.936568  4033 net.cpp:137] Memory required for data: 430528404
I0930 19:59:22.936570  4033 layer_factory.hpp:77] Creating layer Accuracy1
I0930 19:59:22.936575  4033 net.cpp:84] Creating Layer Accuracy1
I0930 19:59:22.936578  4033 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0930 19:59:22.936581  4033 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I0930 19:59:22.936585  4033 net.cpp:380] Accuracy1 -> Accuracy1
I0930 19:59:22.936591  4033 net.cpp:122] Setting up Accuracy1
I0930 19:59:22.936594  4033 net.cpp:129] Top shape: (1)
I0930 19:59:22.936596  4033 net.cpp:137] Memory required for data: 430528408
I0930 19:59:22.936599  4033 net.cpp:200] Accuracy1 does not need backward computation.
I0930 19:59:22.936601  4033 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0930 19:59:22.936610  4033 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0930 19:59:22.936612  4033 net.cpp:198] InnerProduct1 needs backward computation.
I0930 19:59:22.936615  4033 net.cpp:198] Pooling1 needs backward computation.
I0930 19:59:22.936617  4033 net.cpp:198] penlu19 needs backward computation.
I0930 19:59:22.936619  4033 net.cpp:198] Eltwise9 needs backward computation.
I0930 19:59:22.936622  4033 net.cpp:198] Scale21 needs backward computation.
I0930 19:59:22.936625  4033 net.cpp:198] BatchNorm21 needs backward computation.
I0930 19:59:22.936626  4033 net.cpp:198] Convolution21 needs backward computation.
I0930 19:59:22.936628  4033 net.cpp:198] penlu18 needs backward computation.
I0930 19:59:22.936630  4033 net.cpp:198] Scale20 needs backward computation.
I0930 19:59:22.936632  4033 net.cpp:198] BatchNorm20 needs backward computation.
I0930 19:59:22.936635  4033 net.cpp:198] Convolution20 needs backward computation.
I0930 19:59:22.936637  4033 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I0930 19:59:22.936640  4033 net.cpp:198] penlu17 needs backward computation.
I0930 19:59:22.936641  4033 net.cpp:198] Eltwise8 needs backward computation.
I0930 19:59:22.936645  4033 net.cpp:198] Scale19 needs backward computation.
I0930 19:59:22.936646  4033 net.cpp:198] BatchNorm19 needs backward computation.
I0930 19:59:22.936648  4033 net.cpp:198] Convolution19 needs backward computation.
I0930 19:59:22.936651  4033 net.cpp:198] penlu16 needs backward computation.
I0930 19:59:22.936653  4033 net.cpp:198] Scale18 needs backward computation.
I0930 19:59:22.936655  4033 net.cpp:198] BatchNorm18 needs backward computation.
I0930 19:59:22.936657  4033 net.cpp:198] Convolution18 needs backward computation.
I0930 19:59:22.936661  4033 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I0930 19:59:22.936662  4033 net.cpp:198] penlu15 needs backward computation.
I0930 19:59:22.936664  4033 net.cpp:198] Eltwise7 needs backward computation.
I0930 19:59:22.936667  4033 net.cpp:198] Scale17 needs backward computation.
I0930 19:59:22.936669  4033 net.cpp:198] BatchNorm17 needs backward computation.
I0930 19:59:22.936674  4033 net.cpp:198] Convolution17 needs backward computation.
I0930 19:59:22.936677  4033 net.cpp:198] penlu14 needs backward computation.
I0930 19:59:22.936679  4033 net.cpp:198] Scale16 needs backward computation.
I0930 19:59:22.936682  4033 net.cpp:198] BatchNorm16 needs backward computation.
I0930 19:59:22.936684  4033 net.cpp:198] Convolution16 needs backward computation.
I0930 19:59:22.936686  4033 net.cpp:198] Scale15 needs backward computation.
I0930 19:59:22.936689  4033 net.cpp:198] BatchNorm15 needs backward computation.
I0930 19:59:22.936691  4033 net.cpp:198] Convolution15 needs backward computation.
I0930 19:59:22.936694  4033 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I0930 19:59:22.936697  4033 net.cpp:198] penlu13 needs backward computation.
I0930 19:59:22.936699  4033 net.cpp:198] Eltwise6 needs backward computation.
I0930 19:59:22.936702  4033 net.cpp:198] Scale14 needs backward computation.
I0930 19:59:22.936704  4033 net.cpp:198] BatchNorm14 needs backward computation.
I0930 19:59:22.936707  4033 net.cpp:198] Convolution14 needs backward computation.
I0930 19:59:22.936709  4033 net.cpp:198] penlu12 needs backward computation.
I0930 19:59:22.936712  4033 net.cpp:198] Scale13 needs backward computation.
I0930 19:59:22.936713  4033 net.cpp:198] BatchNorm13 needs backward computation.
I0930 19:59:22.937463  4033 net.cpp:198] Convolution13 needs backward computation.
I0930 19:59:22.937471  4033 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I0930 19:59:22.937475  4033 net.cpp:198] penlu11 needs backward computation.
I0930 19:59:22.937479  4033 net.cpp:198] Eltwise5 needs backward computation.
I0930 19:59:22.937484  4033 net.cpp:198] Scale12 needs backward computation.
I0930 19:59:22.937487  4033 net.cpp:198] BatchNorm12 needs backward computation.
I0930 19:59:22.937499  4033 net.cpp:198] Convolution12 needs backward computation.
I0930 19:59:22.937502  4033 net.cpp:198] penlu10 needs backward computation.
I0930 19:59:22.937505  4033 net.cpp:198] Scale11 needs backward computation.
I0930 19:59:22.937506  4033 net.cpp:198] BatchNorm11 needs backward computation.
I0930 19:59:22.937510  4033 net.cpp:198] Convolution11 needs backward computation.
I0930 19:59:22.937513  4033 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I0930 19:59:22.937516  4033 net.cpp:198] penlu9 needs backward computation.
I0930 19:59:22.937518  4033 net.cpp:198] Eltwise4 needs backward computation.
I0930 19:59:22.937521  4033 net.cpp:198] Scale10 needs backward computation.
I0930 19:59:22.937523  4033 net.cpp:198] BatchNorm10 needs backward computation.
I0930 19:59:22.937525  4033 net.cpp:198] Convolution10 needs backward computation.
I0930 19:59:22.937528  4033 net.cpp:198] penlu8 needs backward computation.
I0930 19:59:22.937531  4033 net.cpp:198] Scale9 needs backward computation.
I0930 19:59:22.937533  4033 net.cpp:198] BatchNorm9 needs backward computation.
I0930 19:59:22.937536  4033 net.cpp:198] Convolution9 needs backward computation.
I0930 19:59:22.937538  4033 net.cpp:198] Scale8 needs backward computation.
I0930 19:59:22.937541  4033 net.cpp:198] BatchNorm8 needs backward computation.
I0930 19:59:22.937542  4033 net.cpp:198] Convolution8 needs backward computation.
I0930 19:59:22.937546  4033 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I0930 19:59:22.937547  4033 net.cpp:198] penlu7 needs backward computation.
I0930 19:59:22.937549  4033 net.cpp:198] Eltwise3 needs backward computation.
I0930 19:59:22.937552  4033 net.cpp:198] Scale7 needs backward computation.
I0930 19:59:22.937554  4033 net.cpp:198] BatchNorm7 needs backward computation.
I0930 19:59:22.937557  4033 net.cpp:198] Convolution7 needs backward computation.
I0930 19:59:22.937559  4033 net.cpp:198] penlu6 needs backward computation.
I0930 19:59:22.937561  4033 net.cpp:198] Scale6 needs backward computation.
I0930 19:59:22.937563  4033 net.cpp:198] BatchNorm6 needs backward computation.
I0930 19:59:22.937566  4033 net.cpp:198] Convolution6 needs backward computation.
I0930 19:59:22.937568  4033 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I0930 19:59:22.937571  4033 net.cpp:198] penlu5 needs backward computation.
I0930 19:59:22.937572  4033 net.cpp:198] Eltwise2 needs backward computation.
I0930 19:59:22.937575  4033 net.cpp:198] Scale5 needs backward computation.
I0930 19:59:22.937578  4033 net.cpp:198] BatchNorm5 needs backward computation.
I0930 19:59:22.937580  4033 net.cpp:198] Convolution5 needs backward computation.
I0930 19:59:22.937582  4033 net.cpp:198] penlu4 needs backward computation.
I0930 19:59:22.937585  4033 net.cpp:198] Scale4 needs backward computation.
I0930 19:59:22.937587  4033 net.cpp:198] BatchNorm4 needs backward computation.
I0930 19:59:22.937589  4033 net.cpp:198] Convolution4 needs backward computation.
I0930 19:59:22.937592  4033 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I0930 19:59:22.937594  4033 net.cpp:198] penlu3 needs backward computation.
I0930 19:59:22.937597  4033 net.cpp:198] Eltwise1 needs backward computation.
I0930 19:59:22.937599  4033 net.cpp:198] Scale3 needs backward computation.
I0930 19:59:22.937602  4033 net.cpp:198] BatchNorm3 needs backward computation.
I0930 19:59:22.937603  4033 net.cpp:198] Convolution3 needs backward computation.
I0930 19:59:22.937607  4033 net.cpp:198] penlu2 needs backward computation.
I0930 19:59:22.937608  4033 net.cpp:198] Scale2 needs backward computation.
I0930 19:59:22.937610  4033 net.cpp:198] BatchNorm2 needs backward computation.
I0930 19:59:22.937613  4033 net.cpp:198] Convolution2 needs backward computation.
I0930 19:59:22.937615  4033 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I0930 19:59:22.937618  4033 net.cpp:198] penlu1 needs backward computation.
I0930 19:59:22.937620  4033 net.cpp:198] Scale1 needs backward computation.
I0930 19:59:22.937625  4033 net.cpp:198] BatchNorm1 needs backward computation.
I0930 19:59:22.937628  4033 net.cpp:198] Convolution1 needs backward computation.
I0930 19:59:22.937631  4033 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I0930 19:59:22.937633  4033 net.cpp:200] Data1 does not need backward computation.
I0930 19:59:22.937636  4033 net.cpp:242] This network produces output Accuracy1
I0930 19:59:22.937638  4033 net.cpp:242] This network produces output SoftmaxWithLoss1
I0930 19:59:22.937675  4033 net.cpp:255] Network initialization done.
I0930 19:59:22.937932  4033 solver.cpp:56] Solver scaffolding done.
I0930 19:59:22.943444  4033 caffe.cpp:248] Starting Optimization
I0930 19:59:22.943452  4033 solver.cpp:272] Solving resnet_cifar10
I0930 19:59:22.943454  4033 solver.cpp:273] Learning Rate Policy: multistep
I0930 19:59:22.945348  4033 solver.cpp:330] Iteration 0, Testing net (#0)
I0930 19:59:24.174494  4042 data_layer.cpp:73] Restarting data prefetching from start.
I0930 19:59:24.224231  4033 solver.cpp:397]     Test net output #0: Accuracy1 = 0.1
I0930 19:59:24.224257  4033 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I0930 19:59:24.297500  4033 solver.cpp:218] Iteration 0 (-1.23171e-35 iter/s, 1.35397s/100 iters), loss = 2.3051
I0930 19:59:24.297528  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.3051 (* 1 = 2.3051 loss)
I0930 19:59:24.297538  4033 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I0930 19:59:29.541250  4033 solver.cpp:218] Iteration 100 (19.0705 iter/s, 5.24369s/100 iters), loss = 1.64426
I0930 19:59:29.541290  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.64426 (* 1 = 1.64426 loss)
I0930 19:59:29.541296  4033 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I0930 19:59:34.780124  4033 solver.cpp:218] Iteration 200 (19.0883 iter/s, 5.23881s/100 iters), loss = 1.4706
I0930 19:59:34.780164  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.4706 (* 1 = 1.4706 loss)
I0930 19:59:34.780171  4033 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I0930 19:59:40.016297  4033 solver.cpp:218] Iteration 300 (19.0982 iter/s, 5.2361s/100 iters), loss = 1.33867
I0930 19:59:40.016337  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.33867 (* 1 = 1.33867 loss)
I0930 19:59:40.016343  4033 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I0930 19:59:45.260399  4033 solver.cpp:218] Iteration 400 (19.0693 iter/s, 5.24403s/100 iters), loss = 1.17318
I0930 19:59:45.260429  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.17318 (* 1 = 1.17318 loss)
I0930 19:59:45.260434  4033 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I0930 19:59:50.243543  4041 data_layer.cpp:73] Restarting data prefetching from start.
I0930 19:59:50.453924  4033 solver.cpp:330] Iteration 500, Testing net (#0)
I0930 19:59:51.646605  4042 data_layer.cpp:73] Restarting data prefetching from start.
I0930 19:59:51.696684  4033 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3865
I0930 19:59:51.696710  4033 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.75698 (* 1 = 2.75698 loss)
I0930 19:59:51.748867  4033 solver.cpp:218] Iteration 500 (15.4121 iter/s, 6.48841s/100 iters), loss = 1.14761
I0930 19:59:51.748896  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.14761 (* 1 = 1.14761 loss)
I0930 19:59:51.748903  4033 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I0930 19:59:56.998917  4033 solver.cpp:218] Iteration 600 (19.0476 iter/s, 5.24999s/100 iters), loss = 1.03408
I0930 19:59:56.999083  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03408 (* 1 = 1.03408 loss)
I0930 19:59:56.999090  4033 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I0930 20:00:02.249819  4033 solver.cpp:218] Iteration 700 (19.045 iter/s, 5.25072s/100 iters), loss = 1.19228
I0930 20:00:02.249861  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.19228 (* 1 = 1.19228 loss)
I0930 20:00:02.249867  4033 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I0930 20:00:07.490983  4033 solver.cpp:218] Iteration 800 (19.08 iter/s, 5.24109s/100 iters), loss = 1.00076
I0930 20:00:07.491025  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.00076 (* 1 = 1.00076 loss)
I0930 20:00:07.491037  4033 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I0930 20:00:12.741828  4033 solver.cpp:218] Iteration 900 (19.0449 iter/s, 5.25075s/100 iters), loss = 0.9359
I0930 20:00:12.741863  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.9359 (* 1 = 0.9359 loss)
I0930 20:00:12.741881  4033 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I0930 20:00:17.733433  4041 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:00:17.943014  4033 solver.cpp:330] Iteration 1000, Testing net (#0)
I0930 20:00:19.136438  4042 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:00:19.186792  4033 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3159
I0930 20:00:19.186827  4033 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.59102 (* 1 = 3.59102 loss)
I0930 20:00:19.239712  4033 solver.cpp:218] Iteration 1000 (15.3898 iter/s, 6.49782s/100 iters), loss = 0.891283
I0930 20:00:19.239740  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.891283 (* 1 = 0.891283 loss)
I0930 20:00:19.239748  4033 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I0930 20:00:24.490141  4033 solver.cpp:218] Iteration 1100 (19.0463 iter/s, 5.25038s/100 iters), loss = 0.900561
I0930 20:00:24.490181  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.900561 (* 1 = 0.900561 loss)
I0930 20:00:24.490187  4033 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I0930 20:00:29.741276  4033 solver.cpp:218] Iteration 1200 (19.0438 iter/s, 5.25107s/100 iters), loss = 0.989446
I0930 20:00:29.741428  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.989446 (* 1 = 0.989446 loss)
I0930 20:00:29.741436  4033 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I0930 20:00:34.986788  4033 solver.cpp:218] Iteration 1300 (19.0645 iter/s, 5.24534s/100 iters), loss = 0.932339
I0930 20:00:34.986819  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.932339 (* 1 = 0.932339 loss)
I0930 20:00:34.986824  4033 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I0930 20:00:40.226512  4033 solver.cpp:218] Iteration 1400 (19.0852 iter/s, 5.23967s/100 iters), loss = 0.731801
I0930 20:00:40.226544  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.731801 (* 1 = 0.731801 loss)
I0930 20:00:40.226550  4033 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I0930 20:00:45.214643  4041 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:00:45.424108  4033 solver.cpp:330] Iteration 1500, Testing net (#0)
I0930 20:00:46.622851  4042 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:00:46.673691  4033 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3088
I0930 20:00:46.673717  4033 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.43649 (* 1 = 4.43649 loss)
I0930 20:00:46.727047  4033 solver.cpp:218] Iteration 1500 (15.3835 iter/s, 6.50048s/100 iters), loss = 0.875491
I0930 20:00:46.727082  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.875491 (* 1 = 0.875491 loss)
I0930 20:00:46.727088  4033 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I0930 20:00:51.965623  4033 solver.cpp:218] Iteration 1600 (19.0894 iter/s, 5.23852s/100 iters), loss = 0.795427
I0930 20:00:51.965653  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.795427 (* 1 = 0.795427 loss)
I0930 20:00:51.965658  4033 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I0930 20:00:57.213910  4033 solver.cpp:218] Iteration 1700 (19.054 iter/s, 5.24823s/100 iters), loss = 0.886291
I0930 20:00:57.213943  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.886291 (* 1 = 0.886291 loss)
I0930 20:00:57.213949  4033 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I0930 20:01:02.463404  4033 solver.cpp:218] Iteration 1800 (19.0497 iter/s, 5.24944s/100 iters), loss = 0.741816
I0930 20:01:02.463538  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.741816 (* 1 = 0.741816 loss)
I0930 20:01:02.463557  4033 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I0930 20:01:07.711663  4033 solver.cpp:218] Iteration 1900 (19.0545 iter/s, 5.24811s/100 iters), loss = 0.80955
I0930 20:01:07.711700  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.80955 (* 1 = 0.80955 loss)
I0930 20:01:07.711707  4033 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I0930 20:01:12.707535  4041 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:01:12.916097  4033 solver.cpp:330] Iteration 2000, Testing net (#0)
I0930 20:01:14.112910  4042 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:01:14.161696  4033 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4917
I0930 20:01:14.161731  4033 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.31487 (* 1 = 2.31487 loss)
I0930 20:01:14.213515  4033 solver.cpp:218] Iteration 2000 (15.3804 iter/s, 6.50179s/100 iters), loss = 0.755828
I0930 20:01:14.213541  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.755828 (* 1 = 0.755828 loss)
I0930 20:01:14.213548  4033 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I0930 20:01:19.456722  4033 solver.cpp:218] Iteration 2100 (19.0725 iter/s, 5.24315s/100 iters), loss = 0.636992
I0930 20:01:19.456758  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.636992 (* 1 = 0.636992 loss)
I0930 20:01:19.456766  4033 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I0930 20:01:24.745450  4033 solver.cpp:218] Iteration 2200 (18.9084 iter/s, 5.28867s/100 iters), loss = 0.724899
I0930 20:01:24.745492  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.724899 (* 1 = 0.724899 loss)
I0930 20:01:24.745499  4033 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I0930 20:01:30.008528  4033 solver.cpp:218] Iteration 2300 (19.0005 iter/s, 5.26301s/100 iters), loss = 0.838163
I0930 20:01:30.008569  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.838163 (* 1 = 0.838163 loss)
I0930 20:01:30.008574  4033 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I0930 20:01:35.251814  4033 solver.cpp:218] Iteration 2400 (19.0722 iter/s, 5.24322s/100 iters), loss = 0.693578
I0930 20:01:35.251962  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.693578 (* 1 = 0.693578 loss)
I0930 20:01:35.251971  4033 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I0930 20:01:40.227043  4041 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:01:40.438091  4033 solver.cpp:330] Iteration 2500, Testing net (#0)
I0930 20:01:41.636437  4042 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:01:41.686904  4033 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5759
I0930 20:01:41.686940  4033 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.52666 (* 1 = 1.52666 loss)
I0930 20:01:41.739282  4033 solver.cpp:218] Iteration 2500 (15.4147 iter/s, 6.4873s/100 iters), loss = 0.662298
I0930 20:01:41.739307  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.662298 (* 1 = 0.662298 loss)
I0930 20:01:41.739315  4033 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I0930 20:01:46.985575  4033 solver.cpp:218] Iteration 2600 (19.0613 iter/s, 5.24624s/100 iters), loss = 0.590527
I0930 20:01:46.985615  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.590527 (* 1 = 0.590527 loss)
I0930 20:01:46.985622  4033 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I0930 20:01:52.223294  4033 solver.cpp:218] Iteration 2700 (19.0925 iter/s, 5.23765s/100 iters), loss = 0.694064
I0930 20:01:52.223333  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.694064 (* 1 = 0.694064 loss)
I0930 20:01:52.223340  4033 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I0930 20:01:57.464175  4033 solver.cpp:218] Iteration 2800 (19.081 iter/s, 5.24082s/100 iters), loss = 0.776493
I0930 20:01:57.464216  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.776493 (* 1 = 0.776493 loss)
I0930 20:01:57.464222  4033 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I0930 20:02:02.708487  4033 solver.cpp:218] Iteration 2900 (19.0685 iter/s, 5.24425s/100 iters), loss = 0.569895
I0930 20:02:02.708519  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.569895 (* 1 = 0.569895 loss)
I0930 20:02:02.708525  4033 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I0930 20:02:07.684798  4041 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:02:07.894968  4033 solver.cpp:330] Iteration 3000, Testing net (#0)
I0930 20:02:09.085657  4042 data_layer.cpp:73] Restarting data prefetching from start.
I0930 20:02:09.136286  4033 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5891
I0930 20:02:09.136322  4033 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.44477 (* 1 = 1.44477 loss)
I0930 20:02:09.188567  4033 solver.cpp:218] Iteration 3000 (15.432 iter/s, 6.48002s/100 iters), loss = 0.721873
I0930 20:02:09.188596  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.721873 (* 1 = 0.721873 loss)
I0930 20:02:09.188612  4033 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I0930 20:02:14.428843  4033 solver.cpp:218] Iteration 3100 (19.0832 iter/s, 5.24022s/100 iters), loss = 0.491718
I0930 20:02:14.428874  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.491718 (* 1 = 0.491718 loss)
I0930 20:02:14.428879  4033 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I0930 20:02:19.672430  4033 solver.cpp:218] Iteration 3200 (19.0711 iter/s, 5.24353s/100 iters), loss = 0.798538
I0930 20:02:19.672463  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.798538 (* 1 = 0.798538 loss)
I0930 20:02:19.672471  4033 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I0930 20:02:24.899524  4033 solver.cpp:218] Iteration 3300 (19.1313 iter/s, 5.22704s/100 iters), loss = 87.3365
I0930 20:02:24.899554  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I0930 20:02:24.899560  4033 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I0930 20:02:30.124088  4033 solver.cpp:218] Iteration 3400 (19.1406 iter/s, 5.22451s/100 iters), loss = 87.3365
I0930 20:02:30.124117  4033 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I0930 20:02:30.124122  4033 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
