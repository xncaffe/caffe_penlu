I1001 15:00:40.908130  5332 caffe.cpp:218] Using GPUs 0
I1001 15:00:40.933143  5332 caffe.cpp:223] GPU 0: GeForce GTX 1080
I1001 15:00:41.161985  5332 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 40000
stepvalue: 80000
I1001 15:00:41.162142  5332 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 15:00:41.163671  5332 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 15:00:41.163681  5332 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1001 15:00:41.163827  5332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I1001 15:00:41.163898  5332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I1001 15:00:41.164374  5332 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1001 15:00:41.164752  5332 layer_factory.hpp:77] Creating layer Data1
I1001 15:00:41.164829  5332 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb
I1001 15:00:41.164855  5332 net.cpp:84] Creating Layer Data1
I1001 15:00:41.164860  5332 net.cpp:380] Data1 -> Data1
I1001 15:00:41.164876  5332 net.cpp:380] Data1 -> Data2
I1001 15:00:41.164885  5332 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I1001 15:00:41.166358  5332 data_layer.cpp:45] output data size: 100,3,28,28
I1001 15:00:41.168808  5332 net.cpp:122] Setting up Data1
I1001 15:00:41.168824  5332 net.cpp:129] Top shape: 100 3 28 28 (235200)
I1001 15:00:41.168829  5332 net.cpp:129] Top shape: 100 (100)
I1001 15:00:41.168831  5332 net.cpp:137] Memory required for data: 941200
I1001 15:00:41.168838  5332 layer_factory.hpp:77] Creating layer Convolution1
I1001 15:00:41.168856  5332 net.cpp:84] Creating Layer Convolution1
I1001 15:00:41.168861  5332 net.cpp:406] Convolution1 <- Data1
I1001 15:00:41.168871  5332 net.cpp:380] Convolution1 -> Convolution1
I1001 15:00:41.320097  5332 net.cpp:122] Setting up Convolution1
I1001 15:00:41.320122  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.320125  5332 net.cpp:137] Memory required for data: 5958800
I1001 15:00:41.320139  5332 layer_factory.hpp:77] Creating layer BatchNorm1
I1001 15:00:41.320158  5332 net.cpp:84] Creating Layer BatchNorm1
I1001 15:00:41.320163  5332 net.cpp:406] BatchNorm1 <- Convolution1
I1001 15:00:41.320178  5332 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1001 15:00:41.320338  5332 net.cpp:122] Setting up BatchNorm1
I1001 15:00:41.320344  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.320346  5332 net.cpp:137] Memory required for data: 10976400
I1001 15:00:41.320354  5332 layer_factory.hpp:77] Creating layer Scale1
I1001 15:00:41.320375  5332 net.cpp:84] Creating Layer Scale1
I1001 15:00:41.320379  5332 net.cpp:406] Scale1 <- Convolution1
I1001 15:00:41.320382  5332 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1001 15:00:41.320436  5332 layer_factory.hpp:77] Creating layer Scale1
I1001 15:00:41.320543  5332 net.cpp:122] Setting up Scale1
I1001 15:00:41.320549  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.320551  5332 net.cpp:137] Memory required for data: 15994000
I1001 15:00:41.320555  5332 layer_factory.hpp:77] Creating layer penlu1
I1001 15:00:41.320565  5332 net.cpp:84] Creating Layer penlu1
I1001 15:00:41.320578  5332 net.cpp:406] penlu1 <- Convolution1
I1001 15:00:41.320582  5332 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1001 15:00:41.321197  5332 net.cpp:122] Setting up penlu1
I1001 15:00:41.321207  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.321209  5332 net.cpp:137] Memory required for data: 21011600
I1001 15:00:41.321216  5332 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1001 15:00:41.321221  5332 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1001 15:00:41.321223  5332 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1001 15:00:41.321238  5332 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1001 15:00:41.321245  5332 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1001 15:00:41.321305  5332 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1001 15:00:41.321313  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.321319  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.321321  5332 net.cpp:137] Memory required for data: 31046800
I1001 15:00:41.321323  5332 layer_factory.hpp:77] Creating layer Convolution2
I1001 15:00:41.321331  5332 net.cpp:84] Creating Layer Convolution2
I1001 15:00:41.321343  5332 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1001 15:00:41.321347  5332 net.cpp:380] Convolution2 -> Convolution2
I1001 15:00:41.322199  5332 net.cpp:122] Setting up Convolution2
I1001 15:00:41.322209  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.322211  5332 net.cpp:137] Memory required for data: 36064400
I1001 15:00:41.322216  5332 layer_factory.hpp:77] Creating layer BatchNorm2
I1001 15:00:41.322242  5332 net.cpp:84] Creating Layer BatchNorm2
I1001 15:00:41.322249  5332 net.cpp:406] BatchNorm2 <- Convolution2
I1001 15:00:41.322257  5332 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1001 15:00:41.322412  5332 net.cpp:122] Setting up BatchNorm2
I1001 15:00:41.322419  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.322422  5332 net.cpp:137] Memory required for data: 41082000
I1001 15:00:41.322427  5332 layer_factory.hpp:77] Creating layer Scale2
I1001 15:00:41.322432  5332 net.cpp:84] Creating Layer Scale2
I1001 15:00:41.322434  5332 net.cpp:406] Scale2 <- Convolution2
I1001 15:00:41.322438  5332 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1001 15:00:41.322489  5332 layer_factory.hpp:77] Creating layer Scale2
I1001 15:00:41.322592  5332 net.cpp:122] Setting up Scale2
I1001 15:00:41.322599  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.322602  5332 net.cpp:137] Memory required for data: 46099600
I1001 15:00:41.322608  5332 layer_factory.hpp:77] Creating layer penlu2
I1001 15:00:41.322613  5332 net.cpp:84] Creating Layer penlu2
I1001 15:00:41.322616  5332 net.cpp:406] penlu2 <- Convolution2
I1001 15:00:41.322630  5332 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1001 15:00:41.322741  5332 net.cpp:122] Setting up penlu2
I1001 15:00:41.322747  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.322749  5332 net.cpp:137] Memory required for data: 51117200
I1001 15:00:41.322754  5332 layer_factory.hpp:77] Creating layer Convolution3
I1001 15:00:41.322762  5332 net.cpp:84] Creating Layer Convolution3
I1001 15:00:41.322763  5332 net.cpp:406] Convolution3 <- Convolution2
I1001 15:00:41.322777  5332 net.cpp:380] Convolution3 -> Convolution3
I1001 15:00:41.323624  5332 net.cpp:122] Setting up Convolution3
I1001 15:00:41.323635  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.323638  5332 net.cpp:137] Memory required for data: 56134800
I1001 15:00:41.323642  5332 layer_factory.hpp:77] Creating layer BatchNorm3
I1001 15:00:41.323647  5332 net.cpp:84] Creating Layer BatchNorm3
I1001 15:00:41.323649  5332 net.cpp:406] BatchNorm3 <- Convolution3
I1001 15:00:41.323663  5332 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1001 15:00:41.323786  5332 net.cpp:122] Setting up BatchNorm3
I1001 15:00:41.323791  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.323792  5332 net.cpp:137] Memory required for data: 61152400
I1001 15:00:41.323797  5332 layer_factory.hpp:77] Creating layer Scale3
I1001 15:00:41.323801  5332 net.cpp:84] Creating Layer Scale3
I1001 15:00:41.323804  5332 net.cpp:406] Scale3 <- Convolution3
I1001 15:00:41.323807  5332 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1001 15:00:41.323848  5332 layer_factory.hpp:77] Creating layer Scale3
I1001 15:00:41.323935  5332 net.cpp:122] Setting up Scale3
I1001 15:00:41.323940  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.323941  5332 net.cpp:137] Memory required for data: 66170000
I1001 15:00:41.323945  5332 layer_factory.hpp:77] Creating layer Eltwise1
I1001 15:00:41.323951  5332 net.cpp:84] Creating Layer Eltwise1
I1001 15:00:41.323952  5332 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1001 15:00:41.323956  5332 net.cpp:406] Eltwise1 <- Convolution3
I1001 15:00:41.323958  5332 net.cpp:380] Eltwise1 -> Eltwise1
I1001 15:00:41.323984  5332 net.cpp:122] Setting up Eltwise1
I1001 15:00:41.323998  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.324000  5332 net.cpp:137] Memory required for data: 71187600
I1001 15:00:41.324002  5332 layer_factory.hpp:77] Creating layer penlu3
I1001 15:00:41.324007  5332 net.cpp:84] Creating Layer penlu3
I1001 15:00:41.324009  5332 net.cpp:406] penlu3 <- Eltwise1
I1001 15:00:41.324013  5332 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1001 15:00:41.324123  5332 net.cpp:122] Setting up penlu3
I1001 15:00:41.324128  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.324131  5332 net.cpp:137] Memory required for data: 76205200
I1001 15:00:41.324142  5332 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1001 15:00:41.324146  5332 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1001 15:00:41.324158  5332 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1001 15:00:41.324162  5332 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1001 15:00:41.324167  5332 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1001 15:00:41.324198  5332 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1001 15:00:41.324203  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.324205  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.324208  5332 net.cpp:137] Memory required for data: 86240400
I1001 15:00:41.324209  5332 layer_factory.hpp:77] Creating layer Convolution4
I1001 15:00:41.324226  5332 net.cpp:84] Creating Layer Convolution4
I1001 15:00:41.324229  5332 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1001 15:00:41.324234  5332 net.cpp:380] Convolution4 -> Convolution4
I1001 15:00:41.325078  5332 net.cpp:122] Setting up Convolution4
I1001 15:00:41.325088  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.325093  5332 net.cpp:137] Memory required for data: 91258000
I1001 15:00:41.325098  5332 layer_factory.hpp:77] Creating layer BatchNorm4
I1001 15:00:41.325103  5332 net.cpp:84] Creating Layer BatchNorm4
I1001 15:00:41.325105  5332 net.cpp:406] BatchNorm4 <- Convolution4
I1001 15:00:41.325109  5332 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1001 15:00:41.325222  5332 net.cpp:122] Setting up BatchNorm4
I1001 15:00:41.325227  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.325230  5332 net.cpp:137] Memory required for data: 96275600
I1001 15:00:41.325237  5332 layer_factory.hpp:77] Creating layer Scale4
I1001 15:00:41.325242  5332 net.cpp:84] Creating Layer Scale4
I1001 15:00:41.325244  5332 net.cpp:406] Scale4 <- Convolution4
I1001 15:00:41.325248  5332 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1001 15:00:41.325271  5332 layer_factory.hpp:77] Creating layer Scale4
I1001 15:00:41.325338  5332 net.cpp:122] Setting up Scale4
I1001 15:00:41.325343  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.325346  5332 net.cpp:137] Memory required for data: 101293200
I1001 15:00:41.325351  5332 layer_factory.hpp:77] Creating layer penlu4
I1001 15:00:41.325356  5332 net.cpp:84] Creating Layer penlu4
I1001 15:00:41.325359  5332 net.cpp:406] penlu4 <- Convolution4
I1001 15:00:41.325362  5332 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1001 15:00:41.325454  5332 net.cpp:122] Setting up penlu4
I1001 15:00:41.325459  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.325462  5332 net.cpp:137] Memory required for data: 106310800
I1001 15:00:41.325467  5332 layer_factory.hpp:77] Creating layer Convolution5
I1001 15:00:41.325474  5332 net.cpp:84] Creating Layer Convolution5
I1001 15:00:41.325477  5332 net.cpp:406] Convolution5 <- Convolution4
I1001 15:00:41.325481  5332 net.cpp:380] Convolution5 -> Convolution5
I1001 15:00:41.326310  5332 net.cpp:122] Setting up Convolution5
I1001 15:00:41.326320  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.326324  5332 net.cpp:137] Memory required for data: 111328400
I1001 15:00:41.326329  5332 layer_factory.hpp:77] Creating layer BatchNorm5
I1001 15:00:41.326334  5332 net.cpp:84] Creating Layer BatchNorm5
I1001 15:00:41.326337  5332 net.cpp:406] BatchNorm5 <- Convolution5
I1001 15:00:41.326340  5332 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1001 15:00:41.326458  5332 net.cpp:122] Setting up BatchNorm5
I1001 15:00:41.326463  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.326467  5332 net.cpp:137] Memory required for data: 116346000
I1001 15:00:41.326472  5332 layer_factory.hpp:77] Creating layer Scale5
I1001 15:00:41.326477  5332 net.cpp:84] Creating Layer Scale5
I1001 15:00:41.326478  5332 net.cpp:406] Scale5 <- Convolution5
I1001 15:00:41.326483  5332 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1001 15:00:41.326514  5332 layer_factory.hpp:77] Creating layer Scale5
I1001 15:00:41.326589  5332 net.cpp:122] Setting up Scale5
I1001 15:00:41.326596  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.326598  5332 net.cpp:137] Memory required for data: 121363600
I1001 15:00:41.326602  5332 layer_factory.hpp:77] Creating layer Eltwise2
I1001 15:00:41.326607  5332 net.cpp:84] Creating Layer Eltwise2
I1001 15:00:41.326611  5332 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1001 15:00:41.326613  5332 net.cpp:406] Eltwise2 <- Convolution5
I1001 15:00:41.326617  5332 net.cpp:380] Eltwise2 -> Eltwise2
I1001 15:00:41.326632  5332 net.cpp:122] Setting up Eltwise2
I1001 15:00:41.326637  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.326639  5332 net.cpp:137] Memory required for data: 126381200
I1001 15:00:41.326642  5332 layer_factory.hpp:77] Creating layer penlu5
I1001 15:00:41.326647  5332 net.cpp:84] Creating Layer penlu5
I1001 15:00:41.326650  5332 net.cpp:406] penlu5 <- Eltwise2
I1001 15:00:41.326654  5332 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1001 15:00:41.326750  5332 net.cpp:122] Setting up penlu5
I1001 15:00:41.326756  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.326758  5332 net.cpp:137] Memory required for data: 131398800
I1001 15:00:41.326763  5332 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1001 15:00:41.326767  5332 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1001 15:00:41.326771  5332 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1001 15:00:41.326773  5332 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1001 15:00:41.326778  5332 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1001 15:00:41.326799  5332 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1001 15:00:41.326803  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.326807  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.326809  5332 net.cpp:137] Memory required for data: 141434000
I1001 15:00:41.326812  5332 layer_factory.hpp:77] Creating layer Convolution6
I1001 15:00:41.326819  5332 net.cpp:84] Creating Layer Convolution6
I1001 15:00:41.326822  5332 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1001 15:00:41.326827  5332 net.cpp:380] Convolution6 -> Convolution6
I1001 15:00:41.327657  5332 net.cpp:122] Setting up Convolution6
I1001 15:00:41.327667  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.327672  5332 net.cpp:137] Memory required for data: 146451600
I1001 15:00:41.327675  5332 layer_factory.hpp:77] Creating layer BatchNorm6
I1001 15:00:41.327682  5332 net.cpp:84] Creating Layer BatchNorm6
I1001 15:00:41.327684  5332 net.cpp:406] BatchNorm6 <- Convolution6
I1001 15:00:41.327688  5332 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1001 15:00:41.327810  5332 net.cpp:122] Setting up BatchNorm6
I1001 15:00:41.327814  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.327817  5332 net.cpp:137] Memory required for data: 151469200
I1001 15:00:41.327822  5332 layer_factory.hpp:77] Creating layer Scale6
I1001 15:00:41.327826  5332 net.cpp:84] Creating Layer Scale6
I1001 15:00:41.327831  5332 net.cpp:406] Scale6 <- Convolution6
I1001 15:00:41.327833  5332 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1001 15:00:41.327857  5332 layer_factory.hpp:77] Creating layer Scale6
I1001 15:00:41.327927  5332 net.cpp:122] Setting up Scale6
I1001 15:00:41.327932  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.327935  5332 net.cpp:137] Memory required for data: 156486800
I1001 15:00:41.327939  5332 layer_factory.hpp:77] Creating layer penlu6
I1001 15:00:41.327945  5332 net.cpp:84] Creating Layer penlu6
I1001 15:00:41.327949  5332 net.cpp:406] penlu6 <- Convolution6
I1001 15:00:41.327952  5332 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1001 15:00:41.328047  5332 net.cpp:122] Setting up penlu6
I1001 15:00:41.328052  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.328055  5332 net.cpp:137] Memory required for data: 161504400
I1001 15:00:41.328068  5332 layer_factory.hpp:77] Creating layer Convolution7
I1001 15:00:41.328074  5332 net.cpp:84] Creating Layer Convolution7
I1001 15:00:41.328078  5332 net.cpp:406] Convolution7 <- Convolution6
I1001 15:00:41.328083  5332 net.cpp:380] Convolution7 -> Convolution7
I1001 15:00:41.328593  5332 net.cpp:122] Setting up Convolution7
I1001 15:00:41.328601  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.328604  5332 net.cpp:137] Memory required for data: 166522000
I1001 15:00:41.328609  5332 layer_factory.hpp:77] Creating layer BatchNorm7
I1001 15:00:41.328614  5332 net.cpp:84] Creating Layer BatchNorm7
I1001 15:00:41.328618  5332 net.cpp:406] BatchNorm7 <- Convolution7
I1001 15:00:41.328621  5332 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1001 15:00:41.328740  5332 net.cpp:122] Setting up BatchNorm7
I1001 15:00:41.328745  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.328748  5332 net.cpp:137] Memory required for data: 171539600
I1001 15:00:41.328758  5332 layer_factory.hpp:77] Creating layer Scale7
I1001 15:00:41.328763  5332 net.cpp:84] Creating Layer Scale7
I1001 15:00:41.328766  5332 net.cpp:406] Scale7 <- Convolution7
I1001 15:00:41.328770  5332 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1001 15:00:41.328794  5332 layer_factory.hpp:77] Creating layer Scale7
I1001 15:00:41.328863  5332 net.cpp:122] Setting up Scale7
I1001 15:00:41.328868  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.328872  5332 net.cpp:137] Memory required for data: 176557200
I1001 15:00:41.328876  5332 layer_factory.hpp:77] Creating layer Eltwise3
I1001 15:00:41.328881  5332 net.cpp:84] Creating Layer Eltwise3
I1001 15:00:41.328883  5332 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1001 15:00:41.328886  5332 net.cpp:406] Eltwise3 <- Convolution7
I1001 15:00:41.328891  5332 net.cpp:380] Eltwise3 -> Eltwise3
I1001 15:00:41.328905  5332 net.cpp:122] Setting up Eltwise3
I1001 15:00:41.328909  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.328912  5332 net.cpp:137] Memory required for data: 181574800
I1001 15:00:41.328915  5332 layer_factory.hpp:77] Creating layer penlu7
I1001 15:00:41.328920  5332 net.cpp:84] Creating Layer penlu7
I1001 15:00:41.328923  5332 net.cpp:406] penlu7 <- Eltwise3
I1001 15:00:41.328927  5332 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1001 15:00:41.329026  5332 net.cpp:122] Setting up penlu7
I1001 15:00:41.329031  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.329035  5332 net.cpp:137] Memory required for data: 186592400
I1001 15:00:41.329038  5332 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1001 15:00:41.329043  5332 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1001 15:00:41.329046  5332 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1001 15:00:41.329049  5332 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1001 15:00:41.329054  5332 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1001 15:00:41.329075  5332 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1001 15:00:41.329079  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.329082  5332 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1001 15:00:41.329085  5332 net.cpp:137] Memory required for data: 196627600
I1001 15:00:41.329087  5332 layer_factory.hpp:77] Creating layer Convolution8
I1001 15:00:41.329093  5332 net.cpp:84] Creating Layer Convolution8
I1001 15:00:41.329097  5332 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1001 15:00:41.329100  5332 net.cpp:380] Convolution8 -> Convolution8
I1001 15:00:41.330214  5332 net.cpp:122] Setting up Convolution8
I1001 15:00:41.330224  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.330227  5332 net.cpp:137] Memory required for data: 199136400
I1001 15:00:41.330232  5332 layer_factory.hpp:77] Creating layer BatchNorm8
I1001 15:00:41.330238  5332 net.cpp:84] Creating Layer BatchNorm8
I1001 15:00:41.330241  5332 net.cpp:406] BatchNorm8 <- Convolution8
I1001 15:00:41.330245  5332 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1001 15:00:41.330401  5332 net.cpp:122] Setting up BatchNorm8
I1001 15:00:41.330407  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.330410  5332 net.cpp:137] Memory required for data: 201645200
I1001 15:00:41.330425  5332 layer_factory.hpp:77] Creating layer Scale8
I1001 15:00:41.330430  5332 net.cpp:84] Creating Layer Scale8
I1001 15:00:41.330432  5332 net.cpp:406] Scale8 <- Convolution8
I1001 15:00:41.330436  5332 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1001 15:00:41.330471  5332 layer_factory.hpp:77] Creating layer Scale8
I1001 15:00:41.330544  5332 net.cpp:122] Setting up Scale8
I1001 15:00:41.330550  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.330552  5332 net.cpp:137] Memory required for data: 204154000
I1001 15:00:41.330556  5332 layer_factory.hpp:77] Creating layer Convolution9
I1001 15:00:41.330564  5332 net.cpp:84] Creating Layer Convolution9
I1001 15:00:41.330567  5332 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1001 15:00:41.330571  5332 net.cpp:380] Convolution9 -> Convolution9
I1001 15:00:41.332181  5332 net.cpp:122] Setting up Convolution9
I1001 15:00:41.332195  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.332201  5332 net.cpp:137] Memory required for data: 206662800
I1001 15:00:41.332208  5332 layer_factory.hpp:77] Creating layer BatchNorm9
I1001 15:00:41.332216  5332 net.cpp:84] Creating Layer BatchNorm9
I1001 15:00:41.332221  5332 net.cpp:406] BatchNorm9 <- Convolution9
I1001 15:00:41.332227  5332 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1001 15:00:41.332432  5332 net.cpp:122] Setting up BatchNorm9
I1001 15:00:41.332444  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.332449  5332 net.cpp:137] Memory required for data: 209171600
I1001 15:00:41.332459  5332 layer_factory.hpp:77] Creating layer Scale9
I1001 15:00:41.332468  5332 net.cpp:84] Creating Layer Scale9
I1001 15:00:41.332474  5332 net.cpp:406] Scale9 <- Convolution9
I1001 15:00:41.332481  5332 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1001 15:00:41.332525  5332 layer_factory.hpp:77] Creating layer Scale9
I1001 15:00:41.332612  5332 net.cpp:122] Setting up Scale9
I1001 15:00:41.332623  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.332626  5332 net.cpp:137] Memory required for data: 211680400
I1001 15:00:41.332630  5332 layer_factory.hpp:77] Creating layer penlu8
I1001 15:00:41.332636  5332 net.cpp:84] Creating Layer penlu8
I1001 15:00:41.332639  5332 net.cpp:406] penlu8 <- Convolution9
I1001 15:00:41.332643  5332 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1001 15:00:41.332756  5332 net.cpp:122] Setting up penlu8
I1001 15:00:41.332761  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.332772  5332 net.cpp:137] Memory required for data: 214189200
I1001 15:00:41.332777  5332 layer_factory.hpp:77] Creating layer Convolution10
I1001 15:00:41.332784  5332 net.cpp:84] Creating Layer Convolution10
I1001 15:00:41.332787  5332 net.cpp:406] Convolution10 <- Convolution9
I1001 15:00:41.332792  5332 net.cpp:380] Convolution10 -> Convolution10
I1001 15:00:41.333814  5332 net.cpp:122] Setting up Convolution10
I1001 15:00:41.333824  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.333838  5332 net.cpp:137] Memory required for data: 216698000
I1001 15:00:41.333843  5332 layer_factory.hpp:77] Creating layer BatchNorm10
I1001 15:00:41.333848  5332 net.cpp:84] Creating Layer BatchNorm10
I1001 15:00:41.333850  5332 net.cpp:406] BatchNorm10 <- Convolution10
I1001 15:00:41.333855  5332 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1001 15:00:41.333981  5332 net.cpp:122] Setting up BatchNorm10
I1001 15:00:41.333986  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.333989  5332 net.cpp:137] Memory required for data: 219206800
I1001 15:00:41.333994  5332 layer_factory.hpp:77] Creating layer Scale10
I1001 15:00:41.333999  5332 net.cpp:84] Creating Layer Scale10
I1001 15:00:41.334002  5332 net.cpp:406] Scale10 <- Convolution10
I1001 15:00:41.334013  5332 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1001 15:00:41.334040  5332 layer_factory.hpp:77] Creating layer Scale10
I1001 15:00:41.334121  5332 net.cpp:122] Setting up Scale10
I1001 15:00:41.334126  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.334138  5332 net.cpp:137] Memory required for data: 221715600
I1001 15:00:41.334142  5332 layer_factory.hpp:77] Creating layer Eltwise4
I1001 15:00:41.334147  5332 net.cpp:84] Creating Layer Eltwise4
I1001 15:00:41.334149  5332 net.cpp:406] Eltwise4 <- Convolution8
I1001 15:00:41.334153  5332 net.cpp:406] Eltwise4 <- Convolution10
I1001 15:00:41.334156  5332 net.cpp:380] Eltwise4 -> Eltwise4
I1001 15:00:41.334172  5332 net.cpp:122] Setting up Eltwise4
I1001 15:00:41.334177  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.334179  5332 net.cpp:137] Memory required for data: 224224400
I1001 15:00:41.334182  5332 layer_factory.hpp:77] Creating layer penlu9
I1001 15:00:41.334187  5332 net.cpp:84] Creating Layer penlu9
I1001 15:00:41.334190  5332 net.cpp:406] penlu9 <- Eltwise4
I1001 15:00:41.334194  5332 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1001 15:00:41.334296  5332 net.cpp:122] Setting up penlu9
I1001 15:00:41.334301  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.334305  5332 net.cpp:137] Memory required for data: 226733200
I1001 15:00:41.334309  5332 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1001 15:00:41.334313  5332 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1001 15:00:41.334317  5332 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1001 15:00:41.334321  5332 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1001 15:00:41.334326  5332 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1001 15:00:41.334347  5332 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1001 15:00:41.334352  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.334355  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.334358  5332 net.cpp:137] Memory required for data: 231750800
I1001 15:00:41.334360  5332 layer_factory.hpp:77] Creating layer Convolution11
I1001 15:00:41.334367  5332 net.cpp:84] Creating Layer Convolution11
I1001 15:00:41.334369  5332 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1001 15:00:41.334374  5332 net.cpp:380] Convolution11 -> Convolution11
I1001 15:00:41.335409  5332 net.cpp:122] Setting up Convolution11
I1001 15:00:41.335420  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.335424  5332 net.cpp:137] Memory required for data: 234259600
I1001 15:00:41.335429  5332 layer_factory.hpp:77] Creating layer BatchNorm11
I1001 15:00:41.335435  5332 net.cpp:84] Creating Layer BatchNorm11
I1001 15:00:41.335438  5332 net.cpp:406] BatchNorm11 <- Convolution11
I1001 15:00:41.335443  5332 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1001 15:00:41.335572  5332 net.cpp:122] Setting up BatchNorm11
I1001 15:00:41.335577  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.335580  5332 net.cpp:137] Memory required for data: 236768400
I1001 15:00:41.335585  5332 layer_factory.hpp:77] Creating layer Scale11
I1001 15:00:41.335589  5332 net.cpp:84] Creating Layer Scale11
I1001 15:00:41.335592  5332 net.cpp:406] Scale11 <- Convolution11
I1001 15:00:41.335595  5332 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1001 15:00:41.335630  5332 layer_factory.hpp:77] Creating layer Scale11
I1001 15:00:41.335724  5332 net.cpp:122] Setting up Scale11
I1001 15:00:41.335729  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.335742  5332 net.cpp:137] Memory required for data: 239277200
I1001 15:00:41.335747  5332 layer_factory.hpp:77] Creating layer penlu10
I1001 15:00:41.335752  5332 net.cpp:84] Creating Layer penlu10
I1001 15:00:41.335754  5332 net.cpp:406] penlu10 <- Convolution11
I1001 15:00:41.335757  5332 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1001 15:00:41.335870  5332 net.cpp:122] Setting up penlu10
I1001 15:00:41.335875  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.335893  5332 net.cpp:137] Memory required for data: 241786000
I1001 15:00:41.335898  5332 layer_factory.hpp:77] Creating layer Convolution12
I1001 15:00:41.335906  5332 net.cpp:84] Creating Layer Convolution12
I1001 15:00:41.335911  5332 net.cpp:406] Convolution12 <- Convolution11
I1001 15:00:41.335914  5332 net.cpp:380] Convolution12 -> Convolution12
I1001 15:00:41.336961  5332 net.cpp:122] Setting up Convolution12
I1001 15:00:41.336971  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.336983  5332 net.cpp:137] Memory required for data: 244294800
I1001 15:00:41.336988  5332 layer_factory.hpp:77] Creating layer BatchNorm12
I1001 15:00:41.336993  5332 net.cpp:84] Creating Layer BatchNorm12
I1001 15:00:41.336997  5332 net.cpp:406] BatchNorm12 <- Convolution12
I1001 15:00:41.337002  5332 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1001 15:00:41.337139  5332 net.cpp:122] Setting up BatchNorm12
I1001 15:00:41.337144  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.337155  5332 net.cpp:137] Memory required for data: 246803600
I1001 15:00:41.337162  5332 layer_factory.hpp:77] Creating layer Scale12
I1001 15:00:41.337167  5332 net.cpp:84] Creating Layer Scale12
I1001 15:00:41.337169  5332 net.cpp:406] Scale12 <- Convolution12
I1001 15:00:41.337173  5332 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1001 15:00:41.337208  5332 layer_factory.hpp:77] Creating layer Scale12
I1001 15:00:41.337301  5332 net.cpp:122] Setting up Scale12
I1001 15:00:41.337306  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.337308  5332 net.cpp:137] Memory required for data: 249312400
I1001 15:00:41.337321  5332 layer_factory.hpp:77] Creating layer Eltwise5
I1001 15:00:41.337327  5332 net.cpp:84] Creating Layer Eltwise5
I1001 15:00:41.337329  5332 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1001 15:00:41.337332  5332 net.cpp:406] Eltwise5 <- Convolution12
I1001 15:00:41.337335  5332 net.cpp:380] Eltwise5 -> Eltwise5
I1001 15:00:41.337352  5332 net.cpp:122] Setting up Eltwise5
I1001 15:00:41.337357  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.337358  5332 net.cpp:137] Memory required for data: 251821200
I1001 15:00:41.337360  5332 layer_factory.hpp:77] Creating layer penlu11
I1001 15:00:41.337368  5332 net.cpp:84] Creating Layer penlu11
I1001 15:00:41.337370  5332 net.cpp:406] penlu11 <- Eltwise5
I1001 15:00:41.337374  5332 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1001 15:00:41.337478  5332 net.cpp:122] Setting up penlu11
I1001 15:00:41.337483  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.337486  5332 net.cpp:137] Memory required for data: 254330000
I1001 15:00:41.337491  5332 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1001 15:00:41.337496  5332 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1001 15:00:41.337498  5332 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1001 15:00:41.337502  5332 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1001 15:00:41.337507  5332 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1001 15:00:41.337529  5332 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1001 15:00:41.337533  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.337538  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.337539  5332 net.cpp:137] Memory required for data: 259347600
I1001 15:00:41.337543  5332 layer_factory.hpp:77] Creating layer Convolution13
I1001 15:00:41.337548  5332 net.cpp:84] Creating Layer Convolution13
I1001 15:00:41.337551  5332 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1001 15:00:41.337556  5332 net.cpp:380] Convolution13 -> Convolution13
I1001 15:00:41.338618  5332 net.cpp:122] Setting up Convolution13
I1001 15:00:41.338629  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.338632  5332 net.cpp:137] Memory required for data: 261856400
I1001 15:00:41.338636  5332 layer_factory.hpp:77] Creating layer BatchNorm13
I1001 15:00:41.338642  5332 net.cpp:84] Creating Layer BatchNorm13
I1001 15:00:41.338654  5332 net.cpp:406] BatchNorm13 <- Convolution13
I1001 15:00:41.338659  5332 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1001 15:00:41.338793  5332 net.cpp:122] Setting up BatchNorm13
I1001 15:00:41.338798  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.338801  5332 net.cpp:137] Memory required for data: 264365200
I1001 15:00:41.338806  5332 layer_factory.hpp:77] Creating layer Scale13
I1001 15:00:41.338812  5332 net.cpp:84] Creating Layer Scale13
I1001 15:00:41.338815  5332 net.cpp:406] Scale13 <- Convolution13
I1001 15:00:41.338819  5332 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1001 15:00:41.338848  5332 layer_factory.hpp:77] Creating layer Scale13
I1001 15:00:41.338927  5332 net.cpp:122] Setting up Scale13
I1001 15:00:41.338932  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.338934  5332 net.cpp:137] Memory required for data: 266874000
I1001 15:00:41.338938  5332 layer_factory.hpp:77] Creating layer penlu12
I1001 15:00:41.338944  5332 net.cpp:84] Creating Layer penlu12
I1001 15:00:41.338948  5332 net.cpp:406] penlu12 <- Convolution13
I1001 15:00:41.338953  5332 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1001 15:00:41.339058  5332 net.cpp:122] Setting up penlu12
I1001 15:00:41.339063  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.339066  5332 net.cpp:137] Memory required for data: 269382800
I1001 15:00:41.339071  5332 layer_factory.hpp:77] Creating layer Convolution14
I1001 15:00:41.339078  5332 net.cpp:84] Creating Layer Convolution14
I1001 15:00:41.339082  5332 net.cpp:406] Convolution14 <- Convolution13
I1001 15:00:41.339085  5332 net.cpp:380] Convolution14 -> Convolution14
I1001 15:00:41.340147  5332 net.cpp:122] Setting up Convolution14
I1001 15:00:41.340158  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.340162  5332 net.cpp:137] Memory required for data: 271891600
I1001 15:00:41.340176  5332 layer_factory.hpp:77] Creating layer BatchNorm14
I1001 15:00:41.340186  5332 net.cpp:84] Creating Layer BatchNorm14
I1001 15:00:41.340190  5332 net.cpp:406] BatchNorm14 <- Convolution14
I1001 15:00:41.340194  5332 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1001 15:00:41.340327  5332 net.cpp:122] Setting up BatchNorm14
I1001 15:00:41.340333  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.340337  5332 net.cpp:137] Memory required for data: 274400400
I1001 15:00:41.340342  5332 layer_factory.hpp:77] Creating layer Scale14
I1001 15:00:41.340348  5332 net.cpp:84] Creating Layer Scale14
I1001 15:00:41.340350  5332 net.cpp:406] Scale14 <- Convolution14
I1001 15:00:41.340353  5332 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1001 15:00:41.340380  5332 layer_factory.hpp:77] Creating layer Scale14
I1001 15:00:41.340457  5332 net.cpp:122] Setting up Scale14
I1001 15:00:41.340462  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.340464  5332 net.cpp:137] Memory required for data: 276909200
I1001 15:00:41.340468  5332 layer_factory.hpp:77] Creating layer Eltwise6
I1001 15:00:41.340473  5332 net.cpp:84] Creating Layer Eltwise6
I1001 15:00:41.340476  5332 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1001 15:00:41.340479  5332 net.cpp:406] Eltwise6 <- Convolution14
I1001 15:00:41.340482  5332 net.cpp:380] Eltwise6 -> Eltwise6
I1001 15:00:41.340497  5332 net.cpp:122] Setting up Eltwise6
I1001 15:00:41.340502  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.340503  5332 net.cpp:137] Memory required for data: 279418000
I1001 15:00:41.340505  5332 layer_factory.hpp:77] Creating layer penlu13
I1001 15:00:41.340512  5332 net.cpp:84] Creating Layer penlu13
I1001 15:00:41.340513  5332 net.cpp:406] penlu13 <- Eltwise6
I1001 15:00:41.340517  5332 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1001 15:00:41.340623  5332 net.cpp:122] Setting up penlu13
I1001 15:00:41.340627  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.340629  5332 net.cpp:137] Memory required for data: 281926800
I1001 15:00:41.340634  5332 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1001 15:00:41.340646  5332 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1001 15:00:41.340648  5332 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1001 15:00:41.340651  5332 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1001 15:00:41.340656  5332 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1001 15:00:41.340679  5332 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1001 15:00:41.340683  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.340687  5332 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1001 15:00:41.340688  5332 net.cpp:137] Memory required for data: 286944400
I1001 15:00:41.340690  5332 layer_factory.hpp:77] Creating layer Convolution15
I1001 15:00:41.340697  5332 net.cpp:84] Creating Layer Convolution15
I1001 15:00:41.340699  5332 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1001 15:00:41.340703  5332 net.cpp:380] Convolution15 -> Convolution15
I1001 15:00:41.341608  5332 net.cpp:122] Setting up Convolution15
I1001 15:00:41.341616  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.341619  5332 net.cpp:137] Memory required for data: 288198800
I1001 15:00:41.341624  5332 layer_factory.hpp:77] Creating layer BatchNorm15
I1001 15:00:41.341629  5332 net.cpp:84] Creating Layer BatchNorm15
I1001 15:00:41.341631  5332 net.cpp:406] BatchNorm15 <- Convolution15
I1001 15:00:41.341635  5332 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1001 15:00:41.341766  5332 net.cpp:122] Setting up BatchNorm15
I1001 15:00:41.341770  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.341773  5332 net.cpp:137] Memory required for data: 289453200
I1001 15:00:41.341778  5332 layer_factory.hpp:77] Creating layer Scale15
I1001 15:00:41.341783  5332 net.cpp:84] Creating Layer Scale15
I1001 15:00:41.341785  5332 net.cpp:406] Scale15 <- Convolution15
I1001 15:00:41.341789  5332 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1001 15:00:41.341815  5332 layer_factory.hpp:77] Creating layer Scale15
I1001 15:00:41.341891  5332 net.cpp:122] Setting up Scale15
I1001 15:00:41.341894  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.341897  5332 net.cpp:137] Memory required for data: 290707600
I1001 15:00:41.341900  5332 layer_factory.hpp:77] Creating layer Convolution16
I1001 15:00:41.341908  5332 net.cpp:84] Creating Layer Convolution16
I1001 15:00:41.341912  5332 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1001 15:00:41.341915  5332 net.cpp:380] Convolution16 -> Convolution16
I1001 15:00:41.343736  5332 net.cpp:122] Setting up Convolution16
I1001 15:00:41.343746  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.343750  5332 net.cpp:137] Memory required for data: 291962000
I1001 15:00:41.343755  5332 layer_factory.hpp:77] Creating layer BatchNorm16
I1001 15:00:41.343760  5332 net.cpp:84] Creating Layer BatchNorm16
I1001 15:00:41.343762  5332 net.cpp:406] BatchNorm16 <- Convolution16
I1001 15:00:41.343767  5332 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1001 15:00:41.343901  5332 net.cpp:122] Setting up BatchNorm16
I1001 15:00:41.343906  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.343909  5332 net.cpp:137] Memory required for data: 293216400
I1001 15:00:41.343914  5332 layer_factory.hpp:77] Creating layer Scale16
I1001 15:00:41.343919  5332 net.cpp:84] Creating Layer Scale16
I1001 15:00:41.343920  5332 net.cpp:406] Scale16 <- Convolution16
I1001 15:00:41.343924  5332 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1001 15:00:41.343950  5332 layer_factory.hpp:77] Creating layer Scale16
I1001 15:00:41.344027  5332 net.cpp:122] Setting up Scale16
I1001 15:00:41.344030  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.344033  5332 net.cpp:137] Memory required for data: 294470800
I1001 15:00:41.344036  5332 layer_factory.hpp:77] Creating layer penlu14
I1001 15:00:41.344041  5332 net.cpp:84] Creating Layer penlu14
I1001 15:00:41.344045  5332 net.cpp:406] penlu14 <- Convolution16
I1001 15:00:41.344056  5332 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1001 15:00:41.344172  5332 net.cpp:122] Setting up penlu14
I1001 15:00:41.344177  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.344179  5332 net.cpp:137] Memory required for data: 295725200
I1001 15:00:41.344183  5332 layer_factory.hpp:77] Creating layer Convolution17
I1001 15:00:41.344190  5332 net.cpp:84] Creating Layer Convolution17
I1001 15:00:41.344193  5332 net.cpp:406] Convolution17 <- Convolution16
I1001 15:00:41.344197  5332 net.cpp:380] Convolution17 -> Convolution17
I1001 15:00:41.345851  5332 net.cpp:122] Setting up Convolution17
I1001 15:00:41.345860  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.345862  5332 net.cpp:137] Memory required for data: 296979600
I1001 15:00:41.345867  5332 layer_factory.hpp:77] Creating layer BatchNorm17
I1001 15:00:41.345872  5332 net.cpp:84] Creating Layer BatchNorm17
I1001 15:00:41.345875  5332 net.cpp:406] BatchNorm17 <- Convolution17
I1001 15:00:41.345878  5332 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1001 15:00:41.346012  5332 net.cpp:122] Setting up BatchNorm17
I1001 15:00:41.346016  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.346019  5332 net.cpp:137] Memory required for data: 298234000
I1001 15:00:41.346024  5332 layer_factory.hpp:77] Creating layer Scale17
I1001 15:00:41.346029  5332 net.cpp:84] Creating Layer Scale17
I1001 15:00:41.346031  5332 net.cpp:406] Scale17 <- Convolution17
I1001 15:00:41.346035  5332 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1001 15:00:41.346060  5332 layer_factory.hpp:77] Creating layer Scale17
I1001 15:00:41.346137  5332 net.cpp:122] Setting up Scale17
I1001 15:00:41.346140  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.346143  5332 net.cpp:137] Memory required for data: 299488400
I1001 15:00:41.346146  5332 layer_factory.hpp:77] Creating layer Eltwise7
I1001 15:00:41.346151  5332 net.cpp:84] Creating Layer Eltwise7
I1001 15:00:41.346153  5332 net.cpp:406] Eltwise7 <- Convolution15
I1001 15:00:41.346156  5332 net.cpp:406] Eltwise7 <- Convolution17
I1001 15:00:41.346159  5332 net.cpp:380] Eltwise7 -> Eltwise7
I1001 15:00:41.346175  5332 net.cpp:122] Setting up Eltwise7
I1001 15:00:41.346179  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.346180  5332 net.cpp:137] Memory required for data: 300742800
I1001 15:00:41.346182  5332 layer_factory.hpp:77] Creating layer penlu15
I1001 15:00:41.346189  5332 net.cpp:84] Creating Layer penlu15
I1001 15:00:41.346190  5332 net.cpp:406] penlu15 <- Eltwise7
I1001 15:00:41.346194  5332 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1001 15:00:41.346298  5332 net.cpp:122] Setting up penlu15
I1001 15:00:41.346302  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.346304  5332 net.cpp:137] Memory required for data: 301997200
I1001 15:00:41.346309  5332 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1001 15:00:41.346313  5332 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1001 15:00:41.346315  5332 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1001 15:00:41.346318  5332 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1001 15:00:41.346323  5332 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1001 15:00:41.346344  5332 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1001 15:00:41.346349  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.346350  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.346352  5332 net.cpp:137] Memory required for data: 304506000
I1001 15:00:41.346355  5332 layer_factory.hpp:77] Creating layer Convolution18
I1001 15:00:41.346361  5332 net.cpp:84] Creating Layer Convolution18
I1001 15:00:41.346364  5332 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1001 15:00:41.346367  5332 net.cpp:380] Convolution18 -> Convolution18
I1001 15:00:41.348577  5332 net.cpp:122] Setting up Convolution18
I1001 15:00:41.348588  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.348592  5332 net.cpp:137] Memory required for data: 305760400
I1001 15:00:41.348608  5332 layer_factory.hpp:77] Creating layer BatchNorm18
I1001 15:00:41.348618  5332 net.cpp:84] Creating Layer BatchNorm18
I1001 15:00:41.348623  5332 net.cpp:406] BatchNorm18 <- Convolution18
I1001 15:00:41.348628  5332 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1001 15:00:41.348816  5332 net.cpp:122] Setting up BatchNorm18
I1001 15:00:41.348826  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.348829  5332 net.cpp:137] Memory required for data: 307014800
I1001 15:00:41.348839  5332 layer_factory.hpp:77] Creating layer Scale18
I1001 15:00:41.348845  5332 net.cpp:84] Creating Layer Scale18
I1001 15:00:41.348850  5332 net.cpp:406] Scale18 <- Convolution18
I1001 15:00:41.348855  5332 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1001 15:00:41.348893  5332 layer_factory.hpp:77] Creating layer Scale18
I1001 15:00:41.348992  5332 net.cpp:122] Setting up Scale18
I1001 15:00:41.349002  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.349005  5332 net.cpp:137] Memory required for data: 308269200
I1001 15:00:41.349011  5332 layer_factory.hpp:77] Creating layer penlu16
I1001 15:00:41.349020  5332 net.cpp:84] Creating Layer penlu16
I1001 15:00:41.349025  5332 net.cpp:406] penlu16 <- Convolution18
I1001 15:00:41.349040  5332 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1001 15:00:41.349200  5332 net.cpp:122] Setting up penlu16
I1001 15:00:41.349210  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.349215  5332 net.cpp:137] Memory required for data: 309523600
I1001 15:00:41.349231  5332 layer_factory.hpp:77] Creating layer Convolution19
I1001 15:00:41.349242  5332 net.cpp:84] Creating Layer Convolution19
I1001 15:00:41.349246  5332 net.cpp:406] Convolution19 <- Convolution18
I1001 15:00:41.349253  5332 net.cpp:380] Convolution19 -> Convolution19
I1001 15:00:41.351490  5332 net.cpp:122] Setting up Convolution19
I1001 15:00:41.351501  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.351519  5332 net.cpp:137] Memory required for data: 310778000
I1001 15:00:41.351524  5332 layer_factory.hpp:77] Creating layer BatchNorm19
I1001 15:00:41.351529  5332 net.cpp:84] Creating Layer BatchNorm19
I1001 15:00:41.351541  5332 net.cpp:406] BatchNorm19 <- Convolution19
I1001 15:00:41.351547  5332 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1001 15:00:41.351732  5332 net.cpp:122] Setting up BatchNorm19
I1001 15:00:41.351747  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.351750  5332 net.cpp:137] Memory required for data: 312032400
I1001 15:00:41.351755  5332 layer_factory.hpp:77] Creating layer Scale19
I1001 15:00:41.351769  5332 net.cpp:84] Creating Layer Scale19
I1001 15:00:41.351773  5332 net.cpp:406] Scale19 <- Convolution19
I1001 15:00:41.351776  5332 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1001 15:00:41.351830  5332 layer_factory.hpp:77] Creating layer Scale19
I1001 15:00:41.351927  5332 net.cpp:122] Setting up Scale19
I1001 15:00:41.351933  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.351944  5332 net.cpp:137] Memory required for data: 313286800
I1001 15:00:41.351948  5332 layer_factory.hpp:77] Creating layer Eltwise8
I1001 15:00:41.351953  5332 net.cpp:84] Creating Layer Eltwise8
I1001 15:00:41.351955  5332 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1001 15:00:41.351959  5332 net.cpp:406] Eltwise8 <- Convolution19
I1001 15:00:41.351963  5332 net.cpp:380] Eltwise8 -> Eltwise8
I1001 15:00:41.351979  5332 net.cpp:122] Setting up Eltwise8
I1001 15:00:41.351984  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.351985  5332 net.cpp:137] Memory required for data: 314541200
I1001 15:00:41.351997  5332 layer_factory.hpp:77] Creating layer penlu17
I1001 15:00:41.352002  5332 net.cpp:84] Creating Layer penlu17
I1001 15:00:41.352005  5332 net.cpp:406] penlu17 <- Eltwise8
I1001 15:00:41.352020  5332 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1001 15:00:41.352166  5332 net.cpp:122] Setting up penlu17
I1001 15:00:41.352171  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.352190  5332 net.cpp:137] Memory required for data: 315795600
I1001 15:00:41.352195  5332 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1001 15:00:41.352210  5332 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1001 15:00:41.352214  5332 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1001 15:00:41.352217  5332 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1001 15:00:41.352221  5332 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1001 15:00:41.352246  5332 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1001 15:00:41.352250  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.352254  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.352257  5332 net.cpp:137] Memory required for data: 318304400
I1001 15:00:41.352259  5332 layer_factory.hpp:77] Creating layer Convolution20
I1001 15:00:41.352267  5332 net.cpp:84] Creating Layer Convolution20
I1001 15:00:41.352269  5332 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1001 15:00:41.352273  5332 net.cpp:380] Convolution20 -> Convolution20
I1001 15:00:41.353956  5332 net.cpp:122] Setting up Convolution20
I1001 15:00:41.353966  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.353971  5332 net.cpp:137] Memory required for data: 319558800
I1001 15:00:41.353976  5332 layer_factory.hpp:77] Creating layer BatchNorm20
I1001 15:00:41.353981  5332 net.cpp:84] Creating Layer BatchNorm20
I1001 15:00:41.353983  5332 net.cpp:406] BatchNorm20 <- Convolution20
I1001 15:00:41.353988  5332 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1001 15:00:41.354123  5332 net.cpp:122] Setting up BatchNorm20
I1001 15:00:41.354128  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.354130  5332 net.cpp:137] Memory required for data: 320813200
I1001 15:00:41.354135  5332 layer_factory.hpp:77] Creating layer Scale20
I1001 15:00:41.354140  5332 net.cpp:84] Creating Layer Scale20
I1001 15:00:41.354143  5332 net.cpp:406] Scale20 <- Convolution20
I1001 15:00:41.354147  5332 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1001 15:00:41.354173  5332 layer_factory.hpp:77] Creating layer Scale20
I1001 15:00:41.354261  5332 net.cpp:122] Setting up Scale20
I1001 15:00:41.354266  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.354269  5332 net.cpp:137] Memory required for data: 322067600
I1001 15:00:41.354274  5332 layer_factory.hpp:77] Creating layer penlu18
I1001 15:00:41.354280  5332 net.cpp:84] Creating Layer penlu18
I1001 15:00:41.354284  5332 net.cpp:406] penlu18 <- Convolution20
I1001 15:00:41.354287  5332 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1001 15:00:41.354408  5332 net.cpp:122] Setting up penlu18
I1001 15:00:41.354413  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.354414  5332 net.cpp:137] Memory required for data: 323322000
I1001 15:00:41.354419  5332 layer_factory.hpp:77] Creating layer Convolution21
I1001 15:00:41.354425  5332 net.cpp:84] Creating Layer Convolution21
I1001 15:00:41.354427  5332 net.cpp:406] Convolution21 <- Convolution20
I1001 15:00:41.354431  5332 net.cpp:380] Convolution21 -> Convolution21
I1001 15:00:41.356714  5332 net.cpp:122] Setting up Convolution21
I1001 15:00:41.356724  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.356726  5332 net.cpp:137] Memory required for data: 324576400
I1001 15:00:41.356731  5332 layer_factory.hpp:77] Creating layer BatchNorm21
I1001 15:00:41.356735  5332 net.cpp:84] Creating Layer BatchNorm21
I1001 15:00:41.356739  5332 net.cpp:406] BatchNorm21 <- Convolution21
I1001 15:00:41.356742  5332 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1001 15:00:41.356899  5332 net.cpp:122] Setting up BatchNorm21
I1001 15:00:41.356904  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.356906  5332 net.cpp:137] Memory required for data: 325830800
I1001 15:00:41.356911  5332 layer_factory.hpp:77] Creating layer Scale21
I1001 15:00:41.356915  5332 net.cpp:84] Creating Layer Scale21
I1001 15:00:41.356925  5332 net.cpp:406] Scale21 <- Convolution21
I1001 15:00:41.356930  5332 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1001 15:00:41.356956  5332 layer_factory.hpp:77] Creating layer Scale21
I1001 15:00:41.357036  5332 net.cpp:122] Setting up Scale21
I1001 15:00:41.357040  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.357043  5332 net.cpp:137] Memory required for data: 327085200
I1001 15:00:41.357048  5332 layer_factory.hpp:77] Creating layer Eltwise9
I1001 15:00:41.357050  5332 net.cpp:84] Creating Layer Eltwise9
I1001 15:00:41.357053  5332 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1001 15:00:41.357056  5332 net.cpp:406] Eltwise9 <- Convolution21
I1001 15:00:41.357060  5332 net.cpp:380] Eltwise9 -> Eltwise9
I1001 15:00:41.357076  5332 net.cpp:122] Setting up Eltwise9
I1001 15:00:41.357080  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.357082  5332 net.cpp:137] Memory required for data: 328339600
I1001 15:00:41.357084  5332 layer_factory.hpp:77] Creating layer penlu19
I1001 15:00:41.357089  5332 net.cpp:84] Creating Layer penlu19
I1001 15:00:41.357090  5332 net.cpp:406] penlu19 <- Eltwise9
I1001 15:00:41.357095  5332 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1001 15:00:41.357203  5332 net.cpp:122] Setting up penlu19
I1001 15:00:41.357208  5332 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1001 15:00:41.357209  5332 net.cpp:137] Memory required for data: 329594000
I1001 15:00:41.357214  5332 layer_factory.hpp:77] Creating layer Pooling1
I1001 15:00:41.357218  5332 net.cpp:84] Creating Layer Pooling1
I1001 15:00:41.357221  5332 net.cpp:406] Pooling1 <- Eltwise9
I1001 15:00:41.357224  5332 net.cpp:380] Pooling1 -> Pooling1
I1001 15:00:41.357372  5332 net.cpp:122] Setting up Pooling1
I1001 15:00:41.357379  5332 net.cpp:129] Top shape: 100 64 1 1 (6400)
I1001 15:00:41.357381  5332 net.cpp:137] Memory required for data: 329619600
I1001 15:00:41.357383  5332 layer_factory.hpp:77] Creating layer InnerProduct1
I1001 15:00:41.357393  5332 net.cpp:84] Creating Layer InnerProduct1
I1001 15:00:41.357396  5332 net.cpp:406] InnerProduct1 <- Pooling1
I1001 15:00:41.357400  5332 net.cpp:380] InnerProduct1 -> InnerProduct1
I1001 15:00:41.357494  5332 net.cpp:122] Setting up InnerProduct1
I1001 15:00:41.357499  5332 net.cpp:129] Top shape: 100 10 (1000)
I1001 15:00:41.357501  5332 net.cpp:137] Memory required for data: 329623600
I1001 15:00:41.357506  5332 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 15:00:41.357509  5332 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1001 15:00:41.357511  5332 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1001 15:00:41.357514  5332 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1001 15:00:41.357518  5332 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1001 15:00:41.357523  5332 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 15:00:41.358077  5332 net.cpp:122] Setting up SoftmaxWithLoss1
I1001 15:00:41.358084  5332 net.cpp:129] Top shape: (1)
I1001 15:00:41.358086  5332 net.cpp:132]     with loss weight 1
I1001 15:00:41.358098  5332 net.cpp:137] Memory required for data: 329623604
I1001 15:00:41.358101  5332 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1001 15:00:41.358103  5332 net.cpp:198] InnerProduct1 needs backward computation.
I1001 15:00:41.358105  5332 net.cpp:198] Pooling1 needs backward computation.
I1001 15:00:41.358108  5332 net.cpp:198] penlu19 needs backward computation.
I1001 15:00:41.358109  5332 net.cpp:198] Eltwise9 needs backward computation.
I1001 15:00:41.358113  5332 net.cpp:198] Scale21 needs backward computation.
I1001 15:00:41.358114  5332 net.cpp:198] BatchNorm21 needs backward computation.
I1001 15:00:41.358116  5332 net.cpp:198] Convolution21 needs backward computation.
I1001 15:00:41.358119  5332 net.cpp:198] penlu18 needs backward computation.
I1001 15:00:41.358120  5332 net.cpp:198] Scale20 needs backward computation.
I1001 15:00:41.358122  5332 net.cpp:198] BatchNorm20 needs backward computation.
I1001 15:00:41.358124  5332 net.cpp:198] Convolution20 needs backward computation.
I1001 15:00:41.358131  5332 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1001 15:00:41.358134  5332 net.cpp:198] penlu17 needs backward computation.
I1001 15:00:41.358136  5332 net.cpp:198] Eltwise8 needs backward computation.
I1001 15:00:41.358139  5332 net.cpp:198] Scale19 needs backward computation.
I1001 15:00:41.358140  5332 net.cpp:198] BatchNorm19 needs backward computation.
I1001 15:00:41.358142  5332 net.cpp:198] Convolution19 needs backward computation.
I1001 15:00:41.358144  5332 net.cpp:198] penlu16 needs backward computation.
I1001 15:00:41.358146  5332 net.cpp:198] Scale18 needs backward computation.
I1001 15:00:41.358148  5332 net.cpp:198] BatchNorm18 needs backward computation.
I1001 15:00:41.358150  5332 net.cpp:198] Convolution18 needs backward computation.
I1001 15:00:41.358152  5332 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1001 15:00:41.358155  5332 net.cpp:198] penlu15 needs backward computation.
I1001 15:00:41.358157  5332 net.cpp:198] Eltwise7 needs backward computation.
I1001 15:00:41.358160  5332 net.cpp:198] Scale17 needs backward computation.
I1001 15:00:41.358161  5332 net.cpp:198] BatchNorm17 needs backward computation.
I1001 15:00:41.358165  5332 net.cpp:198] Convolution17 needs backward computation.
I1001 15:00:41.358166  5332 net.cpp:198] penlu14 needs backward computation.
I1001 15:00:41.358168  5332 net.cpp:198] Scale16 needs backward computation.
I1001 15:00:41.358170  5332 net.cpp:198] BatchNorm16 needs backward computation.
I1001 15:00:41.358172  5332 net.cpp:198] Convolution16 needs backward computation.
I1001 15:00:41.358175  5332 net.cpp:198] Scale15 needs backward computation.
I1001 15:00:41.358177  5332 net.cpp:198] BatchNorm15 needs backward computation.
I1001 15:00:41.358180  5332 net.cpp:198] Convolution15 needs backward computation.
I1001 15:00:41.358182  5332 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1001 15:00:41.358184  5332 net.cpp:198] penlu13 needs backward computation.
I1001 15:00:41.358186  5332 net.cpp:198] Eltwise6 needs backward computation.
I1001 15:00:41.358188  5332 net.cpp:198] Scale14 needs backward computation.
I1001 15:00:41.358191  5332 net.cpp:198] BatchNorm14 needs backward computation.
I1001 15:00:41.358192  5332 net.cpp:198] Convolution14 needs backward computation.
I1001 15:00:41.358196  5332 net.cpp:198] penlu12 needs backward computation.
I1001 15:00:41.358197  5332 net.cpp:198] Scale13 needs backward computation.
I1001 15:00:41.358199  5332 net.cpp:198] BatchNorm13 needs backward computation.
I1001 15:00:41.358201  5332 net.cpp:198] Convolution13 needs backward computation.
I1001 15:00:41.358203  5332 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1001 15:00:41.358208  5332 net.cpp:198] penlu11 needs backward computation.
I1001 15:00:41.358211  5332 net.cpp:198] Eltwise5 needs backward computation.
I1001 15:00:41.358213  5332 net.cpp:198] Scale12 needs backward computation.
I1001 15:00:41.358217  5332 net.cpp:198] BatchNorm12 needs backward computation.
I1001 15:00:41.358218  5332 net.cpp:198] Convolution12 needs backward computation.
I1001 15:00:41.358220  5332 net.cpp:198] penlu10 needs backward computation.
I1001 15:00:41.358222  5332 net.cpp:198] Scale11 needs backward computation.
I1001 15:00:41.358224  5332 net.cpp:198] BatchNorm11 needs backward computation.
I1001 15:00:41.358227  5332 net.cpp:198] Convolution11 needs backward computation.
I1001 15:00:41.358229  5332 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1001 15:00:41.358232  5332 net.cpp:198] penlu9 needs backward computation.
I1001 15:00:41.358233  5332 net.cpp:198] Eltwise4 needs backward computation.
I1001 15:00:41.358237  5332 net.cpp:198] Scale10 needs backward computation.
I1001 15:00:41.358238  5332 net.cpp:198] BatchNorm10 needs backward computation.
I1001 15:00:41.358240  5332 net.cpp:198] Convolution10 needs backward computation.
I1001 15:00:41.358243  5332 net.cpp:198] penlu8 needs backward computation.
I1001 15:00:41.358245  5332 net.cpp:198] Scale9 needs backward computation.
I1001 15:00:41.358252  5332 net.cpp:198] BatchNorm9 needs backward computation.
I1001 15:00:41.358253  5332 net.cpp:198] Convolution9 needs backward computation.
I1001 15:00:41.358256  5332 net.cpp:198] Scale8 needs backward computation.
I1001 15:00:41.358258  5332 net.cpp:198] BatchNorm8 needs backward computation.
I1001 15:00:41.358260  5332 net.cpp:198] Convolution8 needs backward computation.
I1001 15:00:41.358263  5332 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1001 15:00:41.358265  5332 net.cpp:198] penlu7 needs backward computation.
I1001 15:00:41.358268  5332 net.cpp:198] Eltwise3 needs backward computation.
I1001 15:00:41.358270  5332 net.cpp:198] Scale7 needs backward computation.
I1001 15:00:41.358273  5332 net.cpp:198] BatchNorm7 needs backward computation.
I1001 15:00:41.358274  5332 net.cpp:198] Convolution7 needs backward computation.
I1001 15:00:41.358276  5332 net.cpp:198] penlu6 needs backward computation.
I1001 15:00:41.358278  5332 net.cpp:198] Scale6 needs backward computation.
I1001 15:00:41.358280  5332 net.cpp:198] BatchNorm6 needs backward computation.
I1001 15:00:41.358283  5332 net.cpp:198] Convolution6 needs backward computation.
I1001 15:00:41.358285  5332 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1001 15:00:41.358288  5332 net.cpp:198] penlu5 needs backward computation.
I1001 15:00:41.358289  5332 net.cpp:198] Eltwise2 needs backward computation.
I1001 15:00:41.358292  5332 net.cpp:198] Scale5 needs backward computation.
I1001 15:00:41.358294  5332 net.cpp:198] BatchNorm5 needs backward computation.
I1001 15:00:41.358296  5332 net.cpp:198] Convolution5 needs backward computation.
I1001 15:00:41.358299  5332 net.cpp:198] penlu4 needs backward computation.
I1001 15:00:41.358301  5332 net.cpp:198] Scale4 needs backward computation.
I1001 15:00:41.358304  5332 net.cpp:198] BatchNorm4 needs backward computation.
I1001 15:00:41.358305  5332 net.cpp:198] Convolution4 needs backward computation.
I1001 15:00:41.358309  5332 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1001 15:00:41.358310  5332 net.cpp:198] penlu3 needs backward computation.
I1001 15:00:41.358312  5332 net.cpp:198] Eltwise1 needs backward computation.
I1001 15:00:41.358315  5332 net.cpp:198] Scale3 needs backward computation.
I1001 15:00:41.358317  5332 net.cpp:198] BatchNorm3 needs backward computation.
I1001 15:00:41.358319  5332 net.cpp:198] Convolution3 needs backward computation.
I1001 15:00:41.358322  5332 net.cpp:198] penlu2 needs backward computation.
I1001 15:00:41.358325  5332 net.cpp:198] Scale2 needs backward computation.
I1001 15:00:41.358326  5332 net.cpp:198] BatchNorm2 needs backward computation.
I1001 15:00:41.358328  5332 net.cpp:198] Convolution2 needs backward computation.
I1001 15:00:41.358331  5332 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1001 15:00:41.358333  5332 net.cpp:198] penlu1 needs backward computation.
I1001 15:00:41.358335  5332 net.cpp:198] Scale1 needs backward computation.
I1001 15:00:41.358338  5332 net.cpp:198] BatchNorm1 needs backward computation.
I1001 15:00:41.358340  5332 net.cpp:198] Convolution1 needs backward computation.
I1001 15:00:41.358342  5332 net.cpp:200] Data1 does not need backward computation.
I1001 15:00:41.358345  5332 net.cpp:242] This network produces output SoftmaxWithLoss1
I1001 15:00:41.358377  5332 net.cpp:255] Network initialization done.
I1001 15:00:41.360069  5332 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 15:00:41.360077  5332 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1001 15:00:41.360081  5332 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/PENLU/neural/resnet/res20/res20_penlu_gauss.prototxt
I1001 15:00:41.360157  5332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I1001 15:00:41.360625  5332 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 1
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I1001 15:00:41.360874  5332 layer_factory.hpp:77] Creating layer Data1
I1001 15:00:41.360911  5332 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb
I1001 15:00:41.360921  5332 net.cpp:84] Creating Layer Data1
I1001 15:00:41.360926  5332 net.cpp:380] Data1 -> Data1
I1001 15:00:41.360932  5332 net.cpp:380] Data1 -> Data2
I1001 15:00:41.360937  5332 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I1001 15:00:41.361047  5332 data_layer.cpp:45] output data size: 100,3,32,32
I1001 15:00:41.364815  5332 net.cpp:122] Setting up Data1
I1001 15:00:41.364835  5332 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1001 15:00:41.364840  5332 net.cpp:129] Top shape: 100 (100)
I1001 15:00:41.364841  5332 net.cpp:137] Memory required for data: 1229200
I1001 15:00:41.364845  5332 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1001 15:00:41.364853  5332 net.cpp:84] Creating Layer Data2_Data1_1_split
I1001 15:00:41.364857  5332 net.cpp:406] Data2_Data1_1_split <- Data2
I1001 15:00:41.364861  5332 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1001 15:00:41.364868  5332 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1001 15:00:41.364938  5332 net.cpp:122] Setting up Data2_Data1_1_split
I1001 15:00:41.364943  5332 net.cpp:129] Top shape: 100 (100)
I1001 15:00:41.364958  5332 net.cpp:129] Top shape: 100 (100)
I1001 15:00:41.364959  5332 net.cpp:137] Memory required for data: 1230000
I1001 15:00:41.364962  5332 layer_factory.hpp:77] Creating layer Convolution1
I1001 15:00:41.364971  5332 net.cpp:84] Creating Layer Convolution1
I1001 15:00:41.364974  5332 net.cpp:406] Convolution1 <- Data1
I1001 15:00:41.364979  5332 net.cpp:380] Convolution1 -> Convolution1
I1001 15:00:41.366129  5332 net.cpp:122] Setting up Convolution1
I1001 15:00:41.366139  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.366142  5332 net.cpp:137] Memory required for data: 7783600
I1001 15:00:41.366149  5332 layer_factory.hpp:77] Creating layer BatchNorm1
I1001 15:00:41.366155  5332 net.cpp:84] Creating Layer BatchNorm1
I1001 15:00:41.366158  5332 net.cpp:406] BatchNorm1 <- Convolution1
I1001 15:00:41.366163  5332 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1001 15:00:41.366300  5332 net.cpp:122] Setting up BatchNorm1
I1001 15:00:41.366305  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.366307  5332 net.cpp:137] Memory required for data: 14337200
I1001 15:00:41.366314  5332 layer_factory.hpp:77] Creating layer Scale1
I1001 15:00:41.366322  5332 net.cpp:84] Creating Layer Scale1
I1001 15:00:41.366325  5332 net.cpp:406] Scale1 <- Convolution1
I1001 15:00:41.366328  5332 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1001 15:00:41.366356  5332 layer_factory.hpp:77] Creating layer Scale1
I1001 15:00:41.366432  5332 net.cpp:122] Setting up Scale1
I1001 15:00:41.366437  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.366439  5332 net.cpp:137] Memory required for data: 20890800
I1001 15:00:41.366443  5332 layer_factory.hpp:77] Creating layer penlu1
I1001 15:00:41.366451  5332 net.cpp:84] Creating Layer penlu1
I1001 15:00:41.366452  5332 net.cpp:406] penlu1 <- Convolution1
I1001 15:00:41.366456  5332 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1001 15:00:41.366607  5332 net.cpp:122] Setting up penlu1
I1001 15:00:41.366613  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.366616  5332 net.cpp:137] Memory required for data: 27444400
I1001 15:00:41.366622  5332 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1001 15:00:41.366626  5332 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1001 15:00:41.366629  5332 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1001 15:00:41.366632  5332 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1001 15:00:41.366636  5332 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1001 15:00:41.366662  5332 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1001 15:00:41.366665  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.366668  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.366670  5332 net.cpp:137] Memory required for data: 40551600
I1001 15:00:41.366677  5332 layer_factory.hpp:77] Creating layer Convolution2
I1001 15:00:41.366683  5332 net.cpp:84] Creating Layer Convolution2
I1001 15:00:41.366688  5332 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1001 15:00:41.366693  5332 net.cpp:380] Convolution2 -> Convolution2
I1001 15:00:41.367296  5332 net.cpp:122] Setting up Convolution2
I1001 15:00:41.367303  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.367306  5332 net.cpp:137] Memory required for data: 47105200
I1001 15:00:41.367311  5332 layer_factory.hpp:77] Creating layer BatchNorm2
I1001 15:00:41.367317  5332 net.cpp:84] Creating Layer BatchNorm2
I1001 15:00:41.367321  5332 net.cpp:406] BatchNorm2 <- Convolution2
I1001 15:00:41.367324  5332 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1001 15:00:41.367561  5332 net.cpp:122] Setting up BatchNorm2
I1001 15:00:41.367566  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.367568  5332 net.cpp:137] Memory required for data: 53658800
I1001 15:00:41.367573  5332 layer_factory.hpp:77] Creating layer Scale2
I1001 15:00:41.367578  5332 net.cpp:84] Creating Layer Scale2
I1001 15:00:41.367588  5332 net.cpp:406] Scale2 <- Convolution2
I1001 15:00:41.367591  5332 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1001 15:00:41.367619  5332 layer_factory.hpp:77] Creating layer Scale2
I1001 15:00:41.367697  5332 net.cpp:122] Setting up Scale2
I1001 15:00:41.367702  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.367703  5332 net.cpp:137] Memory required for data: 60212400
I1001 15:00:41.367710  5332 layer_factory.hpp:77] Creating layer penlu2
I1001 15:00:41.367715  5332 net.cpp:84] Creating Layer penlu2
I1001 15:00:41.367718  5332 net.cpp:406] penlu2 <- Convolution2
I1001 15:00:41.367723  5332 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1001 15:00:41.367838  5332 net.cpp:122] Setting up penlu2
I1001 15:00:41.367843  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.367846  5332 net.cpp:137] Memory required for data: 66766000
I1001 15:00:41.367849  5332 layer_factory.hpp:77] Creating layer Convolution3
I1001 15:00:41.367856  5332 net.cpp:84] Creating Layer Convolution3
I1001 15:00:41.367859  5332 net.cpp:406] Convolution3 <- Convolution2
I1001 15:00:41.367863  5332 net.cpp:380] Convolution3 -> Convolution3
I1001 15:00:41.368930  5332 net.cpp:122] Setting up Convolution3
I1001 15:00:41.368939  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.368943  5332 net.cpp:137] Memory required for data: 73319600
I1001 15:00:41.368948  5332 layer_factory.hpp:77] Creating layer BatchNorm3
I1001 15:00:41.368953  5332 net.cpp:84] Creating Layer BatchNorm3
I1001 15:00:41.368957  5332 net.cpp:406] BatchNorm3 <- Convolution3
I1001 15:00:41.368962  5332 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1001 15:00:41.369096  5332 net.cpp:122] Setting up BatchNorm3
I1001 15:00:41.369104  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.369107  5332 net.cpp:137] Memory required for data: 79873200
I1001 15:00:41.369112  5332 layer_factory.hpp:77] Creating layer Scale3
I1001 15:00:41.369117  5332 net.cpp:84] Creating Layer Scale3
I1001 15:00:41.369118  5332 net.cpp:406] Scale3 <- Convolution3
I1001 15:00:41.369122  5332 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1001 15:00:41.369154  5332 layer_factory.hpp:77] Creating layer Scale3
I1001 15:00:41.369231  5332 net.cpp:122] Setting up Scale3
I1001 15:00:41.369236  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.369238  5332 net.cpp:137] Memory required for data: 86426800
I1001 15:00:41.369242  5332 layer_factory.hpp:77] Creating layer Eltwise1
I1001 15:00:41.369246  5332 net.cpp:84] Creating Layer Eltwise1
I1001 15:00:41.369251  5332 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1001 15:00:41.369253  5332 net.cpp:406] Eltwise1 <- Convolution3
I1001 15:00:41.369256  5332 net.cpp:380] Eltwise1 -> Eltwise1
I1001 15:00:41.369273  5332 net.cpp:122] Setting up Eltwise1
I1001 15:00:41.369279  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.369282  5332 net.cpp:137] Memory required for data: 92980400
I1001 15:00:41.369284  5332 layer_factory.hpp:77] Creating layer penlu3
I1001 15:00:41.369292  5332 net.cpp:84] Creating Layer penlu3
I1001 15:00:41.369295  5332 net.cpp:406] penlu3 <- Eltwise1
I1001 15:00:41.369299  5332 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1001 15:00:41.369418  5332 net.cpp:122] Setting up penlu3
I1001 15:00:41.369422  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.369424  5332 net.cpp:137] Memory required for data: 99534000
I1001 15:00:41.369428  5332 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1001 15:00:41.369433  5332 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1001 15:00:41.369436  5332 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1001 15:00:41.369438  5332 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1001 15:00:41.369442  5332 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1001 15:00:41.369467  5332 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1001 15:00:41.369470  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.369478  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.369482  5332 net.cpp:137] Memory required for data: 112641200
I1001 15:00:41.369483  5332 layer_factory.hpp:77] Creating layer Convolution4
I1001 15:00:41.369490  5332 net.cpp:84] Creating Layer Convolution4
I1001 15:00:41.369493  5332 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1001 15:00:41.369496  5332 net.cpp:380] Convolution4 -> Convolution4
I1001 15:00:41.370584  5332 net.cpp:122] Setting up Convolution4
I1001 15:00:41.370592  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.370595  5332 net.cpp:137] Memory required for data: 119194800
I1001 15:00:41.370600  5332 layer_factory.hpp:77] Creating layer BatchNorm4
I1001 15:00:41.370605  5332 net.cpp:84] Creating Layer BatchNorm4
I1001 15:00:41.370609  5332 net.cpp:406] BatchNorm4 <- Convolution4
I1001 15:00:41.370612  5332 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1001 15:00:41.370748  5332 net.cpp:122] Setting up BatchNorm4
I1001 15:00:41.370754  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.370755  5332 net.cpp:137] Memory required for data: 125748400
I1001 15:00:41.370762  5332 layer_factory.hpp:77] Creating layer Scale4
I1001 15:00:41.370767  5332 net.cpp:84] Creating Layer Scale4
I1001 15:00:41.370774  5332 net.cpp:406] Scale4 <- Convolution4
I1001 15:00:41.370779  5332 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1001 15:00:41.370807  5332 layer_factory.hpp:77] Creating layer Scale4
I1001 15:00:41.370883  5332 net.cpp:122] Setting up Scale4
I1001 15:00:41.370887  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.370890  5332 net.cpp:137] Memory required for data: 132302000
I1001 15:00:41.370893  5332 layer_factory.hpp:77] Creating layer penlu4
I1001 15:00:41.370899  5332 net.cpp:84] Creating Layer penlu4
I1001 15:00:41.370901  5332 net.cpp:406] penlu4 <- Convolution4
I1001 15:00:41.370905  5332 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1001 15:00:41.371023  5332 net.cpp:122] Setting up penlu4
I1001 15:00:41.371027  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.371031  5332 net.cpp:137] Memory required for data: 138855600
I1001 15:00:41.371034  5332 layer_factory.hpp:77] Creating layer Convolution5
I1001 15:00:41.371042  5332 net.cpp:84] Creating Layer Convolution5
I1001 15:00:41.371043  5332 net.cpp:406] Convolution5 <- Convolution4
I1001 15:00:41.371047  5332 net.cpp:380] Convolution5 -> Convolution5
I1001 15:00:41.372304  5332 net.cpp:122] Setting up Convolution5
I1001 15:00:41.372313  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.372316  5332 net.cpp:137] Memory required for data: 145409200
I1001 15:00:41.372321  5332 layer_factory.hpp:77] Creating layer BatchNorm5
I1001 15:00:41.372325  5332 net.cpp:84] Creating Layer BatchNorm5
I1001 15:00:41.372328  5332 net.cpp:406] BatchNorm5 <- Convolution5
I1001 15:00:41.372333  5332 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1001 15:00:41.372474  5332 net.cpp:122] Setting up BatchNorm5
I1001 15:00:41.372478  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.372481  5332 net.cpp:137] Memory required for data: 151962800
I1001 15:00:41.372485  5332 layer_factory.hpp:77] Creating layer Scale5
I1001 15:00:41.372489  5332 net.cpp:84] Creating Layer Scale5
I1001 15:00:41.372493  5332 net.cpp:406] Scale5 <- Convolution5
I1001 15:00:41.372495  5332 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1001 15:00:41.372522  5332 layer_factory.hpp:77] Creating layer Scale5
I1001 15:00:41.372601  5332 net.cpp:122] Setting up Scale5
I1001 15:00:41.372604  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.372607  5332 net.cpp:137] Memory required for data: 158516400
I1001 15:00:41.372611  5332 layer_factory.hpp:77] Creating layer Eltwise2
I1001 15:00:41.372615  5332 net.cpp:84] Creating Layer Eltwise2
I1001 15:00:41.372618  5332 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1001 15:00:41.372622  5332 net.cpp:406] Eltwise2 <- Convolution5
I1001 15:00:41.372624  5332 net.cpp:380] Eltwise2 -> Eltwise2
I1001 15:00:41.372648  5332 net.cpp:122] Setting up Eltwise2
I1001 15:00:41.372653  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.372654  5332 net.cpp:137] Memory required for data: 165070000
I1001 15:00:41.372656  5332 layer_factory.hpp:77] Creating layer penlu5
I1001 15:00:41.372663  5332 net.cpp:84] Creating Layer penlu5
I1001 15:00:41.372664  5332 net.cpp:406] penlu5 <- Eltwise2
I1001 15:00:41.372668  5332 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1001 15:00:41.372786  5332 net.cpp:122] Setting up penlu5
I1001 15:00:41.372790  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.372792  5332 net.cpp:137] Memory required for data: 171623600
I1001 15:00:41.372797  5332 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1001 15:00:41.372800  5332 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1001 15:00:41.372802  5332 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1001 15:00:41.372807  5332 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1001 15:00:41.372810  5332 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1001 15:00:41.372835  5332 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1001 15:00:41.372839  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.372843  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.372844  5332 net.cpp:137] Memory required for data: 184730800
I1001 15:00:41.372846  5332 layer_factory.hpp:77] Creating layer Convolution6
I1001 15:00:41.372853  5332 net.cpp:84] Creating Layer Convolution6
I1001 15:00:41.372854  5332 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1001 15:00:41.372859  5332 net.cpp:380] Convolution6 -> Convolution6
I1001 15:00:41.373808  5332 net.cpp:122] Setting up Convolution6
I1001 15:00:41.373817  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.373821  5332 net.cpp:137] Memory required for data: 191284400
I1001 15:00:41.373824  5332 layer_factory.hpp:77] Creating layer BatchNorm6
I1001 15:00:41.373831  5332 net.cpp:84] Creating Layer BatchNorm6
I1001 15:00:41.373832  5332 net.cpp:406] BatchNorm6 <- Convolution6
I1001 15:00:41.373837  5332 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1001 15:00:41.373976  5332 net.cpp:122] Setting up BatchNorm6
I1001 15:00:41.373980  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.373983  5332 net.cpp:137] Memory required for data: 197838000
I1001 15:00:41.373987  5332 layer_factory.hpp:77] Creating layer Scale6
I1001 15:00:41.373992  5332 net.cpp:84] Creating Layer Scale6
I1001 15:00:41.373996  5332 net.cpp:406] Scale6 <- Convolution6
I1001 15:00:41.373998  5332 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1001 15:00:41.374025  5332 layer_factory.hpp:77] Creating layer Scale6
I1001 15:00:41.374101  5332 net.cpp:122] Setting up Scale6
I1001 15:00:41.374106  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.374109  5332 net.cpp:137] Memory required for data: 204391600
I1001 15:00:41.374112  5332 layer_factory.hpp:77] Creating layer penlu6
I1001 15:00:41.374117  5332 net.cpp:84] Creating Layer penlu6
I1001 15:00:41.374119  5332 net.cpp:406] penlu6 <- Convolution6
I1001 15:00:41.374124  5332 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1001 15:00:41.374240  5332 net.cpp:122] Setting up penlu6
I1001 15:00:41.374245  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.374248  5332 net.cpp:137] Memory required for data: 210945200
I1001 15:00:41.374253  5332 layer_factory.hpp:77] Creating layer Convolution7
I1001 15:00:41.374258  5332 net.cpp:84] Creating Layer Convolution7
I1001 15:00:41.374260  5332 net.cpp:406] Convolution7 <- Convolution6
I1001 15:00:41.374265  5332 net.cpp:380] Convolution7 -> Convolution7
I1001 15:00:41.375207  5332 net.cpp:122] Setting up Convolution7
I1001 15:00:41.375216  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.375219  5332 net.cpp:137] Memory required for data: 217498800
I1001 15:00:41.375223  5332 layer_factory.hpp:77] Creating layer BatchNorm7
I1001 15:00:41.375236  5332 net.cpp:84] Creating Layer BatchNorm7
I1001 15:00:41.375239  5332 net.cpp:406] BatchNorm7 <- Convolution7
I1001 15:00:41.375243  5332 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1001 15:00:41.375383  5332 net.cpp:122] Setting up BatchNorm7
I1001 15:00:41.375388  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.375391  5332 net.cpp:137] Memory required for data: 224052400
I1001 15:00:41.375401  5332 layer_factory.hpp:77] Creating layer Scale7
I1001 15:00:41.375404  5332 net.cpp:84] Creating Layer Scale7
I1001 15:00:41.375406  5332 net.cpp:406] Scale7 <- Convolution7
I1001 15:00:41.375411  5332 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1001 15:00:41.375438  5332 layer_factory.hpp:77] Creating layer Scale7
I1001 15:00:41.375514  5332 net.cpp:122] Setting up Scale7
I1001 15:00:41.375519  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.375521  5332 net.cpp:137] Memory required for data: 230606000
I1001 15:00:41.375524  5332 layer_factory.hpp:77] Creating layer Eltwise3
I1001 15:00:41.375529  5332 net.cpp:84] Creating Layer Eltwise3
I1001 15:00:41.375530  5332 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1001 15:00:41.375533  5332 net.cpp:406] Eltwise3 <- Convolution7
I1001 15:00:41.375537  5332 net.cpp:380] Eltwise3 -> Eltwise3
I1001 15:00:41.375552  5332 net.cpp:122] Setting up Eltwise3
I1001 15:00:41.375556  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.375558  5332 net.cpp:137] Memory required for data: 237159600
I1001 15:00:41.375560  5332 layer_factory.hpp:77] Creating layer penlu7
I1001 15:00:41.375566  5332 net.cpp:84] Creating Layer penlu7
I1001 15:00:41.375567  5332 net.cpp:406] penlu7 <- Eltwise3
I1001 15:00:41.375571  5332 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1001 15:00:41.375686  5332 net.cpp:122] Setting up penlu7
I1001 15:00:41.375690  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.375692  5332 net.cpp:137] Memory required for data: 243713200
I1001 15:00:41.375696  5332 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1001 15:00:41.375700  5332 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1001 15:00:41.375702  5332 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1001 15:00:41.375706  5332 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1001 15:00:41.375711  5332 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1001 15:00:41.375733  5332 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1001 15:00:41.375737  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.375739  5332 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I1001 15:00:41.375741  5332 net.cpp:137] Memory required for data: 256820400
I1001 15:00:41.375743  5332 layer_factory.hpp:77] Creating layer Convolution8
I1001 15:00:41.375749  5332 net.cpp:84] Creating Layer Convolution8
I1001 15:00:41.375752  5332 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1001 15:00:41.375756  5332 net.cpp:380] Convolution8 -> Convolution8
I1001 15:00:41.376631  5332 net.cpp:122] Setting up Convolution8
I1001 15:00:41.376641  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.376642  5332 net.cpp:137] Memory required for data: 260097200
I1001 15:00:41.376647  5332 layer_factory.hpp:77] Creating layer BatchNorm8
I1001 15:00:41.376652  5332 net.cpp:84] Creating Layer BatchNorm8
I1001 15:00:41.376654  5332 net.cpp:406] BatchNorm8 <- Convolution8
I1001 15:00:41.376658  5332 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1001 15:00:41.376790  5332 net.cpp:122] Setting up BatchNorm8
I1001 15:00:41.376793  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.376796  5332 net.cpp:137] Memory required for data: 263374000
I1001 15:00:41.376801  5332 layer_factory.hpp:77] Creating layer Scale8
I1001 15:00:41.376806  5332 net.cpp:84] Creating Layer Scale8
I1001 15:00:41.380597  5332 net.cpp:406] Scale8 <- Convolution8
I1001 15:00:41.380606  5332 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1001 15:00:41.380656  5332 layer_factory.hpp:77] Creating layer Scale8
I1001 15:00:41.380786  5332 net.cpp:122] Setting up Scale8
I1001 15:00:41.380795  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.380798  5332 net.cpp:137] Memory required for data: 266650800
I1001 15:00:41.380806  5332 layer_factory.hpp:77] Creating layer Convolution9
I1001 15:00:41.380817  5332 net.cpp:84] Creating Layer Convolution9
I1001 15:00:41.380822  5332 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1001 15:00:41.380831  5332 net.cpp:380] Convolution9 -> Convolution9
I1001 15:00:41.382323  5332 net.cpp:122] Setting up Convolution9
I1001 15:00:41.382333  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.382339  5332 net.cpp:137] Memory required for data: 269927600
I1001 15:00:41.382345  5332 layer_factory.hpp:77] Creating layer BatchNorm9
I1001 15:00:41.382352  5332 net.cpp:84] Creating Layer BatchNorm9
I1001 15:00:41.382357  5332 net.cpp:406] BatchNorm9 <- Convolution9
I1001 15:00:41.382362  5332 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1001 15:00:41.382578  5332 net.cpp:122] Setting up BatchNorm9
I1001 15:00:41.382588  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.382591  5332 net.cpp:137] Memory required for data: 273204400
I1001 15:00:41.382599  5332 layer_factory.hpp:77] Creating layer Scale9
I1001 15:00:41.382606  5332 net.cpp:84] Creating Layer Scale9
I1001 15:00:41.382609  5332 net.cpp:406] Scale9 <- Convolution9
I1001 15:00:41.382614  5332 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1001 15:00:41.382656  5332 layer_factory.hpp:77] Creating layer Scale9
I1001 15:00:41.382774  5332 net.cpp:122] Setting up Scale9
I1001 15:00:41.382782  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.382786  5332 net.cpp:137] Memory required for data: 276481200
I1001 15:00:41.382792  5332 layer_factory.hpp:77] Creating layer penlu8
I1001 15:00:41.382800  5332 net.cpp:84] Creating Layer penlu8
I1001 15:00:41.382804  5332 net.cpp:406] penlu8 <- Convolution9
I1001 15:00:41.382812  5332 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1001 15:00:41.382985  5332 net.cpp:122] Setting up penlu8
I1001 15:00:41.382992  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.382994  5332 net.cpp:137] Memory required for data: 279758000
I1001 15:00:41.382999  5332 layer_factory.hpp:77] Creating layer Convolution10
I1001 15:00:41.383008  5332 net.cpp:84] Creating Layer Convolution10
I1001 15:00:41.383010  5332 net.cpp:406] Convolution10 <- Convolution9
I1001 15:00:41.383014  5332 net.cpp:380] Convolution10 -> Convolution10
I1001 15:00:41.384428  5332 net.cpp:122] Setting up Convolution10
I1001 15:00:41.384439  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.384443  5332 net.cpp:137] Memory required for data: 283034800
I1001 15:00:41.384449  5332 layer_factory.hpp:77] Creating layer BatchNorm10
I1001 15:00:41.384454  5332 net.cpp:84] Creating Layer BatchNorm10
I1001 15:00:41.384457  5332 net.cpp:406] BatchNorm10 <- Convolution10
I1001 15:00:41.384462  5332 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1001 15:00:41.384661  5332 net.cpp:122] Setting up BatchNorm10
I1001 15:00:41.384668  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.384671  5332 net.cpp:137] Memory required for data: 286311600
I1001 15:00:41.384676  5332 layer_factory.hpp:77] Creating layer Scale10
I1001 15:00:41.384682  5332 net.cpp:84] Creating Layer Scale10
I1001 15:00:41.384685  5332 net.cpp:406] Scale10 <- Convolution10
I1001 15:00:41.384688  5332 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1001 15:00:41.384718  5332 layer_factory.hpp:77] Creating layer Scale10
I1001 15:00:41.384806  5332 net.cpp:122] Setting up Scale10
I1001 15:00:41.384810  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.384812  5332 net.cpp:137] Memory required for data: 289588400
I1001 15:00:41.384816  5332 layer_factory.hpp:77] Creating layer Eltwise4
I1001 15:00:41.384820  5332 net.cpp:84] Creating Layer Eltwise4
I1001 15:00:41.384824  5332 net.cpp:406] Eltwise4 <- Convolution8
I1001 15:00:41.384826  5332 net.cpp:406] Eltwise4 <- Convolution10
I1001 15:00:41.384837  5332 net.cpp:380] Eltwise4 -> Eltwise4
I1001 15:00:41.384851  5332 net.cpp:122] Setting up Eltwise4
I1001 15:00:41.384856  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.384858  5332 net.cpp:137] Memory required for data: 292865200
I1001 15:00:41.384860  5332 layer_factory.hpp:77] Creating layer penlu9
I1001 15:00:41.384865  5332 net.cpp:84] Creating Layer penlu9
I1001 15:00:41.384867  5332 net.cpp:406] penlu9 <- Eltwise4
I1001 15:00:41.384872  5332 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1001 15:00:41.384987  5332 net.cpp:122] Setting up penlu9
I1001 15:00:41.384991  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.384994  5332 net.cpp:137] Memory required for data: 296142000
I1001 15:00:41.384997  5332 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1001 15:00:41.385001  5332 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1001 15:00:41.385004  5332 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1001 15:00:41.385006  5332 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1001 15:00:41.385010  5332 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1001 15:00:41.385035  5332 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1001 15:00:41.385038  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.385040  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.385043  5332 net.cpp:137] Memory required for data: 302695600
I1001 15:00:41.385046  5332 layer_factory.hpp:77] Creating layer Convolution11
I1001 15:00:41.385051  5332 net.cpp:84] Creating Layer Convolution11
I1001 15:00:41.385053  5332 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1001 15:00:41.385057  5332 net.cpp:380] Convolution11 -> Convolution11
I1001 15:00:41.386137  5332 net.cpp:122] Setting up Convolution11
I1001 15:00:41.386147  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.386149  5332 net.cpp:137] Memory required for data: 305972400
I1001 15:00:41.386153  5332 layer_factory.hpp:77] Creating layer BatchNorm11
I1001 15:00:41.386158  5332 net.cpp:84] Creating Layer BatchNorm11
I1001 15:00:41.386162  5332 net.cpp:406] BatchNorm11 <- Convolution11
I1001 15:00:41.386167  5332 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1001 15:00:41.386304  5332 net.cpp:122] Setting up BatchNorm11
I1001 15:00:41.386308  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.386310  5332 net.cpp:137] Memory required for data: 309249200
I1001 15:00:41.386315  5332 layer_factory.hpp:77] Creating layer Scale11
I1001 15:00:41.386319  5332 net.cpp:84] Creating Layer Scale11
I1001 15:00:41.386322  5332 net.cpp:406] Scale11 <- Convolution11
I1001 15:00:41.386325  5332 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1001 15:00:41.386353  5332 layer_factory.hpp:77] Creating layer Scale11
I1001 15:00:41.386430  5332 net.cpp:122] Setting up Scale11
I1001 15:00:41.386433  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.386435  5332 net.cpp:137] Memory required for data: 312526000
I1001 15:00:41.386440  5332 layer_factory.hpp:77] Creating layer penlu10
I1001 15:00:41.386445  5332 net.cpp:84] Creating Layer penlu10
I1001 15:00:41.386447  5332 net.cpp:406] penlu10 <- Convolution11
I1001 15:00:41.386451  5332 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1001 15:00:41.386582  5332 net.cpp:122] Setting up penlu10
I1001 15:00:41.386589  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.386590  5332 net.cpp:137] Memory required for data: 315802800
I1001 15:00:41.386595  5332 layer_factory.hpp:77] Creating layer Convolution12
I1001 15:00:41.386601  5332 net.cpp:84] Creating Layer Convolution12
I1001 15:00:41.386605  5332 net.cpp:406] Convolution12 <- Convolution11
I1001 15:00:41.386610  5332 net.cpp:380] Convolution12 -> Convolution12
I1001 15:00:41.387333  5332 net.cpp:122] Setting up Convolution12
I1001 15:00:41.387341  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.387342  5332 net.cpp:137] Memory required for data: 319079600
I1001 15:00:41.387353  5332 layer_factory.hpp:77] Creating layer BatchNorm12
I1001 15:00:41.387359  5332 net.cpp:84] Creating Layer BatchNorm12
I1001 15:00:41.387362  5332 net.cpp:406] BatchNorm12 <- Convolution12
I1001 15:00:41.387364  5332 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1001 15:00:41.387501  5332 net.cpp:122] Setting up BatchNorm12
I1001 15:00:41.387506  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.387507  5332 net.cpp:137] Memory required for data: 322356400
I1001 15:00:41.387511  5332 layer_factory.hpp:77] Creating layer Scale12
I1001 15:00:41.387516  5332 net.cpp:84] Creating Layer Scale12
I1001 15:00:41.387519  5332 net.cpp:406] Scale12 <- Convolution12
I1001 15:00:41.387522  5332 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1001 15:00:41.387549  5332 layer_factory.hpp:77] Creating layer Scale12
I1001 15:00:41.387624  5332 net.cpp:122] Setting up Scale12
I1001 15:00:41.387629  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.387631  5332 net.cpp:137] Memory required for data: 325633200
I1001 15:00:41.387634  5332 layer_factory.hpp:77] Creating layer Eltwise5
I1001 15:00:41.387639  5332 net.cpp:84] Creating Layer Eltwise5
I1001 15:00:41.387641  5332 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1001 15:00:41.387643  5332 net.cpp:406] Eltwise5 <- Convolution12
I1001 15:00:41.387646  5332 net.cpp:380] Eltwise5 -> Eltwise5
I1001 15:00:41.387660  5332 net.cpp:122] Setting up Eltwise5
I1001 15:00:41.387663  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.387665  5332 net.cpp:137] Memory required for data: 328910000
I1001 15:00:41.387667  5332 layer_factory.hpp:77] Creating layer penlu11
I1001 15:00:41.387671  5332 net.cpp:84] Creating Layer penlu11
I1001 15:00:41.387675  5332 net.cpp:406] penlu11 <- Eltwise5
I1001 15:00:41.387678  5332 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1001 15:00:41.387790  5332 net.cpp:122] Setting up penlu11
I1001 15:00:41.387794  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.387796  5332 net.cpp:137] Memory required for data: 332186800
I1001 15:00:41.387800  5332 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1001 15:00:41.387804  5332 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1001 15:00:41.387806  5332 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1001 15:00:41.387809  5332 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1001 15:00:41.387812  5332 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1001 15:00:41.387838  5332 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1001 15:00:41.387842  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.387845  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.387848  5332 net.cpp:137] Memory required for data: 338740400
I1001 15:00:41.387850  5332 layer_factory.hpp:77] Creating layer Convolution13
I1001 15:00:41.387856  5332 net.cpp:84] Creating Layer Convolution13
I1001 15:00:41.387858  5332 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1001 15:00:41.387862  5332 net.cpp:380] Convolution13 -> Convolution13
I1001 15:00:41.388917  5332 net.cpp:122] Setting up Convolution13
I1001 15:00:41.388926  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.388928  5332 net.cpp:137] Memory required for data: 342017200
I1001 15:00:41.388933  5332 layer_factory.hpp:77] Creating layer BatchNorm13
I1001 15:00:41.388938  5332 net.cpp:84] Creating Layer BatchNorm13
I1001 15:00:41.388941  5332 net.cpp:406] BatchNorm13 <- Convolution13
I1001 15:00:41.388945  5332 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1001 15:00:41.389077  5332 net.cpp:122] Setting up BatchNorm13
I1001 15:00:41.389081  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.389084  5332 net.cpp:137] Memory required for data: 345294000
I1001 15:00:41.389088  5332 layer_factory.hpp:77] Creating layer Scale13
I1001 15:00:41.389092  5332 net.cpp:84] Creating Layer Scale13
I1001 15:00:41.389096  5332 net.cpp:406] Scale13 <- Convolution13
I1001 15:00:41.389106  5332 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1001 15:00:41.389133  5332 layer_factory.hpp:77] Creating layer Scale13
I1001 15:00:41.389214  5332 net.cpp:122] Setting up Scale13
I1001 15:00:41.389217  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.389220  5332 net.cpp:137] Memory required for data: 348570800
I1001 15:00:41.389223  5332 layer_factory.hpp:77] Creating layer penlu12
I1001 15:00:41.389228  5332 net.cpp:84] Creating Layer penlu12
I1001 15:00:41.389230  5332 net.cpp:406] penlu12 <- Convolution13
I1001 15:00:41.389235  5332 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1001 15:00:41.389356  5332 net.cpp:122] Setting up penlu12
I1001 15:00:41.389361  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.389364  5332 net.cpp:137] Memory required for data: 351847600
I1001 15:00:41.389367  5332 layer_factory.hpp:77] Creating layer Convolution14
I1001 15:00:41.389377  5332 net.cpp:84] Creating Layer Convolution14
I1001 15:00:41.389380  5332 net.cpp:406] Convolution14 <- Convolution13
I1001 15:00:41.389384  5332 net.cpp:380] Convolution14 -> Convolution14
I1001 15:00:41.390460  5332 net.cpp:122] Setting up Convolution14
I1001 15:00:41.390470  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.390472  5332 net.cpp:137] Memory required for data: 355124400
I1001 15:00:41.390486  5332 layer_factory.hpp:77] Creating layer BatchNorm14
I1001 15:00:41.390491  5332 net.cpp:84] Creating Layer BatchNorm14
I1001 15:00:41.390494  5332 net.cpp:406] BatchNorm14 <- Convolution14
I1001 15:00:41.390498  5332 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1001 15:00:41.390645  5332 net.cpp:122] Setting up BatchNorm14
I1001 15:00:41.390651  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.390653  5332 net.cpp:137] Memory required for data: 358401200
I1001 15:00:41.390658  5332 layer_factory.hpp:77] Creating layer Scale14
I1001 15:00:41.390663  5332 net.cpp:84] Creating Layer Scale14
I1001 15:00:41.390666  5332 net.cpp:406] Scale14 <- Convolution14
I1001 15:00:41.390669  5332 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1001 15:00:41.390697  5332 layer_factory.hpp:77] Creating layer Scale14
I1001 15:00:41.390803  5332 net.cpp:122] Setting up Scale14
I1001 15:00:41.390808  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.390810  5332 net.cpp:137] Memory required for data: 361678000
I1001 15:00:41.390815  5332 layer_factory.hpp:77] Creating layer Eltwise6
I1001 15:00:41.390818  5332 net.cpp:84] Creating Layer Eltwise6
I1001 15:00:41.390821  5332 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1001 15:00:41.390825  5332 net.cpp:406] Eltwise6 <- Convolution14
I1001 15:00:41.390827  5332 net.cpp:380] Eltwise6 -> Eltwise6
I1001 15:00:41.390841  5332 net.cpp:122] Setting up Eltwise6
I1001 15:00:41.390846  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.390847  5332 net.cpp:137] Memory required for data: 364954800
I1001 15:00:41.390849  5332 layer_factory.hpp:77] Creating layer penlu13
I1001 15:00:41.390853  5332 net.cpp:84] Creating Layer penlu13
I1001 15:00:41.390856  5332 net.cpp:406] penlu13 <- Eltwise6
I1001 15:00:41.390861  5332 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1001 15:00:41.390974  5332 net.cpp:122] Setting up penlu13
I1001 15:00:41.390977  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.390980  5332 net.cpp:137] Memory required for data: 368231600
I1001 15:00:41.390983  5332 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1001 15:00:41.390986  5332 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1001 15:00:41.390990  5332 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1001 15:00:41.390992  5332 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1001 15:00:41.410743  5332 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1001 15:00:41.410797  5332 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1001 15:00:41.410805  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.410811  5332 net.cpp:129] Top shape: 100 32 16 16 (819200)
I1001 15:00:41.410825  5332 net.cpp:137] Memory required for data: 374785200
I1001 15:00:41.410828  5332 layer_factory.hpp:77] Creating layer Convolution15
I1001 15:00:41.410842  5332 net.cpp:84] Creating Layer Convolution15
I1001 15:00:41.410847  5332 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1001 15:00:41.410854  5332 net.cpp:380] Convolution15 -> Convolution15
I1001 15:00:41.412266  5332 net.cpp:122] Setting up Convolution15
I1001 15:00:41.412277  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.412282  5332 net.cpp:137] Memory required for data: 376423600
I1001 15:00:41.412289  5332 layer_factory.hpp:77] Creating layer BatchNorm15
I1001 15:00:41.412297  5332 net.cpp:84] Creating Layer BatchNorm15
I1001 15:00:41.412302  5332 net.cpp:406] BatchNorm15 <- Convolution15
I1001 15:00:41.412308  5332 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1001 15:00:41.412513  5332 net.cpp:122] Setting up BatchNorm15
I1001 15:00:41.412520  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.412524  5332 net.cpp:137] Memory required for data: 378062000
I1001 15:00:41.412533  5332 layer_factory.hpp:77] Creating layer Scale15
I1001 15:00:41.412539  5332 net.cpp:84] Creating Layer Scale15
I1001 15:00:41.412544  5332 net.cpp:406] Scale15 <- Convolution15
I1001 15:00:41.412549  5332 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1001 15:00:41.412591  5332 layer_factory.hpp:77] Creating layer Scale15
I1001 15:00:41.412719  5332 net.cpp:122] Setting up Scale15
I1001 15:00:41.412729  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.412732  5332 net.cpp:137] Memory required for data: 379700400
I1001 15:00:41.412739  5332 layer_factory.hpp:77] Creating layer Convolution16
I1001 15:00:41.412750  5332 net.cpp:84] Creating Layer Convolution16
I1001 15:00:41.412755  5332 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1001 15:00:41.412762  5332 net.cpp:380] Convolution16 -> Convolution16
I1001 15:00:41.414573  5332 net.cpp:122] Setting up Convolution16
I1001 15:00:41.414583  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.414587  5332 net.cpp:137] Memory required for data: 381338800
I1001 15:00:41.414592  5332 layer_factory.hpp:77] Creating layer BatchNorm16
I1001 15:00:41.414597  5332 net.cpp:84] Creating Layer BatchNorm16
I1001 15:00:41.414600  5332 net.cpp:406] BatchNorm16 <- Convolution16
I1001 15:00:41.414604  5332 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1001 15:00:41.414759  5332 net.cpp:122] Setting up BatchNorm16
I1001 15:00:41.414764  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.414767  5332 net.cpp:137] Memory required for data: 382977200
I1001 15:00:41.414772  5332 layer_factory.hpp:77] Creating layer Scale16
I1001 15:00:41.414775  5332 net.cpp:84] Creating Layer Scale16
I1001 15:00:41.414777  5332 net.cpp:406] Scale16 <- Convolution16
I1001 15:00:41.414780  5332 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1001 15:00:41.414808  5332 layer_factory.hpp:77] Creating layer Scale16
I1001 15:00:41.414890  5332 net.cpp:122] Setting up Scale16
I1001 15:00:41.414894  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.414896  5332 net.cpp:137] Memory required for data: 384615600
I1001 15:00:41.414901  5332 layer_factory.hpp:77] Creating layer penlu14
I1001 15:00:41.414906  5332 net.cpp:84] Creating Layer penlu14
I1001 15:00:41.414908  5332 net.cpp:406] penlu14 <- Convolution16
I1001 15:00:41.414911  5332 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1001 15:00:41.415024  5332 net.cpp:122] Setting up penlu14
I1001 15:00:41.415030  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.415031  5332 net.cpp:137] Memory required for data: 386254000
I1001 15:00:41.415035  5332 layer_factory.hpp:77] Creating layer Convolution17
I1001 15:00:41.415043  5332 net.cpp:84] Creating Layer Convolution17
I1001 15:00:41.415046  5332 net.cpp:406] Convolution17 <- Convolution16
I1001 15:00:41.415050  5332 net.cpp:380] Convolution17 -> Convolution17
I1001 15:00:41.416837  5332 net.cpp:122] Setting up Convolution17
I1001 15:00:41.416846  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.416848  5332 net.cpp:137] Memory required for data: 387892400
I1001 15:00:41.416853  5332 layer_factory.hpp:77] Creating layer BatchNorm17
I1001 15:00:41.416858  5332 net.cpp:84] Creating Layer BatchNorm17
I1001 15:00:41.416862  5332 net.cpp:406] BatchNorm17 <- Convolution17
I1001 15:00:41.416865  5332 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1001 15:00:41.417008  5332 net.cpp:122] Setting up BatchNorm17
I1001 15:00:41.417013  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.417016  5332 net.cpp:137] Memory required for data: 389530800
I1001 15:00:41.417019  5332 layer_factory.hpp:77] Creating layer Scale17
I1001 15:00:41.417024  5332 net.cpp:84] Creating Layer Scale17
I1001 15:00:41.417027  5332 net.cpp:406] Scale17 <- Convolution17
I1001 15:00:41.417031  5332 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1001 15:00:41.417058  5332 layer_factory.hpp:77] Creating layer Scale17
I1001 15:00:41.417141  5332 net.cpp:122] Setting up Scale17
I1001 15:00:41.417146  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.417148  5332 net.cpp:137] Memory required for data: 391169200
I1001 15:00:41.417151  5332 layer_factory.hpp:77] Creating layer Eltwise7
I1001 15:00:41.417156  5332 net.cpp:84] Creating Layer Eltwise7
I1001 15:00:41.417160  5332 net.cpp:406] Eltwise7 <- Convolution15
I1001 15:00:41.417161  5332 net.cpp:406] Eltwise7 <- Convolution17
I1001 15:00:41.417165  5332 net.cpp:380] Eltwise7 -> Eltwise7
I1001 15:00:41.417182  5332 net.cpp:122] Setting up Eltwise7
I1001 15:00:41.417186  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.417188  5332 net.cpp:137] Memory required for data: 392807600
I1001 15:00:41.417191  5332 layer_factory.hpp:77] Creating layer penlu15
I1001 15:00:41.417196  5332 net.cpp:84] Creating Layer penlu15
I1001 15:00:41.417197  5332 net.cpp:406] penlu15 <- Eltwise7
I1001 15:00:41.417201  5332 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1001 15:00:41.417315  5332 net.cpp:122] Setting up penlu15
I1001 15:00:41.417318  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.417320  5332 net.cpp:137] Memory required for data: 394446000
I1001 15:00:41.417325  5332 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1001 15:00:41.417328  5332 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1001 15:00:41.417330  5332 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1001 15:00:41.417335  5332 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1001 15:00:41.417338  5332 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1001 15:00:41.417363  5332 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1001 15:00:41.417368  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.417371  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.417372  5332 net.cpp:137] Memory required for data: 397722800
I1001 15:00:41.417376  5332 layer_factory.hpp:77] Creating layer Convolution18
I1001 15:00:41.417382  5332 net.cpp:84] Creating Layer Convolution18
I1001 15:00:41.417384  5332 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1001 15:00:41.417388  5332 net.cpp:380] Convolution18 -> Convolution18
I1001 15:00:41.419085  5332 net.cpp:122] Setting up Convolution18
I1001 15:00:41.419093  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.419096  5332 net.cpp:137] Memory required for data: 399361200
I1001 15:00:41.419100  5332 layer_factory.hpp:77] Creating layer BatchNorm18
I1001 15:00:41.419106  5332 net.cpp:84] Creating Layer BatchNorm18
I1001 15:00:41.419108  5332 net.cpp:406] BatchNorm18 <- Convolution18
I1001 15:00:41.419113  5332 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1001 15:00:41.419253  5332 net.cpp:122] Setting up BatchNorm18
I1001 15:00:41.419258  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.419260  5332 net.cpp:137] Memory required for data: 400999600
I1001 15:00:41.419265  5332 layer_factory.hpp:77] Creating layer Scale18
I1001 15:00:41.419276  5332 net.cpp:84] Creating Layer Scale18
I1001 15:00:41.419279  5332 net.cpp:406] Scale18 <- Convolution18
I1001 15:00:41.419282  5332 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1001 15:00:41.419312  5332 layer_factory.hpp:77] Creating layer Scale18
I1001 15:00:41.419391  5332 net.cpp:122] Setting up Scale18
I1001 15:00:41.419395  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.419397  5332 net.cpp:137] Memory required for data: 402638000
I1001 15:00:41.419401  5332 layer_factory.hpp:77] Creating layer penlu16
I1001 15:00:41.419406  5332 net.cpp:84] Creating Layer penlu16
I1001 15:00:41.419409  5332 net.cpp:406] penlu16 <- Convolution18
I1001 15:00:41.419414  5332 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1001 15:00:41.419523  5332 net.cpp:122] Setting up penlu16
I1001 15:00:41.419528  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.419529  5332 net.cpp:137] Memory required for data: 404276400
I1001 15:00:41.419533  5332 layer_factory.hpp:77] Creating layer Convolution19
I1001 15:00:41.419539  5332 net.cpp:84] Creating Layer Convolution19
I1001 15:00:41.419543  5332 net.cpp:406] Convolution19 <- Convolution18
I1001 15:00:41.419545  5332 net.cpp:380] Convolution19 -> Convolution19
I1001 15:00:41.421239  5332 net.cpp:122] Setting up Convolution19
I1001 15:00:41.421248  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.421250  5332 net.cpp:137] Memory required for data: 405914800
I1001 15:00:41.421255  5332 layer_factory.hpp:77] Creating layer BatchNorm19
I1001 15:00:41.421260  5332 net.cpp:84] Creating Layer BatchNorm19
I1001 15:00:41.421262  5332 net.cpp:406] BatchNorm19 <- Convolution19
I1001 15:00:41.421267  5332 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1001 15:00:41.421406  5332 net.cpp:122] Setting up BatchNorm19
I1001 15:00:41.421411  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.421412  5332 net.cpp:137] Memory required for data: 407553200
I1001 15:00:41.421416  5332 layer_factory.hpp:77] Creating layer Scale19
I1001 15:00:41.421422  5332 net.cpp:84] Creating Layer Scale19
I1001 15:00:41.421423  5332 net.cpp:406] Scale19 <- Convolution19
I1001 15:00:41.421427  5332 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1001 15:00:41.421454  5332 layer_factory.hpp:77] Creating layer Scale19
I1001 15:00:41.421536  5332 net.cpp:122] Setting up Scale19
I1001 15:00:41.421540  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.421542  5332 net.cpp:137] Memory required for data: 409191600
I1001 15:00:41.421546  5332 layer_factory.hpp:77] Creating layer Eltwise8
I1001 15:00:41.421550  5332 net.cpp:84] Creating Layer Eltwise8
I1001 15:00:41.421553  5332 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1001 15:00:41.421556  5332 net.cpp:406] Eltwise8 <- Convolution19
I1001 15:00:41.421560  5332 net.cpp:380] Eltwise8 -> Eltwise8
I1001 15:00:41.421577  5332 net.cpp:122] Setting up Eltwise8
I1001 15:00:41.421581  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.421582  5332 net.cpp:137] Memory required for data: 410830000
I1001 15:00:41.421584  5332 layer_factory.hpp:77] Creating layer penlu17
I1001 15:00:41.421589  5332 net.cpp:84] Creating Layer penlu17
I1001 15:00:41.421592  5332 net.cpp:406] penlu17 <- Eltwise8
I1001 15:00:41.421596  5332 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1001 15:00:41.421717  5332 net.cpp:122] Setting up penlu17
I1001 15:00:41.421722  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.421725  5332 net.cpp:137] Memory required for data: 412468400
I1001 15:00:41.421737  5332 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1001 15:00:41.421741  5332 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1001 15:00:41.421743  5332 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1001 15:00:41.421746  5332 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1001 15:00:41.421751  5332 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1001 15:00:41.421774  5332 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1001 15:00:41.421784  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.421787  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.421789  5332 net.cpp:137] Memory required for data: 415745200
I1001 15:00:41.421792  5332 layer_factory.hpp:77] Creating layer Convolution20
I1001 15:00:41.421797  5332 net.cpp:84] Creating Layer Convolution20
I1001 15:00:41.421800  5332 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1001 15:00:41.421804  5332 net.cpp:380] Convolution20 -> Convolution20
I1001 15:00:41.423794  5332 net.cpp:122] Setting up Convolution20
I1001 15:00:41.423804  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.423806  5332 net.cpp:137] Memory required for data: 417383600
I1001 15:00:41.423811  5332 layer_factory.hpp:77] Creating layer BatchNorm20
I1001 15:00:41.423816  5332 net.cpp:84] Creating Layer BatchNorm20
I1001 15:00:41.423818  5332 net.cpp:406] BatchNorm20 <- Convolution20
I1001 15:00:41.423822  5332 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1001 15:00:41.423985  5332 net.cpp:122] Setting up BatchNorm20
I1001 15:00:41.423990  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.423992  5332 net.cpp:137] Memory required for data: 419022000
I1001 15:00:41.423996  5332 layer_factory.hpp:77] Creating layer Scale20
I1001 15:00:41.424001  5332 net.cpp:84] Creating Layer Scale20
I1001 15:00:41.424005  5332 net.cpp:406] Scale20 <- Convolution20
I1001 15:00:41.424007  5332 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1001 15:00:41.424036  5332 layer_factory.hpp:77] Creating layer Scale20
I1001 15:00:41.424118  5332 net.cpp:122] Setting up Scale20
I1001 15:00:41.424121  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.424124  5332 net.cpp:137] Memory required for data: 420660400
I1001 15:00:41.424127  5332 layer_factory.hpp:77] Creating layer penlu18
I1001 15:00:41.424132  5332 net.cpp:84] Creating Layer penlu18
I1001 15:00:41.424135  5332 net.cpp:406] penlu18 <- Convolution20
I1001 15:00:41.424139  5332 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1001 15:00:41.424249  5332 net.cpp:122] Setting up penlu18
I1001 15:00:41.424254  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.424257  5332 net.cpp:137] Memory required for data: 422298800
I1001 15:00:41.424260  5332 layer_factory.hpp:77] Creating layer Convolution21
I1001 15:00:41.424266  5332 net.cpp:84] Creating Layer Convolution21
I1001 15:00:41.424269  5332 net.cpp:406] Convolution21 <- Convolution20
I1001 15:00:41.424273  5332 net.cpp:380] Convolution21 -> Convolution21
I1001 15:00:41.426264  5332 net.cpp:122] Setting up Convolution21
I1001 15:00:41.426272  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.426275  5332 net.cpp:137] Memory required for data: 423937200
I1001 15:00:41.426280  5332 layer_factory.hpp:77] Creating layer BatchNorm21
I1001 15:00:41.426283  5332 net.cpp:84] Creating Layer BatchNorm21
I1001 15:00:41.426286  5332 net.cpp:406] BatchNorm21 <- Convolution21
I1001 15:00:41.426290  5332 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1001 15:00:41.426430  5332 net.cpp:122] Setting up BatchNorm21
I1001 15:00:41.426434  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.426436  5332 net.cpp:137] Memory required for data: 425575600
I1001 15:00:41.426441  5332 layer_factory.hpp:77] Creating layer Scale21
I1001 15:00:41.426445  5332 net.cpp:84] Creating Layer Scale21
I1001 15:00:41.426447  5332 net.cpp:406] Scale21 <- Convolution21
I1001 15:00:41.426451  5332 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1001 15:00:41.441627  5332 layer_factory.hpp:77] Creating layer Scale21
I1001 15:00:41.441763  5332 net.cpp:122] Setting up Scale21
I1001 15:00:41.441772  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.441776  5332 net.cpp:137] Memory required for data: 427214000
I1001 15:00:41.441784  5332 layer_factory.hpp:77] Creating layer Eltwise9
I1001 15:00:41.441792  5332 net.cpp:84] Creating Layer Eltwise9
I1001 15:00:41.441797  5332 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1001 15:00:41.441812  5332 net.cpp:406] Eltwise9 <- Convolution21
I1001 15:00:41.441819  5332 net.cpp:380] Eltwise9 -> Eltwise9
I1001 15:00:41.441848  5332 net.cpp:122] Setting up Eltwise9
I1001 15:00:41.441855  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.441859  5332 net.cpp:137] Memory required for data: 428852400
I1001 15:00:41.441864  5332 layer_factory.hpp:77] Creating layer penlu19
I1001 15:00:41.441871  5332 net.cpp:84] Creating Layer penlu19
I1001 15:00:41.441875  5332 net.cpp:406] penlu19 <- Eltwise9
I1001 15:00:41.441884  5332 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1001 15:00:41.442066  5332 net.cpp:122] Setting up penlu19
I1001 15:00:41.442075  5332 net.cpp:129] Top shape: 100 64 8 8 (409600)
I1001 15:00:41.442078  5332 net.cpp:137] Memory required for data: 430490800
I1001 15:00:41.442087  5332 layer_factory.hpp:77] Creating layer Pooling1
I1001 15:00:41.442095  5332 net.cpp:84] Creating Layer Pooling1
I1001 15:00:41.442098  5332 net.cpp:406] Pooling1 <- Eltwise9
I1001 15:00:41.442104  5332 net.cpp:380] Pooling1 -> Pooling1
I1001 15:00:41.442315  5332 net.cpp:122] Setting up Pooling1
I1001 15:00:41.442325  5332 net.cpp:129] Top shape: 100 64 1 1 (6400)
I1001 15:00:41.442329  5332 net.cpp:137] Memory required for data: 430516400
I1001 15:00:41.442334  5332 layer_factory.hpp:77] Creating layer InnerProduct1
I1001 15:00:41.442343  5332 net.cpp:84] Creating Layer InnerProduct1
I1001 15:00:41.442348  5332 net.cpp:406] InnerProduct1 <- Pooling1
I1001 15:00:41.442354  5332 net.cpp:380] InnerProduct1 -> InnerProduct1
I1001 15:00:41.442502  5332 net.cpp:122] Setting up InnerProduct1
I1001 15:00:41.442510  5332 net.cpp:129] Top shape: 100 10 (1000)
I1001 15:00:41.442514  5332 net.cpp:137] Memory required for data: 430520400
I1001 15:00:41.442530  5332 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1001 15:00:41.442538  5332 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1001 15:00:41.442543  5332 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1001 15:00:41.442550  5332 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1001 15:00:41.442558  5332 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1001 15:00:41.442603  5332 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1001 15:00:41.442612  5332 net.cpp:129] Top shape: 100 10 (1000)
I1001 15:00:41.442617  5332 net.cpp:129] Top shape: 100 10 (1000)
I1001 15:00:41.442621  5332 net.cpp:137] Memory required for data: 430528400
I1001 15:00:41.442625  5332 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 15:00:41.442632  5332 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1001 15:00:41.442636  5332 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I1001 15:00:41.442642  5332 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I1001 15:00:41.442647  5332 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1001 15:00:41.442656  5332 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1001 15:00:41.442941  5332 net.cpp:122] Setting up SoftmaxWithLoss1
I1001 15:00:41.442951  5332 net.cpp:129] Top shape: (1)
I1001 15:00:41.442956  5332 net.cpp:132]     with loss weight 1
I1001 15:00:41.442966  5332 net.cpp:137] Memory required for data: 430528404
I1001 15:00:41.442971  5332 layer_factory.hpp:77] Creating layer Accuracy1
I1001 15:00:41.442979  5332 net.cpp:84] Creating Layer Accuracy1
I1001 15:00:41.442984  5332 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I1001 15:00:41.442991  5332 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I1001 15:00:41.442996  5332 net.cpp:380] Accuracy1 -> Accuracy1
I1001 15:00:41.443006  5332 net.cpp:122] Setting up Accuracy1
I1001 15:00:41.443011  5332 net.cpp:129] Top shape: (1)
I1001 15:00:41.443017  5332 net.cpp:137] Memory required for data: 430528408
I1001 15:00:41.443020  5332 net.cpp:200] Accuracy1 does not need backward computation.
I1001 15:00:41.443025  5332 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1001 15:00:41.443038  5332 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1001 15:00:41.443043  5332 net.cpp:198] InnerProduct1 needs backward computation.
I1001 15:00:41.443048  5332 net.cpp:198] Pooling1 needs backward computation.
I1001 15:00:41.443051  5332 net.cpp:198] penlu19 needs backward computation.
I1001 15:00:41.443056  5332 net.cpp:198] Eltwise9 needs backward computation.
I1001 15:00:41.443060  5332 net.cpp:198] Scale21 needs backward computation.
I1001 15:00:41.443064  5332 net.cpp:198] BatchNorm21 needs backward computation.
I1001 15:00:41.443068  5332 net.cpp:198] Convolution21 needs backward computation.
I1001 15:00:41.443073  5332 net.cpp:198] penlu18 needs backward computation.
I1001 15:00:41.443078  5332 net.cpp:198] Scale20 needs backward computation.
I1001 15:00:41.443081  5332 net.cpp:198] BatchNorm20 needs backward computation.
I1001 15:00:41.443085  5332 net.cpp:198] Convolution20 needs backward computation.
I1001 15:00:41.443089  5332 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1001 15:00:41.443094  5332 net.cpp:198] penlu17 needs backward computation.
I1001 15:00:41.443099  5332 net.cpp:198] Eltwise8 needs backward computation.
I1001 15:00:41.443102  5332 net.cpp:198] Scale19 needs backward computation.
I1001 15:00:41.443106  5332 net.cpp:198] BatchNorm19 needs backward computation.
I1001 15:00:41.443110  5332 net.cpp:198] Convolution19 needs backward computation.
I1001 15:00:41.443114  5332 net.cpp:198] penlu16 needs backward computation.
I1001 15:00:41.443119  5332 net.cpp:198] Scale18 needs backward computation.
I1001 15:00:41.443122  5332 net.cpp:198] BatchNorm18 needs backward computation.
I1001 15:00:41.443126  5332 net.cpp:198] Convolution18 needs backward computation.
I1001 15:00:41.443131  5332 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1001 15:00:41.443135  5332 net.cpp:198] penlu15 needs backward computation.
I1001 15:00:41.443140  5332 net.cpp:198] Eltwise7 needs backward computation.
I1001 15:00:41.443145  5332 net.cpp:198] Scale17 needs backward computation.
I1001 15:00:41.443148  5332 net.cpp:198] BatchNorm17 needs backward computation.
I1001 15:00:41.443152  5332 net.cpp:198] Convolution17 needs backward computation.
I1001 15:00:41.443156  5332 net.cpp:198] penlu14 needs backward computation.
I1001 15:00:41.443161  5332 net.cpp:198] Scale16 needs backward computation.
I1001 15:00:41.443164  5332 net.cpp:198] BatchNorm16 needs backward computation.
I1001 15:00:41.443168  5332 net.cpp:198] Convolution16 needs backward computation.
I1001 15:00:41.443173  5332 net.cpp:198] Scale15 needs backward computation.
I1001 15:00:41.443177  5332 net.cpp:198] BatchNorm15 needs backward computation.
I1001 15:00:41.443181  5332 net.cpp:198] Convolution15 needs backward computation.
I1001 15:00:41.443186  5332 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1001 15:00:41.443190  5332 net.cpp:198] penlu13 needs backward computation.
I1001 15:00:41.443194  5332 net.cpp:198] Eltwise6 needs backward computation.
I1001 15:00:41.443199  5332 net.cpp:198] Scale14 needs backward computation.
I1001 15:00:41.443203  5332 net.cpp:198] BatchNorm14 needs backward computation.
I1001 15:00:41.443207  5332 net.cpp:198] Convolution14 needs backward computation.
I1001 15:00:41.443214  5332 net.cpp:198] penlu12 needs backward computation.
I1001 15:00:41.443218  5332 net.cpp:198] Scale13 needs backward computation.
I1001 15:00:41.443222  5332 net.cpp:198] BatchNorm13 needs backward computation.
I1001 15:00:41.444324  5332 net.cpp:198] Convolution13 needs backward computation.
I1001 15:00:41.444339  5332 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1001 15:00:41.444342  5332 net.cpp:198] penlu11 needs backward computation.
I1001 15:00:41.444345  5332 net.cpp:198] Eltwise5 needs backward computation.
I1001 15:00:41.444349  5332 net.cpp:198] Scale12 needs backward computation.
I1001 15:00:41.444350  5332 net.cpp:198] BatchNorm12 needs backward computation.
I1001 15:00:41.444360  5332 net.cpp:198] Convolution12 needs backward computation.
I1001 15:00:41.444363  5332 net.cpp:198] penlu10 needs backward computation.
I1001 15:00:41.444365  5332 net.cpp:198] Scale11 needs backward computation.
I1001 15:00:41.444367  5332 net.cpp:198] BatchNorm11 needs backward computation.
I1001 15:00:41.444370  5332 net.cpp:198] Convolution11 needs backward computation.
I1001 15:00:41.444372  5332 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1001 15:00:41.444375  5332 net.cpp:198] penlu9 needs backward computation.
I1001 15:00:41.444377  5332 net.cpp:198] Eltwise4 needs backward computation.
I1001 15:00:41.444380  5332 net.cpp:198] Scale10 needs backward computation.
I1001 15:00:41.444383  5332 net.cpp:198] BatchNorm10 needs backward computation.
I1001 15:00:41.444386  5332 net.cpp:198] Convolution10 needs backward computation.
I1001 15:00:41.444388  5332 net.cpp:198] penlu8 needs backward computation.
I1001 15:00:41.444391  5332 net.cpp:198] Scale9 needs backward computation.
I1001 15:00:41.444393  5332 net.cpp:198] BatchNorm9 needs backward computation.
I1001 15:00:41.444396  5332 net.cpp:198] Convolution9 needs backward computation.
I1001 15:00:41.444398  5332 net.cpp:198] Scale8 needs backward computation.
I1001 15:00:41.444401  5332 net.cpp:198] BatchNorm8 needs backward computation.
I1001 15:00:41.444402  5332 net.cpp:198] Convolution8 needs backward computation.
I1001 15:00:41.444406  5332 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1001 15:00:41.444408  5332 net.cpp:198] penlu7 needs backward computation.
I1001 15:00:41.444411  5332 net.cpp:198] Eltwise3 needs backward computation.
I1001 15:00:41.444413  5332 net.cpp:198] Scale7 needs backward computation.
I1001 15:00:41.444416  5332 net.cpp:198] BatchNorm7 needs backward computation.
I1001 15:00:41.444418  5332 net.cpp:198] Convolution7 needs backward computation.
I1001 15:00:41.444420  5332 net.cpp:198] penlu6 needs backward computation.
I1001 15:00:41.444423  5332 net.cpp:198] Scale6 needs backward computation.
I1001 15:00:41.444425  5332 net.cpp:198] BatchNorm6 needs backward computation.
I1001 15:00:41.444427  5332 net.cpp:198] Convolution6 needs backward computation.
I1001 15:00:41.444430  5332 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1001 15:00:41.444432  5332 net.cpp:198] penlu5 needs backward computation.
I1001 15:00:41.444435  5332 net.cpp:198] Eltwise2 needs backward computation.
I1001 15:00:41.444438  5332 net.cpp:198] Scale5 needs backward computation.
I1001 15:00:41.444440  5332 net.cpp:198] BatchNorm5 needs backward computation.
I1001 15:00:41.444442  5332 net.cpp:198] Convolution5 needs backward computation.
I1001 15:00:41.444445  5332 net.cpp:198] penlu4 needs backward computation.
I1001 15:00:41.444447  5332 net.cpp:198] Scale4 needs backward computation.
I1001 15:00:41.444449  5332 net.cpp:198] BatchNorm4 needs backward computation.
I1001 15:00:41.444453  5332 net.cpp:198] Convolution4 needs backward computation.
I1001 15:00:41.444454  5332 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1001 15:00:41.444458  5332 net.cpp:198] penlu3 needs backward computation.
I1001 15:00:41.444460  5332 net.cpp:198] Eltwise1 needs backward computation.
I1001 15:00:41.444463  5332 net.cpp:198] Scale3 needs backward computation.
I1001 15:00:41.444465  5332 net.cpp:198] BatchNorm3 needs backward computation.
I1001 15:00:41.444468  5332 net.cpp:198] Convolution3 needs backward computation.
I1001 15:00:41.444470  5332 net.cpp:198] penlu2 needs backward computation.
I1001 15:00:41.444473  5332 net.cpp:198] Scale2 needs backward computation.
I1001 15:00:41.444475  5332 net.cpp:198] BatchNorm2 needs backward computation.
I1001 15:00:41.444478  5332 net.cpp:198] Convolution2 needs backward computation.
I1001 15:00:41.444480  5332 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1001 15:00:41.444483  5332 net.cpp:198] penlu1 needs backward computation.
I1001 15:00:41.444485  5332 net.cpp:198] Scale1 needs backward computation.
I1001 15:00:41.444491  5332 net.cpp:198] BatchNorm1 needs backward computation.
I1001 15:00:41.444494  5332 net.cpp:198] Convolution1 needs backward computation.
I1001 15:00:41.444496  5332 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1001 15:00:41.444499  5332 net.cpp:200] Data1 does not need backward computation.
I1001 15:00:41.444501  5332 net.cpp:242] This network produces output Accuracy1
I1001 15:00:41.444504  5332 net.cpp:242] This network produces output SoftmaxWithLoss1
I1001 15:00:41.444545  5332 net.cpp:255] Network initialization done.
I1001 15:00:41.444808  5332 solver.cpp:56] Solver scaffolding done.
I1001 15:00:41.450134  5332 caffe.cpp:248] Starting Optimization
I1001 15:00:41.450141  5332 solver.cpp:272] Solving resnet_cifar10
I1001 15:00:41.450143  5332 solver.cpp:273] Learning Rate Policy: multistep
I1001 15:00:41.452077  5332 solver.cpp:330] Iteration 0, Testing net (#0)
I1001 15:00:42.681869  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:00:42.731396  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.1
I1001 15:00:42.731433  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I1001 15:00:42.804432  5332 solver.cpp:218] Iteration 0 (0.168104 iter/s, 1.35421s/100 iters), loss = 2.30124
I1001 15:00:42.804456  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.30124 (* 1 = 2.30124 loss)
I1001 15:00:42.804466  5332 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1001 15:00:48.034171  5332 solver.cpp:218] Iteration 100 (19.1217 iter/s, 5.22966s/100 iters), loss = 1.57624
I1001 15:00:48.034200  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.57624 (* 1 = 1.57624 loss)
I1001 15:00:48.034215  5332 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1001 15:00:53.260902  5332 solver.cpp:218] Iteration 200 (19.1327 iter/s, 5.22664s/100 iters), loss = 1.55594
I1001 15:00:53.260932  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.55594 (* 1 = 1.55594 loss)
I1001 15:00:53.260948  5332 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1001 15:00:58.483219  5332 solver.cpp:218] Iteration 300 (19.1489 iter/s, 5.22223s/100 iters), loss = 1.26385
I1001 15:00:58.483273  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.26385 (* 1 = 1.26385 loss)
I1001 15:00:58.483281  5332 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1001 15:01:03.703001  5332 solver.cpp:218] Iteration 400 (19.1584 iter/s, 5.21964s/100 iters), loss = 1.04972
I1001 15:01:03.703042  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.04972 (* 1 = 1.04972 loss)
I1001 15:01:03.703048  5332 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1001 15:01:08.669124  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:01:08.877661  5332 solver.cpp:330] Iteration 500, Testing net (#0)
I1001 15:01:10.065330  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:01:10.115557  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4795
I1001 15:01:10.115592  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.71861 (* 1 = 1.71861 loss)
I1001 15:01:10.167744  5332 solver.cpp:218] Iteration 500 (15.4688 iter/s, 6.46464s/100 iters), loss = 1.13441
I1001 15:01:10.167770  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.13441 (* 1 = 1.13441 loss)
I1001 15:01:10.167778  5332 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1001 15:01:15.402377  5332 solver.cpp:218] Iteration 600 (19.1038 iter/s, 5.23455s/100 iters), loss = 1.10298
I1001 15:01:15.402460  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.10298 (* 1 = 1.10298 loss)
I1001 15:01:15.402467  5332 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1001 15:01:20.625241  5332 solver.cpp:218] Iteration 700 (19.1471 iter/s, 5.22273s/100 iters), loss = 1.06151
I1001 15:01:20.625280  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.06151 (* 1 = 1.06151 loss)
I1001 15:01:20.625286  5332 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1001 15:01:25.851160  5332 solver.cpp:218] Iteration 800 (19.1357 iter/s, 5.22582s/100 iters), loss = 1.01975
I1001 15:01:25.851200  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.01975 (* 1 = 1.01975 loss)
I1001 15:01:25.851207  5332 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1001 15:01:31.073225  5332 solver.cpp:218] Iteration 900 (19.1499 iter/s, 5.22197s/100 iters), loss = 0.73944
I1001 15:01:31.073253  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.73944 (* 1 = 0.73944 loss)
I1001 15:01:31.073258  5332 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1001 15:01:36.049125  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:01:36.258143  5332 solver.cpp:330] Iteration 1000, Testing net (#0)
I1001 15:01:37.450704  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:01:37.502046  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5711
I1001 15:01:37.502082  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.19759 (* 1 = 1.19759 loss)
I1001 15:01:37.556202  5332 solver.cpp:218] Iteration 1000 (15.4252 iter/s, 6.48288s/100 iters), loss = 0.875051
I1001 15:01:37.556237  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.875051 (* 1 = 0.875051 loss)
I1001 15:01:37.556244  5332 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1001 15:01:42.781896  5332 solver.cpp:218] Iteration 1100 (19.1365 iter/s, 5.2256s/100 iters), loss = 0.719587
I1001 15:01:42.781929  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.719587 (* 1 = 0.719587 loss)
I1001 15:01:42.781936  5332 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1001 15:01:48.018724  5332 solver.cpp:218] Iteration 1200 (19.0958 iter/s, 5.23674s/100 iters), loss = 0.812851
I1001 15:01:48.018885  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.812851 (* 1 = 0.812851 loss)
I1001 15:01:48.018895  5332 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1001 15:01:53.259085  5332 solver.cpp:218] Iteration 1300 (19.0834 iter/s, 5.24015s/100 iters), loss = 0.777173
I1001 15:01:53.259116  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.777173 (* 1 = 0.777173 loss)
I1001 15:01:53.259125  5332 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1001 15:01:58.494663  5332 solver.cpp:218] Iteration 1400 (19.1004 iter/s, 5.23549s/100 iters), loss = 0.721622
I1001 15:01:58.494710  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.721622 (* 1 = 0.721622 loss)
I1001 15:01:58.494719  5332 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1001 15:02:03.465324  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:02:03.675006  5332 solver.cpp:330] Iteration 1500, Testing net (#0)
I1001 15:02:04.868281  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:02:04.917513  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4987
I1001 15:02:04.917549  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.34671 (* 1 = 1.34671 loss)
I1001 15:02:04.969900  5332 solver.cpp:218] Iteration 1500 (15.4438 iter/s, 6.47509s/100 iters), loss = 0.757223
I1001 15:02:04.969931  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.757223 (* 1 = 0.757223 loss)
I1001 15:02:04.969939  5332 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1001 15:02:10.193389  5332 solver.cpp:218] Iteration 1600 (19.1446 iter/s, 5.22341s/100 iters), loss = 0.55841
I1001 15:02:10.193418  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.55841 (* 1 = 0.55841 loss)
I1001 15:02:10.193424  5332 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1001 15:02:15.416761  5332 solver.cpp:218] Iteration 1700 (19.145 iter/s, 5.22329s/100 iters), loss = 0.678105
I1001 15:02:15.416801  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.678105 (* 1 = 0.678105 loss)
I1001 15:02:15.416807  5332 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1001 15:02:20.652808  5332 solver.cpp:218] Iteration 1800 (19.0987 iter/s, 5.23595s/100 iters), loss = 0.647465
I1001 15:02:20.652958  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.647465 (* 1 = 0.647465 loss)
I1001 15:02:20.652966  5332 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1001 15:02:25.890836  5332 solver.cpp:218] Iteration 1900 (19.0919 iter/s, 5.23783s/100 iters), loss = 0.640334
I1001 15:02:25.890866  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.640334 (* 1 = 0.640334 loss)
I1001 15:02:25.890873  5332 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1001 15:02:30.857786  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:02:31.067281  5332 solver.cpp:330] Iteration 2000, Testing net (#0)
I1001 15:02:32.266752  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:02:32.316617  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6095
I1001 15:02:32.316642  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.12997 (* 1 = 1.12997 loss)
I1001 15:02:32.368854  5332 solver.cpp:218] Iteration 2000 (15.437 iter/s, 6.47793s/100 iters), loss = 0.678692
I1001 15:02:32.368878  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.678692 (* 1 = 0.678692 loss)
I1001 15:02:32.368885  5332 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1001 15:02:37.599196  5332 solver.cpp:218] Iteration 2100 (19.1195 iter/s, 5.23026s/100 iters), loss = 0.462684
I1001 15:02:37.599239  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.462684 (* 1 = 0.462684 loss)
I1001 15:02:37.599246  5332 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1001 15:02:42.832448  5332 solver.cpp:218] Iteration 2200 (19.1091 iter/s, 5.23312s/100 iters), loss = 0.674562
I1001 15:02:42.832489  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.674562 (* 1 = 0.674562 loss)
I1001 15:02:42.832494  5332 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1001 15:02:48.072561  5332 solver.cpp:218] Iteration 2300 (19.0839 iter/s, 5.24002s/100 iters), loss = 0.588603
I1001 15:02:48.072590  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.588603 (* 1 = 0.588603 loss)
I1001 15:02:48.072595  5332 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1001 15:02:53.303584  5332 solver.cpp:218] Iteration 2400 (19.117 iter/s, 5.23094s/100 iters), loss = 0.574181
I1001 15:02:53.303750  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.574181 (* 1 = 0.574181 loss)
I1001 15:02:53.303758  5332 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1001 15:02:58.264967  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:02:58.477179  5332 solver.cpp:330] Iteration 2500, Testing net (#0)
I1001 15:02:59.668342  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:02:59.718540  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5703
I1001 15:02:59.718585  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.22741 (* 1 = 1.22741 loss)
I1001 15:02:59.770306  5332 solver.cpp:218] Iteration 2500 (15.4643 iter/s, 6.4665s/100 iters), loss = 0.579276
I1001 15:02:59.770335  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.579276 (* 1 = 0.579276 loss)
I1001 15:02:59.770342  5332 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1001 15:03:05.008815  5332 solver.cpp:218] Iteration 2600 (19.0897 iter/s, 5.23842s/100 iters), loss = 0.44542
I1001 15:03:05.008857  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44542 (* 1 = 0.44542 loss)
I1001 15:03:05.008863  5332 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1001 15:03:10.237197  5332 solver.cpp:218] Iteration 2700 (19.1267 iter/s, 5.22829s/100 iters), loss = 0.558026
I1001 15:03:10.237226  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.558026 (* 1 = 0.558026 loss)
I1001 15:03:10.237231  5332 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1001 15:03:15.480968  5332 solver.cpp:218] Iteration 2800 (19.0705 iter/s, 5.24369s/100 iters), loss = 0.532086
I1001 15:03:15.480998  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.532086 (* 1 = 0.532086 loss)
I1001 15:03:15.481003  5332 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1001 15:03:20.720875  5332 solver.cpp:218] Iteration 2900 (19.0846 iter/s, 5.23983s/100 iters), loss = 0.521507
I1001 15:03:20.720903  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.521507 (* 1 = 0.521507 loss)
I1001 15:03:20.720908  5332 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1001 15:03:25.704530  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:03:25.914008  5332 solver.cpp:330] Iteration 3000, Testing net (#0)
I1001 15:03:27.105213  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:03:27.155017  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5882
I1001 15:03:27.155051  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.18492 (* 1 = 1.18492 loss)
I1001 15:03:27.207634  5332 solver.cpp:218] Iteration 3000 (15.4162 iter/s, 6.48668s/100 iters), loss = 0.532408
I1001 15:03:27.207660  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.532408 (* 1 = 0.532408 loss)
I1001 15:03:27.207666  5332 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1001 15:03:32.451179  5332 solver.cpp:218] Iteration 3100 (19.0713 iter/s, 5.24347s/100 iters), loss = 0.306055
I1001 15:03:32.451210  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306055 (* 1 = 0.306055 loss)
I1001 15:03:32.451225  5332 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1001 15:03:37.692128  5332 solver.cpp:218] Iteration 3200 (19.0808 iter/s, 5.24087s/100 iters), loss = 0.63014
I1001 15:03:37.692162  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.63014 (* 1 = 0.63014 loss)
I1001 15:03:37.692169  5332 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1001 15:03:42.921103  5332 solver.cpp:218] Iteration 3300 (19.1245 iter/s, 5.22889s/100 iters), loss = 0.588511
I1001 15:03:42.921131  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.588511 (* 1 = 0.588511 loss)
I1001 15:03:42.921147  5332 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1001 15:03:48.159308  5332 solver.cpp:218] Iteration 3400 (19.0908 iter/s, 5.23813s/100 iters), loss = 0.516537
I1001 15:03:48.159337  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.516537 (* 1 = 0.516537 loss)
I1001 15:03:48.159343  5332 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1001 15:03:53.136353  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:03:53.345135  5332 solver.cpp:330] Iteration 3500, Testing net (#0)
I1001 15:03:54.534967  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:03:54.585242  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7126
I1001 15:03:54.585275  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.846169 (* 1 = 0.846169 loss)
I1001 15:03:54.637471  5332 solver.cpp:218] Iteration 3500 (15.4367 iter/s, 6.47808s/100 iters), loss = 0.492462
I1001 15:03:54.637495  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.492462 (* 1 = 0.492462 loss)
I1001 15:03:54.637501  5332 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1001 15:03:59.882961  5332 solver.cpp:218] Iteration 3600 (19.0643 iter/s, 5.24542s/100 iters), loss = 0.399567
I1001 15:03:59.883075  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.399567 (* 1 = 0.399567 loss)
I1001 15:03:59.883082  5332 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1001 15:04:05.129779  5332 solver.cpp:218] Iteration 3700 (19.0597 iter/s, 5.24667s/100 iters), loss = 0.474249
I1001 15:04:05.129820  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.474249 (* 1 = 0.474249 loss)
I1001 15:04:05.129827  5332 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1001 15:04:10.366524  5332 solver.cpp:218] Iteration 3800 (19.0962 iter/s, 5.23666s/100 iters), loss = 0.52086
I1001 15:04:10.366555  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.52086 (* 1 = 0.52086 loss)
I1001 15:04:10.366562  5332 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1001 15:04:15.612566  5332 solver.cpp:218] Iteration 3900 (19.0623 iter/s, 5.24597s/100 iters), loss = 0.425626
I1001 15:04:15.612597  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.425626 (* 1 = 0.425626 loss)
I1001 15:04:15.612612  5332 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1001 15:04:20.597131  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:04:20.806840  5332 solver.cpp:330] Iteration 4000, Testing net (#0)
I1001 15:04:21.997972  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:04:22.047965  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.647
I1001 15:04:22.048001  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.04048 (* 1 = 1.04048 loss)
I1001 15:04:22.100488  5332 solver.cpp:218] Iteration 4000 (15.4135 iter/s, 6.48784s/100 iters), loss = 0.445027
I1001 15:04:22.100512  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.445027 (* 1 = 0.445027 loss)
I1001 15:04:22.100519  5332 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1001 15:04:27.348587  5332 solver.cpp:218] Iteration 4100 (19.0548 iter/s, 5.24803s/100 iters), loss = 0.400682
I1001 15:04:27.348616  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.400682 (* 1 = 0.400682 loss)
I1001 15:04:27.348623  5332 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1001 15:04:32.586506  5332 solver.cpp:218] Iteration 4200 (19.0918 iter/s, 5.23784s/100 iters), loss = 0.435152
I1001 15:04:32.586647  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.435152 (* 1 = 0.435152 loss)
I1001 15:04:32.586655  5332 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1001 15:04:37.826160  5332 solver.cpp:218] Iteration 4300 (19.0859 iter/s, 5.23947s/100 iters), loss = 0.471007
I1001 15:04:37.826190  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.471007 (* 1 = 0.471007 loss)
I1001 15:04:37.826197  5332 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1001 15:04:43.056579  5332 solver.cpp:218] Iteration 4400 (19.1192 iter/s, 5.23035s/100 iters), loss = 0.467979
I1001 15:04:43.056610  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.467979 (* 1 = 0.467979 loss)
I1001 15:04:43.056615  5332 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1001 15:04:48.035387  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:04:48.244580  5332 solver.cpp:330] Iteration 4500, Testing net (#0)
I1001 15:04:49.434933  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:04:49.486130  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7684
I1001 15:04:49.486167  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.697516 (* 1 = 0.697516 loss)
I1001 15:04:49.541136  5332 solver.cpp:218] Iteration 4500 (15.4214 iter/s, 6.48448s/100 iters), loss = 0.441876
I1001 15:04:49.541175  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.441876 (* 1 = 0.441876 loss)
I1001 15:04:49.541183  5332 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1001 15:04:54.777216  5332 solver.cpp:218] Iteration 4600 (19.0986 iter/s, 5.236s/100 iters), loss = 0.360618
I1001 15:04:54.777256  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.360618 (* 1 = 0.360618 loss)
I1001 15:04:54.777261  5332 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1001 15:05:00.022826  5332 solver.cpp:218] Iteration 4700 (19.0638 iter/s, 5.24553s/100 iters), loss = 0.407297
I1001 15:05:00.022855  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.407297 (* 1 = 0.407297 loss)
I1001 15:05:00.022861  5332 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1001 15:05:05.268184  5332 solver.cpp:218] Iteration 4800 (19.0647 iter/s, 5.24529s/100 iters), loss = 0.475767
I1001 15:05:05.268290  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.475767 (* 1 = 0.475767 loss)
I1001 15:05:05.268297  5332 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1001 15:05:10.504019  5332 solver.cpp:218] Iteration 4900 (19.0997 iter/s, 5.23569s/100 iters), loss = 0.355573
I1001 15:05:10.504066  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.355573 (* 1 = 0.355573 loss)
I1001 15:05:10.504084  5332 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1001 15:05:15.480931  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:05:15.690136  5332 solver.cpp:330] Iteration 5000, Testing net (#0)
I1001 15:05:16.887568  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:05:16.936969  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6476
I1001 15:05:16.937005  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.953724 (* 1 = 0.953724 loss)
I1001 15:05:16.990159  5332 solver.cpp:218] Iteration 5000 (15.4177 iter/s, 6.48605s/100 iters), loss = 0.332519
I1001 15:05:16.990190  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332519 (* 1 = 0.332519 loss)
I1001 15:05:16.990196  5332 sgd_solver.cpp:105] Iteration 5000, lr = 0.1
I1001 15:05:22.221016  5332 solver.cpp:218] Iteration 5100 (19.1176 iter/s, 5.23079s/100 iters), loss = 0.273368
I1001 15:05:22.221045  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273368 (* 1 = 0.273368 loss)
I1001 15:05:22.221051  5332 sgd_solver.cpp:105] Iteration 5100, lr = 0.1
I1001 15:05:27.461628  5332 solver.cpp:218] Iteration 5200 (19.082 iter/s, 5.24055s/100 iters), loss = 0.42639
I1001 15:05:27.461658  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.42639 (* 1 = 0.42639 loss)
I1001 15:05:27.461664  5332 sgd_solver.cpp:105] Iteration 5200, lr = 0.1
I1001 15:05:32.698276  5332 solver.cpp:218] Iteration 5300 (19.0964 iter/s, 5.23659s/100 iters), loss = 0.418825
I1001 15:05:32.698305  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.418824 (* 1 = 0.418824 loss)
I1001 15:05:32.698312  5332 sgd_solver.cpp:105] Iteration 5300, lr = 0.1
I1001 15:05:37.940038  5332 solver.cpp:218] Iteration 5400 (19.0778 iter/s, 5.2417s/100 iters), loss = 0.34876
I1001 15:05:37.940135  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.34876 (* 1 = 0.34876 loss)
I1001 15:05:37.940141  5332 sgd_solver.cpp:105] Iteration 5400, lr = 0.1
I1001 15:05:42.910598  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:05:43.119868  5332 solver.cpp:330] Iteration 5500, Testing net (#0)
I1001 15:05:44.316931  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:05:44.367005  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7716
I1001 15:05:44.367040  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.682971 (* 1 = 0.682971 loss)
I1001 15:05:44.419744  5332 solver.cpp:218] Iteration 5500 (15.4331 iter/s, 6.47958s/100 iters), loss = 0.366389
I1001 15:05:44.419766  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.366388 (* 1 = 0.366388 loss)
I1001 15:05:44.419773  5332 sgd_solver.cpp:105] Iteration 5500, lr = 0.1
I1001 15:05:49.663790  5332 solver.cpp:218] Iteration 5600 (19.0695 iter/s, 5.24399s/100 iters), loss = 0.381731
I1001 15:05:49.663823  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.381731 (* 1 = 0.381731 loss)
I1001 15:05:49.663830  5332 sgd_solver.cpp:105] Iteration 5600, lr = 0.1
I1001 15:05:54.898226  5332 solver.cpp:218] Iteration 5700 (19.1046 iter/s, 5.23434s/100 iters), loss = 0.361418
I1001 15:05:54.898257  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.361418 (* 1 = 0.361418 loss)
I1001 15:05:54.898262  5332 sgd_solver.cpp:105] Iteration 5700, lr = 0.1
I1001 15:06:00.140955  5332 solver.cpp:218] Iteration 5800 (19.0743 iter/s, 5.24267s/100 iters), loss = 0.436546
I1001 15:06:00.140985  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.436546 (* 1 = 0.436546 loss)
I1001 15:06:00.140990  5332 sgd_solver.cpp:105] Iteration 5800, lr = 0.1
I1001 15:06:05.377588  5332 solver.cpp:218] Iteration 5900 (19.0965 iter/s, 5.23657s/100 iters), loss = 0.319176
I1001 15:06:05.377616  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319176 (* 1 = 0.319176 loss)
I1001 15:06:05.377624  5332 sgd_solver.cpp:105] Iteration 5900, lr = 0.1
I1001 15:06:10.344836  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:06:10.559554  5332 solver.cpp:330] Iteration 6000, Testing net (#0)
I1001 15:06:11.753015  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:06:11.803216  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6936
I1001 15:06:11.803241  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.886146 (* 1 = 0.886146 loss)
I1001 15:06:11.855612  5332 solver.cpp:218] Iteration 6000 (15.4369 iter/s, 6.47796s/100 iters), loss = 0.376064
I1001 15:06:11.855638  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376064 (* 1 = 0.376064 loss)
I1001 15:06:11.855648  5332 sgd_solver.cpp:105] Iteration 6000, lr = 0.1
I1001 15:06:17.099565  5332 solver.cpp:218] Iteration 6100 (19.0698 iter/s, 5.2439s/100 iters), loss = 0.265316
I1001 15:06:17.099597  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265315 (* 1 = 0.265315 loss)
I1001 15:06:17.099606  5332 sgd_solver.cpp:105] Iteration 6100, lr = 0.1
I1001 15:06:22.329578  5332 solver.cpp:218] Iteration 6200 (19.1206 iter/s, 5.22995s/100 iters), loss = 0.338188
I1001 15:06:22.329610  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.338188 (* 1 = 0.338188 loss)
I1001 15:06:22.329628  5332 sgd_solver.cpp:105] Iteration 6200, lr = 0.1
I1001 15:06:27.573302  5332 solver.cpp:218] Iteration 6300 (19.0706 iter/s, 5.24367s/100 iters), loss = 0.417057
I1001 15:06:27.573333  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.417057 (* 1 = 0.417057 loss)
I1001 15:06:27.573341  5332 sgd_solver.cpp:105] Iteration 6300, lr = 0.1
I1001 15:06:32.819836  5332 solver.cpp:218] Iteration 6400 (19.0604 iter/s, 5.24648s/100 iters), loss = 0.437898
I1001 15:06:32.819869  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.437897 (* 1 = 0.437897 loss)
I1001 15:06:32.819876  5332 sgd_solver.cpp:105] Iteration 6400, lr = 0.1
I1001 15:06:37.799302  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:06:38.008277  5332 solver.cpp:330] Iteration 6500, Testing net (#0)
I1001 15:06:39.197994  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:06:39.248095  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7174
I1001 15:06:39.248121  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.822427 (* 1 = 0.822427 loss)
I1001 15:06:39.300108  5332 solver.cpp:218] Iteration 6500 (15.4316 iter/s, 6.48021s/100 iters), loss = 0.316983
I1001 15:06:39.300140  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316983 (* 1 = 0.316983 loss)
I1001 15:06:39.300149  5332 sgd_solver.cpp:105] Iteration 6500, lr = 0.1
I1001 15:06:44.543911  5332 solver.cpp:218] Iteration 6600 (19.0704 iter/s, 5.24374s/100 iters), loss = 0.33645
I1001 15:06:44.544061  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.33645 (* 1 = 0.33645 loss)
I1001 15:06:44.544090  5332 sgd_solver.cpp:105] Iteration 6600, lr = 0.1
I1001 15:06:49.784198  5332 solver.cpp:218] Iteration 6700 (19.0835 iter/s, 5.24012s/100 iters), loss = 0.399896
I1001 15:06:49.784230  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.399895 (* 1 = 0.399895 loss)
I1001 15:06:49.784240  5332 sgd_solver.cpp:105] Iteration 6700, lr = 0.1
I1001 15:06:55.018602  5332 solver.cpp:218] Iteration 6800 (19.1046 iter/s, 5.23434s/100 iters), loss = 0.406929
I1001 15:06:55.018635  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.406929 (* 1 = 0.406929 loss)
I1001 15:06:55.018652  5332 sgd_solver.cpp:105] Iteration 6800, lr = 0.1
I1001 15:07:00.262501  5332 solver.cpp:218] Iteration 6900 (19.07 iter/s, 5.24384s/100 iters), loss = 0.435265
I1001 15:07:00.262540  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.435264 (* 1 = 0.435264 loss)
I1001 15:07:00.262558  5332 sgd_solver.cpp:105] Iteration 6900, lr = 0.1
I1001 15:07:05.241376  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:07:05.450706  5332 solver.cpp:330] Iteration 7000, Testing net (#0)
I1001 15:07:06.638219  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:07:06.688231  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7837
I1001 15:07:06.688257  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.620602 (* 1 = 0.620602 loss)
I1001 15:07:06.740478  5332 solver.cpp:218] Iteration 7000 (15.4371 iter/s, 6.47791s/100 iters), loss = 0.39772
I1001 15:07:06.740504  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.39772 (* 1 = 0.39772 loss)
I1001 15:07:06.740514  5332 sgd_solver.cpp:105] Iteration 7000, lr = 0.1
I1001 15:07:11.983386  5332 solver.cpp:218] Iteration 7100 (19.0736 iter/s, 5.24286s/100 iters), loss = 0.289247
I1001 15:07:11.983418  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289247 (* 1 = 0.289247 loss)
I1001 15:07:11.983427  5332 sgd_solver.cpp:105] Iteration 7100, lr = 0.1
I1001 15:07:17.230798  5332 solver.cpp:218] Iteration 7200 (19.0572 iter/s, 5.24735s/100 iters), loss = 0.387004
I1001 15:07:17.230926  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.387004 (* 1 = 0.387004 loss)
I1001 15:07:17.230945  5332 sgd_solver.cpp:105] Iteration 7200, lr = 0.1
I1001 15:07:22.468979  5332 solver.cpp:218] Iteration 7300 (19.0911 iter/s, 5.23803s/100 iters), loss = 0.447163
I1001 15:07:22.469032  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.447163 (* 1 = 0.447163 loss)
I1001 15:07:22.469050  5332 sgd_solver.cpp:105] Iteration 7300, lr = 0.1
I1001 15:07:27.708879  5332 solver.cpp:218] Iteration 7400 (19.0847 iter/s, 5.23979s/100 iters), loss = 0.40342
I1001 15:07:27.708909  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.40342 (* 1 = 0.40342 loss)
I1001 15:07:27.708914  5332 sgd_solver.cpp:105] Iteration 7400, lr = 0.1
I1001 15:07:32.694306  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:07:32.904098  5332 solver.cpp:330] Iteration 7500, Testing net (#0)
I1001 15:07:34.094161  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:07:34.144655  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.637
I1001 15:07:34.144680  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.34204 (* 1 = 1.34204 loss)
I1001 15:07:34.196857  5332 solver.cpp:218] Iteration 7500 (15.4133 iter/s, 6.48792s/100 iters), loss = 0.325638
I1001 15:07:34.196885  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.325638 (* 1 = 0.325638 loss)
I1001 15:07:34.196893  5332 sgd_solver.cpp:105] Iteration 7500, lr = 0.1
I1001 15:07:39.446780  5332 solver.cpp:218] Iteration 7600 (19.0481 iter/s, 5.24987s/100 iters), loss = 0.267864
I1001 15:07:39.446810  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267864 (* 1 = 0.267864 loss)
I1001 15:07:39.446816  5332 sgd_solver.cpp:105] Iteration 7600, lr = 0.1
I1001 15:07:44.693097  5332 solver.cpp:218] Iteration 7700 (19.0612 iter/s, 5.24626s/100 iters), loss = 0.370917
I1001 15:07:44.693136  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.370917 (* 1 = 0.370917 loss)
I1001 15:07:44.693143  5332 sgd_solver.cpp:105] Iteration 7700, lr = 0.1
I1001 15:07:49.932799  5332 solver.cpp:218] Iteration 7800 (19.0853 iter/s, 5.23963s/100 iters), loss = 0.432537
I1001 15:07:49.932917  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.432537 (* 1 = 0.432537 loss)
I1001 15:07:49.932942  5332 sgd_solver.cpp:105] Iteration 7800, lr = 0.1
I1001 15:07:55.167443  5332 solver.cpp:218] Iteration 7900 (19.104 iter/s, 5.2345s/100 iters), loss = 0.278179
I1001 15:07:55.167484  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278179 (* 1 = 0.278179 loss)
I1001 15:07:55.167490  5332 sgd_solver.cpp:105] Iteration 7900, lr = 0.1
I1001 15:08:00.150501  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:08:00.360554  5332 solver.cpp:330] Iteration 8000, Testing net (#0)
I1001 15:08:01.553351  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:08:01.604390  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7604
I1001 15:08:01.604427  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.742615 (* 1 = 0.742615 loss)
I1001 15:08:01.657999  5332 solver.cpp:218] Iteration 8000 (15.4072 iter/s, 6.49048s/100 iters), loss = 0.320985
I1001 15:08:01.658030  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320984 (* 1 = 0.320984 loss)
I1001 15:08:01.658037  5332 sgd_solver.cpp:105] Iteration 8000, lr = 0.1
I1001 15:08:06.896096  5332 solver.cpp:218] Iteration 8100 (19.0911 iter/s, 5.23804s/100 iters), loss = 0.313342
I1001 15:08:06.896136  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313342 (* 1 = 0.313342 loss)
I1001 15:08:06.896142  5332 sgd_solver.cpp:105] Iteration 8100, lr = 0.1
I1001 15:08:12.140791  5332 solver.cpp:218] Iteration 8200 (19.0671 iter/s, 5.24463s/100 iters), loss = 0.327252
I1001 15:08:12.140820  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.327252 (* 1 = 0.327252 loss)
I1001 15:08:12.140827  5332 sgd_solver.cpp:105] Iteration 8200, lr = 0.1
I1001 15:08:17.382616  5332 solver.cpp:218] Iteration 8300 (19.0775 iter/s, 5.24177s/100 iters), loss = 0.440145
I1001 15:08:17.382643  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.440145 (* 1 = 0.440145 loss)
I1001 15:08:17.382649  5332 sgd_solver.cpp:105] Iteration 8300, lr = 0.1
I1001 15:08:22.625669  5332 solver.cpp:218] Iteration 8400 (19.0731 iter/s, 5.24299s/100 iters), loss = 0.373147
I1001 15:08:22.625797  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.373146 (* 1 = 0.373146 loss)
I1001 15:08:22.625818  5332 sgd_solver.cpp:105] Iteration 8400, lr = 0.1
I1001 15:08:27.597158  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:08:27.807214  5332 solver.cpp:330] Iteration 8500, Testing net (#0)
I1001 15:08:29.003612  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:08:29.053330  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7749
I1001 15:08:29.053365  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.677487 (* 1 = 0.677487 loss)
I1001 15:08:29.105515  5332 solver.cpp:218] Iteration 8500 (15.4329 iter/s, 6.47967s/100 iters), loss = 0.285479
I1001 15:08:29.105548  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285479 (* 1 = 0.285479 loss)
I1001 15:08:29.105556  5332 sgd_solver.cpp:105] Iteration 8500, lr = 0.1
I1001 15:08:34.339869  5332 solver.cpp:218] Iteration 8600 (19.1048 iter/s, 5.23429s/100 iters), loss = 0.356243
I1001 15:08:34.339900  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.356243 (* 1 = 0.356243 loss)
I1001 15:08:34.339905  5332 sgd_solver.cpp:105] Iteration 8600, lr = 0.1
I1001 15:08:39.581209  5332 solver.cpp:218] Iteration 8700 (19.0793 iter/s, 5.24128s/100 iters), loss = 0.390557
I1001 15:08:39.581249  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390557 (* 1 = 0.390557 loss)
I1001 15:08:39.581255  5332 sgd_solver.cpp:105] Iteration 8700, lr = 0.1
I1001 15:08:44.822021  5332 solver.cpp:218] Iteration 8800 (19.0813 iter/s, 5.24074s/100 iters), loss = 0.44147
I1001 15:08:44.822049  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44147 (* 1 = 0.44147 loss)
I1001 15:08:44.822055  5332 sgd_solver.cpp:105] Iteration 8800, lr = 0.1
I1001 15:08:50.071216  5332 solver.cpp:218] Iteration 8900 (19.0507 iter/s, 5.24914s/100 iters), loss = 0.35762
I1001 15:08:50.071256  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.35762 (* 1 = 0.35762 loss)
I1001 15:08:50.071262  5332 sgd_solver.cpp:105] Iteration 8900, lr = 0.1
I1001 15:08:55.046198  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:08:55.255269  5332 solver.cpp:330] Iteration 9000, Testing net (#0)
I1001 15:08:56.455622  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:08:56.505511  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7966
I1001 15:08:56.505537  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.597052 (* 1 = 0.597052 loss)
I1001 15:08:56.557931  5332 solver.cpp:218] Iteration 9000 (15.4163 iter/s, 6.48664s/100 iters), loss = 0.372258
I1001 15:08:56.557967  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372258 (* 1 = 0.372258 loss)
I1001 15:08:56.557974  5332 sgd_solver.cpp:105] Iteration 9000, lr = 0.1
I1001 15:09:01.809283  5332 solver.cpp:218] Iteration 9100 (19.0429 iter/s, 5.25129s/100 iters), loss = 0.316401
I1001 15:09:01.809312  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316401 (* 1 = 0.316401 loss)
I1001 15:09:01.809319  5332 sgd_solver.cpp:105] Iteration 9100, lr = 0.1
I1001 15:09:07.047446  5332 solver.cpp:218] Iteration 9200 (19.0909 iter/s, 5.2381s/100 iters), loss = 0.37662
I1001 15:09:07.047477  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.37662 (* 1 = 0.37662 loss)
I1001 15:09:07.047483  5332 sgd_solver.cpp:105] Iteration 9200, lr = 0.1
I1001 15:09:12.295079  5332 solver.cpp:218] Iteration 9300 (19.0564 iter/s, 5.24758s/100 iters), loss = 0.335143
I1001 15:09:12.295109  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.335143 (* 1 = 0.335143 loss)
I1001 15:09:12.295115  5332 sgd_solver.cpp:105] Iteration 9300, lr = 0.1
I1001 15:09:17.538209  5332 solver.cpp:218] Iteration 9400 (19.0728 iter/s, 5.24307s/100 iters), loss = 0.34977
I1001 15:09:17.538249  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.34977 (* 1 = 0.34977 loss)
I1001 15:09:17.538255  5332 sgd_solver.cpp:105] Iteration 9400, lr = 0.1
I1001 15:09:22.511716  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:09:22.726106  5332 solver.cpp:330] Iteration 9500, Testing net (#0)
I1001 15:09:23.916612  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:09:23.966715  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.766
I1001 15:09:23.966742  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.687973 (* 1 = 0.687973 loss)
I1001 15:09:24.018894  5332 solver.cpp:218] Iteration 9500 (15.4306 iter/s, 6.48062s/100 iters), loss = 0.342958
I1001 15:09:24.018921  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.342958 (* 1 = 0.342958 loss)
I1001 15:09:24.018931  5332 sgd_solver.cpp:105] Iteration 9500, lr = 0.1
I1001 15:09:29.262049  5332 solver.cpp:218] Iteration 9600 (19.0727 iter/s, 5.2431s/100 iters), loss = 0.353133
I1001 15:09:29.262243  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353133 (* 1 = 0.353133 loss)
I1001 15:09:29.262264  5332 sgd_solver.cpp:105] Iteration 9600, lr = 0.1
I1001 15:09:34.504370  5332 solver.cpp:218] Iteration 9700 (19.0763 iter/s, 5.24212s/100 iters), loss = 0.298435
I1001 15:09:34.504410  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.298435 (* 1 = 0.298435 loss)
I1001 15:09:34.504420  5332 sgd_solver.cpp:105] Iteration 9700, lr = 0.1
I1001 15:09:39.747675  5332 solver.cpp:218] Iteration 9800 (19.0722 iter/s, 5.24324s/100 iters), loss = 0.377817
I1001 15:09:39.747705  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.377817 (* 1 = 0.377817 loss)
I1001 15:09:39.747721  5332 sgd_solver.cpp:105] Iteration 9800, lr = 0.1
I1001 15:09:44.995199  5332 solver.cpp:218] Iteration 9900 (19.0568 iter/s, 5.24747s/100 iters), loss = 0.340712
I1001 15:09:44.995229  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340712 (* 1 = 0.340712 loss)
I1001 15:09:44.995245  5332 sgd_solver.cpp:105] Iteration 9900, lr = 0.1
I1001 15:09:49.978514  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:09:50.187520  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_10000.caffemodel
I1001 15:09:50.196048  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_10000.solverstate
I1001 15:09:50.197397  5332 solver.cpp:330] Iteration 10000, Testing net (#0)
I1001 15:09:51.387836  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:09:51.438534  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7212
I1001 15:09:51.438568  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.852488 (* 1 = 0.852488 loss)
I1001 15:09:51.490660  5332 solver.cpp:218] Iteration 10000 (15.3955 iter/s, 6.4954s/100 iters), loss = 0.304294
I1001 15:09:51.490691  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.304294 (* 1 = 0.304294 loss)
I1001 15:09:51.490698  5332 sgd_solver.cpp:105] Iteration 10000, lr = 0.1
I1001 15:09:56.737131  5332 solver.cpp:218] Iteration 10100 (19.0606 iter/s, 5.24642s/100 iters), loss = 0.220432
I1001 15:09:56.737162  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220432 (* 1 = 0.220432 loss)
I1001 15:09:56.737169  5332 sgd_solver.cpp:105] Iteration 10100, lr = 0.1
I1001 15:10:01.978196  5332 solver.cpp:218] Iteration 10200 (19.0803 iter/s, 5.24101s/100 iters), loss = 0.355214
I1001 15:10:01.978324  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.355214 (* 1 = 0.355214 loss)
I1001 15:10:01.978332  5332 sgd_solver.cpp:105] Iteration 10200, lr = 0.1
I1001 15:10:07.211308  5332 solver.cpp:218] Iteration 10300 (19.1096 iter/s, 5.23297s/100 iters), loss = 0.424933
I1001 15:10:07.211347  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.424933 (* 1 = 0.424933 loss)
I1001 15:10:07.211354  5332 sgd_solver.cpp:105] Iteration 10300, lr = 0.1
I1001 15:10:12.448839  5332 solver.cpp:218] Iteration 10400 (19.0932 iter/s, 5.23747s/100 iters), loss = 0.253379
I1001 15:10:12.448868  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253379 (* 1 = 0.253379 loss)
I1001 15:10:12.448873  5332 sgd_solver.cpp:105] Iteration 10400, lr = 0.1
I1001 15:10:17.428007  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:10:17.637362  5332 solver.cpp:330] Iteration 10500, Testing net (#0)
I1001 15:10:18.827170  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:10:18.877537  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7137
I1001 15:10:18.877571  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.872862 (* 1 = 0.872862 loss)
I1001 15:10:18.929843  5332 solver.cpp:218] Iteration 10500 (15.4298 iter/s, 6.48095s/100 iters), loss = 0.343287
I1001 15:10:18.929868  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.343287 (* 1 = 0.343287 loss)
I1001 15:10:18.929874  5332 sgd_solver.cpp:105] Iteration 10500, lr = 0.1
I1001 15:10:24.168126  5332 solver.cpp:218] Iteration 10600 (19.0904 iter/s, 5.23823s/100 iters), loss = 0.312272
I1001 15:10:24.168166  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312272 (* 1 = 0.312272 loss)
I1001 15:10:24.168171  5332 sgd_solver.cpp:105] Iteration 10600, lr = 0.1
I1001 15:10:29.411511  5332 solver.cpp:218] Iteration 10700 (19.0719 iter/s, 5.24332s/100 iters), loss = 0.32664
I1001 15:10:29.411543  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.32664 (* 1 = 0.32664 loss)
I1001 15:10:29.411551  5332 sgd_solver.cpp:105] Iteration 10700, lr = 0.1
I1001 15:10:34.659801  5332 solver.cpp:218] Iteration 10800 (19.054 iter/s, 5.24823s/100 iters), loss = 0.427788
I1001 15:10:34.659921  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.427788 (* 1 = 0.427788 loss)
I1001 15:10:34.659932  5332 sgd_solver.cpp:105] Iteration 10800, lr = 0.1
I1001 15:10:39.899026  5332 solver.cpp:218] Iteration 10900 (19.0873 iter/s, 5.23909s/100 iters), loss = 0.390015
I1001 15:10:39.899060  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390015 (* 1 = 0.390015 loss)
I1001 15:10:39.899067  5332 sgd_solver.cpp:105] Iteration 10900, lr = 0.1
I1001 15:10:44.884579  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:10:45.093281  5332 solver.cpp:330] Iteration 11000, Testing net (#0)
I1001 15:10:46.283098  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:10:46.333187  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7925
I1001 15:10:46.333214  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.629569 (* 1 = 0.629569 loss)
I1001 15:10:46.385498  5332 solver.cpp:218] Iteration 11000 (15.4168 iter/s, 6.48641s/100 iters), loss = 0.296522
I1001 15:10:46.385527  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296522 (* 1 = 0.296522 loss)
I1001 15:10:46.385536  5332 sgd_solver.cpp:105] Iteration 11000, lr = 0.1
I1001 15:10:51.626137  5332 solver.cpp:218] Iteration 11100 (19.0818 iter/s, 5.24059s/100 iters), loss = 0.265034
I1001 15:10:51.626175  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265034 (* 1 = 0.265034 loss)
I1001 15:10:51.626181  5332 sgd_solver.cpp:105] Iteration 11100, lr = 0.1
I1001 15:10:56.865777  5332 solver.cpp:218] Iteration 11200 (19.0855 iter/s, 5.23958s/100 iters), loss = 0.326085
I1001 15:10:56.865818  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.326085 (* 1 = 0.326085 loss)
I1001 15:10:56.865823  5332 sgd_solver.cpp:105] Iteration 11200, lr = 0.1
I1001 15:11:02.105217  5332 solver.cpp:218] Iteration 11300 (19.0863 iter/s, 5.23937s/100 iters), loss = 0.400742
I1001 15:11:02.105257  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.400742 (* 1 = 0.400742 loss)
I1001 15:11:02.105262  5332 sgd_solver.cpp:105] Iteration 11300, lr = 0.1
I1001 15:11:07.332733  5332 solver.cpp:218] Iteration 11400 (19.1298 iter/s, 5.22745s/100 iters), loss = 0.284092
I1001 15:11:07.332872  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284092 (* 1 = 0.284092 loss)
I1001 15:11:07.332890  5332 sgd_solver.cpp:105] Iteration 11400, lr = 0.1
I1001 15:11:12.306077  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:11:12.516137  5332 solver.cpp:330] Iteration 11500, Testing net (#0)
I1001 15:11:13.714268  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:11:13.764470  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7917
I1001 15:11:13.764497  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.631674 (* 1 = 0.631674 loss)
I1001 15:11:13.816849  5332 solver.cpp:218] Iteration 11500 (15.4227 iter/s, 6.48395s/100 iters), loss = 0.219648
I1001 15:11:13.816879  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219648 (* 1 = 0.219648 loss)
I1001 15:11:13.816887  5332 sgd_solver.cpp:105] Iteration 11500, lr = 0.1
I1001 15:11:19.054183  5332 solver.cpp:218] Iteration 11600 (19.0939 iter/s, 5.23728s/100 iters), loss = 0.323604
I1001 15:11:19.054211  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.323604 (* 1 = 0.323604 loss)
I1001 15:11:19.054217  5332 sgd_solver.cpp:105] Iteration 11600, lr = 0.1
I1001 15:11:24.299026  5332 solver.cpp:218] Iteration 11700 (19.0665 iter/s, 5.24479s/100 iters), loss = 0.357146
I1001 15:11:24.299064  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.357146 (* 1 = 0.357146 loss)
I1001 15:11:24.299070  5332 sgd_solver.cpp:105] Iteration 11700, lr = 0.1
I1001 15:11:29.547756  5332 solver.cpp:218] Iteration 11800 (19.0525 iter/s, 5.24866s/100 iters), loss = 0.399973
I1001 15:11:29.547796  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.399972 (* 1 = 0.399972 loss)
I1001 15:11:29.547801  5332 sgd_solver.cpp:105] Iteration 11800, lr = 0.1
I1001 15:11:34.793678  5332 solver.cpp:218] Iteration 11900 (19.0627 iter/s, 5.24585s/100 iters), loss = 0.258088
I1001 15:11:34.793720  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258088 (* 1 = 0.258088 loss)
I1001 15:11:34.793727  5332 sgd_solver.cpp:105] Iteration 11900, lr = 0.1
I1001 15:11:39.767388  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:11:39.977423  5332 solver.cpp:330] Iteration 12000, Testing net (#0)
I1001 15:11:41.173749  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:11:41.223623  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8103
I1001 15:11:41.223659  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.554382 (* 1 = 0.554382 loss)
I1001 15:11:41.276407  5332 solver.cpp:218] Iteration 12000 (15.4258 iter/s, 6.48266s/100 iters), loss = 0.231741
I1001 15:11:41.276439  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231741 (* 1 = 0.231741 loss)
I1001 15:11:41.276448  5332 sgd_solver.cpp:105] Iteration 12000, lr = 0.1
I1001 15:11:46.513736  5332 solver.cpp:218] Iteration 12100 (19.0939 iter/s, 5.23727s/100 iters), loss = 0.297751
I1001 15:11:46.513770  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297751 (* 1 = 0.297751 loss)
I1001 15:11:46.513777  5332 sgd_solver.cpp:105] Iteration 12100, lr = 0.1
I1001 15:11:51.749682  5332 solver.cpp:218] Iteration 12200 (19.099 iter/s, 5.23588s/100 iters), loss = 0.439043
I1001 15:11:51.749723  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.439043 (* 1 = 0.439043 loss)
I1001 15:11:51.749729  5332 sgd_solver.cpp:105] Iteration 12200, lr = 0.1
I1001 15:11:56.992322  5332 solver.cpp:218] Iteration 12300 (19.0746 iter/s, 5.24257s/100 iters), loss = 0.37207
I1001 15:11:56.992352  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.37207 (* 1 = 0.37207 loss)
I1001 15:11:56.992358  5332 sgd_solver.cpp:105] Iteration 12300, lr = 0.1
I1001 15:12:02.241567  5332 solver.cpp:218] Iteration 12400 (19.0506 iter/s, 5.24919s/100 iters), loss = 0.321658
I1001 15:12:02.241597  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.321658 (* 1 = 0.321658 loss)
I1001 15:12:02.241603  5332 sgd_solver.cpp:105] Iteration 12400, lr = 0.1
I1001 15:12:07.219117  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:12:07.428757  5332 solver.cpp:330] Iteration 12500, Testing net (#0)
I1001 15:12:08.625476  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:12:08.675714  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6726
I1001 15:12:08.675739  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.26376 (* 1 = 1.26376 loss)
I1001 15:12:08.727928  5332 solver.cpp:218] Iteration 12500 (15.4171 iter/s, 6.4863s/100 iters), loss = 0.386454
I1001 15:12:08.727952  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.386454 (* 1 = 0.386454 loss)
I1001 15:12:08.727959  5332 sgd_solver.cpp:105] Iteration 12500, lr = 0.1
I1001 15:12:13.972415  5332 solver.cpp:218] Iteration 12600 (19.0678 iter/s, 5.24443s/100 iters), loss = 0.295583
I1001 15:12:13.972539  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.295583 (* 1 = 0.295583 loss)
I1001 15:12:13.972548  5332 sgd_solver.cpp:105] Iteration 12600, lr = 0.1
I1001 15:12:19.204857  5332 solver.cpp:218] Iteration 12700 (19.1121 iter/s, 5.23229s/100 iters), loss = 0.310448
I1001 15:12:19.204893  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310447 (* 1 = 0.310447 loss)
I1001 15:12:19.204901  5332 sgd_solver.cpp:105] Iteration 12700, lr = 0.1
I1001 15:12:24.448071  5332 solver.cpp:218] Iteration 12800 (19.0725 iter/s, 5.24315s/100 iters), loss = 0.39483
I1001 15:12:24.448101  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.39483 (* 1 = 0.39483 loss)
I1001 15:12:24.448107  5332 sgd_solver.cpp:105] Iteration 12800, lr = 0.1
I1001 15:12:29.688841  5332 solver.cpp:218] Iteration 12900 (19.0814 iter/s, 5.24072s/100 iters), loss = 0.373995
I1001 15:12:29.688869  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.373995 (* 1 = 0.373995 loss)
I1001 15:12:29.688885  5332 sgd_solver.cpp:105] Iteration 12900, lr = 0.1
I1001 15:12:34.665926  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:12:34.876781  5332 solver.cpp:330] Iteration 13000, Testing net (#0)
I1001 15:12:36.066081  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:12:36.115974  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7889
I1001 15:12:36.116001  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.678788 (* 1 = 0.678788 loss)
I1001 15:12:36.168213  5332 solver.cpp:218] Iteration 13000 (15.4337 iter/s, 6.47932s/100 iters), loss = 0.281621
I1001 15:12:36.168241  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281621 (* 1 = 0.281621 loss)
I1001 15:12:36.168248  5332 sgd_solver.cpp:105] Iteration 13000, lr = 0.1
I1001 15:12:41.412618  5332 solver.cpp:218] Iteration 13100 (19.0681 iter/s, 5.24435s/100 iters), loss = 0.266328
I1001 15:12:41.412658  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266327 (* 1 = 0.266327 loss)
I1001 15:12:41.412664  5332 sgd_solver.cpp:105] Iteration 13100, lr = 0.1
I1001 15:12:46.655067  5332 solver.cpp:218] Iteration 13200 (19.0753 iter/s, 5.24238s/100 iters), loss = 0.240375
I1001 15:12:46.655227  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240375 (* 1 = 0.240375 loss)
I1001 15:12:46.655236  5332 sgd_solver.cpp:105] Iteration 13200, lr = 0.1
I1001 15:12:51.892199  5332 solver.cpp:218] Iteration 13300 (19.0951 iter/s, 5.23696s/100 iters), loss = 0.387037
I1001 15:12:51.892230  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.387037 (* 1 = 0.387037 loss)
I1001 15:12:51.892235  5332 sgd_solver.cpp:105] Iteration 13300, lr = 0.1
I1001 15:12:57.142498  5332 solver.cpp:218] Iteration 13400 (19.0467 iter/s, 5.25024s/100 iters), loss = 0.210722
I1001 15:12:57.142540  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210722 (* 1 = 0.210722 loss)
I1001 15:12:57.142546  5332 sgd_solver.cpp:105] Iteration 13400, lr = 0.1
I1001 15:13:02.127427  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:13:02.337152  5332 solver.cpp:330] Iteration 13500, Testing net (#0)
I1001 15:13:03.527964  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:13:03.578050  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7299
I1001 15:13:03.578075  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.841397 (* 1 = 0.841397 loss)
I1001 15:13:03.630573  5332 solver.cpp:218] Iteration 13500 (15.4131 iter/s, 6.48801s/100 iters), loss = 0.257727
I1001 15:13:03.630599  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257727 (* 1 = 0.257727 loss)
I1001 15:13:03.630606  5332 sgd_solver.cpp:105] Iteration 13500, lr = 0.1
I1001 15:13:08.876533  5332 solver.cpp:218] Iteration 13600 (19.0625 iter/s, 5.24591s/100 iters), loss = 0.264611
I1001 15:13:08.876572  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264611 (* 1 = 0.264611 loss)
I1001 15:13:08.876579  5332 sgd_solver.cpp:105] Iteration 13600, lr = 0.1
I1001 15:13:14.121240  5332 solver.cpp:218] Iteration 13700 (19.0671 iter/s, 5.24464s/100 iters), loss = 0.212515
I1001 15:13:14.121271  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212515 (* 1 = 0.212515 loss)
I1001 15:13:14.121278  5332 sgd_solver.cpp:105] Iteration 13700, lr = 0.1
I1001 15:13:19.351625  5332 solver.cpp:218] Iteration 13800 (19.1193 iter/s, 5.23033s/100 iters), loss = 0.33153
I1001 15:13:19.351745  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.33153 (* 1 = 0.33153 loss)
I1001 15:13:19.351752  5332 sgd_solver.cpp:105] Iteration 13800, lr = 0.1
I1001 15:13:24.594079  5332 solver.cpp:218] Iteration 13900 (19.0756 iter/s, 5.24231s/100 iters), loss = 0.268029
I1001 15:13:24.594110  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268029 (* 1 = 0.268029 loss)
I1001 15:13:24.594115  5332 sgd_solver.cpp:105] Iteration 13900, lr = 0.1
I1001 15:13:29.573458  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:13:29.782254  5332 solver.cpp:330] Iteration 14000, Testing net (#0)
I1001 15:13:30.970106  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:13:31.020175  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7453
I1001 15:13:31.020210  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.789857 (* 1 = 0.789857 loss)
I1001 15:13:31.072665  5332 solver.cpp:218] Iteration 14000 (15.4356 iter/s, 6.47853s/100 iters), loss = 0.234114
I1001 15:13:31.072690  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234114 (* 1 = 0.234114 loss)
I1001 15:13:31.072698  5332 sgd_solver.cpp:105] Iteration 14000, lr = 0.1
I1001 15:13:36.316267  5332 solver.cpp:218] Iteration 14100 (19.071 iter/s, 5.24355s/100 iters), loss = 0.319117
I1001 15:13:36.316308  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319117 (* 1 = 0.319117 loss)
I1001 15:13:36.316313  5332 sgd_solver.cpp:105] Iteration 14100, lr = 0.1
I1001 15:13:41.557189  5332 solver.cpp:218] Iteration 14200 (19.0809 iter/s, 5.24086s/100 iters), loss = 0.399292
I1001 15:13:41.557230  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.399292 (* 1 = 0.399292 loss)
I1001 15:13:41.557236  5332 sgd_solver.cpp:105] Iteration 14200, lr = 0.1
I1001 15:13:46.803180  5332 solver.cpp:218] Iteration 14300 (19.0624 iter/s, 5.24592s/100 iters), loss = 0.354378
I1001 15:13:46.803210  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.354378 (* 1 = 0.354378 loss)
I1001 15:13:46.803217  5332 sgd_solver.cpp:105] Iteration 14300, lr = 0.1
I1001 15:13:52.039618  5332 solver.cpp:218] Iteration 14400 (19.0972 iter/s, 5.23638s/100 iters), loss = 0.253084
I1001 15:13:52.039739  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253084 (* 1 = 0.253084 loss)
I1001 15:13:52.039747  5332 sgd_solver.cpp:105] Iteration 14400, lr = 0.1
I1001 15:13:57.023185  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:13:57.233088  5332 solver.cpp:330] Iteration 14500, Testing net (#0)
I1001 15:13:58.423313  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:13:58.474108  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7533
I1001 15:13:58.474134  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.771599 (* 1 = 0.771599 loss)
I1001 15:13:58.528789  5332 solver.cpp:218] Iteration 14500 (15.4106 iter/s, 6.48902s/100 iters), loss = 0.320311
I1001 15:13:58.528825  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320311 (* 1 = 0.320311 loss)
I1001 15:13:58.528833  5332 sgd_solver.cpp:105] Iteration 14500, lr = 0.1
I1001 15:14:03.772707  5332 solver.cpp:218] Iteration 14600 (19.0699 iter/s, 5.24385s/100 iters), loss = 0.21673
I1001 15:14:03.772747  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21673 (* 1 = 0.21673 loss)
I1001 15:14:03.772753  5332 sgd_solver.cpp:105] Iteration 14600, lr = 0.1
I1001 15:14:09.016541  5332 solver.cpp:218] Iteration 14700 (19.0702 iter/s, 5.24377s/100 iters), loss = 0.462322
I1001 15:14:09.016571  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.462322 (* 1 = 0.462322 loss)
I1001 15:14:09.016577  5332 sgd_solver.cpp:105] Iteration 14700, lr = 0.1
I1001 15:14:14.260859  5332 solver.cpp:218] Iteration 14800 (19.0685 iter/s, 5.24426s/100 iters), loss = 0.262019
I1001 15:14:14.260898  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262019 (* 1 = 0.262019 loss)
I1001 15:14:14.260905  5332 sgd_solver.cpp:105] Iteration 14800, lr = 0.1
I1001 15:14:19.499244  5332 solver.cpp:218] Iteration 14900 (19.0901 iter/s, 5.23832s/100 iters), loss = 0.271646
I1001 15:14:19.499279  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.271647 (* 1 = 0.271647 loss)
I1001 15:14:19.499286  5332 sgd_solver.cpp:105] Iteration 14900, lr = 0.1
I1001 15:14:24.479771  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:14:24.689679  5332 solver.cpp:330] Iteration 15000, Testing net (#0)
I1001 15:14:25.886307  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:14:25.936667  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8192
I1001 15:14:25.936693  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.529566 (* 1 = 0.529566 loss)
I1001 15:14:25.988865  5332 solver.cpp:218] Iteration 15000 (15.4094 iter/s, 6.48956s/100 iters), loss = 0.367325
I1001 15:14:25.988898  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.367325 (* 1 = 0.367325 loss)
I1001 15:14:25.988903  5332 sgd_solver.cpp:105] Iteration 15000, lr = 0.1
I1001 15:14:31.223275  5332 solver.cpp:218] Iteration 15100 (19.1046 iter/s, 5.23435s/100 iters), loss = 0.177952
I1001 15:14:31.223315  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177953 (* 1 = 0.177953 loss)
I1001 15:14:31.223321  5332 sgd_solver.cpp:105] Iteration 15100, lr = 0.1
I1001 15:14:36.467154  5332 solver.cpp:218] Iteration 15200 (19.0701 iter/s, 5.24381s/100 iters), loss = 0.253588
I1001 15:14:36.467193  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253588 (* 1 = 0.253588 loss)
I1001 15:14:36.467200  5332 sgd_solver.cpp:105] Iteration 15200, lr = 0.1
I1001 15:14:41.710304  5332 solver.cpp:218] Iteration 15300 (19.0727 iter/s, 5.24309s/100 iters), loss = 0.267591
I1001 15:14:41.710345  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267591 (* 1 = 0.267591 loss)
I1001 15:14:41.710350  5332 sgd_solver.cpp:105] Iteration 15300, lr = 0.1
I1001 15:14:46.955165  5332 solver.cpp:218] Iteration 15400 (19.0665 iter/s, 5.24479s/100 iters), loss = 0.376935
I1001 15:14:46.955195  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376935 (* 1 = 0.376935 loss)
I1001 15:14:46.955201  5332 sgd_solver.cpp:105] Iteration 15400, lr = 0.1
I1001 15:14:51.928195  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:14:52.138087  5332 solver.cpp:330] Iteration 15500, Testing net (#0)
I1001 15:14:53.337339  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:14:53.387534  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6271
I1001 15:14:53.387568  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.29841 (* 1 = 1.29841 loss)
I1001 15:14:53.439663  5332 solver.cpp:218] Iteration 15500 (15.4215 iter/s, 6.48444s/100 iters), loss = 0.266337
I1001 15:14:53.439692  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266338 (* 1 = 0.266338 loss)
I1001 15:14:53.439700  5332 sgd_solver.cpp:105] Iteration 15500, lr = 0.1
I1001 15:14:58.695132  5332 solver.cpp:218] Iteration 15600 (19.028 iter/s, 5.25542s/100 iters), loss = 0.281824
I1001 15:14:58.695267  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281824 (* 1 = 0.281824 loss)
I1001 15:14:58.695276  5332 sgd_solver.cpp:105] Iteration 15600, lr = 0.1
I1001 15:15:03.941351  5332 solver.cpp:218] Iteration 15700 (19.0619 iter/s, 5.24607s/100 iters), loss = 0.393481
I1001 15:15:03.941390  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.393481 (* 1 = 0.393481 loss)
I1001 15:15:03.941397  5332 sgd_solver.cpp:105] Iteration 15700, lr = 0.1
I1001 15:15:09.198227  5332 solver.cpp:218] Iteration 15800 (19.0229 iter/s, 5.25681s/100 iters), loss = 0.310372
I1001 15:15:09.198257  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310372 (* 1 = 0.310372 loss)
I1001 15:15:09.198273  5332 sgd_solver.cpp:105] Iteration 15800, lr = 0.1
I1001 15:15:14.455449  5332 solver.cpp:218] Iteration 15900 (19.0217 iter/s, 5.25717s/100 iters), loss = 0.329208
I1001 15:15:14.455478  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329209 (* 1 = 0.329209 loss)
I1001 15:15:14.455484  5332 sgd_solver.cpp:105] Iteration 15900, lr = 0.1
I1001 15:15:19.441792  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:15:19.659335  5332 solver.cpp:330] Iteration 16000, Testing net (#0)
I1001 15:15:20.852346  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:15:20.902724  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5954
I1001 15:15:20.902758  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.52561 (* 1 = 1.52561 loss)
I1001 15:15:20.955130  5332 solver.cpp:218] Iteration 16000 (15.3855 iter/s, 6.49963s/100 iters), loss = 0.277453
I1001 15:15:20.955164  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277453 (* 1 = 0.277453 loss)
I1001 15:15:20.955171  5332 sgd_solver.cpp:105] Iteration 16000, lr = 0.1
I1001 15:15:26.212266  5332 solver.cpp:218] Iteration 16100 (19.022 iter/s, 5.25708s/100 iters), loss = 0.285407
I1001 15:15:26.212297  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285407 (* 1 = 0.285407 loss)
I1001 15:15:26.212304  5332 sgd_solver.cpp:105] Iteration 16100, lr = 0.1
I1001 15:15:31.461830  5332 solver.cpp:218] Iteration 16200 (19.0494 iter/s, 5.2495s/100 iters), loss = 0.384281
I1001 15:15:31.461971  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.384281 (* 1 = 0.384281 loss)
I1001 15:15:31.461992  5332 sgd_solver.cpp:105] Iteration 16200, lr = 0.1
I1001 15:15:36.718449  5332 solver.cpp:218] Iteration 16300 (19.0243 iter/s, 5.25644s/100 iters), loss = 0.284262
I1001 15:15:36.718488  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284262 (* 1 = 0.284262 loss)
I1001 15:15:36.718494  5332 sgd_solver.cpp:105] Iteration 16300, lr = 0.1
I1001 15:15:41.968406  5332 solver.cpp:218] Iteration 16400 (19.048 iter/s, 5.24989s/100 iters), loss = 0.243129
I1001 15:15:41.968436  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24313 (* 1 = 0.24313 loss)
I1001 15:15:41.968441  5332 sgd_solver.cpp:105] Iteration 16400, lr = 0.1
I1001 15:15:46.947697  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:15:47.157634  5332 solver.cpp:330] Iteration 16500, Testing net (#0)
I1001 15:15:48.349537  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:15:48.399758  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.668
I1001 15:15:48.399792  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.18179 (* 1 = 1.18179 loss)
I1001 15:15:48.452298  5332 solver.cpp:218] Iteration 16500 (15.423 iter/s, 6.48384s/100 iters), loss = 0.2067
I1001 15:15:48.452323  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206701 (* 1 = 0.206701 loss)
I1001 15:15:48.452330  5332 sgd_solver.cpp:105] Iteration 16500, lr = 0.1
I1001 15:15:53.708202  5332 solver.cpp:218] Iteration 16600 (19.0264 iter/s, 5.25585s/100 iters), loss = 0.25197
I1001 15:15:53.708241  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25197 (* 1 = 0.25197 loss)
I1001 15:15:53.708247  5332 sgd_solver.cpp:105] Iteration 16600, lr = 0.1
I1001 15:15:58.963480  5332 solver.cpp:218] Iteration 16700 (19.0287 iter/s, 5.25521s/100 iters), loss = 0.261901
I1001 15:15:58.963521  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261901 (* 1 = 0.261901 loss)
I1001 15:15:58.963527  5332 sgd_solver.cpp:105] Iteration 16700, lr = 0.1
I1001 15:16:04.215245  5332 solver.cpp:218] Iteration 16800 (19.0415 iter/s, 5.2517s/100 iters), loss = 0.426559
I1001 15:16:04.215363  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.426559 (* 1 = 0.426559 loss)
I1001 15:16:04.215378  5332 sgd_solver.cpp:105] Iteration 16800, lr = 0.1
I1001 15:16:09.474618  5332 solver.cpp:218] Iteration 16900 (19.0141 iter/s, 5.25925s/100 iters), loss = 0.290176
I1001 15:16:09.474659  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.290176 (* 1 = 0.290176 loss)
I1001 15:16:09.474665  5332 sgd_solver.cpp:105] Iteration 16900, lr = 0.1
I1001 15:16:14.476184  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:16:14.686998  5332 solver.cpp:330] Iteration 17000, Testing net (#0)
I1001 15:16:15.878478  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:16:15.928979  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6735
I1001 15:16:15.929014  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.16748 (* 1 = 1.16748 loss)
I1001 15:16:15.981483  5332 solver.cpp:218] Iteration 17000 (15.3685 iter/s, 6.5068s/100 iters), loss = 0.297446
I1001 15:16:15.981513  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297446 (* 1 = 0.297446 loss)
I1001 15:16:15.981519  5332 sgd_solver.cpp:105] Iteration 17000, lr = 0.1
I1001 15:16:21.240341  5332 solver.cpp:218] Iteration 17100 (19.0157 iter/s, 5.2588s/100 iters), loss = 0.320261
I1001 15:16:21.240377  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320262 (* 1 = 0.320262 loss)
I1001 15:16:21.240394  5332 sgd_solver.cpp:105] Iteration 17100, lr = 0.1
I1001 15:16:26.495998  5332 solver.cpp:218] Iteration 17200 (19.0273 iter/s, 5.2556s/100 iters), loss = 0.292777
I1001 15:16:26.496028  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292777 (* 1 = 0.292777 loss)
I1001 15:16:26.496034  5332 sgd_solver.cpp:105] Iteration 17200, lr = 0.1
I1001 15:16:31.748651  5332 solver.cpp:218] Iteration 17300 (19.0382 iter/s, 5.2526s/100 iters), loss = 0.36561
I1001 15:16:31.748684  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.36561 (* 1 = 0.36561 loss)
I1001 15:16:31.748692  5332 sgd_solver.cpp:105] Iteration 17300, lr = 0.1
I1001 15:16:36.997728  5332 solver.cpp:218] Iteration 17400 (19.0512 iter/s, 5.24902s/100 iters), loss = 0.225714
I1001 15:16:36.997862  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.225714 (* 1 = 0.225714 loss)
I1001 15:16:36.997870  5332 sgd_solver.cpp:105] Iteration 17400, lr = 0.1
I1001 15:16:41.991067  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:16:42.201167  5332 solver.cpp:330] Iteration 17500, Testing net (#0)
I1001 15:16:43.393184  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:16:43.443542  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7417
I1001 15:16:43.443579  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.839258 (* 1 = 0.839258 loss)
I1001 15:16:43.497076  5332 solver.cpp:218] Iteration 17500 (15.3865 iter/s, 6.4992s/100 iters), loss = 0.269082
I1001 15:16:43.497107  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269083 (* 1 = 0.269083 loss)
I1001 15:16:43.497113  5332 sgd_solver.cpp:105] Iteration 17500, lr = 0.1
I1001 15:16:48.754676  5332 solver.cpp:218] Iteration 17600 (19.0203 iter/s, 5.25755s/100 iters), loss = 0.287144
I1001 15:16:48.754705  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.287144 (* 1 = 0.287144 loss)
I1001 15:16:48.754711  5332 sgd_solver.cpp:105] Iteration 17600, lr = 0.1
I1001 15:16:54.012344  5332 solver.cpp:218] Iteration 17700 (19.02 iter/s, 5.25762s/100 iters), loss = 0.305062
I1001 15:16:54.012383  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305062 (* 1 = 0.305062 loss)
I1001 15:16:54.012389  5332 sgd_solver.cpp:105] Iteration 17700, lr = 0.1
I1001 15:16:59.269850  5332 solver.cpp:218] Iteration 17800 (19.0207 iter/s, 5.25744s/100 iters), loss = 0.310651
I1001 15:16:59.269891  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310651 (* 1 = 0.310651 loss)
I1001 15:16:59.269898  5332 sgd_solver.cpp:105] Iteration 17800, lr = 0.1
I1001 15:17:04.525709  5332 solver.cpp:218] Iteration 17900 (19.0266 iter/s, 5.25579s/100 iters), loss = 0.278438
I1001 15:17:04.525748  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278438 (* 1 = 0.278438 loss)
I1001 15:17:04.525756  5332 sgd_solver.cpp:105] Iteration 17900, lr = 0.1
I1001 15:17:09.520915  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:17:09.730882  5332 solver.cpp:330] Iteration 18000, Testing net (#0)
I1001 15:17:10.931845  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:17:10.981868  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7716
I1001 15:17:10.981904  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.709405 (* 1 = 0.709405 loss)
I1001 15:17:11.034591  5332 solver.cpp:218] Iteration 18000 (15.3638 iter/s, 6.50879s/100 iters), loss = 0.200182
I1001 15:17:11.034624  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200182 (* 1 = 0.200182 loss)
I1001 15:17:11.034631  5332 sgd_solver.cpp:105] Iteration 18000, lr = 0.1
I1001 15:17:16.287361  5332 solver.cpp:218] Iteration 18100 (19.0378 iter/s, 5.25271s/100 iters), loss = 0.276007
I1001 15:17:16.287394  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276008 (* 1 = 0.276008 loss)
I1001 15:17:16.287401  5332 sgd_solver.cpp:105] Iteration 18100, lr = 0.1
I1001 15:17:21.540099  5332 solver.cpp:218] Iteration 18200 (19.0379 iter/s, 5.25268s/100 iters), loss = 0.262963
I1001 15:17:21.540130  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262963 (* 1 = 0.262963 loss)
I1001 15:17:21.540138  5332 sgd_solver.cpp:105] Iteration 18200, lr = 0.1
I1001 15:17:26.793406  5332 solver.cpp:218] Iteration 18300 (19.0358 iter/s, 5.25325s/100 iters), loss = 0.298256
I1001 15:17:26.793447  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.298256 (* 1 = 0.298256 loss)
I1001 15:17:26.793453  5332 sgd_solver.cpp:105] Iteration 18300, lr = 0.1
I1001 15:17:32.049299  5332 solver.cpp:218] Iteration 18400 (19.0265 iter/s, 5.25583s/100 iters), loss = 0.243135
I1001 15:17:32.049329  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243135 (* 1 = 0.243135 loss)
I1001 15:17:32.049336  5332 sgd_solver.cpp:105] Iteration 18400, lr = 0.1
I1001 15:17:37.029423  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:17:37.239990  5332 solver.cpp:330] Iteration 18500, Testing net (#0)
I1001 15:17:38.440873  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:17:38.491013  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7979
I1001 15:17:38.491047  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.615423 (* 1 = 0.615423 loss)
I1001 15:17:38.543351  5332 solver.cpp:218] Iteration 18500 (15.3988 iter/s, 6.494s/100 iters), loss = 0.189839
I1001 15:17:38.543376  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18984 (* 1 = 0.18984 loss)
I1001 15:17:38.543383  5332 sgd_solver.cpp:105] Iteration 18500, lr = 0.1
I1001 15:17:43.804597  5332 solver.cpp:218] Iteration 18600 (19.0071 iter/s, 5.26119s/100 iters), loss = 0.248519
I1001 15:17:43.804759  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248519 (* 1 = 0.248519 loss)
I1001 15:17:43.804766  5332 sgd_solver.cpp:105] Iteration 18600, lr = 0.1
I1001 15:17:49.056048  5332 solver.cpp:218] Iteration 18700 (19.043 iter/s, 5.25127s/100 iters), loss = 0.275441
I1001 15:17:49.056077  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.275441 (* 1 = 0.275441 loss)
I1001 15:17:49.056082  5332 sgd_solver.cpp:105] Iteration 18700, lr = 0.1
I1001 15:17:54.315459  5332 solver.cpp:218] Iteration 18800 (19.0137 iter/s, 5.25936s/100 iters), loss = 0.345392
I1001 15:17:54.315490  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.345392 (* 1 = 0.345392 loss)
I1001 15:17:54.315508  5332 sgd_solver.cpp:105] Iteration 18800, lr = 0.1
I1001 15:17:59.577775  5332 solver.cpp:218] Iteration 18900 (19.0032 iter/s, 5.26226s/100 iters), loss = 0.255736
I1001 15:17:59.577808  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255736 (* 1 = 0.255736 loss)
I1001 15:17:59.577827  5332 sgd_solver.cpp:105] Iteration 18900, lr = 0.1
I1001 15:18:04.566854  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:18:04.780838  5332 solver.cpp:330] Iteration 19000, Testing net (#0)
I1001 15:18:05.973167  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:18:06.023517  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7991
I1001 15:18:06.023545  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.625731 (* 1 = 0.625731 loss)
I1001 15:18:06.075891  5332 solver.cpp:218] Iteration 19000 (15.3892 iter/s, 6.49806s/100 iters), loss = 0.168618
I1001 15:18:06.075922  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168618 (* 1 = 0.168618 loss)
I1001 15:18:06.075932  5332 sgd_solver.cpp:105] Iteration 19000, lr = 0.1
I1001 15:18:11.327008  5332 solver.cpp:218] Iteration 19100 (19.0437 iter/s, 5.25107s/100 iters), loss = 0.385144
I1001 15:18:11.327041  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.385144 (* 1 = 0.385144 loss)
I1001 15:18:11.327049  5332 sgd_solver.cpp:105] Iteration 19100, lr = 0.1
I1001 15:18:16.574096  5332 solver.cpp:218] Iteration 19200 (19.0584 iter/s, 5.24703s/100 iters), loss = 0.293761
I1001 15:18:16.574259  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293761 (* 1 = 0.293761 loss)
I1001 15:18:16.574270  5332 sgd_solver.cpp:105] Iteration 19200, lr = 0.1
I1001 15:18:21.825320  5332 solver.cpp:218] Iteration 19300 (19.0438 iter/s, 5.25104s/100 iters), loss = 0.318721
I1001 15:18:21.825351  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.318721 (* 1 = 0.318721 loss)
I1001 15:18:21.825357  5332 sgd_solver.cpp:105] Iteration 19300, lr = 0.1
I1001 15:18:27.079155  5332 solver.cpp:218] Iteration 19400 (19.0339 iter/s, 5.25378s/100 iters), loss = 0.297085
I1001 15:18:27.079193  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297085 (* 1 = 0.297085 loss)
I1001 15:18:27.079200  5332 sgd_solver.cpp:105] Iteration 19400, lr = 0.1
I1001 15:18:32.070585  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:18:32.280652  5332 solver.cpp:330] Iteration 19500, Testing net (#0)
I1001 15:18:33.474517  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:18:33.524827  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7017
I1001 15:18:33.524862  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.12346 (* 1 = 1.12346 loss)
I1001 15:18:33.577083  5332 solver.cpp:218] Iteration 19500 (15.3897 iter/s, 6.49787s/100 iters), loss = 0.333882
I1001 15:18:33.577107  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.333882 (* 1 = 0.333882 loss)
I1001 15:18:33.577114  5332 sgd_solver.cpp:105] Iteration 19500, lr = 0.1
I1001 15:18:38.841478  5332 solver.cpp:218] Iteration 19600 (18.9957 iter/s, 5.26435s/100 iters), loss = 0.261445
I1001 15:18:38.841518  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261445 (* 1 = 0.261445 loss)
I1001 15:18:38.841523  5332 sgd_solver.cpp:105] Iteration 19600, lr = 0.1
I1001 15:18:44.104094  5332 solver.cpp:218] Iteration 19700 (19.0022 iter/s, 5.26255s/100 iters), loss = 0.277812
I1001 15:18:44.104135  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277812 (* 1 = 0.277812 loss)
I1001 15:18:44.104140  5332 sgd_solver.cpp:105] Iteration 19700, lr = 0.1
I1001 15:18:49.347718  5332 solver.cpp:218] Iteration 19800 (19.071 iter/s, 5.24356s/100 iters), loss = 0.274913
I1001 15:18:49.347823  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.274913 (* 1 = 0.274913 loss)
I1001 15:18:49.347831  5332 sgd_solver.cpp:105] Iteration 19800, lr = 0.1
I1001 15:18:54.604810  5332 solver.cpp:218] Iteration 19900 (19.0224 iter/s, 5.25696s/100 iters), loss = 0.236842
I1001 15:18:54.604840  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236842 (* 1 = 0.236842 loss)
I1001 15:18:54.604856  5332 sgd_solver.cpp:105] Iteration 19900, lr = 0.1
I1001 15:18:59.600916  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:18:59.810546  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_20000.caffemodel
I1001 15:18:59.815769  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_20000.solverstate
I1001 15:18:59.817158  5332 solver.cpp:330] Iteration 20000, Testing net (#0)
I1001 15:19:01.008770  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:19:01.059257  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7793
I1001 15:19:01.059283  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.677369 (* 1 = 0.677369 loss)
I1001 15:19:01.111994  5332 solver.cpp:218] Iteration 20000 (15.3678 iter/s, 6.50713s/100 iters), loss = 0.33304
I1001 15:19:01.112030  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.33304 (* 1 = 0.33304 loss)
I1001 15:19:01.112037  5332 sgd_solver.cpp:105] Iteration 20000, lr = 0.1
I1001 15:19:06.364243  5332 solver.cpp:218] Iteration 20100 (19.0397 iter/s, 5.25219s/100 iters), loss = 0.207443
I1001 15:19:06.364271  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207443 (* 1 = 0.207443 loss)
I1001 15:19:06.364277  5332 sgd_solver.cpp:105] Iteration 20100, lr = 0.1
I1001 15:19:11.617125  5332 solver.cpp:218] Iteration 20200 (19.0373 iter/s, 5.25283s/100 iters), loss = 0.286234
I1001 15:19:11.617158  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286234 (* 1 = 0.286234 loss)
I1001 15:19:11.617167  5332 sgd_solver.cpp:105] Iteration 20200, lr = 0.1
I1001 15:19:16.874297  5332 solver.cpp:218] Iteration 20300 (19.0218 iter/s, 5.25712s/100 iters), loss = 0.313286
I1001 15:19:16.874331  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313287 (* 1 = 0.313287 loss)
I1001 15:19:16.874341  5332 sgd_solver.cpp:105] Iteration 20300, lr = 0.1
I1001 15:19:22.122967  5332 solver.cpp:218] Iteration 20400 (19.0526 iter/s, 5.24862s/100 iters), loss = 0.246552
I1001 15:19:22.123096  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246552 (* 1 = 0.246552 loss)
I1001 15:19:22.123142  5332 sgd_solver.cpp:105] Iteration 20400, lr = 0.1
I1001 15:19:27.120481  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:19:27.330682  5332 solver.cpp:330] Iteration 20500, Testing net (#0)
I1001 15:19:28.527827  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:19:28.578629  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7092
I1001 15:19:28.578657  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.99731 (* 1 = 0.99731 loss)
I1001 15:19:28.632695  5332 solver.cpp:218] Iteration 20500 (15.362 iter/s, 6.50959s/100 iters), loss = 0.259422
I1001 15:19:28.632735  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.259422 (* 1 = 0.259422 loss)
I1001 15:19:28.632745  5332 sgd_solver.cpp:105] Iteration 20500, lr = 0.1
I1001 15:19:33.887812  5332 solver.cpp:218] Iteration 20600 (19.0293 iter/s, 5.25506s/100 iters), loss = 0.207982
I1001 15:19:33.887845  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207982 (* 1 = 0.207982 loss)
I1001 15:19:33.887864  5332 sgd_solver.cpp:105] Iteration 20600, lr = 0.1
I1001 15:19:39.141806  5332 solver.cpp:218] Iteration 20700 (19.0333 iter/s, 5.25394s/100 iters), loss = 0.270236
I1001 15:19:39.141845  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270236 (* 1 = 0.270236 loss)
I1001 15:19:39.141851  5332 sgd_solver.cpp:105] Iteration 20700, lr = 0.1
I1001 15:19:44.395546  5332 solver.cpp:218] Iteration 20800 (19.0343 iter/s, 5.25368s/100 iters), loss = 0.311322
I1001 15:19:44.395577  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.311322 (* 1 = 0.311322 loss)
I1001 15:19:44.395583  5332 sgd_solver.cpp:105] Iteration 20800, lr = 0.1
I1001 15:19:49.643317  5332 solver.cpp:218] Iteration 20900 (19.0559 iter/s, 5.24772s/100 iters), loss = 0.329216
I1001 15:19:49.643353  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329216 (* 1 = 0.329216 loss)
I1001 15:19:49.643371  5332 sgd_solver.cpp:105] Iteration 20900, lr = 0.1
I1001 15:19:54.627219  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:19:54.836871  5332 solver.cpp:330] Iteration 21000, Testing net (#0)
I1001 15:19:56.035887  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:19:56.086431  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7354
I1001 15:19:56.086467  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.857242 (* 1 = 0.857242 loss)
I1001 15:19:56.139187  5332 solver.cpp:218] Iteration 21000 (15.3946 iter/s, 6.49577s/100 iters), loss = 0.127048
I1001 15:19:56.139219  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127048 (* 1 = 0.127048 loss)
I1001 15:19:56.139226  5332 sgd_solver.cpp:105] Iteration 21000, lr = 0.1
I1001 15:20:01.384467  5332 solver.cpp:218] Iteration 21100 (19.0649 iter/s, 5.24523s/100 iters), loss = 0.207869
I1001 15:20:01.384496  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207869 (* 1 = 0.207869 loss)
I1001 15:20:01.384502  5332 sgd_solver.cpp:105] Iteration 21100, lr = 0.1
I1001 15:20:06.638619  5332 solver.cpp:218] Iteration 21200 (19.0327 iter/s, 5.2541s/100 iters), loss = 0.278476
I1001 15:20:06.638648  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278476 (* 1 = 0.278476 loss)
I1001 15:20:06.638654  5332 sgd_solver.cpp:105] Iteration 21200, lr = 0.1
I1001 15:20:11.894915  5332 solver.cpp:218] Iteration 21300 (19.025 iter/s, 5.25625s/100 iters), loss = 0.390473
I1001 15:20:11.894945  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390473 (* 1 = 0.390473 loss)
I1001 15:20:11.894951  5332 sgd_solver.cpp:105] Iteration 21300, lr = 0.1
I1001 15:20:17.147836  5332 solver.cpp:218] Iteration 21400 (19.0372 iter/s, 5.25287s/100 iters), loss = 0.205585
I1001 15:20:17.147866  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205585 (* 1 = 0.205585 loss)
I1001 15:20:17.147872  5332 sgd_solver.cpp:105] Iteration 21400, lr = 0.1
I1001 15:20:22.130102  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:20:22.339567  5332 solver.cpp:330] Iteration 21500, Testing net (#0)
I1001 15:20:23.541496  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:20:23.591611  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8257
I1001 15:20:23.591636  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.531084 (* 1 = 0.531084 loss)
I1001 15:20:23.644464  5332 solver.cpp:218] Iteration 21500 (15.3927 iter/s, 6.49658s/100 iters), loss = 0.133968
I1001 15:20:23.644490  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133968 (* 1 = 0.133968 loss)
I1001 15:20:23.644507  5332 sgd_solver.cpp:105] Iteration 21500, lr = 0.1
I1001 15:20:28.904162  5332 solver.cpp:218] Iteration 21600 (19.0127 iter/s, 5.25965s/100 iters), loss = 0.18925
I1001 15:20:28.904296  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18925 (* 1 = 0.18925 loss)
I1001 15:20:28.904314  5332 sgd_solver.cpp:105] Iteration 21600, lr = 0.1
I1001 15:20:34.149899  5332 solver.cpp:218] Iteration 21700 (19.0636 iter/s, 5.24559s/100 iters), loss = 0.320487
I1001 15:20:34.149927  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320487 (* 1 = 0.320487 loss)
I1001 15:20:34.149933  5332 sgd_solver.cpp:105] Iteration 21700, lr = 0.1
I1001 15:20:39.403964  5332 solver.cpp:218] Iteration 21800 (19.0331 iter/s, 5.25401s/100 iters), loss = 0.20659
I1001 15:20:39.404003  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20659 (* 1 = 0.20659 loss)
I1001 15:20:39.404009  5332 sgd_solver.cpp:105] Iteration 21800, lr = 0.1
I1001 15:20:44.658331  5332 solver.cpp:218] Iteration 21900 (19.032 iter/s, 5.25431s/100 iters), loss = 0.226953
I1001 15:20:44.658371  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226954 (* 1 = 0.226954 loss)
I1001 15:20:44.658377  5332 sgd_solver.cpp:105] Iteration 21900, lr = 0.1
I1001 15:20:49.652046  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:20:49.865139  5332 solver.cpp:330] Iteration 22000, Testing net (#0)
I1001 15:20:51.056596  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:20:51.106806  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7724
I1001 15:20:51.106850  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.708709 (* 1 = 0.708709 loss)
I1001 15:20:51.159464  5332 solver.cpp:218] Iteration 22000 (15.3821 iter/s, 6.50107s/100 iters), loss = 0.22561
I1001 15:20:51.159488  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22561 (* 1 = 0.22561 loss)
I1001 15:20:51.159495  5332 sgd_solver.cpp:105] Iteration 22000, lr = 0.1
I1001 15:20:56.416541  5332 solver.cpp:218] Iteration 22100 (19.0222 iter/s, 5.25703s/100 iters), loss = 0.22925
I1001 15:20:56.416573  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22925 (* 1 = 0.22925 loss)
I1001 15:20:56.416579  5332 sgd_solver.cpp:105] Iteration 22100, lr = 0.1
I1001 15:21:01.671627  5332 solver.cpp:218] Iteration 22200 (19.0294 iter/s, 5.25503s/100 iters), loss = 0.27583
I1001 15:21:01.671814  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27583 (* 1 = 0.27583 loss)
I1001 15:21:01.671835  5332 sgd_solver.cpp:105] Iteration 22200, lr = 0.1
I1001 15:21:06.923349  5332 solver.cpp:218] Iteration 22300 (19.0421 iter/s, 5.25152s/100 iters), loss = 0.32678
I1001 15:21:06.923379  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.326781 (* 1 = 0.326781 loss)
I1001 15:21:06.923384  5332 sgd_solver.cpp:105] Iteration 22300, lr = 0.1
I1001 15:21:12.184039  5332 solver.cpp:218] Iteration 22400 (19.0091 iter/s, 5.26064s/100 iters), loss = 0.281365
I1001 15:21:12.184068  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281365 (* 1 = 0.281365 loss)
I1001 15:21:12.184074  5332 sgd_solver.cpp:105] Iteration 22400, lr = 0.1
I1001 15:21:17.178910  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:21:17.389855  5332 solver.cpp:330] Iteration 22500, Testing net (#0)
I1001 15:21:18.582857  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:21:18.633121  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7764
I1001 15:21:18.633155  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.664456 (* 1 = 0.664456 loss)
I1001 15:21:18.685549  5332 solver.cpp:218] Iteration 22500 (15.3812 iter/s, 6.50146s/100 iters), loss = 0.204456
I1001 15:21:18.685578  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204456 (* 1 = 0.204456 loss)
I1001 15:21:18.685595  5332 sgd_solver.cpp:105] Iteration 22500, lr = 0.1
I1001 15:21:23.936776  5332 solver.cpp:218] Iteration 22600 (19.0434 iter/s, 5.25117s/100 iters), loss = 0.229738
I1001 15:21:23.936817  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229739 (* 1 = 0.229739 loss)
I1001 15:21:23.936825  5332 sgd_solver.cpp:105] Iteration 22600, lr = 0.1
I1001 15:21:29.188223  5332 solver.cpp:218] Iteration 22700 (19.0426 iter/s, 5.25138s/100 iters), loss = 0.211037
I1001 15:21:29.188253  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.211037 (* 1 = 0.211037 loss)
I1001 15:21:29.188259  5332 sgd_solver.cpp:105] Iteration 22700, lr = 0.1
I1001 15:21:34.438395  5332 solver.cpp:218] Iteration 22800 (19.0472 iter/s, 5.25012s/100 iters), loss = 0.241029
I1001 15:21:34.438560  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241029 (* 1 = 0.241029 loss)
I1001 15:21:34.438568  5332 sgd_solver.cpp:105] Iteration 22800, lr = 0.1
I1001 15:21:39.694104  5332 solver.cpp:218] Iteration 22900 (19.0276 iter/s, 5.25553s/100 iters), loss = 0.196894
I1001 15:21:39.694134  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196894 (* 1 = 0.196894 loss)
I1001 15:21:39.694150  5332 sgd_solver.cpp:105] Iteration 22900, lr = 0.1
I1001 15:21:44.691279  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:21:44.901536  5332 solver.cpp:330] Iteration 23000, Testing net (#0)
I1001 15:21:46.091261  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:21:46.141196  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8292
I1001 15:21:46.141230  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.533991 (* 1 = 0.533991 loss)
I1001 15:21:46.193998  5332 solver.cpp:218] Iteration 23000 (15.385 iter/s, 6.49984s/100 iters), loss = 0.221498
I1001 15:21:46.194022  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221498 (* 1 = 0.221498 loss)
I1001 15:21:46.194030  5332 sgd_solver.cpp:105] Iteration 23000, lr = 0.1
I1001 15:21:51.451565  5332 solver.cpp:218] Iteration 23100 (19.0204 iter/s, 5.25752s/100 iters), loss = 0.31827
I1001 15:21:51.451593  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.31827 (* 1 = 0.31827 loss)
I1001 15:21:51.451599  5332 sgd_solver.cpp:105] Iteration 23100, lr = 0.1
I1001 15:21:56.709189  5332 solver.cpp:218] Iteration 23200 (19.0202 iter/s, 5.25757s/100 iters), loss = 0.300647
I1001 15:21:56.709219  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.300647 (* 1 = 0.300647 loss)
I1001 15:21:56.709235  5332 sgd_solver.cpp:105] Iteration 23200, lr = 0.1
I1001 15:22:01.966956  5332 solver.cpp:218] Iteration 23300 (19.0197 iter/s, 5.25771s/100 iters), loss = 0.287875
I1001 15:22:01.966987  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.287876 (* 1 = 0.287876 loss)
I1001 15:22:01.967003  5332 sgd_solver.cpp:105] Iteration 23300, lr = 0.1
I1001 15:22:07.216260  5332 solver.cpp:218] Iteration 23400 (19.0503 iter/s, 5.24925s/100 iters), loss = 0.187961
I1001 15:22:07.216413  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.187961 (* 1 = 0.187961 loss)
I1001 15:22:07.216421  5332 sgd_solver.cpp:105] Iteration 23400, lr = 0.1
I1001 15:22:12.206516  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:22:12.417382  5332 solver.cpp:330] Iteration 23500, Testing net (#0)
I1001 15:22:13.612773  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:22:13.663102  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6808
I1001 15:22:13.663138  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.27244 (* 1 = 1.27244 loss)
I1001 15:22:13.717320  5332 solver.cpp:218] Iteration 23500 (15.3825 iter/s, 6.5009s/100 iters), loss = 0.237282
I1001 15:22:13.717365  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237283 (* 1 = 0.237283 loss)
I1001 15:22:13.717373  5332 sgd_solver.cpp:105] Iteration 23500, lr = 0.1
I1001 15:22:18.964012  5332 solver.cpp:218] Iteration 23600 (19.0599 iter/s, 5.24662s/100 iters), loss = 0.196252
I1001 15:22:18.964042  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196252 (* 1 = 0.196252 loss)
I1001 15:22:18.964048  5332 sgd_solver.cpp:105] Iteration 23600, lr = 0.1
I1001 15:22:24.216500  5332 solver.cpp:218] Iteration 23700 (19.0388 iter/s, 5.25244s/100 iters), loss = 0.198892
I1001 15:22:24.216531  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198892 (* 1 = 0.198892 loss)
I1001 15:22:24.216536  5332 sgd_solver.cpp:105] Iteration 23700, lr = 0.1
I1001 15:22:29.473930  5332 solver.cpp:218] Iteration 23800 (19.0209 iter/s, 5.25738s/100 iters), loss = 0.303601
I1001 15:22:29.473975  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303602 (* 1 = 0.303602 loss)
I1001 15:22:29.473983  5332 sgd_solver.cpp:105] Iteration 23800, lr = 0.1
I1001 15:22:34.730206  5332 solver.cpp:218] Iteration 23900 (19.0251 iter/s, 5.25621s/100 iters), loss = 0.208893
I1001 15:22:34.730239  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208894 (* 1 = 0.208894 loss)
I1001 15:22:34.730247  5332 sgd_solver.cpp:105] Iteration 23900, lr = 0.1
I1001 15:22:39.715960  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:22:39.924674  5332 solver.cpp:330] Iteration 24000, Testing net (#0)
I1001 15:22:41.124892  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:22:41.175130  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.764
I1001 15:22:41.175155  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.802035 (* 1 = 0.802035 loss)
I1001 15:22:41.227656  5332 solver.cpp:218] Iteration 24000 (15.3908 iter/s, 6.4974s/100 iters), loss = 0.149003
I1001 15:22:41.227680  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149003 (* 1 = 0.149003 loss)
I1001 15:22:41.227689  5332 sgd_solver.cpp:105] Iteration 24000, lr = 0.1
I1001 15:22:46.475770  5332 solver.cpp:218] Iteration 24100 (19.0546 iter/s, 5.24806s/100 iters), loss = 0.17733
I1001 15:22:46.475813  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177331 (* 1 = 0.177331 loss)
I1001 15:22:46.475821  5332 sgd_solver.cpp:105] Iteration 24100, lr = 0.1
I1001 15:22:51.726665  5332 solver.cpp:218] Iteration 24200 (19.0446 iter/s, 5.25082s/100 iters), loss = 0.204541
I1001 15:22:51.726706  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204541 (* 1 = 0.204541 loss)
I1001 15:22:51.726712  5332 sgd_solver.cpp:105] Iteration 24200, lr = 0.1
I1001 15:22:56.978443  5332 solver.cpp:218] Iteration 24300 (19.0414 iter/s, 5.25171s/100 iters), loss = 0.271396
I1001 15:22:56.978473  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.271396 (* 1 = 0.271396 loss)
I1001 15:22:56.978479  5332 sgd_solver.cpp:105] Iteration 24300, lr = 0.1
I1001 15:23:02.237035  5332 solver.cpp:218] Iteration 24400 (19.0167 iter/s, 5.25854s/100 iters), loss = 0.289835
I1001 15:23:02.237076  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289835 (* 1 = 0.289835 loss)
I1001 15:23:02.237082  5332 sgd_solver.cpp:105] Iteration 24400, lr = 0.1
I1001 15:23:07.220185  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:23:07.429564  5332 solver.cpp:330] Iteration 24500, Testing net (#0)
I1001 15:23:08.628476  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:23:08.678915  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7907
I1001 15:23:08.678949  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.693167 (* 1 = 0.693167 loss)
I1001 15:23:08.731237  5332 solver.cpp:218] Iteration 24500 (15.3985 iter/s, 6.49414s/100 iters), loss = 0.246723
I1001 15:23:08.731266  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246723 (* 1 = 0.246723 loss)
I1001 15:23:08.731273  5332 sgd_solver.cpp:105] Iteration 24500, lr = 0.1
I1001 15:23:13.984593  5332 solver.cpp:218] Iteration 24600 (19.0356 iter/s, 5.2533s/100 iters), loss = 0.220608
I1001 15:23:13.984730  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220608 (* 1 = 0.220608 loss)
I1001 15:23:13.984740  5332 sgd_solver.cpp:105] Iteration 24600, lr = 0.1
I1001 15:23:19.233954  5332 solver.cpp:218] Iteration 24700 (19.0505 iter/s, 5.24921s/100 iters), loss = 0.302399
I1001 15:23:19.233983  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.302399 (* 1 = 0.302399 loss)
I1001 15:23:19.233989  5332 sgd_solver.cpp:105] Iteration 24700, lr = 0.1
I1001 15:23:24.492704  5332 solver.cpp:218] Iteration 24800 (19.0161 iter/s, 5.2587s/100 iters), loss = 0.297673
I1001 15:23:24.492743  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297673 (* 1 = 0.297673 loss)
I1001 15:23:24.492749  5332 sgd_solver.cpp:105] Iteration 24800, lr = 0.1
I1001 15:23:29.748956  5332 solver.cpp:218] Iteration 24900 (19.0252 iter/s, 5.25619s/100 iters), loss = 0.234381
I1001 15:23:29.748986  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234381 (* 1 = 0.234381 loss)
I1001 15:23:29.748991  5332 sgd_solver.cpp:105] Iteration 24900, lr = 0.1
I1001 15:23:34.741511  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:23:34.952806  5332 solver.cpp:330] Iteration 25000, Testing net (#0)
I1001 15:23:36.144243  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:23:36.194646  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6962
I1001 15:23:36.194671  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.12555 (* 1 = 1.12555 loss)
I1001 15:23:36.247261  5332 solver.cpp:218] Iteration 25000 (15.3888 iter/s, 6.49825s/100 iters), loss = 0.233926
I1001 15:23:36.247287  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233926 (* 1 = 0.233926 loss)
I1001 15:23:36.247310  5332 sgd_solver.cpp:105] Iteration 25000, lr = 0.1
I1001 15:23:41.506754  5332 solver.cpp:218] Iteration 25100 (19.0134 iter/s, 5.25945s/100 iters), loss = 0.216113
I1001 15:23:41.506784  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216113 (* 1 = 0.216113 loss)
I1001 15:23:41.506790  5332 sgd_solver.cpp:105] Iteration 25100, lr = 0.1
I1001 15:23:46.755554  5332 solver.cpp:218] Iteration 25200 (19.0522 iter/s, 5.24875s/100 iters), loss = 0.246708
I1001 15:23:46.755671  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246708 (* 1 = 0.246708 loss)
I1001 15:23:46.755689  5332 sgd_solver.cpp:105] Iteration 25200, lr = 0.1
I1001 15:23:52.003774  5332 solver.cpp:218] Iteration 25300 (19.0545 iter/s, 5.24809s/100 iters), loss = 0.228179
I1001 15:23:52.003803  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.228179 (* 1 = 0.228179 loss)
I1001 15:23:52.003809  5332 sgd_solver.cpp:105] Iteration 25300, lr = 0.1
I1001 15:23:57.260334  5332 solver.cpp:218] Iteration 25400 (19.024 iter/s, 5.25651s/100 iters), loss = 0.197071
I1001 15:23:57.260365  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197071 (* 1 = 0.197071 loss)
I1001 15:23:57.260370  5332 sgd_solver.cpp:105] Iteration 25400, lr = 0.1
I1001 15:24:02.256486  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:24:02.466123  5332 solver.cpp:330] Iteration 25500, Testing net (#0)
I1001 15:24:03.661375  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:24:03.711794  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.828
I1001 15:24:03.711828  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.503425 (* 1 = 0.503425 loss)
I1001 15:24:03.764181  5332 solver.cpp:218] Iteration 25500 (15.3756 iter/s, 6.5038s/100 iters), loss = 0.203952
I1001 15:24:03.764206  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203952 (* 1 = 0.203952 loss)
I1001 15:24:03.764214  5332 sgd_solver.cpp:105] Iteration 25500, lr = 0.1
I1001 15:24:09.023663  5332 solver.cpp:218] Iteration 25600 (19.0135 iter/s, 5.25943s/100 iters), loss = 0.267456
I1001 15:24:09.023691  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267456 (* 1 = 0.267456 loss)
I1001 15:24:09.023697  5332 sgd_solver.cpp:105] Iteration 25600, lr = 0.1
I1001 15:24:14.276842  5332 solver.cpp:218] Iteration 25700 (19.0363 iter/s, 5.25313s/100 iters), loss = 0.219386
I1001 15:24:14.276870  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219386 (* 1 = 0.219386 loss)
I1001 15:24:14.276875  5332 sgd_solver.cpp:105] Iteration 25700, lr = 0.1
I1001 15:24:19.524875  5332 solver.cpp:218] Iteration 25800 (19.055 iter/s, 5.24797s/100 iters), loss = 0.263862
I1001 15:24:19.525039  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263863 (* 1 = 0.263863 loss)
I1001 15:24:19.525053  5332 sgd_solver.cpp:105] Iteration 25800, lr = 0.1
I1001 15:24:24.783519  5332 solver.cpp:218] Iteration 25900 (19.0169 iter/s, 5.25848s/100 iters), loss = 0.238602
I1001 15:24:24.783551  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238602 (* 1 = 0.238602 loss)
I1001 15:24:24.783560  5332 sgd_solver.cpp:105] Iteration 25900, lr = 0.1
I1001 15:24:29.774562  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:24:29.985049  5332 solver.cpp:330] Iteration 26000, Testing net (#0)
I1001 15:24:31.176273  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:24:31.226805  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7341
I1001 15:24:31.226830  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.893629 (* 1 = 0.893629 loss)
I1001 15:24:31.279245  5332 solver.cpp:218] Iteration 26000 (15.3949 iter/s, 6.49567s/100 iters), loss = 0.196254
I1001 15:24:31.279269  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196254 (* 1 = 0.196254 loss)
I1001 15:24:31.279276  5332 sgd_solver.cpp:105] Iteration 26000, lr = 0.1
I1001 15:24:36.537660  5332 solver.cpp:218] Iteration 26100 (19.0173 iter/s, 5.25837s/100 iters), loss = 0.200087
I1001 15:24:36.537689  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200087 (* 1 = 0.200087 loss)
I1001 15:24:36.537695  5332 sgd_solver.cpp:105] Iteration 26100, lr = 0.1
I1001 15:24:41.789814  5332 solver.cpp:218] Iteration 26200 (19.04 iter/s, 5.2521s/100 iters), loss = 0.260096
I1001 15:24:41.789844  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260096 (* 1 = 0.260096 loss)
I1001 15:24:41.789849  5332 sgd_solver.cpp:105] Iteration 26200, lr = 0.1
I1001 15:24:47.048279  5332 solver.cpp:218] Iteration 26300 (19.0172 iter/s, 5.25841s/100 iters), loss = 0.292988
I1001 15:24:47.048310  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292988 (* 1 = 0.292988 loss)
I1001 15:24:47.048328  5332 sgd_solver.cpp:105] Iteration 26300, lr = 0.1
I1001 15:24:52.300027  5332 solver.cpp:218] Iteration 26400 (19.0415 iter/s, 5.2517s/100 iters), loss = 0.186359
I1001 15:24:52.300151  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18636 (* 1 = 0.18636 loss)
I1001 15:24:52.300173  5332 sgd_solver.cpp:105] Iteration 26400, lr = 0.1
I1001 15:24:57.296816  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:24:57.506063  5332 solver.cpp:330] Iteration 26500, Testing net (#0)
I1001 15:24:58.702894  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:24:58.752800  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7481
I1001 15:24:58.752828  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.843157 (* 1 = 0.843157 loss)
I1001 15:24:58.806309  5332 solver.cpp:218] Iteration 26500 (15.3701 iter/s, 6.50615s/100 iters), loss = 0.198518
I1001 15:24:58.806344  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198518 (* 1 = 0.198518 loss)
I1001 15:24:58.806354  5332 sgd_solver.cpp:105] Iteration 26500, lr = 0.1
I1001 15:25:04.058084  5332 solver.cpp:218] Iteration 26600 (19.0414 iter/s, 5.25171s/100 iters), loss = 0.179309
I1001 15:25:04.058125  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179309 (* 1 = 0.179309 loss)
I1001 15:25:04.058130  5332 sgd_solver.cpp:105] Iteration 26600, lr = 0.1
I1001 15:25:09.317553  5332 solver.cpp:218] Iteration 26700 (19.0135 iter/s, 5.25941s/100 iters), loss = 0.306663
I1001 15:25:09.317584  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306664 (* 1 = 0.306664 loss)
I1001 15:25:09.317592  5332 sgd_solver.cpp:105] Iteration 26700, lr = 0.1
I1001 15:25:14.577610  5332 solver.cpp:218] Iteration 26800 (19.0114 iter/s, 5.26001s/100 iters), loss = 0.229535
I1001 15:25:14.577639  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229535 (* 1 = 0.229535 loss)
I1001 15:25:14.577646  5332 sgd_solver.cpp:105] Iteration 26800, lr = 0.1
I1001 15:25:19.834672  5332 solver.cpp:218] Iteration 26900 (19.0222 iter/s, 5.25701s/100 iters), loss = 0.21393
I1001 15:25:19.834715  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21393 (* 1 = 0.21393 loss)
I1001 15:25:19.834722  5332 sgd_solver.cpp:105] Iteration 26900, lr = 0.1
I1001 15:25:24.823230  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:25:25.033926  5332 solver.cpp:330] Iteration 27000, Testing net (#0)
I1001 15:25:26.233909  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:25:26.284154  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7274
I1001 15:25:26.284180  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.899647 (* 1 = 0.899647 loss)
I1001 15:25:26.337023  5332 solver.cpp:218] Iteration 27000 (15.3792 iter/s, 6.50229s/100 iters), loss = 0.216202
I1001 15:25:26.337052  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216202 (* 1 = 0.216202 loss)
I1001 15:25:26.337059  5332 sgd_solver.cpp:105] Iteration 27000, lr = 0.1
I1001 15:25:31.591176  5332 solver.cpp:218] Iteration 27100 (19.0328 iter/s, 5.2541s/100 iters), loss = 0.179715
I1001 15:25:31.591209  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179716 (* 1 = 0.179716 loss)
I1001 15:25:31.591217  5332 sgd_solver.cpp:105] Iteration 27100, lr = 0.1
I1001 15:25:36.844285  5332 solver.cpp:218] Iteration 27200 (19.0366 iter/s, 5.25305s/100 iters), loss = 0.135278
I1001 15:25:36.844326  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135278 (* 1 = 0.135278 loss)
I1001 15:25:36.844331  5332 sgd_solver.cpp:105] Iteration 27200, lr = 0.1
I1001 15:25:42.107755  5332 solver.cpp:218] Iteration 27300 (18.9991 iter/s, 5.26341s/100 iters), loss = 0.28008
I1001 15:25:42.107784  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28008 (* 1 = 0.28008 loss)
I1001 15:25:42.107789  5332 sgd_solver.cpp:105] Iteration 27300, lr = 0.1
I1001 15:25:47.367914  5332 solver.cpp:218] Iteration 27400 (19.011 iter/s, 5.26011s/100 iters), loss = 0.251333
I1001 15:25:47.367944  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.251333 (* 1 = 0.251333 loss)
I1001 15:25:47.367949  5332 sgd_solver.cpp:105] Iteration 27400, lr = 0.1
I1001 15:25:52.353353  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:25:52.565412  5332 solver.cpp:330] Iteration 27500, Testing net (#0)
I1001 15:25:53.766490  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:25:53.816555  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7829
I1001 15:25:53.816591  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.662968 (* 1 = 0.662968 loss)
I1001 15:25:53.869426  5332 solver.cpp:218] Iteration 27500 (15.3812 iter/s, 6.50146s/100 iters), loss = 0.249623
I1001 15:25:53.869451  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249623 (* 1 = 0.249623 loss)
I1001 15:25:53.869457  5332 sgd_solver.cpp:105] Iteration 27500, lr = 0.1
I1001 15:25:59.131644  5332 solver.cpp:218] Iteration 27600 (19.0036 iter/s, 5.26217s/100 iters), loss = 0.242596
I1001 15:25:59.131814  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242596 (* 1 = 0.242596 loss)
I1001 15:25:59.131837  5332 sgd_solver.cpp:105] Iteration 27600, lr = 0.1
I1001 15:26:04.383739  5332 solver.cpp:218] Iteration 27700 (19.0407 iter/s, 5.25191s/100 iters), loss = 0.245976
I1001 15:26:04.383772  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245977 (* 1 = 0.245977 loss)
I1001 15:26:04.383790  5332 sgd_solver.cpp:105] Iteration 27700, lr = 0.1
I1001 15:26:09.643179  5332 solver.cpp:218] Iteration 27800 (19.0136 iter/s, 5.25939s/100 iters), loss = 0.332572
I1001 15:26:09.643219  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332573 (* 1 = 0.332573 loss)
I1001 15:26:09.643229  5332 sgd_solver.cpp:105] Iteration 27800, lr = 0.1
I1001 15:26:14.898252  5332 solver.cpp:218] Iteration 27900 (19.0294 iter/s, 5.25502s/100 iters), loss = 0.149302
I1001 15:26:14.898283  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149303 (* 1 = 0.149303 loss)
I1001 15:26:14.898300  5332 sgd_solver.cpp:105] Iteration 27900, lr = 0.1
I1001 15:26:19.890688  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:26:20.100672  5332 solver.cpp:330] Iteration 28000, Testing net (#0)
I1001 15:26:21.293526  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:26:21.343583  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7082
I1001 15:26:21.343607  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.993303 (* 1 = 0.993303 loss)
I1001 15:26:21.396144  5332 solver.cpp:218] Iteration 28000 (15.3897 iter/s, 6.49784s/100 iters), loss = 0.189308
I1001 15:26:21.396173  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.189309 (* 1 = 0.189309 loss)
I1001 15:26:21.396181  5332 sgd_solver.cpp:105] Iteration 28000, lr = 0.1
I1001 15:26:26.653276  5332 solver.cpp:218] Iteration 28100 (19.022 iter/s, 5.25708s/100 iters), loss = 0.270169
I1001 15:26:26.653308  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270169 (* 1 = 0.270169 loss)
I1001 15:26:26.653317  5332 sgd_solver.cpp:105] Iteration 28100, lr = 0.1
I1001 15:26:31.912739  5332 solver.cpp:218] Iteration 28200 (19.0136 iter/s, 5.25941s/100 iters), loss = 0.199098
I1001 15:26:31.912902  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199098 (* 1 = 0.199098 loss)
I1001 15:26:31.912928  5332 sgd_solver.cpp:105] Iteration 28200, lr = 0.1
I1001 15:26:37.165045  5332 solver.cpp:218] Iteration 28300 (19.0399 iter/s, 5.25212s/100 iters), loss = 0.35184
I1001 15:26:37.165076  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.35184 (* 1 = 0.35184 loss)
I1001 15:26:37.165084  5332 sgd_solver.cpp:105] Iteration 28300, lr = 0.1
I1001 15:26:42.422952  5332 solver.cpp:218] Iteration 28400 (19.0192 iter/s, 5.25786s/100 iters), loss = 0.216552
I1001 15:26:42.422982  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216552 (* 1 = 0.216552 loss)
I1001 15:26:42.422998  5332 sgd_solver.cpp:105] Iteration 28400, lr = 0.1
I1001 15:26:47.420032  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:26:47.629866  5332 solver.cpp:330] Iteration 28500, Testing net (#0)
I1001 15:26:48.822484  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:26:48.872767  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.773
I1001 15:26:48.872802  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.674466 (* 1 = 0.674466 loss)
I1001 15:26:48.925052  5332 solver.cpp:218] Iteration 28500 (15.3798 iter/s, 6.50205s/100 iters), loss = 0.207053
I1001 15:26:48.925076  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207053 (* 1 = 0.207053 loss)
I1001 15:26:48.925082  5332 sgd_solver.cpp:105] Iteration 28500, lr = 0.1
I1001 15:26:54.185556  5332 solver.cpp:218] Iteration 28600 (19.0098 iter/s, 5.26045s/100 iters), loss = 0.207002
I1001 15:26:54.185595  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207002 (* 1 = 0.207002 loss)
I1001 15:26:54.185601  5332 sgd_solver.cpp:105] Iteration 28600, lr = 0.1
I1001 15:26:59.441373  5332 solver.cpp:218] Iteration 28700 (19.0268 iter/s, 5.25576s/100 iters), loss = 0.290864
I1001 15:26:59.441413  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.290865 (* 1 = 0.290865 loss)
I1001 15:26:59.441419  5332 sgd_solver.cpp:105] Iteration 28700, lr = 0.1
I1001 15:27:04.698695  5332 solver.cpp:218] Iteration 28800 (19.0213 iter/s, 5.25726s/100 iters), loss = 0.248516
I1001 15:27:04.698822  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248516 (* 1 = 0.248516 loss)
I1001 15:27:04.698842  5332 sgd_solver.cpp:105] Iteration 28800, lr = 0.1
I1001 15:27:09.953506  5332 solver.cpp:218] Iteration 28900 (19.0307 iter/s, 5.25466s/100 iters), loss = 0.160519
I1001 15:27:09.953547  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.16052 (* 1 = 0.16052 loss)
I1001 15:27:09.953553  5332 sgd_solver.cpp:105] Iteration 28900, lr = 0.1
I1001 15:27:14.955911  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:27:15.166705  5332 solver.cpp:330] Iteration 29000, Testing net (#0)
I1001 15:27:16.359629  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:27:16.409844  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7808
I1001 15:27:16.409868  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.684196 (* 1 = 0.684196 loss)
I1001 15:27:16.462062  5332 solver.cpp:218] Iteration 29000 (15.3645 iter/s, 6.50849s/100 iters), loss = 0.196408
I1001 15:27:16.462086  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196408 (* 1 = 0.196408 loss)
I1001 15:27:16.462093  5332 sgd_solver.cpp:105] Iteration 29000, lr = 0.1
I1001 15:27:21.714850  5332 solver.cpp:218] Iteration 29100 (19.0377 iter/s, 5.25274s/100 iters), loss = 0.253787
I1001 15:27:21.714881  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253788 (* 1 = 0.253788 loss)
I1001 15:27:21.714900  5332 sgd_solver.cpp:105] Iteration 29100, lr = 0.1
I1001 15:27:26.972954  5332 solver.cpp:218] Iteration 29200 (19.0185 iter/s, 5.25805s/100 iters), loss = 0.213479
I1001 15:27:26.972986  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213479 (* 1 = 0.213479 loss)
I1001 15:27:26.972995  5332 sgd_solver.cpp:105] Iteration 29200, lr = 0.1
I1001 15:27:32.228796  5332 solver.cpp:218] Iteration 29300 (19.0266 iter/s, 5.25579s/100 iters), loss = 0.322077
I1001 15:27:32.228827  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.322078 (* 1 = 0.322078 loss)
I1001 15:27:32.228837  5332 sgd_solver.cpp:105] Iteration 29300, lr = 0.1
I1001 15:27:37.476523  5332 solver.cpp:218] Iteration 29400 (19.0561 iter/s, 5.24767s/100 iters), loss = 0.28605
I1001 15:27:37.476688  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286051 (* 1 = 0.286051 loss)
I1001 15:27:37.476716  5332 sgd_solver.cpp:105] Iteration 29400, lr = 0.1
I1001 15:27:42.469032  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:27:42.678607  5332 solver.cpp:330] Iteration 29500, Testing net (#0)
I1001 15:27:43.875392  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:27:43.925909  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7604
I1001 15:27:43.925943  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.756673 (* 1 = 0.756673 loss)
I1001 15:27:43.978818  5332 solver.cpp:218] Iteration 29500 (15.3796 iter/s, 6.50211s/100 iters), loss = 0.239604
I1001 15:27:43.978848  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239604 (* 1 = 0.239604 loss)
I1001 15:27:43.978854  5332 sgd_solver.cpp:105] Iteration 29500, lr = 0.1
I1001 15:27:49.226924  5332 solver.cpp:218] Iteration 29600 (19.0547 iter/s, 5.24805s/100 iters), loss = 0.201958
I1001 15:27:49.226966  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201958 (* 1 = 0.201958 loss)
I1001 15:27:49.226972  5332 sgd_solver.cpp:105] Iteration 29600, lr = 0.1
I1001 15:27:54.480119  5332 solver.cpp:218] Iteration 29700 (19.0362 iter/s, 5.25314s/100 iters), loss = 0.217424
I1001 15:27:54.480149  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217424 (* 1 = 0.217424 loss)
I1001 15:27:54.480155  5332 sgd_solver.cpp:105] Iteration 29700, lr = 0.1
I1001 15:27:59.736145  5332 solver.cpp:218] Iteration 29800 (19.026 iter/s, 5.25598s/100 iters), loss = 0.234271
I1001 15:27:59.736186  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234271 (* 1 = 0.234271 loss)
I1001 15:27:59.736191  5332 sgd_solver.cpp:105] Iteration 29800, lr = 0.1
I1001 15:28:04.996122  5332 solver.cpp:218] Iteration 29900 (19.0117 iter/s, 5.25991s/100 iters), loss = 0.178681
I1001 15:28:04.996152  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178682 (* 1 = 0.178682 loss)
I1001 15:28:04.996160  5332 sgd_solver.cpp:105] Iteration 29900, lr = 0.1
I1001 15:28:09.983340  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:28:10.193316  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_30000.caffemodel
I1001 15:28:10.198374  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_30000.solverstate
I1001 15:28:10.199753  5332 solver.cpp:330] Iteration 30000, Testing net (#0)
I1001 15:28:11.400261  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:28:11.450700  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7772
I1001 15:28:11.450734  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.702368 (* 1 = 0.702368 loss)
I1001 15:28:11.503456  5332 solver.cpp:218] Iteration 30000 (15.3674 iter/s, 6.50728s/100 iters), loss = 0.250469
I1001 15:28:11.503487  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250469 (* 1 = 0.250469 loss)
I1001 15:28:11.503494  5332 sgd_solver.cpp:105] Iteration 30000, lr = 0.1
I1001 15:28:16.761991  5332 solver.cpp:218] Iteration 30100 (19.0169 iter/s, 5.25848s/100 iters), loss = 0.222076
I1001 15:28:16.762024  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222077 (* 1 = 0.222077 loss)
I1001 15:28:16.762032  5332 sgd_solver.cpp:105] Iteration 30100, lr = 0.1
I1001 15:28:22.013188  5332 solver.cpp:218] Iteration 30200 (19.0435 iter/s, 5.25114s/100 iters), loss = 0.218271
I1001 15:28:22.013216  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218272 (* 1 = 0.218272 loss)
I1001 15:28:22.013221  5332 sgd_solver.cpp:105] Iteration 30200, lr = 0.1
I1001 15:28:27.280434  5332 solver.cpp:218] Iteration 30300 (18.9854 iter/s, 5.2672s/100 iters), loss = 0.161015
I1001 15:28:27.280463  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.161016 (* 1 = 0.161016 loss)
I1001 15:28:27.280469  5332 sgd_solver.cpp:105] Iteration 30300, lr = 0.1
I1001 15:28:32.537405  5332 solver.cpp:218] Iteration 30400 (19.0225 iter/s, 5.25692s/100 iters), loss = 0.203605
I1001 15:28:32.537443  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203606 (* 1 = 0.203606 loss)
I1001 15:28:32.537451  5332 sgd_solver.cpp:105] Iteration 30400, lr = 0.1
I1001 15:28:37.518118  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:28:37.732906  5332 solver.cpp:330] Iteration 30500, Testing net (#0)
I1001 15:28:38.928009  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:28:38.978338  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7621
I1001 15:28:38.978361  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.774778 (* 1 = 0.774778 loss)
I1001 15:28:39.031148  5332 solver.cpp:218] Iteration 30500 (15.3996 iter/s, 6.49368s/100 iters), loss = 0.159847
I1001 15:28:39.031179  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159848 (* 1 = 0.159848 loss)
I1001 15:28:39.031186  5332 sgd_solver.cpp:105] Iteration 30500, lr = 0.1
I1001 15:28:44.288971  5332 solver.cpp:218] Iteration 30600 (19.0195 iter/s, 5.25777s/100 iters), loss = 0.18766
I1001 15:28:44.289100  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18766 (* 1 = 0.18766 loss)
I1001 15:28:44.289108  5332 sgd_solver.cpp:105] Iteration 30600, lr = 0.1
I1001 15:28:49.542932  5332 solver.cpp:218] Iteration 30700 (19.0338 iter/s, 5.25381s/100 iters), loss = 0.233119
I1001 15:28:49.542996  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233119 (* 1 = 0.233119 loss)
I1001 15:28:49.543004  5332 sgd_solver.cpp:105] Iteration 30700, lr = 0.1
I1001 15:28:54.805235  5332 solver.cpp:218] Iteration 30800 (19.0034 iter/s, 5.26223s/100 iters), loss = 0.237439
I1001 15:28:54.805265  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237439 (* 1 = 0.237439 loss)
I1001 15:28:54.805270  5332 sgd_solver.cpp:105] Iteration 30800, lr = 0.1
I1001 15:29:00.073715  5332 solver.cpp:218] Iteration 30900 (18.981 iter/s, 5.26843s/100 iters), loss = 0.206177
I1001 15:29:00.073745  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206177 (* 1 = 0.206177 loss)
I1001 15:29:00.073760  5332 sgd_solver.cpp:105] Iteration 30900, lr = 0.1
I1001 15:29:05.073721  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:29:05.283759  5332 solver.cpp:330] Iteration 31000, Testing net (#0)
I1001 15:29:06.475637  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:29:06.525996  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7778
I1001 15:29:06.526029  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.757632 (* 1 = 0.757632 loss)
I1001 15:29:06.578764  5332 solver.cpp:218] Iteration 31000 (15.3728 iter/s, 6.505s/100 iters), loss = 0.200791
I1001 15:29:06.578788  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200791 (* 1 = 0.200791 loss)
I1001 15:29:06.578795  5332 sgd_solver.cpp:105] Iteration 31000, lr = 0.1
I1001 15:29:11.833855  5332 solver.cpp:218] Iteration 31100 (19.0293 iter/s, 5.25504s/100 iters), loss = 0.228167
I1001 15:29:11.833884  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.228167 (* 1 = 0.228167 loss)
I1001 15:29:11.833889  5332 sgd_solver.cpp:105] Iteration 31100, lr = 0.1
I1001 15:29:17.092953  5332 solver.cpp:218] Iteration 31200 (19.0148 iter/s, 5.25905s/100 iters), loss = 0.265972
I1001 15:29:17.093075  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265973 (* 1 = 0.265973 loss)
I1001 15:29:17.093082  5332 sgd_solver.cpp:105] Iteration 31200, lr = 0.1
I1001 15:29:22.339632  5332 solver.cpp:218] Iteration 31300 (19.0602 iter/s, 5.24654s/100 iters), loss = 0.286783
I1001 15:29:22.339673  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286784 (* 1 = 0.286784 loss)
I1001 15:29:22.339679  5332 sgd_solver.cpp:105] Iteration 31300, lr = 0.1
I1001 15:29:27.597848  5332 solver.cpp:218] Iteration 31400 (19.0181 iter/s, 5.25815s/100 iters), loss = 0.160783
I1001 15:29:27.597878  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160784 (* 1 = 0.160784 loss)
I1001 15:29:27.597884  5332 sgd_solver.cpp:105] Iteration 31400, lr = 0.1
I1001 15:29:32.595183  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:29:32.805096  5332 solver.cpp:330] Iteration 31500, Testing net (#0)
I1001 15:29:33.998612  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:29:34.048817  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8129
I1001 15:29:34.048842  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.57981 (* 1 = 0.57981 loss)
I1001 15:29:34.101326  5332 solver.cpp:218] Iteration 31500 (15.3765 iter/s, 6.50342s/100 iters), loss = 0.238055
I1001 15:29:34.101353  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238055 (* 1 = 0.238055 loss)
I1001 15:29:34.101361  5332 sgd_solver.cpp:105] Iteration 31500, lr = 0.1
I1001 15:29:39.360080  5332 solver.cpp:218] Iteration 31600 (19.0161 iter/s, 5.2587s/100 iters), loss = 0.303969
I1001 15:29:39.360119  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303969 (* 1 = 0.303969 loss)
I1001 15:29:39.360126  5332 sgd_solver.cpp:105] Iteration 31600, lr = 0.1
I1001 15:29:44.619613  5332 solver.cpp:218] Iteration 31700 (19.0133 iter/s, 5.25947s/100 iters), loss = 0.305988
I1001 15:29:44.619653  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305989 (* 1 = 0.305989 loss)
I1001 15:29:44.619660  5332 sgd_solver.cpp:105] Iteration 31700, lr = 0.1
I1001 15:29:49.873960  5332 solver.cpp:218] Iteration 31800 (19.0321 iter/s, 5.25428s/100 iters), loss = 0.259302
I1001 15:29:49.874099  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.259302 (* 1 = 0.259302 loss)
I1001 15:29:49.874119  5332 sgd_solver.cpp:105] Iteration 31800, lr = 0.1
I1001 15:29:55.131374  5332 solver.cpp:218] Iteration 31900 (19.0213 iter/s, 5.25726s/100 iters), loss = 0.186677
I1001 15:29:55.131404  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186677 (* 1 = 0.186677 loss)
I1001 15:29:55.131410  5332 sgd_solver.cpp:105] Iteration 31900, lr = 0.1
I1001 15:30:00.132589  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:30:00.342833  5332 solver.cpp:330] Iteration 32000, Testing net (#0)
I1001 15:30:01.534662  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:30:01.585984  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.813
I1001 15:30:01.586012  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.595127 (* 1 = 0.595127 loss)
I1001 15:30:01.640558  5332 solver.cpp:218] Iteration 32000 (15.363 iter/s, 6.50913s/100 iters), loss = 0.231459
I1001 15:30:01.640604  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231459 (* 1 = 0.231459 loss)
I1001 15:30:01.640611  5332 sgd_solver.cpp:105] Iteration 32000, lr = 0.1
I1001 15:30:06.894719  5332 solver.cpp:218] Iteration 32100 (19.0329 iter/s, 5.25406s/100 iters), loss = 0.295622
I1001 15:30:06.894752  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.295622 (* 1 = 0.295622 loss)
I1001 15:30:06.894768  5332 sgd_solver.cpp:105] Iteration 32100, lr = 0.1
I1001 15:30:12.147280  5332 solver.cpp:218] Iteration 32200 (19.0385 iter/s, 5.25251s/100 iters), loss = 0.283678
I1001 15:30:12.147310  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283678 (* 1 = 0.283678 loss)
I1001 15:30:12.147315  5332 sgd_solver.cpp:105] Iteration 32200, lr = 0.1
I1001 15:30:17.403302  5332 solver.cpp:218] Iteration 32300 (19.026 iter/s, 5.25597s/100 iters), loss = 0.296756
I1001 15:30:17.403342  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296756 (* 1 = 0.296756 loss)
I1001 15:30:17.403348  5332 sgd_solver.cpp:105] Iteration 32300, lr = 0.1
I1001 15:30:22.657819  5332 solver.cpp:218] Iteration 32400 (19.0315 iter/s, 5.25445s/100 iters), loss = 0.148921
I1001 15:30:22.657938  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148921 (* 1 = 0.148921 loss)
I1001 15:30:22.657963  5332 sgd_solver.cpp:105] Iteration 32400, lr = 0.1
I1001 15:30:27.652484  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:30:27.862030  5332 solver.cpp:330] Iteration 32500, Testing net (#0)
I1001 15:30:29.061203  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:30:29.111750  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7657
I1001 15:30:29.111776  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.758777 (* 1 = 0.758777 loss)
I1001 15:30:29.164803  5332 solver.cpp:218] Iteration 32500 (15.3685 iter/s, 6.50682s/100 iters), loss = 0.15315
I1001 15:30:29.164832  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15315 (* 1 = 0.15315 loss)
I1001 15:30:29.164839  5332 sgd_solver.cpp:105] Iteration 32500, lr = 0.1
I1001 15:30:34.417697  5332 solver.cpp:218] Iteration 32600 (19.0373 iter/s, 5.25284s/100 iters), loss = 0.156788
I1001 15:30:34.417737  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156788 (* 1 = 0.156788 loss)
I1001 15:30:34.417742  5332 sgd_solver.cpp:105] Iteration 32600, lr = 0.1
I1001 15:30:39.678830  5332 solver.cpp:218] Iteration 32700 (19.0075 iter/s, 5.26107s/100 iters), loss = 0.241099
I1001 15:30:39.678859  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241099 (* 1 = 0.241099 loss)
I1001 15:30:39.678865  5332 sgd_solver.cpp:105] Iteration 32700, lr = 0.1
I1001 15:30:44.940979  5332 solver.cpp:218] Iteration 32800 (19.0038 iter/s, 5.2621s/100 iters), loss = 0.268595
I1001 15:30:44.941018  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268596 (* 1 = 0.268596 loss)
I1001 15:30:44.941023  5332 sgd_solver.cpp:105] Iteration 32800, lr = 0.1
I1001 15:30:50.207633  5332 solver.cpp:218] Iteration 32900 (18.9876 iter/s, 5.26659s/100 iters), loss = 0.276812
I1001 15:30:50.207672  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276812 (* 1 = 0.276812 loss)
I1001 15:30:50.207679  5332 sgd_solver.cpp:105] Iteration 32900, lr = 0.1
I1001 15:30:55.194351  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:30:55.404865  5332 solver.cpp:330] Iteration 33000, Testing net (#0)
I1001 15:30:56.603569  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:30:56.653719  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7558
I1001 15:30:56.653754  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.779201 (* 1 = 0.779201 loss)
I1001 15:30:56.706459  5332 solver.cpp:218] Iteration 33000 (15.3875 iter/s, 6.49876s/100 iters), loss = 0.273481
I1001 15:30:56.706487  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273481 (* 1 = 0.273481 loss)
I1001 15:30:56.706495  5332 sgd_solver.cpp:105] Iteration 33000, lr = 0.1
I1001 15:31:01.959517  5332 solver.cpp:218] Iteration 33100 (19.0367 iter/s, 5.25301s/100 iters), loss = 0.274552
I1001 15:31:01.959547  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.274552 (* 1 = 0.274552 loss)
I1001 15:31:01.959553  5332 sgd_solver.cpp:105] Iteration 33100, lr = 0.1
I1001 15:31:07.206603  5332 solver.cpp:218] Iteration 33200 (19.0584 iter/s, 5.24703s/100 iters), loss = 0.258341
I1001 15:31:07.206632  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258341 (* 1 = 0.258341 loss)
I1001 15:31:07.206638  5332 sgd_solver.cpp:105] Iteration 33200, lr = 0.1
I1001 15:31:12.463711  5332 solver.cpp:218] Iteration 33300 (19.0221 iter/s, 5.25705s/100 iters), loss = 0.205641
I1001 15:31:12.463752  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205642 (* 1 = 0.205642 loss)
I1001 15:31:12.463757  5332 sgd_solver.cpp:105] Iteration 33300, lr = 0.1
I1001 15:31:17.723832  5332 solver.cpp:218] Iteration 33400 (19.0112 iter/s, 5.26006s/100 iters), loss = 0.20384
I1001 15:31:17.723871  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203841 (* 1 = 0.203841 loss)
I1001 15:31:17.723877  5332 sgd_solver.cpp:105] Iteration 33400, lr = 0.1
I1001 15:31:22.721925  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:31:22.934556  5332 solver.cpp:330] Iteration 33500, Testing net (#0)
I1001 15:31:24.127471  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:31:24.177974  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.762
I1001 15:31:24.178009  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.692281 (* 1 = 0.692281 loss)
I1001 15:31:24.230214  5332 solver.cpp:218] Iteration 33500 (15.3697 iter/s, 6.50632s/100 iters), loss = 0.183241
I1001 15:31:24.230239  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183242 (* 1 = 0.183242 loss)
I1001 15:31:24.230245  5332 sgd_solver.cpp:105] Iteration 33500, lr = 0.1
I1001 15:31:29.496436  5332 solver.cpp:218] Iteration 33600 (18.9891 iter/s, 5.26617s/100 iters), loss = 0.228368
I1001 15:31:29.496573  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.228369 (* 1 = 0.228369 loss)
I1001 15:31:29.496590  5332 sgd_solver.cpp:105] Iteration 33600, lr = 0.1
I1001 15:31:34.760265  5332 solver.cpp:218] Iteration 33700 (18.9981 iter/s, 5.26368s/100 iters), loss = 0.175992
I1001 15:31:34.760299  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175993 (* 1 = 0.175993 loss)
I1001 15:31:34.760306  5332 sgd_solver.cpp:105] Iteration 33700, lr = 0.1
I1001 15:31:40.013800  5332 solver.cpp:218] Iteration 33800 (19.035 iter/s, 5.25348s/100 iters), loss = 0.233153
I1001 15:31:40.013841  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233154 (* 1 = 0.233154 loss)
I1001 15:31:40.013847  5332 sgd_solver.cpp:105] Iteration 33800, lr = 0.1
I1001 15:31:45.273861  5332 solver.cpp:218] Iteration 33900 (19.0114 iter/s, 5.26s/100 iters), loss = 0.190755
I1001 15:31:45.273900  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190756 (* 1 = 0.190756 loss)
I1001 15:31:45.273906  5332 sgd_solver.cpp:105] Iteration 33900, lr = 0.1
I1001 15:31:50.270001  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:31:50.480243  5332 solver.cpp:330] Iteration 34000, Testing net (#0)
I1001 15:31:51.670179  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:31:51.720396  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7321
I1001 15:31:51.720429  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.932861 (* 1 = 0.932861 loss)
I1001 15:31:51.772555  5332 solver.cpp:218] Iteration 34000 (15.3878 iter/s, 6.49863s/100 iters), loss = 0.2144
I1001 15:31:51.772588  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2144 (* 1 = 0.2144 loss)
I1001 15:31:51.772595  5332 sgd_solver.cpp:105] Iteration 34000, lr = 0.1
I1001 15:31:57.038451  5332 solver.cpp:218] Iteration 34100 (18.9903 iter/s, 5.26584s/100 iters), loss = 0.248674
I1001 15:31:57.038492  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248675 (* 1 = 0.248675 loss)
I1001 15:31:57.038499  5332 sgd_solver.cpp:105] Iteration 34100, lr = 0.1
I1001 15:32:02.303805  5332 solver.cpp:218] Iteration 34200 (18.9923 iter/s, 5.26529s/100 iters), loss = 0.195726
I1001 15:32:02.303951  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195727 (* 1 = 0.195727 loss)
I1001 15:32:02.303968  5332 sgd_solver.cpp:105] Iteration 34200, lr = 0.1
I1001 15:32:07.564213  5332 solver.cpp:218] Iteration 34300 (19.0105 iter/s, 5.26024s/100 iters), loss = 0.22178
I1001 15:32:07.564246  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22178 (* 1 = 0.22178 loss)
I1001 15:32:07.564254  5332 sgd_solver.cpp:105] Iteration 34300, lr = 0.1
I1001 15:32:12.825654  5332 solver.cpp:218] Iteration 34400 (19.0065 iter/s, 5.26135s/100 iters), loss = 0.197248
I1001 15:32:12.825682  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197249 (* 1 = 0.197249 loss)
I1001 15:32:12.825698  5332 sgd_solver.cpp:105] Iteration 34400, lr = 0.1
I1001 15:32:17.830238  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:32:18.040390  5332 solver.cpp:330] Iteration 34500, Testing net (#0)
I1001 15:32:19.231833  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:32:19.281766  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5582
I1001 15:32:19.281800  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.96223 (* 1 = 1.96223 loss)
I1001 15:32:19.334112  5332 solver.cpp:218] Iteration 34500 (15.3647 iter/s, 6.50841s/100 iters), loss = 0.332542
I1001 15:32:19.334136  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332542 (* 1 = 0.332542 loss)
I1001 15:32:19.334142  5332 sgd_solver.cpp:105] Iteration 34500, lr = 0.1
I1001 15:32:24.602001  5332 solver.cpp:218] Iteration 34600 (18.9831 iter/s, 5.26784s/100 iters), loss = 0.0861186
I1001 15:32:24.602041  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0861191 (* 1 = 0.0861191 loss)
I1001 15:32:24.602046  5332 sgd_solver.cpp:105] Iteration 34600, lr = 0.1
I1001 15:32:29.860177  5332 solver.cpp:218] Iteration 34700 (19.0182 iter/s, 5.25811s/100 iters), loss = 0.217794
I1001 15:32:29.860216  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217795 (* 1 = 0.217795 loss)
I1001 15:32:29.860222  5332 sgd_solver.cpp:105] Iteration 34700, lr = 0.1
I1001 15:32:35.118546  5332 solver.cpp:218] Iteration 34800 (19.0176 iter/s, 5.25829s/100 iters), loss = 0.232241
I1001 15:32:35.118670  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232242 (* 1 = 0.232242 loss)
I1001 15:32:35.118679  5332 sgd_solver.cpp:105] Iteration 34800, lr = 0.1
I1001 15:32:40.370952  5332 solver.cpp:218] Iteration 34900 (19.0394 iter/s, 5.25226s/100 iters), loss = 0.108768
I1001 15:32:40.370981  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108768 (* 1 = 0.108768 loss)
I1001 15:32:40.370988  5332 sgd_solver.cpp:105] Iteration 34900, lr = 0.1
I1001 15:32:45.367869  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:32:45.578795  5332 solver.cpp:330] Iteration 35000, Testing net (#0)
I1001 15:32:46.776777  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:32:46.827937  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7872
I1001 15:32:46.827965  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.65777 (* 1 = 0.65777 loss)
I1001 15:32:46.880642  5332 solver.cpp:218] Iteration 35000 (15.3618 iter/s, 6.50964s/100 iters), loss = 0.201544
I1001 15:32:46.880673  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201544 (* 1 = 0.201544 loss)
I1001 15:32:46.880681  5332 sgd_solver.cpp:105] Iteration 35000, lr = 0.1
I1001 15:32:52.132815  5332 solver.cpp:218] Iteration 35100 (19.0399 iter/s, 5.25212s/100 iters), loss = 0.25177
I1001 15:32:52.132853  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25177 (* 1 = 0.25177 loss)
I1001 15:32:52.132859  5332 sgd_solver.cpp:105] Iteration 35100, lr = 0.1
I1001 15:32:57.391090  5332 solver.cpp:218] Iteration 35200 (19.0179 iter/s, 5.25821s/100 iters), loss = 0.27975
I1001 15:32:57.391120  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279751 (* 1 = 0.279751 loss)
I1001 15:32:57.391126  5332 sgd_solver.cpp:105] Iteration 35200, lr = 0.1
I1001 15:33:02.651145  5332 solver.cpp:218] Iteration 35300 (19.0114 iter/s, 5.26s/100 iters), loss = 0.279981
I1001 15:33:02.651175  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279982 (* 1 = 0.279982 loss)
I1001 15:33:02.651191  5332 sgd_solver.cpp:105] Iteration 35300, lr = 0.1
I1001 15:33:07.911855  5332 solver.cpp:218] Iteration 35400 (19.009 iter/s, 5.26066s/100 iters), loss = 0.187573
I1001 15:33:07.911972  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.187574 (* 1 = 0.187574 loss)
I1001 15:33:07.911979  5332 sgd_solver.cpp:105] Iteration 35400, lr = 0.1
I1001 15:33:12.897790  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:33:13.108331  5332 solver.cpp:330] Iteration 35500, Testing net (#0)
I1001 15:33:14.310066  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:33:14.360584  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6623
I1001 15:33:14.360618  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.41499 (* 1 = 1.41499 loss)
I1001 15:33:14.413558  5332 solver.cpp:218] Iteration 35500 (15.3809 iter/s, 6.50158s/100 iters), loss = 0.199682
I1001 15:33:14.413589  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199683 (* 1 = 0.199683 loss)
I1001 15:33:14.413595  5332 sgd_solver.cpp:105] Iteration 35500, lr = 0.1
I1001 15:33:19.668097  5332 solver.cpp:218] Iteration 35600 (19.0314 iter/s, 5.25448s/100 iters), loss = 0.240846
I1001 15:33:19.668144  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240846 (* 1 = 0.240846 loss)
I1001 15:33:19.668153  5332 sgd_solver.cpp:105] Iteration 35600, lr = 0.1
I1001 15:33:24.916553  5332 solver.cpp:218] Iteration 35700 (19.0536 iter/s, 5.24835s/100 iters), loss = 0.220193
I1001 15:33:24.916584  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220194 (* 1 = 0.220194 loss)
I1001 15:33:24.916600  5332 sgd_solver.cpp:105] Iteration 35700, lr = 0.1
I1001 15:33:30.177780  5332 solver.cpp:218] Iteration 35800 (19.0072 iter/s, 5.26117s/100 iters), loss = 0.234473
I1001 15:33:30.177820  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234474 (* 1 = 0.234474 loss)
I1001 15:33:30.177827  5332 sgd_solver.cpp:105] Iteration 35800, lr = 0.1
I1001 15:33:35.439985  5332 solver.cpp:218] Iteration 35900 (19.0037 iter/s, 5.26214s/100 iters), loss = 0.193716
I1001 15:33:35.440013  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193717 (* 1 = 0.193717 loss)
I1001 15:33:35.440019  5332 sgd_solver.cpp:105] Iteration 35900, lr = 0.1
I1001 15:33:40.415532  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:33:40.628499  5332 solver.cpp:330] Iteration 36000, Testing net (#0)
I1001 15:33:41.825435  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:33:41.875788  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7359
I1001 15:33:41.875823  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.875001 (* 1 = 0.875001 loss)
I1001 15:33:41.928361  5332 solver.cpp:218] Iteration 36000 (15.4123 iter/s, 6.48833s/100 iters), loss = 0.242696
I1001 15:33:41.928386  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242697 (* 1 = 0.242697 loss)
I1001 15:33:41.928393  5332 sgd_solver.cpp:105] Iteration 36000, lr = 0.1
I1001 15:33:47.191399  5332 solver.cpp:218] Iteration 36100 (19.0006 iter/s, 5.26298s/100 iters), loss = 0.245728
I1001 15:33:47.191428  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245728 (* 1 = 0.245728 loss)
I1001 15:33:47.191434  5332 sgd_solver.cpp:105] Iteration 36100, lr = 0.1
I1001 15:33:52.441867  5332 solver.cpp:218] Iteration 36200 (19.0461 iter/s, 5.25041s/100 iters), loss = 0.210635
I1001 15:33:52.441908  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210636 (* 1 = 0.210636 loss)
I1001 15:33:52.441915  5332 sgd_solver.cpp:105] Iteration 36200, lr = 0.1
I1001 15:33:57.705060  5332 solver.cpp:218] Iteration 36300 (19.0001 iter/s, 5.26313s/100 iters), loss = 0.238827
I1001 15:33:57.705090  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238828 (* 1 = 0.238828 loss)
I1001 15:33:57.705096  5332 sgd_solver.cpp:105] Iteration 36300, lr = 0.1
I1001 15:34:02.969730  5332 solver.cpp:218] Iteration 36400 (18.9947 iter/s, 5.26462s/100 iters), loss = 0.191579
I1001 15:34:02.969770  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191579 (* 1 = 0.191579 loss)
I1001 15:34:02.969776  5332 sgd_solver.cpp:105] Iteration 36400, lr = 0.1
I1001 15:34:07.964565  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:34:08.174595  5332 solver.cpp:330] Iteration 36500, Testing net (#0)
I1001 15:34:09.368242  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:34:09.418697  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7259
I1001 15:34:09.418721  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.922378 (* 1 = 0.922378 loss)
I1001 15:34:09.471560  5332 solver.cpp:218] Iteration 36500 (15.3804 iter/s, 6.50177s/100 iters), loss = 0.227146
I1001 15:34:09.471583  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227147 (* 1 = 0.227147 loss)
I1001 15:34:09.471590  5332 sgd_solver.cpp:105] Iteration 36500, lr = 0.1
I1001 15:34:14.733355  5332 solver.cpp:218] Iteration 36600 (19.0051 iter/s, 5.26175s/100 iters), loss = 0.204686
I1001 15:34:14.733481  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204686 (* 1 = 0.204686 loss)
I1001 15:34:14.733500  5332 sgd_solver.cpp:105] Iteration 36600, lr = 0.1
I1001 15:34:19.991997  5332 solver.cpp:218] Iteration 36700 (19.0168 iter/s, 5.2585s/100 iters), loss = 0.249797
I1001 15:34:19.992038  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249797 (* 1 = 0.249797 loss)
I1001 15:34:19.992043  5332 sgd_solver.cpp:105] Iteration 36700, lr = 0.1
I1001 15:34:25.242228  5332 solver.cpp:218] Iteration 36800 (19.047 iter/s, 5.25017s/100 iters), loss = 0.254536
I1001 15:34:25.242257  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254537 (* 1 = 0.254537 loss)
I1001 15:34:25.242274  5332 sgd_solver.cpp:105] Iteration 36800, lr = 0.1
I1001 15:34:30.506290  5332 solver.cpp:218] Iteration 36900 (18.9969 iter/s, 5.26401s/100 iters), loss = 0.243545
I1001 15:34:30.506320  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243545 (* 1 = 0.243545 loss)
I1001 15:34:30.506336  5332 sgd_solver.cpp:105] Iteration 36900, lr = 0.1
I1001 15:34:35.506880  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:34:35.717293  5332 solver.cpp:330] Iteration 37000, Testing net (#0)
I1001 15:34:36.909785  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:34:36.960239  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8316
I1001 15:34:36.960274  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.526061 (* 1 = 0.526061 loss)
I1001 15:34:37.013044  5332 solver.cpp:218] Iteration 37000 (15.3688 iter/s, 6.5067s/100 iters), loss = 0.213651
I1001 15:34:37.013073  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213652 (* 1 = 0.213652 loss)
I1001 15:34:37.013080  5332 sgd_solver.cpp:105] Iteration 37000, lr = 0.1
I1001 15:34:42.276412  5332 solver.cpp:218] Iteration 37100 (18.9994 iter/s, 5.26332s/100 iters), loss = 0.229273
I1001 15:34:42.276451  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229274 (* 1 = 0.229274 loss)
I1001 15:34:42.276456  5332 sgd_solver.cpp:105] Iteration 37100, lr = 0.1
I1001 15:34:47.539151  5332 solver.cpp:218] Iteration 37200 (19.0017 iter/s, 5.26268s/100 iters), loss = 0.24916
I1001 15:34:47.539300  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249161 (* 1 = 0.249161 loss)
I1001 15:34:47.539309  5332 sgd_solver.cpp:105] Iteration 37200, lr = 0.1
I1001 15:34:52.796399  5332 solver.cpp:218] Iteration 37300 (19.022 iter/s, 5.25708s/100 iters), loss = 0.292638
I1001 15:34:52.796433  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292639 (* 1 = 0.292639 loss)
I1001 15:34:52.796440  5332 sgd_solver.cpp:105] Iteration 37300, lr = 0.1
I1001 15:34:58.053128  5332 solver.cpp:218] Iteration 37400 (19.0236 iter/s, 5.25664s/100 iters), loss = 0.196709
I1001 15:34:58.053169  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19671 (* 1 = 0.19671 loss)
I1001 15:34:58.053174  5332 sgd_solver.cpp:105] Iteration 37400, lr = 0.1
I1001 15:35:03.048257  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:35:03.259016  5332 solver.cpp:330] Iteration 37500, Testing net (#0)
I1001 15:35:04.454797  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:35:04.505153  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6935
I1001 15:35:04.505189  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.20485 (* 1 = 1.20485 loss)
I1001 15:35:04.557559  5332 solver.cpp:218] Iteration 37500 (15.3743 iter/s, 6.50437s/100 iters), loss = 0.238435
I1001 15:35:04.557601  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238435 (* 1 = 0.238435 loss)
I1001 15:35:04.557608  5332 sgd_solver.cpp:105] Iteration 37500, lr = 0.1
I1001 15:35:09.817839  5332 solver.cpp:218] Iteration 37600 (19.0106 iter/s, 5.26022s/100 iters), loss = 0.207872
I1001 15:35:09.817878  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207873 (* 1 = 0.207873 loss)
I1001 15:35:09.817884  5332 sgd_solver.cpp:105] Iteration 37600, lr = 0.1
I1001 15:35:15.081480  5332 solver.cpp:218] Iteration 37700 (18.9985 iter/s, 5.26358s/100 iters), loss = 0.214715
I1001 15:35:15.081518  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214716 (* 1 = 0.214716 loss)
I1001 15:35:15.081524  5332 sgd_solver.cpp:105] Iteration 37700, lr = 0.1
I1001 15:35:20.348129  5332 solver.cpp:218] Iteration 37800 (18.9876 iter/s, 5.26659s/100 iters), loss = 0.284937
I1001 15:35:20.348284  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284938 (* 1 = 0.284938 loss)
I1001 15:35:20.348292  5332 sgd_solver.cpp:105] Iteration 37800, lr = 0.1
I1001 15:35:25.605434  5332 solver.cpp:218] Iteration 37900 (19.0218 iter/s, 5.25714s/100 iters), loss = 0.237626
I1001 15:35:25.605469  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237627 (* 1 = 0.237627 loss)
I1001 15:35:25.605478  5332 sgd_solver.cpp:105] Iteration 37900, lr = 0.1
I1001 15:35:30.602128  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:35:30.812444  5332 solver.cpp:330] Iteration 38000, Testing net (#0)
I1001 15:35:32.012697  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:35:32.062928  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7685
I1001 15:35:32.062963  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.845169 (* 1 = 0.845169 loss)
I1001 15:35:32.115622  5332 solver.cpp:218] Iteration 38000 (15.3607 iter/s, 6.51013s/100 iters), loss = 0.0866695
I1001 15:35:32.115662  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0866701 (* 1 = 0.0866701 loss)
I1001 15:35:32.115670  5332 sgd_solver.cpp:105] Iteration 38000, lr = 0.1
I1001 15:35:37.368155  5332 solver.cpp:218] Iteration 38100 (19.0387 iter/s, 5.25247s/100 iters), loss = 0.213427
I1001 15:35:37.368185  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213428 (* 1 = 0.213428 loss)
I1001 15:35:37.368191  5332 sgd_solver.cpp:105] Iteration 38100, lr = 0.1
I1001 15:35:42.620842  5332 solver.cpp:218] Iteration 38200 (19.0381 iter/s, 5.25263s/100 iters), loss = 0.207522
I1001 15:35:42.620872  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207523 (* 1 = 0.207523 loss)
I1001 15:35:42.620878  5332 sgd_solver.cpp:105] Iteration 38200, lr = 0.1
I1001 15:35:47.881309  5332 solver.cpp:218] Iteration 38300 (19.0099 iter/s, 5.26041s/100 iters), loss = 0.220314
I1001 15:35:47.881340  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220315 (* 1 = 0.220315 loss)
I1001 15:35:47.881345  5332 sgd_solver.cpp:105] Iteration 38300, lr = 0.1
I1001 15:35:53.137096  5332 solver.cpp:218] Iteration 38400 (19.0268 iter/s, 5.25573s/100 iters), loss = 0.170328
I1001 15:35:53.137214  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170329 (* 1 = 0.170329 loss)
I1001 15:35:53.137223  5332 sgd_solver.cpp:105] Iteration 38400, lr = 0.1
I1001 15:35:58.125154  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:35:58.335600  5332 solver.cpp:330] Iteration 38500, Testing net (#0)
I1001 15:35:59.537307  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:35:59.587556  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7672
I1001 15:35:59.587581  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.789389 (* 1 = 0.789389 loss)
I1001 15:35:59.640163  5332 solver.cpp:218] Iteration 38500 (15.3777 iter/s, 6.50293s/100 iters), loss = 0.220573
I1001 15:35:59.640187  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220573 (* 1 = 0.220573 loss)
I1001 15:35:59.640193  5332 sgd_solver.cpp:105] Iteration 38500, lr = 0.1
I1001 15:36:04.907742  5332 solver.cpp:218] Iteration 38600 (18.9842 iter/s, 5.26753s/100 iters), loss = 0.297018
I1001 15:36:04.907773  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297019 (* 1 = 0.297019 loss)
I1001 15:36:04.907778  5332 sgd_solver.cpp:105] Iteration 38600, lr = 0.1
I1001 15:36:10.164417  5332 solver.cpp:218] Iteration 38700 (19.0236 iter/s, 5.25662s/100 iters), loss = 0.261497
I1001 15:36:10.164456  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261497 (* 1 = 0.261497 loss)
I1001 15:36:10.164463  5332 sgd_solver.cpp:105] Iteration 38700, lr = 0.1
I1001 15:36:15.430205  5332 solver.cpp:218] Iteration 38800 (18.9907 iter/s, 5.26573s/100 iters), loss = 0.239604
I1001 15:36:15.430235  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239605 (* 1 = 0.239605 loss)
I1001 15:36:15.430241  5332 sgd_solver.cpp:105] Iteration 38800, lr = 0.1
I1001 15:36:20.693593  5332 solver.cpp:218] Iteration 38900 (18.9994 iter/s, 5.26333s/100 iters), loss = 0.145519
I1001 15:36:20.693621  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14552 (* 1 = 0.14552 loss)
I1001 15:36:20.693627  5332 sgd_solver.cpp:105] Iteration 38900, lr = 0.1
I1001 15:36:25.690493  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:36:25.905243  5332 solver.cpp:330] Iteration 39000, Testing net (#0)
I1001 15:36:27.099020  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:36:27.149219  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6836
I1001 15:36:27.149255  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.23187 (* 1 = 1.23187 loss)
I1001 15:36:27.201869  5332 solver.cpp:218] Iteration 39000 (15.3652 iter/s, 6.50823s/100 iters), loss = 0.297764
I1001 15:36:27.201894  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297765 (* 1 = 0.297765 loss)
I1001 15:36:27.201901  5332 sgd_solver.cpp:105] Iteration 39000, lr = 0.1
I1001 15:36:32.459596  5332 solver.cpp:218] Iteration 39100 (19.0198 iter/s, 5.25768s/100 iters), loss = 0.237392
I1001 15:36:32.459636  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237393 (* 1 = 0.237393 loss)
I1001 15:36:32.459642  5332 sgd_solver.cpp:105] Iteration 39100, lr = 0.1
I1001 15:36:37.710270  5332 solver.cpp:218] Iteration 39200 (19.0454 iter/s, 5.25061s/100 iters), loss = 0.140954
I1001 15:36:37.710304  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140955 (* 1 = 0.140955 loss)
I1001 15:36:37.710310  5332 sgd_solver.cpp:105] Iteration 39200, lr = 0.1
I1001 15:36:42.963022  5332 solver.cpp:218] Iteration 39300 (19.0378 iter/s, 5.2527s/100 iters), loss = 0.157894
I1001 15:36:42.963050  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.157894 (* 1 = 0.157894 loss)
I1001 15:36:42.963055  5332 sgd_solver.cpp:105] Iteration 39300, lr = 0.1
I1001 15:36:48.220914  5332 solver.cpp:218] Iteration 39400 (19.0192 iter/s, 5.25784s/100 iters), loss = 0.0983877
I1001 15:36:48.220943  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0983885 (* 1 = 0.0983885 loss)
I1001 15:36:48.220948  5332 sgd_solver.cpp:105] Iteration 39400, lr = 0.1
I1001 15:36:53.220880  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:36:53.430832  5332 solver.cpp:330] Iteration 39500, Testing net (#0)
I1001 15:36:54.624960  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:36:54.675262  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8027
I1001 15:36:54.675297  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.618246 (* 1 = 0.618246 loss)
I1001 15:36:54.727684  5332 solver.cpp:218] Iteration 39500 (15.3687 iter/s, 6.50672s/100 iters), loss = 0.194871
I1001 15:36:54.727712  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194872 (* 1 = 0.194872 loss)
I1001 15:36:54.727720  5332 sgd_solver.cpp:105] Iteration 39500, lr = 0.1
I1001 15:36:59.987908  5332 solver.cpp:218] Iteration 39600 (19.0108 iter/s, 5.26018s/100 iters), loss = 0.283156
I1001 15:36:59.988023  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283156 (* 1 = 0.283156 loss)
I1001 15:36:59.988031  5332 sgd_solver.cpp:105] Iteration 39600, lr = 0.1
I1001 15:37:05.246901  5332 solver.cpp:218] Iteration 39700 (19.0155 iter/s, 5.25886s/100 iters), loss = 0.151557
I1001 15:37:05.246933  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151558 (* 1 = 0.151558 loss)
I1001 15:37:05.246949  5332 sgd_solver.cpp:105] Iteration 39700, lr = 0.1
I1001 15:37:10.500229  5332 solver.cpp:218] Iteration 39800 (19.0357 iter/s, 5.25328s/100 iters), loss = 0.235003
I1001 15:37:10.500258  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235004 (* 1 = 0.235004 loss)
I1001 15:37:10.500275  5332 sgd_solver.cpp:105] Iteration 39800, lr = 0.1
I1001 15:37:15.767025  5332 solver.cpp:218] Iteration 39900 (18.9871 iter/s, 5.26675s/100 iters), loss = 0.142569
I1001 15:37:15.767055  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14257 (* 1 = 0.14257 loss)
I1001 15:37:15.767061  5332 sgd_solver.cpp:105] Iteration 39900, lr = 0.1
I1001 15:37:20.761742  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:37:20.971444  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_40000.caffemodel
I1001 15:37:20.976588  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_40000.solverstate
I1001 15:37:20.977962  5332 solver.cpp:330] Iteration 40000, Testing net (#0)
I1001 15:37:22.170119  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:37:22.220494  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7717
I1001 15:37:22.220520  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.755984 (* 1 = 0.755984 loss)
I1001 15:37:22.273483  5332 solver.cpp:218] Iteration 40000 (15.3695 iter/s, 6.50641s/100 iters), loss = 0.20886
I1001 15:37:22.273507  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20886 (* 1 = 0.20886 loss)
I1001 15:37:22.273512  5332 sgd_solver.cpp:46] MultiStep Status: Iteration 40000, step = 1
I1001 15:37:22.273516  5332 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I1001 15:37:27.532927  5332 solver.cpp:218] Iteration 40100 (19.0136 iter/s, 5.2594s/100 iters), loss = 0.21785
I1001 15:37:27.532965  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217851 (* 1 = 0.217851 loss)
I1001 15:37:27.532971  5332 sgd_solver.cpp:105] Iteration 40100, lr = 0.01
I1001 15:37:32.786123  5332 solver.cpp:218] Iteration 40200 (19.0362 iter/s, 5.25314s/100 iters), loss = 0.153818
I1001 15:37:32.786267  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.153819 (* 1 = 0.153819 loss)
I1001 15:37:32.786274  5332 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I1001 15:37:38.050209  5332 solver.cpp:218] Iteration 40300 (18.9972 iter/s, 5.26393s/100 iters), loss = 0.174605
I1001 15:37:38.050238  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.174605 (* 1 = 0.174605 loss)
I1001 15:37:38.050246  5332 sgd_solver.cpp:105] Iteration 40300, lr = 0.01
I1001 15:37:43.302245  5332 solver.cpp:218] Iteration 40400 (19.0404 iter/s, 5.25199s/100 iters), loss = 0.0835547
I1001 15:37:43.302284  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0835554 (* 1 = 0.0835554 loss)
I1001 15:37:43.302290  5332 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I1001 15:37:48.300948  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:37:48.510829  5332 solver.cpp:330] Iteration 40500, Testing net (#0)
I1001 15:37:49.708988  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:37:49.761113  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8881
I1001 15:37:49.761148  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.326161 (* 1 = 0.326161 loss)
I1001 15:37:49.815065  5332 solver.cpp:218] Iteration 40500 (15.3545 iter/s, 6.51275s/100 iters), loss = 0.113706
I1001 15:37:49.815114  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113707 (* 1 = 0.113707 loss)
I1001 15:37:49.815121  5332 sgd_solver.cpp:105] Iteration 40500, lr = 0.01
I1001 15:37:55.073870  5332 solver.cpp:218] Iteration 40600 (19.0161 iter/s, 5.2587s/100 iters), loss = 0.151047
I1001 15:37:55.073899  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151048 (* 1 = 0.151048 loss)
I1001 15:37:55.073904  5332 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I1001 15:38:00.337110  5332 solver.cpp:218] Iteration 40700 (18.9999 iter/s, 5.26319s/100 iters), loss = 0.137083
I1001 15:38:00.337138  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.137084 (* 1 = 0.137084 loss)
I1001 15:38:00.337144  5332 sgd_solver.cpp:105] Iteration 40700, lr = 0.01
I1001 15:38:05.597316  5332 solver.cpp:218] Iteration 40800 (19.0109 iter/s, 5.26015s/100 iters), loss = 0.122376
I1001 15:38:05.597477  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.122377 (* 1 = 0.122377 loss)
I1001 15:38:05.597486  5332 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I1001 15:38:10.859424  5332 solver.cpp:218] Iteration 40900 (19.0044 iter/s, 5.26194s/100 iters), loss = 0.0698686
I1001 15:38:10.859467  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0698694 (* 1 = 0.0698694 loss)
I1001 15:38:10.859475  5332 sgd_solver.cpp:105] Iteration 40900, lr = 0.01
I1001 15:38:15.849633  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:38:16.059326  5332 solver.cpp:330] Iteration 41000, Testing net (#0)
I1001 15:38:17.257117  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:38:17.307453  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9017
I1001 15:38:17.307488  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.285887 (* 1 = 0.285887 loss)
I1001 15:38:17.359948  5332 solver.cpp:218] Iteration 41000 (15.3836 iter/s, 6.50043s/100 iters), loss = 0.114141
I1001 15:38:17.359977  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.114142 (* 1 = 0.114142 loss)
I1001 15:38:17.359983  5332 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I1001 15:38:22.613919  5332 solver.cpp:218] Iteration 41100 (19.0334 iter/s, 5.25392s/100 iters), loss = 0.149498
I1001 15:38:22.613952  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149499 (* 1 = 0.149499 loss)
I1001 15:38:22.613958  5332 sgd_solver.cpp:105] Iteration 41100, lr = 0.01
I1001 15:38:27.877545  5332 solver.cpp:218] Iteration 41200 (18.9985 iter/s, 5.26358s/100 iters), loss = 0.0993646
I1001 15:38:27.877584  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0993653 (* 1 = 0.0993653 loss)
I1001 15:38:27.877590  5332 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I1001 15:38:33.145340  5332 solver.cpp:218] Iteration 41300 (18.9835 iter/s, 5.26773s/100 iters), loss = 0.102287
I1001 15:38:33.145370  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.102287 (* 1 = 0.102287 loss)
I1001 15:38:33.145375  5332 sgd_solver.cpp:105] Iteration 41300, lr = 0.01
I1001 15:38:38.411016  5332 solver.cpp:218] Iteration 41400 (18.9911 iter/s, 5.26562s/100 iters), loss = 0.0906911
I1001 15:38:38.411134  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0906918 (* 1 = 0.0906918 loss)
I1001 15:38:38.411145  5332 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I1001 15:38:43.403424  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:38:43.614701  5332 solver.cpp:330] Iteration 41500, Testing net (#0)
I1001 15:38:44.817276  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:38:44.867499  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9011
I1001 15:38:44.867527  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.288436 (* 1 = 0.288436 loss)
I1001 15:38:44.920131  5332 solver.cpp:218] Iteration 41500 (15.3634 iter/s, 6.50898s/100 iters), loss = 0.0554527
I1001 15:38:44.920158  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0554535 (* 1 = 0.0554535 loss)
I1001 15:38:44.920168  5332 sgd_solver.cpp:105] Iteration 41500, lr = 0.01
I1001 15:38:50.182148  5332 solver.cpp:218] Iteration 41600 (19.0043 iter/s, 5.26197s/100 iters), loss = 0.0844328
I1001 15:38:50.182178  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0844336 (* 1 = 0.0844336 loss)
I1001 15:38:50.182183  5332 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I1001 15:38:55.431773  5332 solver.cpp:218] Iteration 41700 (19.0492 iter/s, 5.24957s/100 iters), loss = 0.0959943
I1001 15:38:55.431805  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0959951 (* 1 = 0.0959951 loss)
I1001 15:38:55.431812  5332 sgd_solver.cpp:105] Iteration 41700, lr = 0.01
I1001 15:39:00.691887  5332 solver.cpp:218] Iteration 41800 (19.0112 iter/s, 5.26006s/100 iters), loss = 0.0929164
I1001 15:39:00.691927  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0929171 (* 1 = 0.0929171 loss)
I1001 15:39:00.691933  5332 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I1001 15:39:05.953402  5332 solver.cpp:218] Iteration 41900 (19.0061 iter/s, 5.26146s/100 iters), loss = 0.0429237
I1001 15:39:05.953433  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0429245 (* 1 = 0.0429245 loss)
I1001 15:39:05.953447  5332 sgd_solver.cpp:105] Iteration 41900, lr = 0.01
I1001 15:39:10.949915  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:39:11.160310  5332 solver.cpp:330] Iteration 42000, Testing net (#0)
I1001 15:39:12.351680  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:39:12.402083  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9029
I1001 15:39:12.402109  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.285941 (* 1 = 0.285941 loss)
I1001 15:39:12.455051  5332 solver.cpp:218] Iteration 42000 (15.3808 iter/s, 6.5016s/100 iters), loss = 0.0710737
I1001 15:39:12.455075  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0710745 (* 1 = 0.0710745 loss)
I1001 15:39:12.455082  5332 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I1001 15:39:17.726547  5332 solver.cpp:218] Iteration 42100 (18.9701 iter/s, 5.27145s/100 iters), loss = 0.0975074
I1001 15:39:17.726593  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0975082 (* 1 = 0.0975082 loss)
I1001 15:39:17.726600  5332 sgd_solver.cpp:105] Iteration 42100, lr = 0.01
I1001 15:39:22.988988  5332 solver.cpp:218] Iteration 42200 (19.0028 iter/s, 5.26237s/100 iters), loss = 0.091692
I1001 15:39:22.989019  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0916927 (* 1 = 0.0916927 loss)
I1001 15:39:22.989025  5332 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I1001 15:39:28.245574  5332 solver.cpp:218] Iteration 42300 (19.0239 iter/s, 5.25653s/100 iters), loss = 0.0932649
I1001 15:39:28.245616  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0932657 (* 1 = 0.0932657 loss)
I1001 15:39:28.245622  5332 sgd_solver.cpp:105] Iteration 42300, lr = 0.01
I1001 15:39:33.512775  5332 solver.cpp:218] Iteration 42400 (18.9856 iter/s, 5.26714s/100 iters), loss = 0.0673365
I1001 15:39:33.512807  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0673373 (* 1 = 0.0673373 loss)
I1001 15:39:33.512814  5332 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I1001 15:39:38.515275  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:39:38.726250  5332 solver.cpp:330] Iteration 42500, Testing net (#0)
I1001 15:39:39.920413  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:39:39.970966  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9056
I1001 15:39:39.971000  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.279332 (* 1 = 0.279332 loss)
I1001 15:39:40.023418  5332 solver.cpp:218] Iteration 42500 (15.3596 iter/s, 6.51059s/100 iters), loss = 0.0614082
I1001 15:39:40.023440  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.061409 (* 1 = 0.061409 loss)
I1001 15:39:40.023447  5332 sgd_solver.cpp:105] Iteration 42500, lr = 0.01
I1001 15:39:45.279778  5332 solver.cpp:218] Iteration 42600 (19.0247 iter/s, 5.25632s/100 iters), loss = 0.115189
I1001 15:39:45.279917  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.115189 (* 1 = 0.115189 loss)
I1001 15:39:45.279924  5332 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I1001 15:39:50.537067  5332 solver.cpp:218] Iteration 42700 (19.0218 iter/s, 5.25713s/100 iters), loss = 0.126946
I1001 15:39:50.537106  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126947 (* 1 = 0.126947 loss)
I1001 15:39:50.537111  5332 sgd_solver.cpp:105] Iteration 42700, lr = 0.01
I1001 15:39:55.793455  5332 solver.cpp:218] Iteration 42800 (19.0247 iter/s, 5.25633s/100 iters), loss = 0.0352301
I1001 15:39:55.793490  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0352309 (* 1 = 0.0352309 loss)
I1001 15:39:55.793498  5332 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I1001 15:40:01.043700  5332 solver.cpp:218] Iteration 42900 (19.0469 iter/s, 5.25019s/100 iters), loss = 0.036337
I1001 15:40:01.043728  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0363378 (* 1 = 0.0363378 loss)
I1001 15:40:01.043735  5332 sgd_solver.cpp:105] Iteration 42900, lr = 0.01
I1001 15:40:06.040649  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:40:06.250320  5332 solver.cpp:330] Iteration 43000, Testing net (#0)
I1001 15:40:07.441589  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:40:07.491926  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9026
I1001 15:40:07.491961  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.291317 (* 1 = 0.291317 loss)
I1001 15:40:07.544533  5332 solver.cpp:218] Iteration 43000 (15.3828 iter/s, 6.50078s/100 iters), loss = 0.0329296
I1001 15:40:07.544556  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0329304 (* 1 = 0.0329304 loss)
I1001 15:40:07.544564  5332 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I1001 15:40:12.807497  5332 solver.cpp:218] Iteration 43100 (19.0009 iter/s, 5.26292s/100 iters), loss = 0.0721719
I1001 15:40:12.807536  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0721727 (* 1 = 0.0721727 loss)
I1001 15:40:12.807543  5332 sgd_solver.cpp:105] Iteration 43100, lr = 0.01
I1001 15:40:18.070392  5332 solver.cpp:218] Iteration 43200 (19.0011 iter/s, 5.26284s/100 iters), loss = 0.103903
I1001 15:40:18.070456  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103904 (* 1 = 0.103904 loss)
I1001 15:40:18.070473  5332 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I1001 15:40:23.333703  5332 solver.cpp:218] Iteration 43300 (18.9997 iter/s, 5.26323s/100 iters), loss = 0.0971709
I1001 15:40:23.333751  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0971717 (* 1 = 0.0971717 loss)
I1001 15:40:23.333768  5332 sgd_solver.cpp:105] Iteration 43300, lr = 0.01
I1001 15:40:28.586158  5332 solver.cpp:218] Iteration 43400 (19.0389 iter/s, 5.25239s/100 iters), loss = 0.0668718
I1001 15:40:28.586187  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0668726 (* 1 = 0.0668726 loss)
I1001 15:40:28.586194  5332 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I1001 15:40:33.583498  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:40:33.793846  5332 solver.cpp:330] Iteration 43500, Testing net (#0)
I1001 15:40:34.994408  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:40:35.044940  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.902
I1001 15:40:35.044975  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.293415 (* 1 = 0.293415 loss)
I1001 15:40:35.097777  5332 solver.cpp:218] Iteration 43500 (15.3573 iter/s, 6.51157s/100 iters), loss = 0.0902626
I1001 15:40:35.097808  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0902634 (* 1 = 0.0902634 loss)
I1001 15:40:35.097815  5332 sgd_solver.cpp:105] Iteration 43500, lr = 0.01
I1001 15:40:40.349771  5332 solver.cpp:218] Iteration 43600 (19.0406 iter/s, 5.25194s/100 iters), loss = 0.0537063
I1001 15:40:40.349799  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0537071 (* 1 = 0.0537071 loss)
I1001 15:40:40.349807  5332 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I1001 15:40:45.610124  5332 solver.cpp:218] Iteration 43700 (19.0103 iter/s, 5.2603s/100 iters), loss = 0.0842881
I1001 15:40:45.610164  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0842889 (* 1 = 0.0842889 loss)
I1001 15:40:45.610170  5332 sgd_solver.cpp:105] Iteration 43700, lr = 0.01
I1001 15:40:50.871014  5332 solver.cpp:218] Iteration 43800 (19.0084 iter/s, 5.26083s/100 iters), loss = 0.117662
I1001 15:40:50.871134  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117663 (* 1 = 0.117663 loss)
I1001 15:40:50.871142  5332 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I1001 15:40:56.133409  5332 solver.cpp:218] Iteration 43900 (19.0032 iter/s, 5.26226s/100 iters), loss = 0.0856778
I1001 15:40:56.133440  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0856786 (* 1 = 0.0856786 loss)
I1001 15:40:56.133455  5332 sgd_solver.cpp:105] Iteration 43900, lr = 0.01
I1001 15:41:01.125726  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:41:01.336515  5332 solver.cpp:330] Iteration 44000, Testing net (#0)
I1001 15:41:02.536432  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:41:02.586858  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9036
I1001 15:41:02.586892  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.297312 (* 1 = 0.297312 loss)
I1001 15:41:02.639446  5332 solver.cpp:218] Iteration 44000 (15.3705 iter/s, 6.50599s/100 iters), loss = 0.0719456
I1001 15:41:02.639469  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0719464 (* 1 = 0.0719464 loss)
I1001 15:41:02.639477  5332 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I1001 15:41:07.903436  5332 solver.cpp:218] Iteration 44100 (18.9972 iter/s, 5.26394s/100 iters), loss = 0.104465
I1001 15:41:07.903468  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.104466 (* 1 = 0.104466 loss)
I1001 15:41:07.903475  5332 sgd_solver.cpp:105] Iteration 44100, lr = 0.01
I1001 15:41:13.159137  5332 solver.cpp:218] Iteration 44200 (19.0272 iter/s, 5.25565s/100 iters), loss = 0.12398
I1001 15:41:13.159176  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.123981 (* 1 = 0.123981 loss)
I1001 15:41:13.159183  5332 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I1001 15:41:18.422190  5332 solver.cpp:218] Iteration 44300 (19.0006 iter/s, 5.26299s/100 iters), loss = 0.0392676
I1001 15:41:18.422241  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0392684 (* 1 = 0.0392684 loss)
I1001 15:41:18.422247  5332 sgd_solver.cpp:105] Iteration 44300, lr = 0.01
I1001 15:41:23.685963  5332 solver.cpp:218] Iteration 44400 (18.998 iter/s, 5.2637s/100 iters), loss = 0.117462
I1001 15:41:23.686102  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117463 (* 1 = 0.117463 loss)
I1001 15:41:23.686110  5332 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I1001 15:41:28.678282  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:41:28.894472  5332 solver.cpp:330] Iteration 44500, Testing net (#0)
I1001 15:41:30.090327  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:41:30.140954  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9025
I1001 15:41:30.140988  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.300682 (* 1 = 0.300682 loss)
I1001 15:41:30.193511  5332 solver.cpp:218] Iteration 44500 (15.3671 iter/s, 6.50739s/100 iters), loss = 0.0638748
I1001 15:41:30.193542  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0638756 (* 1 = 0.0638756 loss)
I1001 15:41:30.193548  5332 sgd_solver.cpp:105] Iteration 44500, lr = 0.01
I1001 15:41:35.450471  5332 solver.cpp:218] Iteration 44600 (19.0226 iter/s, 5.25691s/100 iters), loss = 0.0784344
I1001 15:41:35.450500  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0784353 (* 1 = 0.0784353 loss)
I1001 15:41:35.450506  5332 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I1001 15:41:40.703476  5332 solver.cpp:218] Iteration 44700 (19.0369 iter/s, 5.25295s/100 iters), loss = 0.0647664
I1001 15:41:40.703511  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0647673 (* 1 = 0.0647673 loss)
I1001 15:41:40.703518  5332 sgd_solver.cpp:105] Iteration 44700, lr = 0.01
I1001 15:41:45.959511  5332 solver.cpp:218] Iteration 44800 (19.026 iter/s, 5.25598s/100 iters), loss = 0.0815536
I1001 15:41:45.959542  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0815544 (* 1 = 0.0815544 loss)
I1001 15:41:45.959547  5332 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I1001 15:41:51.224546  5332 solver.cpp:218] Iteration 44900 (18.9934 iter/s, 5.26498s/100 iters), loss = 0.0407593
I1001 15:41:51.224576  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0407601 (* 1 = 0.0407601 loss)
I1001 15:41:51.224582  5332 sgd_solver.cpp:105] Iteration 44900, lr = 0.01
I1001 15:41:56.226927  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:41:56.437846  5332 solver.cpp:330] Iteration 45000, Testing net (#0)
I1001 15:41:57.630101  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:41:57.680307  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9039
I1001 15:41:57.680341  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.295918 (* 1 = 0.295918 loss)
I1001 15:41:57.732847  5332 solver.cpp:218] Iteration 45000 (15.3651 iter/s, 6.50825s/100 iters), loss = 0.0448858
I1001 15:41:57.732868  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0448866 (* 1 = 0.0448866 loss)
I1001 15:41:57.732875  5332 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I1001 15:42:02.996651  5332 solver.cpp:218] Iteration 45100 (18.9978 iter/s, 5.26376s/100 iters), loss = 0.0970124
I1001 15:42:02.996685  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0970133 (* 1 = 0.0970133 loss)
I1001 15:42:02.996693  5332 sgd_solver.cpp:105] Iteration 45100, lr = 0.01
I1001 15:42:08.253633  5332 solver.cpp:218] Iteration 45200 (19.0225 iter/s, 5.25693s/100 iters), loss = 0.0769496
I1001 15:42:08.253666  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0769505 (* 1 = 0.0769505 loss)
I1001 15:42:08.253685  5332 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I1001 15:42:13.502468  5332 solver.cpp:218] Iteration 45300 (19.052 iter/s, 5.24878s/100 iters), loss = 0.0557357
I1001 15:42:13.502506  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0557366 (* 1 = 0.0557366 loss)
I1001 15:42:13.502516  5332 sgd_solver.cpp:105] Iteration 45300, lr = 0.01
I1001 15:42:18.759147  5332 solver.cpp:218] Iteration 45400 (19.0236 iter/s, 5.25663s/100 iters), loss = 0.029537
I1001 15:42:18.759177  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0295378 (* 1 = 0.0295378 loss)
I1001 15:42:18.759183  5332 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I1001 15:42:23.759117  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:42:23.969943  5332 solver.cpp:330] Iteration 45500, Testing net (#0)
I1001 15:42:25.162701  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:42:25.213194  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9009
I1001 15:42:25.213230  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.306851 (* 1 = 0.306851 loss)
I1001 15:42:25.266100  5332 solver.cpp:218] Iteration 45500 (15.3683 iter/s, 6.5069s/100 iters), loss = 0.0710365
I1001 15:42:25.266125  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0710373 (* 1 = 0.0710373 loss)
I1001 15:42:25.266132  5332 sgd_solver.cpp:105] Iteration 45500, lr = 0.01
I1001 15:42:30.530150  5332 solver.cpp:218] Iteration 45600 (18.9969 iter/s, 5.264s/100 iters), loss = 0.0563016
I1001 15:42:30.530289  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0563025 (* 1 = 0.0563025 loss)
I1001 15:42:30.530308  5332 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I1001 15:42:35.788908  5332 solver.cpp:218] Iteration 45700 (19.0165 iter/s, 5.2586s/100 iters), loss = 0.0792817
I1001 15:42:35.788938  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0792826 (* 1 = 0.0792826 loss)
I1001 15:42:35.788944  5332 sgd_solver.cpp:105] Iteration 45700, lr = 0.01
I1001 15:42:41.054221  5332 solver.cpp:218] Iteration 45800 (18.9924 iter/s, 5.26526s/100 iters), loss = 0.0480715
I1001 15:42:41.054250  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0480723 (* 1 = 0.0480723 loss)
I1001 15:42:41.054257  5332 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I1001 15:42:46.302883  5332 solver.cpp:218] Iteration 45900 (19.0527 iter/s, 5.24861s/100 iters), loss = 0.0675303
I1001 15:42:46.302913  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0675312 (* 1 = 0.0675312 loss)
I1001 15:42:46.302919  5332 sgd_solver.cpp:105] Iteration 45900, lr = 0.01
I1001 15:42:51.303344  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:42:51.513306  5332 solver.cpp:330] Iteration 46000, Testing net (#0)
I1001 15:42:52.706964  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:42:52.757165  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9033
I1001 15:42:52.757190  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.302664 (* 1 = 0.302664 loss)
I1001 15:42:52.810843  5332 solver.cpp:218] Iteration 46000 (15.3659 iter/s, 6.5079s/100 iters), loss = 0.0667532
I1001 15:42:52.810901  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.066754 (* 1 = 0.066754 loss)
I1001 15:42:52.810909  5332 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I1001 15:42:58.064790  5332 solver.cpp:218] Iteration 46100 (19.0337 iter/s, 5.25383s/100 iters), loss = 0.0376289
I1001 15:42:58.064831  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0376297 (* 1 = 0.0376297 loss)
I1001 15:42:58.064836  5332 sgd_solver.cpp:105] Iteration 46100, lr = 0.01
I1001 15:43:03.322057  5332 solver.cpp:218] Iteration 46200 (19.0215 iter/s, 5.2572s/100 iters), loss = 0.0654905
I1001 15:43:03.322165  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0654914 (* 1 = 0.0654914 loss)
I1001 15:43:03.322173  5332 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I1001 15:43:08.578560  5332 solver.cpp:218] Iteration 46300 (19.0245 iter/s, 5.25637s/100 iters), loss = 0.106633
I1001 15:43:08.578600  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106634 (* 1 = 0.106634 loss)
I1001 15:43:08.578608  5332 sgd_solver.cpp:105] Iteration 46300, lr = 0.01
I1001 15:43:13.836418  5332 solver.cpp:218] Iteration 46400 (19.0194 iter/s, 5.25779s/100 iters), loss = 0.0459158
I1001 15:43:13.836457  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0459166 (* 1 = 0.0459166 loss)
I1001 15:43:13.836463  5332 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I1001 15:43:18.827710  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:43:19.038050  5332 solver.cpp:330] Iteration 46500, Testing net (#0)
I1001 15:43:20.240980  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:43:20.291088  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8999
I1001 15:43:20.291123  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.309531 (* 1 = 0.309531 loss)
I1001 15:43:20.344033  5332 solver.cpp:218] Iteration 46500 (15.3667 iter/s, 6.50756s/100 iters), loss = 0.0255155
I1001 15:43:20.344056  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0255163 (* 1 = 0.0255163 loss)
I1001 15:43:20.344063  5332 sgd_solver.cpp:105] Iteration 46500, lr = 0.01
I1001 15:43:25.594426  5332 solver.cpp:218] Iteration 46600 (19.0464 iter/s, 5.25035s/100 iters), loss = 0.04155
I1001 15:43:25.594465  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0415508 (* 1 = 0.0415508 loss)
I1001 15:43:25.594471  5332 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I1001 15:43:30.857888  5332 solver.cpp:218] Iteration 46700 (18.9991 iter/s, 5.2634s/100 iters), loss = 0.105426
I1001 15:43:30.857928  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105427 (* 1 = 0.105427 loss)
I1001 15:43:30.857933  5332 sgd_solver.cpp:105] Iteration 46700, lr = 0.01
I1001 15:43:36.122253  5332 solver.cpp:218] Iteration 46800 (18.9959 iter/s, 5.2643s/100 iters), loss = 0.0723331
I1001 15:43:36.122372  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0723339 (* 1 = 0.0723339 loss)
I1001 15:43:36.122380  5332 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I1001 15:43:41.378886  5332 solver.cpp:218] Iteration 46900 (19.0241 iter/s, 5.2565s/100 iters), loss = 0.0734826
I1001 15:43:41.378914  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0734834 (* 1 = 0.0734834 loss)
I1001 15:43:41.378921  5332 sgd_solver.cpp:105] Iteration 46900, lr = 0.01
I1001 15:43:46.364356  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:43:46.573511  5332 solver.cpp:330] Iteration 47000, Testing net (#0)
I1001 15:43:47.770653  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:43:47.820816  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9034
I1001 15:43:47.820842  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.305577 (* 1 = 0.305577 loss)
I1001 15:43:47.873464  5332 solver.cpp:218] Iteration 47000 (15.3976 iter/s, 6.49452s/100 iters), loss = 0.0346359
I1001 15:43:47.873488  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0346366 (* 1 = 0.0346366 loss)
I1001 15:43:47.873495  5332 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I1001 15:43:53.130283  5332 solver.cpp:218] Iteration 47100 (19.0231 iter/s, 5.25677s/100 iters), loss = 0.059016
I1001 15:43:53.130314  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0590167 (* 1 = 0.0590167 loss)
I1001 15:43:53.130321  5332 sgd_solver.cpp:105] Iteration 47100, lr = 0.01
I1001 15:43:58.381391  5332 solver.cpp:218] Iteration 47200 (19.0438 iter/s, 5.25105s/100 iters), loss = 0.0793566
I1001 15:43:58.381429  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0793574 (* 1 = 0.0793574 loss)
I1001 15:43:58.381435  5332 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I1001 15:44:03.635757  5332 solver.cpp:218] Iteration 47300 (19.032 iter/s, 5.2543s/100 iters), loss = 0.0682911
I1001 15:44:03.635795  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0682919 (* 1 = 0.0682919 loss)
I1001 15:44:03.635802  5332 sgd_solver.cpp:105] Iteration 47300, lr = 0.01
I1001 15:44:08.899636  5332 solver.cpp:218] Iteration 47400 (18.9976 iter/s, 5.26382s/100 iters), loss = 0.0456798
I1001 15:44:08.899742  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0456806 (* 1 = 0.0456806 loss)
I1001 15:44:08.899750  5332 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I1001 15:44:13.900737  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:44:14.111563  5332 solver.cpp:330] Iteration 47500, Testing net (#0)
I1001 15:44:15.304023  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:44:15.354328  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8997
I1001 15:44:15.354372  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.328385 (* 1 = 0.328385 loss)
I1001 15:44:15.407392  5332 solver.cpp:218] Iteration 47500 (15.3666 iter/s, 6.50763s/100 iters), loss = 0.0601715
I1001 15:44:15.407418  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0601723 (* 1 = 0.0601723 loss)
I1001 15:44:15.407425  5332 sgd_solver.cpp:105] Iteration 47500, lr = 0.01
I1001 15:44:20.671908  5332 solver.cpp:218] Iteration 47600 (18.9953 iter/s, 5.26447s/100 iters), loss = 0.0889215
I1001 15:44:20.671937  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0889224 (* 1 = 0.0889224 loss)
I1001 15:44:20.671943  5332 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I1001 15:44:25.931994  5332 solver.cpp:218] Iteration 47700 (19.0113 iter/s, 5.26003s/100 iters), loss = 0.0473477
I1001 15:44:25.932027  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0473486 (* 1 = 0.0473486 loss)
I1001 15:44:25.932034  5332 sgd_solver.cpp:105] Iteration 47700, lr = 0.01
I1001 15:44:31.186262  5332 solver.cpp:218] Iteration 47800 (19.0323 iter/s, 5.25421s/100 iters), loss = 0.0367267
I1001 15:44:31.186292  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0367276 (* 1 = 0.0367276 loss)
I1001 15:44:31.186308  5332 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I1001 15:44:36.446377  5332 solver.cpp:218] Iteration 47900 (19.0112 iter/s, 5.26006s/100 iters), loss = 0.0538017
I1001 15:44:36.446406  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0538025 (* 1 = 0.0538025 loss)
I1001 15:44:36.446422  5332 sgd_solver.cpp:105] Iteration 47900, lr = 0.01
I1001 15:44:41.443115  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:44:41.654088  5332 solver.cpp:330] Iteration 48000, Testing net (#0)
I1001 15:44:42.845752  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:44:42.895982  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9
I1001 15:44:42.896008  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.330293 (* 1 = 0.330293 loss)
I1001 15:44:42.948675  5332 solver.cpp:218] Iteration 48000 (15.3793 iter/s, 6.50224s/100 iters), loss = 0.0890565
I1001 15:44:42.948698  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0890573 (* 1 = 0.0890573 loss)
I1001 15:44:42.948705  5332 sgd_solver.cpp:105] Iteration 48000, lr = 0.01
I1001 15:44:48.212189  5332 solver.cpp:218] Iteration 48100 (18.9989 iter/s, 5.26346s/100 iters), loss = 0.0572829
I1001 15:44:48.212230  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0572837 (* 1 = 0.0572837 loss)
I1001 15:44:48.212236  5332 sgd_solver.cpp:105] Iteration 48100, lr = 0.01
I1001 15:44:53.474114  5332 solver.cpp:218] Iteration 48200 (19.0047 iter/s, 5.26186s/100 iters), loss = 0.0308859
I1001 15:44:53.474153  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0308868 (* 1 = 0.0308868 loss)
I1001 15:44:53.474159  5332 sgd_solver.cpp:105] Iteration 48200, lr = 0.01
I1001 15:44:58.730463  5332 solver.cpp:218] Iteration 48300 (19.0248 iter/s, 5.25628s/100 iters), loss = 0.0576202
I1001 15:44:58.730499  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.057621 (* 1 = 0.057621 loss)
I1001 15:44:58.730506  5332 sgd_solver.cpp:105] Iteration 48300, lr = 0.01
I1001 15:45:03.993077  5332 solver.cpp:218] Iteration 48400 (19.0022 iter/s, 5.26256s/100 iters), loss = 0.0174702
I1001 15:45:03.993106  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.017471 (* 1 = 0.017471 loss)
I1001 15:45:03.993122  5332 sgd_solver.cpp:105] Iteration 48400, lr = 0.01
I1001 15:45:08.992981  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:45:09.202985  5332 solver.cpp:330] Iteration 48500, Testing net (#0)
I1001 15:45:10.397457  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:45:10.447412  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9036
I1001 15:45:10.447448  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.309662 (* 1 = 0.309662 loss)
I1001 15:45:10.499927  5332 solver.cpp:218] Iteration 48500 (15.3685 iter/s, 6.5068s/100 iters), loss = 0.0673961
I1001 15:45:10.499949  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0673969 (* 1 = 0.0673969 loss)
I1001 15:45:10.499956  5332 sgd_solver.cpp:105] Iteration 48500, lr = 0.01
I1001 15:45:15.761766  5332 solver.cpp:218] Iteration 48600 (19.0049 iter/s, 5.26179s/100 iters), loss = 0.0688322
I1001 15:45:15.761873  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.068833 (* 1 = 0.068833 loss)
I1001 15:45:15.761890  5332 sgd_solver.cpp:105] Iteration 48600, lr = 0.01
I1001 15:45:21.019650  5332 solver.cpp:218] Iteration 48700 (19.0195 iter/s, 5.25776s/100 iters), loss = 0.0997921
I1001 15:45:21.019690  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0997929 (* 1 = 0.0997929 loss)
I1001 15:45:21.019695  5332 sgd_solver.cpp:105] Iteration 48700, lr = 0.01
I1001 15:45:26.281008  5332 solver.cpp:218] Iteration 48800 (19.0067 iter/s, 5.2613s/100 iters), loss = 0.0534521
I1001 15:45:26.281049  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0534529 (* 1 = 0.0534529 loss)
I1001 15:45:26.281056  5332 sgd_solver.cpp:105] Iteration 48800, lr = 0.01
I1001 15:45:31.531319  5332 solver.cpp:218] Iteration 48900 (19.0467 iter/s, 5.25025s/100 iters), loss = 0.0343691
I1001 15:45:31.531347  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0343699 (* 1 = 0.0343699 loss)
I1001 15:45:31.531352  5332 sgd_solver.cpp:105] Iteration 48900, lr = 0.01
I1001 15:45:36.527115  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:45:36.737776  5332 solver.cpp:330] Iteration 49000, Testing net (#0)
I1001 15:45:37.938844  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:45:37.989387  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.905
I1001 15:45:37.989423  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.313858 (* 1 = 0.313858 loss)
I1001 15:45:38.042476  5332 solver.cpp:218] Iteration 49000 (15.3584 iter/s, 6.51111s/100 iters), loss = 0.0513209
I1001 15:45:38.042506  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0513217 (* 1 = 0.0513217 loss)
I1001 15:45:38.042513  5332 sgd_solver.cpp:105] Iteration 49000, lr = 0.01
I1001 15:45:43.295675  5332 solver.cpp:218] Iteration 49100 (19.0362 iter/s, 5.25314s/100 iters), loss = 0.0721389
I1001 15:45:43.295722  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0721398 (* 1 = 0.0721398 loss)
I1001 15:45:43.295730  5332 sgd_solver.cpp:105] Iteration 49100, lr = 0.01
I1001 15:45:48.560286  5332 solver.cpp:218] Iteration 49200 (18.995 iter/s, 5.26454s/100 iters), loss = 0.0569538
I1001 15:45:48.560381  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0569546 (* 1 = 0.0569546 loss)
I1001 15:45:48.560389  5332 sgd_solver.cpp:105] Iteration 49200, lr = 0.01
I1001 15:45:53.821099  5332 solver.cpp:218] Iteration 49300 (19.0089 iter/s, 5.26069s/100 iters), loss = 0.0691481
I1001 15:45:53.821137  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0691489 (* 1 = 0.0691489 loss)
I1001 15:45:53.821143  5332 sgd_solver.cpp:105] Iteration 49300, lr = 0.01
I1001 15:45:59.085146  5332 solver.cpp:218] Iteration 49400 (18.997 iter/s, 5.26399s/100 iters), loss = 0.0618427
I1001 15:45:59.085177  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0618435 (* 1 = 0.0618435 loss)
I1001 15:45:59.085185  5332 sgd_solver.cpp:105] Iteration 49400, lr = 0.01
I1001 15:46:04.073671  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:46:04.283588  5332 solver.cpp:330] Iteration 49500, Testing net (#0)
I1001 15:46:05.482455  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:46:05.532341  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8969
I1001 15:46:05.532363  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329211 (* 1 = 0.329211 loss)
I1001 15:46:05.584957  5332 solver.cpp:218] Iteration 49500 (15.3852 iter/s, 6.49976s/100 iters), loss = 0.0747907
I1001 15:46:05.584980  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0747915 (* 1 = 0.0747915 loss)
I1001 15:46:05.584987  5332 sgd_solver.cpp:105] Iteration 49500, lr = 0.01
I1001 15:46:10.837767  5332 solver.cpp:218] Iteration 49600 (19.0376 iter/s, 5.25276s/100 iters), loss = 0.0406279
I1001 15:46:10.837800  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0406287 (* 1 = 0.0406287 loss)
I1001 15:46:10.837807  5332 sgd_solver.cpp:105] Iteration 49600, lr = 0.01
I1001 15:46:16.094620  5332 solver.cpp:218] Iteration 49700 (19.023 iter/s, 5.25679s/100 iters), loss = 0.0625108
I1001 15:46:16.094651  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0625117 (* 1 = 0.0625117 loss)
I1001 15:46:16.094657  5332 sgd_solver.cpp:105] Iteration 49700, lr = 0.01
I1001 15:46:21.355226  5332 solver.cpp:218] Iteration 49800 (19.0094 iter/s, 5.26055s/100 iters), loss = 0.0191764
I1001 15:46:21.355401  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0191773 (* 1 = 0.0191773 loss)
I1001 15:46:21.355410  5332 sgd_solver.cpp:105] Iteration 49800, lr = 0.01
I1001 15:46:26.616746  5332 solver.cpp:218] Iteration 49900 (19.0066 iter/s, 5.26134s/100 iters), loss = 0.0186256
I1001 15:46:26.616776  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0186265 (* 1 = 0.0186265 loss)
I1001 15:46:26.616793  5332 sgd_solver.cpp:105] Iteration 49900, lr = 0.01
I1001 15:46:31.605487  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:46:31.822572  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_50000.caffemodel
I1001 15:46:31.827821  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_50000.solverstate
I1001 15:46:31.829186  5332 solver.cpp:330] Iteration 50000, Testing net (#0)
I1001 15:46:33.024194  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:46:33.074822  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9001
I1001 15:46:33.074858  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.319559 (* 1 = 0.319559 loss)
I1001 15:46:33.127308  5332 solver.cpp:218] Iteration 50000 (15.3598 iter/s, 6.51051s/100 iters), loss = 0.0750895
I1001 15:46:33.127331  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0750903 (* 1 = 0.0750903 loss)
I1001 15:46:33.127338  5332 sgd_solver.cpp:105] Iteration 50000, lr = 0.01
I1001 15:46:38.394896  5332 solver.cpp:218] Iteration 50100 (18.9842 iter/s, 5.26754s/100 iters), loss = 0.0313927
I1001 15:46:38.394937  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313935 (* 1 = 0.0313935 loss)
I1001 15:46:38.394942  5332 sgd_solver.cpp:105] Iteration 50100, lr = 0.01
I1001 15:46:43.642235  5332 solver.cpp:218] Iteration 50200 (19.0575 iter/s, 5.24728s/100 iters), loss = 0.041963
I1001 15:46:43.642277  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0419638 (* 1 = 0.0419638 loss)
I1001 15:46:43.642287  5332 sgd_solver.cpp:105] Iteration 50200, lr = 0.01
I1001 15:46:48.902468  5332 solver.cpp:218] Iteration 50300 (19.0109 iter/s, 5.26014s/100 iters), loss = 0.0473635
I1001 15:46:48.902498  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0473643 (* 1 = 0.0473643 loss)
I1001 15:46:48.902503  5332 sgd_solver.cpp:105] Iteration 50300, lr = 0.01
I1001 15:46:54.161010  5332 solver.cpp:218] Iteration 50400 (19.0169 iter/s, 5.25849s/100 iters), loss = 0.0299355
I1001 15:46:54.161121  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0299363 (* 1 = 0.0299363 loss)
I1001 15:46:54.161129  5332 sgd_solver.cpp:105] Iteration 50400, lr = 0.01
I1001 15:46:59.160394  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:46:59.369886  5332 solver.cpp:330] Iteration 50500, Testing net (#0)
I1001 15:47:00.564790  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:47:00.615306  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8976
I1001 15:47:00.615341  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.325717 (* 1 = 0.325717 loss)
I1001 15:47:00.667981  5332 solver.cpp:218] Iteration 50500 (15.3684 iter/s, 6.50685s/100 iters), loss = 0.03556
I1001 15:47:00.668005  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0355608 (* 1 = 0.0355608 loss)
I1001 15:47:00.668011  5332 sgd_solver.cpp:105] Iteration 50500, lr = 0.01
I1001 15:47:05.930126  5332 solver.cpp:218] Iteration 50600 (19.0038 iter/s, 5.2621s/100 iters), loss = 0.0687003
I1001 15:47:05.930155  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.068701 (* 1 = 0.068701 loss)
I1001 15:47:05.930161  5332 sgd_solver.cpp:105] Iteration 50600, lr = 0.01
I1001 15:47:11.189340  5332 solver.cpp:218] Iteration 50700 (19.0144 iter/s, 5.25917s/100 iters), loss = 0.0601458
I1001 15:47:11.189369  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0601466 (* 1 = 0.0601466 loss)
I1001 15:47:11.189376  5332 sgd_solver.cpp:105] Iteration 50700, lr = 0.01
I1001 15:47:16.448760  5332 solver.cpp:218] Iteration 50800 (19.0137 iter/s, 5.25937s/100 iters), loss = 0.0513219
I1001 15:47:16.448789  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0513227 (* 1 = 0.0513227 loss)
I1001 15:47:16.448796  5332 sgd_solver.cpp:105] Iteration 50800, lr = 0.01
I1001 15:47:21.714931  5332 solver.cpp:218] Iteration 50900 (18.9893 iter/s, 5.26612s/100 iters), loss = 0.0563028
I1001 15:47:21.714969  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0563035 (* 1 = 0.0563035 loss)
I1001 15:47:21.714975  5332 sgd_solver.cpp:105] Iteration 50900, lr = 0.01
I1001 15:47:26.718214  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:47:26.928486  5332 solver.cpp:330] Iteration 51000, Testing net (#0)
I1001 15:47:28.117782  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:47:28.168243  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9006
I1001 15:47:28.168278  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.337757 (* 1 = 0.337757 loss)
I1001 15:47:28.220746  5332 solver.cpp:218] Iteration 51000 (15.371 iter/s, 6.50576s/100 iters), loss = 0.0321726
I1001 15:47:28.220773  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0321734 (* 1 = 0.0321734 loss)
I1001 15:47:28.220782  5332 sgd_solver.cpp:105] Iteration 51000, lr = 0.01
I1001 15:47:33.485463  5332 solver.cpp:218] Iteration 51100 (18.9946 iter/s, 5.26467s/100 iters), loss = 0.0818647
I1001 15:47:33.485502  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0818654 (* 1 = 0.0818654 loss)
I1001 15:47:33.485508  5332 sgd_solver.cpp:105] Iteration 51100, lr = 0.01
I1001 15:47:38.745820  5332 solver.cpp:218] Iteration 51200 (19.0103 iter/s, 5.2603s/100 iters), loss = 0.0295974
I1001 15:47:38.745848  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0295981 (* 1 = 0.0295981 loss)
I1001 15:47:38.745854  5332 sgd_solver.cpp:105] Iteration 51200, lr = 0.01
I1001 15:47:44.004921  5332 solver.cpp:218] Iteration 51300 (19.0148 iter/s, 5.25905s/100 iters), loss = 0.0362038
I1001 15:47:44.004951  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0362045 (* 1 = 0.0362045 loss)
I1001 15:47:44.004958  5332 sgd_solver.cpp:105] Iteration 51300, lr = 0.01
I1001 15:47:49.258775  5332 solver.cpp:218] Iteration 51400 (19.0338 iter/s, 5.2538s/100 iters), loss = 0.0519622
I1001 15:47:49.258805  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.051963 (* 1 = 0.051963 loss)
I1001 15:47:49.258810  5332 sgd_solver.cpp:105] Iteration 51400, lr = 0.01
I1001 15:47:54.257532  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:47:54.467259  5332 solver.cpp:330] Iteration 51500, Testing net (#0)
I1001 15:47:55.661872  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:47:55.712208  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8993
I1001 15:47:55.712244  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.336576 (* 1 = 0.336576 loss)
I1001 15:47:55.765769  5332 solver.cpp:218] Iteration 51500 (15.3682 iter/s, 6.50694s/100 iters), loss = 0.046404
I1001 15:47:55.765811  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0464048 (* 1 = 0.0464048 loss)
I1001 15:47:55.765818  5332 sgd_solver.cpp:105] Iteration 51500, lr = 0.01
I1001 15:48:01.030477  5332 solver.cpp:218] Iteration 51600 (18.9948 iter/s, 5.26461s/100 iters), loss = 0.0262527
I1001 15:48:01.030603  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0262534 (* 1 = 0.0262534 loss)
I1001 15:48:01.030611  5332 sgd_solver.cpp:105] Iteration 51600, lr = 0.01
I1001 15:48:06.292744  5332 solver.cpp:218] Iteration 51700 (19.0037 iter/s, 5.26212s/100 iters), loss = 0.0272926
I1001 15:48:06.292783  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0272933 (* 1 = 0.0272933 loss)
I1001 15:48:06.292789  5332 sgd_solver.cpp:105] Iteration 51700, lr = 0.01
I1001 15:48:11.555429  5332 solver.cpp:218] Iteration 51800 (19.0019 iter/s, 5.26262s/100 iters), loss = 0.0554809
I1001 15:48:11.555469  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0554816 (* 1 = 0.0554816 loss)
I1001 15:48:11.555474  5332 sgd_solver.cpp:105] Iteration 51800, lr = 0.01
I1001 15:48:16.814311  5332 solver.cpp:218] Iteration 51900 (19.0157 iter/s, 5.25882s/100 iters), loss = 0.0358792
I1001 15:48:16.814347  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0358799 (* 1 = 0.0358799 loss)
I1001 15:48:16.814354  5332 sgd_solver.cpp:105] Iteration 51900, lr = 0.01
I1001 15:48:21.808285  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:48:22.018580  5332 solver.cpp:330] Iteration 52000, Testing net (#0)
I1001 15:48:23.217839  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:48:23.267608  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8976
I1001 15:48:23.267642  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.34288 (* 1 = 0.34288 loss)
I1001 15:48:23.320149  5332 solver.cpp:218] Iteration 52000 (15.3709 iter/s, 6.50578s/100 iters), loss = 0.0537479
I1001 15:48:23.320175  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0537487 (* 1 = 0.0537487 loss)
I1001 15:48:23.320183  5332 sgd_solver.cpp:105] Iteration 52000, lr = 0.01
I1001 15:48:28.572712  5332 solver.cpp:218] Iteration 52100 (19.0385 iter/s, 5.25252s/100 iters), loss = 0.028414
I1001 15:48:28.572743  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0284148 (* 1 = 0.0284148 loss)
I1001 15:48:28.572749  5332 sgd_solver.cpp:105] Iteration 52100, lr = 0.01
I1001 15:48:33.833771  5332 solver.cpp:218] Iteration 52200 (19.0078 iter/s, 5.261s/100 iters), loss = 0.071072
I1001 15:48:33.833930  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0710728 (* 1 = 0.0710728 loss)
I1001 15:48:33.833938  5332 sgd_solver.cpp:105] Iteration 52200, lr = 0.01
I1001 15:48:39.097321  5332 solver.cpp:218] Iteration 52300 (18.9992 iter/s, 5.26338s/100 iters), loss = 0.0238502
I1001 15:48:39.097350  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.023851 (* 1 = 0.023851 loss)
I1001 15:48:39.097357  5332 sgd_solver.cpp:105] Iteration 52300, lr = 0.01
I1001 15:48:44.358460  5332 solver.cpp:218] Iteration 52400 (19.0075 iter/s, 5.26109s/100 iters), loss = 0.0736048
I1001 15:48:44.358489  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0736056 (* 1 = 0.0736056 loss)
I1001 15:48:44.358495  5332 sgd_solver.cpp:105] Iteration 52400, lr = 0.01
I1001 15:48:49.350533  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:48:49.561683  5332 solver.cpp:330] Iteration 52500, Testing net (#0)
I1001 15:48:50.764783  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:48:50.815127  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9021
I1001 15:48:50.815162  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.327383 (* 1 = 0.327383 loss)
I1001 15:48:50.867751  5332 solver.cpp:218] Iteration 52500 (15.3628 iter/s, 6.50924s/100 iters), loss = 0.0224728
I1001 15:48:50.867779  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0224736 (* 1 = 0.0224736 loss)
I1001 15:48:50.867785  5332 sgd_solver.cpp:105] Iteration 52500, lr = 0.01
I1001 15:48:56.132202  5332 solver.cpp:218] Iteration 52600 (18.9955 iter/s, 5.2644s/100 iters), loss = 0.0161343
I1001 15:48:56.132233  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.016135 (* 1 = 0.016135 loss)
I1001 15:48:56.132238  5332 sgd_solver.cpp:105] Iteration 52600, lr = 0.01
I1001 15:49:01.388583  5332 solver.cpp:218] Iteration 52700 (19.0247 iter/s, 5.25633s/100 iters), loss = 0.0599229
I1001 15:49:01.388612  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0599236 (* 1 = 0.0599236 loss)
I1001 15:49:01.388618  5332 sgd_solver.cpp:105] Iteration 52700, lr = 0.01
I1001 15:49:06.645431  5332 solver.cpp:218] Iteration 52800 (19.023 iter/s, 5.25679s/100 iters), loss = 0.0254261
I1001 15:49:06.645560  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0254268 (* 1 = 0.0254268 loss)
I1001 15:49:06.645567  5332 sgd_solver.cpp:105] Iteration 52800, lr = 0.01
I1001 15:49:11.911901  5332 solver.cpp:218] Iteration 52900 (18.9885 iter/s, 5.26634s/100 iters), loss = 0.0516128
I1001 15:49:11.911932  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0516136 (* 1 = 0.0516136 loss)
I1001 15:49:11.911939  5332 sgd_solver.cpp:105] Iteration 52900, lr = 0.01
I1001 15:49:16.908913  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:49:17.120278  5332 solver.cpp:330] Iteration 53000, Testing net (#0)
I1001 15:49:18.311466  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:49:18.361907  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9005
I1001 15:49:18.361932  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.347227 (* 1 = 0.347227 loss)
I1001 15:49:18.414535  5332 solver.cpp:218] Iteration 53000 (15.3785 iter/s, 6.50258s/100 iters), loss = 0.0142507
I1001 15:49:18.414561  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142515 (* 1 = 0.0142515 loss)
I1001 15:49:18.414568  5332 sgd_solver.cpp:105] Iteration 53000, lr = 0.01
I1001 15:49:23.671380  5332 solver.cpp:218] Iteration 53100 (19.023 iter/s, 5.2568s/100 iters), loss = 0.0659652
I1001 15:49:23.671411  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.065966 (* 1 = 0.065966 loss)
I1001 15:49:23.671416  5332 sgd_solver.cpp:105] Iteration 53100, lr = 0.01
I1001 15:49:28.921211  5332 solver.cpp:218] Iteration 53200 (19.0484 iter/s, 5.24978s/100 iters), loss = 0.0764172
I1001 15:49:28.921243  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.076418 (* 1 = 0.076418 loss)
I1001 15:49:28.921250  5332 sgd_solver.cpp:105] Iteration 53200, lr = 0.01
I1001 15:49:34.170833  5332 solver.cpp:218] Iteration 53300 (19.0492 iter/s, 5.24957s/100 iters), loss = 0.0694309
I1001 15:49:34.170872  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0694316 (* 1 = 0.0694316 loss)
I1001 15:49:34.170878  5332 sgd_solver.cpp:105] Iteration 53300, lr = 0.01
I1001 15:49:39.429770  5332 solver.cpp:218] Iteration 53400 (19.0155 iter/s, 5.25888s/100 iters), loss = 0.0140593
I1001 15:49:39.429896  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140601 (* 1 = 0.0140601 loss)
I1001 15:49:39.429914  5332 sgd_solver.cpp:105] Iteration 53400, lr = 0.01
I1001 15:49:44.429484  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:49:44.640568  5332 solver.cpp:330] Iteration 53500, Testing net (#0)
I1001 15:49:45.835009  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:49:45.885423  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8927
I1001 15:49:45.885448  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.374931 (* 1 = 0.374931 loss)
I1001 15:49:45.937722  5332 solver.cpp:218] Iteration 53500 (15.3662 iter/s, 6.50781s/100 iters), loss = 0.0434913
I1001 15:49:45.937744  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0434921 (* 1 = 0.0434921 loss)
I1001 15:49:45.937752  5332 sgd_solver.cpp:105] Iteration 53500, lr = 0.01
I1001 15:49:51.197726  5332 solver.cpp:218] Iteration 53600 (19.0116 iter/s, 5.25996s/100 iters), loss = 0.0462724
I1001 15:49:51.197757  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0462732 (* 1 = 0.0462732 loss)
I1001 15:49:51.197765  5332 sgd_solver.cpp:105] Iteration 53600, lr = 0.01
I1001 15:49:56.457312  5332 solver.cpp:218] Iteration 53700 (19.0131 iter/s, 5.25953s/100 iters), loss = 0.0493205
I1001 15:49:56.457345  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0493213 (* 1 = 0.0493213 loss)
I1001 15:49:56.457353  5332 sgd_solver.cpp:105] Iteration 53700, lr = 0.01
I1001 15:50:01.709694  5332 solver.cpp:218] Iteration 53800 (19.0392 iter/s, 5.25233s/100 iters), loss = 0.0542808
I1001 15:50:01.709733  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0542816 (* 1 = 0.0542816 loss)
I1001 15:50:01.709741  5332 sgd_solver.cpp:105] Iteration 53800, lr = 0.01
I1001 15:50:06.967794  5332 solver.cpp:218] Iteration 53900 (19.0185 iter/s, 5.25804s/100 iters), loss = 0.0408949
I1001 15:50:06.967826  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0408957 (* 1 = 0.0408957 loss)
I1001 15:50:06.967844  5332 sgd_solver.cpp:105] Iteration 53900, lr = 0.01
I1001 15:50:11.963311  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:50:12.173604  5332 solver.cpp:330] Iteration 54000, Testing net (#0)
I1001 15:50:13.364871  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:50:13.415287  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9002
I1001 15:50:13.415313  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.346369 (* 1 = 0.346369 loss)
I1001 15:50:13.467629  5332 solver.cpp:218] Iteration 54000 (15.3851 iter/s, 6.49978s/100 iters), loss = 0.0292923
I1001 15:50:13.467655  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.029293 (* 1 = 0.029293 loss)
I1001 15:50:13.467664  5332 sgd_solver.cpp:105] Iteration 54000, lr = 0.01
I1001 15:50:18.725298  5332 solver.cpp:218] Iteration 54100 (19.02 iter/s, 5.25762s/100 iters), loss = 0.0266167
I1001 15:50:18.725330  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0266174 (* 1 = 0.0266174 loss)
I1001 15:50:18.725338  5332 sgd_solver.cpp:105] Iteration 54100, lr = 0.01
I1001 15:50:23.985924  5332 solver.cpp:218] Iteration 54200 (19.0094 iter/s, 5.26057s/100 iters), loss = 0.0361223
I1001 15:50:23.985956  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0361231 (* 1 = 0.0361231 loss)
I1001 15:50:23.985965  5332 sgd_solver.cpp:105] Iteration 54200, lr = 0.01
I1001 15:50:29.250458  5332 solver.cpp:218] Iteration 54300 (18.9952 iter/s, 5.26448s/100 iters), loss = 0.012052
I1001 15:50:29.250494  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0120528 (* 1 = 0.0120528 loss)
I1001 15:50:29.250501  5332 sgd_solver.cpp:105] Iteration 54300, lr = 0.01
I1001 15:50:34.501437  5332 solver.cpp:218] Iteration 54400 (19.0443 iter/s, 5.25093s/100 iters), loss = 0.0596883
I1001 15:50:34.501471  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.059689 (* 1 = 0.059689 loss)
I1001 15:50:34.501478  5332 sgd_solver.cpp:105] Iteration 54400, lr = 0.01
I1001 15:50:39.503063  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:50:39.713855  5332 solver.cpp:330] Iteration 54500, Testing net (#0)
I1001 15:50:40.912372  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:50:40.963567  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9002
I1001 15:50:40.963593  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.348556 (* 1 = 0.348556 loss)
I1001 15:50:41.016360  5332 solver.cpp:218] Iteration 54500 (15.3495 iter/s, 6.51487s/100 iters), loss = 0.0148
I1001 15:50:41.016391  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0148007 (* 1 = 0.0148007 loss)
I1001 15:50:41.016397  5332 sgd_solver.cpp:105] Iteration 54500, lr = 0.01
I1001 15:50:46.266356  5332 solver.cpp:218] Iteration 54600 (19.0478 iter/s, 5.24994s/100 iters), loss = 0.0336846
I1001 15:50:46.266477  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0336854 (* 1 = 0.0336854 loss)
I1001 15:50:46.266499  5332 sgd_solver.cpp:105] Iteration 54600, lr = 0.01
I1001 15:50:51.527525  5332 solver.cpp:218] Iteration 54700 (19.0076 iter/s, 5.26104s/100 iters), loss = 0.0707034
I1001 15:50:51.527565  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0707042 (* 1 = 0.0707042 loss)
I1001 15:50:51.527570  5332 sgd_solver.cpp:105] Iteration 54700, lr = 0.01
I1001 15:50:56.788573  5332 solver.cpp:218] Iteration 54800 (19.0078 iter/s, 5.26099s/100 iters), loss = 0.0352276
I1001 15:50:56.788602  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0352284 (* 1 = 0.0352284 loss)
I1001 15:50:56.788625  5332 sgd_solver.cpp:105] Iteration 54800, lr = 0.01
I1001 15:51:02.045819  5332 solver.cpp:218] Iteration 54900 (19.0216 iter/s, 5.25719s/100 iters), loss = 0.0455701
I1001 15:51:02.045852  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0455709 (* 1 = 0.0455709 loss)
I1001 15:51:02.045869  5332 sgd_solver.cpp:105] Iteration 54900, lr = 0.01
I1001 15:51:07.034325  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:51:07.244953  5332 solver.cpp:330] Iteration 55000, Testing net (#0)
I1001 15:51:08.445536  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:51:08.495864  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9017
I1001 15:51:08.495899  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.349082 (* 1 = 0.349082 loss)
I1001 15:51:08.548951  5332 solver.cpp:218] Iteration 55000 (15.3773 iter/s, 6.50308s/100 iters), loss = 0.048108
I1001 15:51:08.548976  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0481088 (* 1 = 0.0481088 loss)
I1001 15:51:08.548984  5332 sgd_solver.cpp:105] Iteration 55000, lr = 0.01
I1001 15:51:13.805096  5332 solver.cpp:218] Iteration 55100 (19.0255 iter/s, 5.25609s/100 iters), loss = 0.0531264
I1001 15:51:13.805145  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0531272 (* 1 = 0.0531272 loss)
I1001 15:51:13.805155  5332 sgd_solver.cpp:105] Iteration 55100, lr = 0.01
I1001 15:51:19.059777  5332 solver.cpp:218] Iteration 55200 (19.0309 iter/s, 5.25461s/100 iters), loss = 0.0330065
I1001 15:51:19.059907  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0330073 (* 1 = 0.0330073 loss)
I1001 15:51:19.059914  5332 sgd_solver.cpp:105] Iteration 55200, lr = 0.01
I1001 15:51:24.324100  5332 solver.cpp:218] Iteration 55300 (18.9963 iter/s, 5.26418s/100 iters), loss = 0.0685743
I1001 15:51:24.324128  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0685751 (* 1 = 0.0685751 loss)
I1001 15:51:24.324133  5332 sgd_solver.cpp:105] Iteration 55300, lr = 0.01
I1001 15:51:29.589879  5332 solver.cpp:218] Iteration 55400 (18.9907 iter/s, 5.26573s/100 iters), loss = 0.0443137
I1001 15:51:29.589908  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0443146 (* 1 = 0.0443146 loss)
I1001 15:51:29.589915  5332 sgd_solver.cpp:105] Iteration 55400, lr = 0.01
I1001 15:51:34.582676  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:51:34.798192  5332 solver.cpp:330] Iteration 55500, Testing net (#0)
I1001 15:51:35.997107  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:51:36.047360  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8993
I1001 15:51:36.047394  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.347376 (* 1 = 0.347376 loss)
I1001 15:51:36.099903  5332 solver.cpp:218] Iteration 55500 (15.361 iter/s, 6.50997s/100 iters), loss = 0.0328791
I1001 15:51:36.099931  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0328799 (* 1 = 0.0328799 loss)
I1001 15:51:36.099936  5332 sgd_solver.cpp:105] Iteration 55500, lr = 0.01
I1001 15:51:41.359537  5332 solver.cpp:218] Iteration 55600 (19.0129 iter/s, 5.25958s/100 iters), loss = 0.0157688
I1001 15:51:41.359578  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0157697 (* 1 = 0.0157697 loss)
I1001 15:51:41.359585  5332 sgd_solver.cpp:105] Iteration 55600, lr = 0.01
I1001 15:51:46.612870  5332 solver.cpp:218] Iteration 55700 (19.0358 iter/s, 5.25327s/100 iters), loss = 0.0481754
I1001 15:51:46.612913  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0481762 (* 1 = 0.0481762 loss)
I1001 15:51:46.612920  5332 sgd_solver.cpp:105] Iteration 55700, lr = 0.01
I1001 15:51:51.868113  5332 solver.cpp:218] Iteration 55800 (19.0288 iter/s, 5.25518s/100 iters), loss = 0.033408
I1001 15:51:51.868271  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0334089 (* 1 = 0.0334089 loss)
I1001 15:51:51.868278  5332 sgd_solver.cpp:105] Iteration 55800, lr = 0.01
I1001 15:51:57.126600  5332 solver.cpp:218] Iteration 55900 (19.0175 iter/s, 5.25832s/100 iters), loss = 0.0426903
I1001 15:51:57.126628  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0426911 (* 1 = 0.0426911 loss)
I1001 15:51:57.126634  5332 sgd_solver.cpp:105] Iteration 55900, lr = 0.01
I1001 15:52:02.126807  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:52:02.336740  5332 solver.cpp:330] Iteration 56000, Testing net (#0)
I1001 15:52:03.528888  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:52:03.579378  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8916
I1001 15:52:03.579402  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.37738 (* 1 = 0.37738 loss)
I1001 15:52:03.632082  5332 solver.cpp:218] Iteration 56000 (15.3718 iter/s, 6.50543s/100 iters), loss = 0.063863
I1001 15:52:03.632107  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0638638 (* 1 = 0.0638638 loss)
I1001 15:52:03.632113  5332 sgd_solver.cpp:105] Iteration 56000, lr = 0.01
I1001 15:52:08.896955  5332 solver.cpp:218] Iteration 56100 (18.994 iter/s, 5.26483s/100 iters), loss = 0.0324635
I1001 15:52:08.896982  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0324643 (* 1 = 0.0324643 loss)
I1001 15:52:08.896998  5332 sgd_solver.cpp:105] Iteration 56100, lr = 0.01
I1001 15:52:14.160055  5332 solver.cpp:218] Iteration 56200 (19.0004 iter/s, 5.26305s/100 iters), loss = 0.0315201
I1001 15:52:14.160087  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0315209 (* 1 = 0.0315209 loss)
I1001 15:52:14.160096  5332 sgd_solver.cpp:105] Iteration 56200, lr = 0.01
I1001 15:52:19.413851  5332 solver.cpp:218] Iteration 56300 (19.0341 iter/s, 5.25374s/100 iters), loss = 0.0615577
I1001 15:52:19.413882  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0615586 (* 1 = 0.0615586 loss)
I1001 15:52:19.413890  5332 sgd_solver.cpp:105] Iteration 56300, lr = 0.01
I1001 15:52:24.672967  5332 solver.cpp:218] Iteration 56400 (19.0148 iter/s, 5.25906s/100 iters), loss = 0.0102423
I1001 15:52:24.673100  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0102431 (* 1 = 0.0102431 loss)
I1001 15:52:24.673126  5332 sgd_solver.cpp:105] Iteration 56400, lr = 0.01
I1001 15:52:29.673882  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:52:29.884650  5332 solver.cpp:330] Iteration 56500, Testing net (#0)
I1001 15:52:31.076905  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:52:31.127326  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8968
I1001 15:52:31.127362  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.37693 (* 1 = 0.37693 loss)
I1001 15:52:31.180157  5332 solver.cpp:218] Iteration 56500 (15.368 iter/s, 6.50704s/100 iters), loss = 0.0201444
I1001 15:52:31.180182  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0201452 (* 1 = 0.0201452 loss)
I1001 15:52:31.180188  5332 sgd_solver.cpp:105] Iteration 56500, lr = 0.01
I1001 15:52:36.438868  5332 solver.cpp:218] Iteration 56600 (19.0162 iter/s, 5.25867s/100 iters), loss = 0.0627234
I1001 15:52:36.438897  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0627242 (* 1 = 0.0627242 loss)
I1001 15:52:36.438904  5332 sgd_solver.cpp:105] Iteration 56600, lr = 0.01
I1001 15:52:41.699126  5332 solver.cpp:218] Iteration 56700 (19.0107 iter/s, 5.26021s/100 iters), loss = 0.0643893
I1001 15:52:41.699157  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0643901 (* 1 = 0.0643901 loss)
I1001 15:52:41.699165  5332 sgd_solver.cpp:105] Iteration 56700, lr = 0.01
I1001 15:52:46.962169  5332 solver.cpp:218] Iteration 56800 (19.0006 iter/s, 5.26299s/100 iters), loss = 0.0929697
I1001 15:52:46.962205  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0929706 (* 1 = 0.0929706 loss)
I1001 15:52:46.962215  5332 sgd_solver.cpp:105] Iteration 56800, lr = 0.01
I1001 15:52:52.222343  5332 solver.cpp:218] Iteration 56900 (19.011 iter/s, 5.26012s/100 iters), loss = 0.00796108
I1001 15:52:52.222373  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00796195 (* 1 = 0.00796195 loss)
I1001 15:52:52.222381  5332 sgd_solver.cpp:105] Iteration 56900, lr = 0.01
I1001 15:52:57.227888  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:52:57.439121  5332 solver.cpp:330] Iteration 57000, Testing net (#0)
I1001 15:52:58.633057  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:52:58.683841  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8997
I1001 15:52:58.683877  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.354346 (* 1 = 0.354346 loss)
I1001 15:52:58.738296  5332 solver.cpp:218] Iteration 57000 (15.3471 iter/s, 6.5159s/100 iters), loss = 0.0603019
I1001 15:52:58.738332  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0603028 (* 1 = 0.0603028 loss)
I1001 15:52:58.738338  5332 sgd_solver.cpp:105] Iteration 57000, lr = 0.01
I1001 15:53:04.000402  5332 solver.cpp:218] Iteration 57100 (19.004 iter/s, 5.26205s/100 iters), loss = 0.0324025
I1001 15:53:04.000432  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0324034 (* 1 = 0.0324034 loss)
I1001 15:53:04.000437  5332 sgd_solver.cpp:105] Iteration 57100, lr = 0.01
I1001 15:53:09.245055  5332 solver.cpp:218] Iteration 57200 (19.0672 iter/s, 5.2446s/100 iters), loss = 0.0133458
I1001 15:53:09.245085  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0133467 (* 1 = 0.0133467 loss)
I1001 15:53:09.245091  5332 sgd_solver.cpp:105] Iteration 57200, lr = 0.01
I1001 15:53:14.509965  5332 solver.cpp:218] Iteration 57300 (18.9939 iter/s, 5.26485s/100 iters), loss = 0.0282413
I1001 15:53:14.509994  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0282422 (* 1 = 0.0282422 loss)
I1001 15:53:14.510000  5332 sgd_solver.cpp:105] Iteration 57300, lr = 0.01
I1001 15:53:19.764345  5332 solver.cpp:218] Iteration 57400 (19.0319 iter/s, 5.25433s/100 iters), loss = 0.0313526
I1001 15:53:19.764389  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313534 (* 1 = 0.0313534 loss)
I1001 15:53:19.764397  5332 sgd_solver.cpp:105] Iteration 57400, lr = 0.01
I1001 15:53:24.757580  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:53:24.968616  5332 solver.cpp:330] Iteration 57500, Testing net (#0)
I1001 15:53:26.169970  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:53:26.220434  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9025
I1001 15:53:26.220475  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.352788 (* 1 = 0.352788 loss)
I1001 15:53:26.273360  5332 solver.cpp:218] Iteration 57500 (15.3635 iter/s, 6.50895s/100 iters), loss = 0.0243996
I1001 15:53:26.273389  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0244004 (* 1 = 0.0244004 loss)
I1001 15:53:26.273396  5332 sgd_solver.cpp:105] Iteration 57500, lr = 0.01
I1001 15:53:31.523341  5332 solver.cpp:218] Iteration 57600 (19.0479 iter/s, 5.24993s/100 iters), loss = 0.0252467
I1001 15:53:31.523452  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0252476 (* 1 = 0.0252476 loss)
I1001 15:53:31.523468  5332 sgd_solver.cpp:105] Iteration 57600, lr = 0.01
I1001 15:53:36.780627  5332 solver.cpp:218] Iteration 57700 (19.0216 iter/s, 5.25717s/100 iters), loss = 0.0448542
I1001 15:53:36.780666  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.044855 (* 1 = 0.044855 loss)
I1001 15:53:36.780673  5332 sgd_solver.cpp:105] Iteration 57700, lr = 0.01
I1001 15:53:42.044942  5332 solver.cpp:218] Iteration 57800 (18.996 iter/s, 5.26425s/100 iters), loss = 0.0197971
I1001 15:53:42.044986  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.019798 (* 1 = 0.019798 loss)
I1001 15:53:42.044992  5332 sgd_solver.cpp:105] Iteration 57800, lr = 0.01
I1001 15:53:47.308425  5332 solver.cpp:218] Iteration 57900 (18.999 iter/s, 5.26342s/100 iters), loss = 0.0251002
I1001 15:53:47.308466  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.025101 (* 1 = 0.025101 loss)
I1001 15:53:47.308472  5332 sgd_solver.cpp:105] Iteration 57900, lr = 0.01
I1001 15:53:52.301584  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:53:52.512562  5332 solver.cpp:330] Iteration 58000, Testing net (#0)
I1001 15:53:53.714217  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:53:53.764678  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897
I1001 15:53:53.764713  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.374603 (* 1 = 0.374603 loss)
I1001 15:53:53.817268  5332 solver.cpp:218] Iteration 58000 (15.3639 iter/s, 6.50878s/100 iters), loss = 0.038467
I1001 15:53:53.817291  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0384679 (* 1 = 0.0384679 loss)
I1001 15:53:53.817298  5332 sgd_solver.cpp:105] Iteration 58000, lr = 0.01
I1001 15:53:59.081874  5332 solver.cpp:218] Iteration 58100 (18.9949 iter/s, 5.26456s/100 iters), loss = 0.0400431
I1001 15:53:59.081915  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.040044 (* 1 = 0.040044 loss)
I1001 15:53:59.081923  5332 sgd_solver.cpp:105] Iteration 58100, lr = 0.01
I1001 15:54:04.335300  5332 solver.cpp:218] Iteration 58200 (19.0354 iter/s, 5.25336s/100 iters), loss = 0.055891
I1001 15:54:04.335482  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0558919 (* 1 = 0.0558919 loss)
I1001 15:54:04.335502  5332 sgd_solver.cpp:105] Iteration 58200, lr = 0.01
I1001 15:54:09.589181  5332 solver.cpp:218] Iteration 58300 (19.0342 iter/s, 5.2537s/100 iters), loss = 0.0249929
I1001 15:54:09.589221  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0249937 (* 1 = 0.0249937 loss)
I1001 15:54:09.589227  5332 sgd_solver.cpp:105] Iteration 58300, lr = 0.01
I1001 15:54:14.846448  5332 solver.cpp:218] Iteration 58400 (19.0215 iter/s, 5.25721s/100 iters), loss = 0.00744838
I1001 15:54:14.846477  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00744924 (* 1 = 0.00744924 loss)
I1001 15:54:14.846483  5332 sgd_solver.cpp:105] Iteration 58400, lr = 0.01
I1001 15:54:19.837390  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:54:20.051108  5332 solver.cpp:330] Iteration 58500, Testing net (#0)
I1001 15:54:21.244381  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:54:21.295102  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8957
I1001 15:54:21.295136  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.388849 (* 1 = 0.388849 loss)
I1001 15:54:21.347918  5332 solver.cpp:218] Iteration 58500 (15.3813 iter/s, 6.50142s/100 iters), loss = 0.0361218
I1001 15:54:21.347944  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0361226 (* 1 = 0.0361226 loss)
I1001 15:54:21.347950  5332 sgd_solver.cpp:105] Iteration 58500, lr = 0.01
I1001 15:54:26.616191  5332 solver.cpp:218] Iteration 58600 (18.9817 iter/s, 5.26822s/100 iters), loss = 0.0375345
I1001 15:54:26.616225  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0375353 (* 1 = 0.0375353 loss)
I1001 15:54:26.616230  5332 sgd_solver.cpp:105] Iteration 58600, lr = 0.01
I1001 15:54:31.878412  5332 solver.cpp:218] Iteration 58700 (19.0036 iter/s, 5.26216s/100 iters), loss = 0.0180285
I1001 15:54:31.878446  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0180293 (* 1 = 0.0180293 loss)
I1001 15:54:31.878453  5332 sgd_solver.cpp:105] Iteration 58700, lr = 0.01
I1001 15:54:37.136253  5332 solver.cpp:218] Iteration 58800 (19.0194 iter/s, 5.25779s/100 iters), loss = 0.053172
I1001 15:54:37.136374  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0531729 (* 1 = 0.0531729 loss)
I1001 15:54:37.136382  5332 sgd_solver.cpp:105] Iteration 58800, lr = 0.01
I1001 15:54:42.403185  5332 solver.cpp:218] Iteration 58900 (18.9869 iter/s, 5.2668s/100 iters), loss = 0.0157194
I1001 15:54:42.403214  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0157203 (* 1 = 0.0157203 loss)
I1001 15:54:42.403220  5332 sgd_solver.cpp:105] Iteration 58900, lr = 0.01
I1001 15:54:47.403621  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:54:47.613943  5332 solver.cpp:330] Iteration 59000, Testing net (#0)
I1001 15:54:48.807085  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:54:48.856745  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.902
I1001 15:54:48.856770  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.347541 (* 1 = 0.347541 loss)
I1001 15:54:48.909337  5332 solver.cpp:218] Iteration 59000 (15.3702 iter/s, 6.5061s/100 iters), loss = 0.0384571
I1001 15:54:48.909363  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.038458 (* 1 = 0.038458 loss)
I1001 15:54:48.909369  5332 sgd_solver.cpp:105] Iteration 59000, lr = 0.01
I1001 15:54:54.169817  5332 solver.cpp:218] Iteration 59100 (19.0098 iter/s, 5.26044s/100 iters), loss = 0.040768
I1001 15:54:54.169847  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0407689 (* 1 = 0.0407689 loss)
I1001 15:54:54.169852  5332 sgd_solver.cpp:105] Iteration 59100, lr = 0.01
I1001 15:54:59.430389  5332 solver.cpp:218] Iteration 59200 (19.0095 iter/s, 5.26052s/100 iters), loss = 0.014093
I1001 15:54:59.430418  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140939 (* 1 = 0.0140939 loss)
I1001 15:54:59.430424  5332 sgd_solver.cpp:105] Iteration 59200, lr = 0.01
I1001 15:55:04.681838  5332 solver.cpp:218] Iteration 59300 (19.0426 iter/s, 5.2514s/100 iters), loss = 0.0208235
I1001 15:55:04.681870  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0208244 (* 1 = 0.0208244 loss)
I1001 15:55:04.681876  5332 sgd_solver.cpp:105] Iteration 59300, lr = 0.01
I1001 15:55:09.941380  5332 solver.cpp:218] Iteration 59400 (19.0133 iter/s, 5.25949s/100 iters), loss = 0.00855356
I1001 15:55:09.941535  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00855445 (* 1 = 0.00855445 loss)
I1001 15:55:09.941541  5332 sgd_solver.cpp:105] Iteration 59400, lr = 0.01
I1001 15:55:14.938385  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:55:15.148445  5332 solver.cpp:330] Iteration 59500, Testing net (#0)
I1001 15:55:16.340731  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:55:16.390700  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8951
I1001 15:55:16.390734  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.392497 (* 1 = 0.392497 loss)
I1001 15:55:16.443591  5332 solver.cpp:218] Iteration 59500 (15.3798 iter/s, 6.50203s/100 iters), loss = 0.0284578
I1001 15:55:16.443616  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0284587 (* 1 = 0.0284587 loss)
I1001 15:55:16.443624  5332 sgd_solver.cpp:105] Iteration 59500, lr = 0.01
I1001 15:55:21.714084  5332 solver.cpp:218] Iteration 59600 (18.9737 iter/s, 5.27045s/100 iters), loss = 0.0599011
I1001 15:55:21.714123  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.059902 (* 1 = 0.059902 loss)
I1001 15:55:21.714129  5332 sgd_solver.cpp:105] Iteration 59600, lr = 0.01
I1001 15:55:26.985275  5332 solver.cpp:218] Iteration 59700 (18.9713 iter/s, 5.27113s/100 iters), loss = 0.0218742
I1001 15:55:26.985312  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0218751 (* 1 = 0.0218751 loss)
I1001 15:55:26.985318  5332 sgd_solver.cpp:105] Iteration 59700, lr = 0.01
I1001 15:55:32.256875  5332 solver.cpp:218] Iteration 59800 (18.9698 iter/s, 5.27154s/100 iters), loss = 0.0364259
I1001 15:55:32.256916  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0364268 (* 1 = 0.0364268 loss)
I1001 15:55:32.256922  5332 sgd_solver.cpp:105] Iteration 59800, lr = 0.01
I1001 15:55:37.515969  5332 solver.cpp:218] Iteration 59900 (19.0149 iter/s, 5.25903s/100 iters), loss = 0.0405993
I1001 15:55:37.516010  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0406002 (* 1 = 0.0406002 loss)
I1001 15:55:37.516016  5332 sgd_solver.cpp:105] Iteration 59900, lr = 0.01
I1001 15:55:42.513073  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:55:42.723223  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_60000.caffemodel
I1001 15:55:42.728112  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_60000.solverstate
I1001 15:55:42.729498  5332 solver.cpp:330] Iteration 60000, Testing net (#0)
I1001 15:55:43.927693  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:55:43.979285  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.902
I1001 15:55:43.979321  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.373251 (* 1 = 0.373251 loss)
I1001 15:55:44.032186  5332 solver.cpp:218] Iteration 60000 (15.3465 iter/s, 6.51616s/100 iters), loss = 0.0156129
I1001 15:55:44.032224  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0156138 (* 1 = 0.0156138 loss)
I1001 15:55:44.032232  5332 sgd_solver.cpp:105] Iteration 60000, lr = 0.01
I1001 15:55:49.288151  5332 solver.cpp:218] Iteration 60100 (19.0262 iter/s, 5.25591s/100 iters), loss = 0.0214695
I1001 15:55:49.288182  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0214705 (* 1 = 0.0214705 loss)
I1001 15:55:49.288187  5332 sgd_solver.cpp:105] Iteration 60100, lr = 0.01
I1001 15:55:54.550741  5332 solver.cpp:218] Iteration 60200 (19.0022 iter/s, 5.26254s/100 iters), loss = 0.054789
I1001 15:55:54.550771  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0547899 (* 1 = 0.0547899 loss)
I1001 15:55:54.550787  5332 sgd_solver.cpp:105] Iteration 60200, lr = 0.01
I1001 15:55:59.816892  5332 solver.cpp:218] Iteration 60300 (18.9894 iter/s, 5.2661s/100 iters), loss = 0.0184227
I1001 15:55:59.816921  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0184236 (* 1 = 0.0184236 loss)
I1001 15:55:59.816926  5332 sgd_solver.cpp:105] Iteration 60300, lr = 0.01
I1001 15:56:05.085320  5332 solver.cpp:218] Iteration 60400 (18.9812 iter/s, 5.26838s/100 iters), loss = 0.0433845
I1001 15:56:05.085351  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0433854 (* 1 = 0.0433854 loss)
I1001 15:56:05.085367  5332 sgd_solver.cpp:105] Iteration 60400, lr = 0.01
I1001 15:56:10.082258  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:56:10.292949  5332 solver.cpp:330] Iteration 60500, Testing net (#0)
I1001 15:56:11.498286  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:56:11.547780  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8979
I1001 15:56:11.547814  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.376661 (* 1 = 0.376661 loss)
I1001 15:56:11.600564  5332 solver.cpp:218] Iteration 60500 (15.3487 iter/s, 6.51519s/100 iters), loss = 0.039439
I1001 15:56:11.600589  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0394399 (* 1 = 0.0394399 loss)
I1001 15:56:11.600596  5332 sgd_solver.cpp:105] Iteration 60500, lr = 0.01
I1001 15:56:16.865584  5332 solver.cpp:218] Iteration 60600 (18.9935 iter/s, 5.26497s/100 iters), loss = 0.0268698
I1001 15:56:16.865725  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0268707 (* 1 = 0.0268707 loss)
I1001 15:56:16.865732  5332 sgd_solver.cpp:105] Iteration 60600, lr = 0.01
I1001 15:56:22.120059  5332 solver.cpp:218] Iteration 60700 (19.032 iter/s, 5.25432s/100 iters), loss = 0.0517981
I1001 15:56:22.120087  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.051799 (* 1 = 0.051799 loss)
I1001 15:56:22.120093  5332 sgd_solver.cpp:105] Iteration 60700, lr = 0.01
I1001 15:56:27.377964  5332 solver.cpp:218] Iteration 60800 (19.0192 iter/s, 5.25786s/100 iters), loss = 0.0239131
I1001 15:56:27.377992  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.023914 (* 1 = 0.023914 loss)
I1001 15:56:27.377998  5332 sgd_solver.cpp:105] Iteration 60800, lr = 0.01
I1001 15:56:32.638671  5332 solver.cpp:218] Iteration 60900 (19.009 iter/s, 5.26066s/100 iters), loss = 0.0174329
I1001 15:56:32.638703  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0174338 (* 1 = 0.0174338 loss)
I1001 15:56:32.638710  5332 sgd_solver.cpp:105] Iteration 60900, lr = 0.01
I1001 15:56:37.628005  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:56:37.842365  5332 solver.cpp:330] Iteration 61000, Testing net (#0)
I1001 15:56:39.039324  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:56:39.089470  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.895
I1001 15:56:39.089504  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.423715 (* 1 = 0.423715 loss)
I1001 15:56:39.142084  5332 solver.cpp:218] Iteration 61000 (15.3767 iter/s, 6.50336s/100 iters), loss = 0.0376485
I1001 15:56:39.142112  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0376494 (* 1 = 0.0376494 loss)
I1001 15:56:39.142119  5332 sgd_solver.cpp:105] Iteration 61000, lr = 0.01
I1001 15:56:44.397091  5332 solver.cpp:218] Iteration 61100 (19.0296 iter/s, 5.25496s/100 iters), loss = 0.0304744
I1001 15:56:44.397132  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0304753 (* 1 = 0.0304753 loss)
I1001 15:56:44.397138  5332 sgd_solver.cpp:105] Iteration 61100, lr = 0.01
I1001 15:56:49.650811  5332 solver.cpp:218] Iteration 61200 (19.0343 iter/s, 5.25366s/100 iters), loss = 0.0288886
I1001 15:56:49.650979  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0288895 (* 1 = 0.0288895 loss)
I1001 15:56:49.651001  5332 sgd_solver.cpp:105] Iteration 61200, lr = 0.01
I1001 15:56:54.918826  5332 solver.cpp:218] Iteration 61300 (18.9831 iter/s, 5.26783s/100 iters), loss = 0.0140271
I1001 15:56:54.918865  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140281 (* 1 = 0.0140281 loss)
I1001 15:56:54.918871  5332 sgd_solver.cpp:105] Iteration 61300, lr = 0.01
I1001 15:57:00.188522  5332 solver.cpp:218] Iteration 61400 (18.9767 iter/s, 5.26963s/100 iters), loss = 0.0169413
I1001 15:57:00.188560  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0169422 (* 1 = 0.0169422 loss)
I1001 15:57:00.188567  5332 sgd_solver.cpp:105] Iteration 61400, lr = 0.01
I1001 15:57:05.187705  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:57:05.398322  5332 solver.cpp:330] Iteration 61500, Testing net (#0)
I1001 15:57:06.590827  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:57:06.640908  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9009
I1001 15:57:06.640931  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.37833 (* 1 = 0.37833 loss)
I1001 15:57:06.693692  5332 solver.cpp:218] Iteration 61500 (15.3725 iter/s, 6.50511s/100 iters), loss = 0.0164182
I1001 15:57:06.693714  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0164191 (* 1 = 0.0164191 loss)
I1001 15:57:06.693722  5332 sgd_solver.cpp:105] Iteration 61500, lr = 0.01
I1001 15:57:11.955644  5332 solver.cpp:218] Iteration 61600 (19.0045 iter/s, 5.2619s/100 iters), loss = 0.0248448
I1001 15:57:11.955673  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0248458 (* 1 = 0.0248458 loss)
I1001 15:57:11.955680  5332 sgd_solver.cpp:105] Iteration 61600, lr = 0.01
I1001 15:57:17.214642  5332 solver.cpp:218] Iteration 61700 (19.0152 iter/s, 5.25895s/100 iters), loss = 0.0297256
I1001 15:57:17.214670  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0297266 (* 1 = 0.0297266 loss)
I1001 15:57:17.214678  5332 sgd_solver.cpp:105] Iteration 61700, lr = 0.01
I1001 15:57:22.466564  5332 solver.cpp:218] Iteration 61800 (19.0408 iter/s, 5.25187s/100 iters), loss = 0.022897
I1001 15:57:22.466727  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0228979 (* 1 = 0.0228979 loss)
I1001 15:57:22.466753  5332 sgd_solver.cpp:105] Iteration 61800, lr = 0.01
I1001 15:57:27.724344  5332 solver.cpp:218] Iteration 61900 (19.0201 iter/s, 5.25761s/100 iters), loss = 0.00470121
I1001 15:57:27.724383  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00470214 (* 1 = 0.00470214 loss)
I1001 15:57:27.724390  5332 sgd_solver.cpp:105] Iteration 61900, lr = 0.01
I1001 15:57:32.721155  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:57:32.931252  5332 solver.cpp:330] Iteration 62000, Testing net (#0)
I1001 15:57:34.122241  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:57:34.172559  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898
I1001 15:57:34.172585  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.399753 (* 1 = 0.399753 loss)
I1001 15:57:34.225256  5332 solver.cpp:218] Iteration 62000 (15.3826 iter/s, 6.50085s/100 iters), loss = 0.0141451
I1001 15:57:34.225281  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.014146 (* 1 = 0.014146 loss)
I1001 15:57:34.225288  5332 sgd_solver.cpp:105] Iteration 62000, lr = 0.01
I1001 15:57:39.491226  5332 solver.cpp:218] Iteration 62100 (18.99 iter/s, 5.26592s/100 iters), loss = 0.0181527
I1001 15:57:39.491256  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0181536 (* 1 = 0.0181536 loss)
I1001 15:57:39.491261  5332 sgd_solver.cpp:105] Iteration 62100, lr = 0.01
I1001 15:57:44.754746  5332 solver.cpp:218] Iteration 62200 (18.9989 iter/s, 5.26347s/100 iters), loss = 0.0544142
I1001 15:57:44.754791  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0544151 (* 1 = 0.0544151 loss)
I1001 15:57:44.754799  5332 sgd_solver.cpp:105] Iteration 62200, lr = 0.01
I1001 15:57:50.017820  5332 solver.cpp:218] Iteration 62300 (19.0005 iter/s, 5.26301s/100 iters), loss = 0.0226226
I1001 15:57:50.017865  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0226235 (* 1 = 0.0226235 loss)
I1001 15:57:50.017874  5332 sgd_solver.cpp:105] Iteration 62300, lr = 0.01
I1001 15:57:55.273653  5332 solver.cpp:218] Iteration 62400 (19.0267 iter/s, 5.25577s/100 iters), loss = 0.015194
I1001 15:57:55.273782  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0151949 (* 1 = 0.0151949 loss)
I1001 15:57:55.273789  5332 sgd_solver.cpp:105] Iteration 62400, lr = 0.01
I1001 15:58:00.278869  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:58:00.489679  5332 solver.cpp:330] Iteration 62500, Testing net (#0)
I1001 15:58:01.682513  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:58:01.733749  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9023
I1001 15:58:01.733775  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.362877 (* 1 = 0.362877 loss)
I1001 15:58:01.787856  5332 solver.cpp:218] Iteration 62500 (15.3514 iter/s, 6.51405s/100 iters), loss = 0.0141229
I1001 15:58:01.787904  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0141238 (* 1 = 0.0141238 loss)
I1001 15:58:01.787912  5332 sgd_solver.cpp:105] Iteration 62500, lr = 0.01
I1001 15:58:07.044688  5332 solver.cpp:218] Iteration 62600 (19.0233 iter/s, 5.25672s/100 iters), loss = 0.0555785
I1001 15:58:07.044718  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0555793 (* 1 = 0.0555793 loss)
I1001 15:58:07.044734  5332 sgd_solver.cpp:105] Iteration 62600, lr = 0.01
I1001 15:58:12.303338  5332 solver.cpp:218] Iteration 62700 (19.0165 iter/s, 5.2586s/100 iters), loss = 0.0112413
I1001 15:58:12.303367  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112422 (* 1 = 0.0112422 loss)
I1001 15:58:12.303382  5332 sgd_solver.cpp:105] Iteration 62700, lr = 0.01
I1001 15:58:17.561375  5332 solver.cpp:218] Iteration 62800 (19.0187 iter/s, 5.25798s/100 iters), loss = 0.0446532
I1001 15:58:17.561409  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446541 (* 1 = 0.0446541 loss)
I1001 15:58:17.561429  5332 sgd_solver.cpp:105] Iteration 62800, lr = 0.01
I1001 15:58:22.814868  5332 solver.cpp:218] Iteration 62900 (19.0352 iter/s, 5.25344s/100 iters), loss = 0.0282594
I1001 15:58:22.814906  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0282604 (* 1 = 0.0282604 loss)
I1001 15:58:22.814925  5332 sgd_solver.cpp:105] Iteration 62900, lr = 0.01
I1001 15:58:27.811525  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:58:28.021847  5332 solver.cpp:330] Iteration 63000, Testing net (#0)
I1001 15:58:29.221139  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:58:29.271355  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.889
I1001 15:58:29.271380  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417861 (* 1 = 0.417861 loss)
I1001 15:58:29.323801  5332 solver.cpp:218] Iteration 63000 (15.3636 iter/s, 6.50887s/100 iters), loss = 0.029737
I1001 15:58:29.323843  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.029738 (* 1 = 0.029738 loss)
I1001 15:58:29.323849  5332 sgd_solver.cpp:105] Iteration 63000, lr = 0.01
I1001 15:58:34.581655  5332 solver.cpp:218] Iteration 63100 (19.0194 iter/s, 5.25779s/100 iters), loss = 0.0323918
I1001 15:58:34.581696  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0323928 (* 1 = 0.0323928 loss)
I1001 15:58:34.581701  5332 sgd_solver.cpp:105] Iteration 63100, lr = 0.01
I1001 15:58:39.844017  5332 solver.cpp:218] Iteration 63200 (19.0031 iter/s, 5.2623s/100 iters), loss = 0.0446966
I1001 15:58:39.844056  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446975 (* 1 = 0.0446975 loss)
I1001 15:58:39.844063  5332 sgd_solver.cpp:105] Iteration 63200, lr = 0.01
I1001 15:58:45.106909  5332 solver.cpp:218] Iteration 63300 (19.0012 iter/s, 5.26283s/100 iters), loss = 0.0056166
I1001 15:58:45.106937  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00561754 (* 1 = 0.00561754 loss)
I1001 15:58:45.106943  5332 sgd_solver.cpp:105] Iteration 63300, lr = 0.01
I1001 15:58:50.372799  5332 solver.cpp:218] Iteration 63400 (18.9903 iter/s, 5.26584s/100 iters), loss = 0.0611786
I1001 15:58:50.372838  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0611796 (* 1 = 0.0611796 loss)
I1001 15:58:50.372845  5332 sgd_solver.cpp:105] Iteration 63400, lr = 0.01
I1001 15:58:55.361647  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:58:55.571626  5332 solver.cpp:330] Iteration 63500, Testing net (#0)
I1001 15:58:56.771781  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:58:56.822003  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8944
I1001 15:58:56.822034  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385575 (* 1 = 0.385575 loss)
I1001 15:58:56.874716  5332 solver.cpp:218] Iteration 63500 (15.3802 iter/s, 6.50186s/100 iters), loss = 0.00939231
I1001 15:58:56.874745  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00939327 (* 1 = 0.00939327 loss)
I1001 15:58:56.874752  5332 sgd_solver.cpp:105] Iteration 63500, lr = 0.01
I1001 15:59:02.138430  5332 solver.cpp:218] Iteration 63600 (18.9982 iter/s, 5.26366s/100 iters), loss = 0.0313946
I1001 15:59:02.138584  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313956 (* 1 = 0.0313956 loss)
I1001 15:59:02.138593  5332 sgd_solver.cpp:105] Iteration 63600, lr = 0.01
I1001 15:59:07.388828  5332 solver.cpp:218] Iteration 63700 (19.0468 iter/s, 5.25023s/100 iters), loss = 0.050091
I1001 15:59:07.388857  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.050092 (* 1 = 0.050092 loss)
I1001 15:59:07.388864  5332 sgd_solver.cpp:105] Iteration 63700, lr = 0.01
I1001 15:59:12.646852  5332 solver.cpp:218] Iteration 63800 (19.0187 iter/s, 5.25797s/100 iters), loss = 0.0243652
I1001 15:59:12.646883  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0243661 (* 1 = 0.0243661 loss)
I1001 15:59:12.646899  5332 sgd_solver.cpp:105] Iteration 63800, lr = 0.01
I1001 15:59:17.906779  5332 solver.cpp:218] Iteration 63900 (19.0119 iter/s, 5.25988s/100 iters), loss = 0.00793311
I1001 15:59:17.906807  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00793407 (* 1 = 0.00793407 loss)
I1001 15:59:17.906813  5332 sgd_solver.cpp:105] Iteration 63900, lr = 0.01
I1001 15:59:22.903275  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:59:23.117002  5332 solver.cpp:330] Iteration 64000, Testing net (#0)
I1001 15:59:24.307339  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:59:24.357544  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8979
I1001 15:59:24.357580  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3879 (* 1 = 0.3879 loss)
I1001 15:59:24.410049  5332 solver.cpp:218] Iteration 64000 (15.377 iter/s, 6.50322s/100 iters), loss = 0.0262892
I1001 15:59:24.410076  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0262902 (* 1 = 0.0262902 loss)
I1001 15:59:24.410082  5332 sgd_solver.cpp:105] Iteration 64000, lr = 0.01
I1001 15:59:29.679144  5332 solver.cpp:218] Iteration 64100 (18.9788 iter/s, 5.26904s/100 iters), loss = 0.0462923
I1001 15:59:29.679179  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0462932 (* 1 = 0.0462932 loss)
I1001 15:59:29.679186  5332 sgd_solver.cpp:105] Iteration 64100, lr = 0.01
I1001 15:59:34.932674  5332 solver.cpp:218] Iteration 64200 (19.035 iter/s, 5.25347s/100 iters), loss = 0.0345025
I1001 15:59:34.932818  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0345034 (* 1 = 0.0345034 loss)
I1001 15:59:34.932827  5332 sgd_solver.cpp:105] Iteration 64200, lr = 0.01
I1001 15:59:40.185286  5332 solver.cpp:218] Iteration 64300 (19.0387 iter/s, 5.25246s/100 iters), loss = 0.0284651
I1001 15:59:40.185322  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.028466 (* 1 = 0.028466 loss)
I1001 15:59:40.185328  5332 sgd_solver.cpp:105] Iteration 64300, lr = 0.01
I1001 15:59:45.452674  5332 solver.cpp:218] Iteration 64400 (18.9849 iter/s, 5.26733s/100 iters), loss = 0.0267873
I1001 15:59:45.452714  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0267882 (* 1 = 0.0267882 loss)
I1001 15:59:45.452720  5332 sgd_solver.cpp:105] Iteration 64400, lr = 0.01
I1001 15:59:50.448225  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:59:50.658452  5332 solver.cpp:330] Iteration 64500, Testing net (#0)
I1001 15:59:51.850538  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 15:59:51.900970  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9009
I1001 15:59:51.900996  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.366941 (* 1 = 0.366941 loss)
I1001 15:59:51.953626  5332 solver.cpp:218] Iteration 64500 (15.3825 iter/s, 6.50089s/100 iters), loss = 0.00669342
I1001 15:59:51.953651  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0066944 (* 1 = 0.0066944 loss)
I1001 15:59:51.953657  5332 sgd_solver.cpp:105] Iteration 64500, lr = 0.01
I1001 15:59:57.213421  5332 solver.cpp:218] Iteration 64600 (19.0123 iter/s, 5.25975s/100 iters), loss = 0.0441682
I1001 15:59:57.213450  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0441692 (* 1 = 0.0441692 loss)
I1001 15:59:57.213456  5332 sgd_solver.cpp:105] Iteration 64600, lr = 0.01
I1001 16:00:02.476902  5332 solver.cpp:218] Iteration 64700 (18.999 iter/s, 5.26343s/100 iters), loss = 0.0282468
I1001 16:00:02.476941  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0282477 (* 1 = 0.0282477 loss)
I1001 16:00:02.476948  5332 sgd_solver.cpp:105] Iteration 64700, lr = 0.01
I1001 16:00:07.732019  5332 solver.cpp:218] Iteration 64800 (19.0293 iter/s, 5.25504s/100 iters), loss = 0.03852
I1001 16:00:07.732158  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.038521 (* 1 = 0.038521 loss)
I1001 16:00:07.732167  5332 sgd_solver.cpp:105] Iteration 64800, lr = 0.01
I1001 16:00:12.995210  5332 solver.cpp:218] Iteration 64900 (19.0005 iter/s, 5.26301s/100 iters), loss = 0.0432592
I1001 16:00:12.995240  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0432602 (* 1 = 0.0432602 loss)
I1001 16:00:12.995255  5332 sgd_solver.cpp:105] Iteration 64900, lr = 0.01
I1001 16:00:18.000294  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:00:18.210979  5332 solver.cpp:330] Iteration 65000, Testing net (#0)
I1001 16:00:19.401475  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:00:19.451865  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.893
I1001 16:00:19.451900  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.418347 (* 1 = 0.418347 loss)
I1001 16:00:19.504489  5332 solver.cpp:218] Iteration 65000 (15.3628 iter/s, 6.50923s/100 iters), loss = 0.034798
I1001 16:00:19.504513  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.034799 (* 1 = 0.034799 loss)
I1001 16:00:19.504519  5332 sgd_solver.cpp:105] Iteration 65000, lr = 0.01
I1001 16:00:24.768035  5332 solver.cpp:218] Iteration 65100 (18.9988 iter/s, 5.2635s/100 iters), loss = 0.0135527
I1001 16:00:24.768074  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0135537 (* 1 = 0.0135537 loss)
I1001 16:00:24.768080  5332 sgd_solver.cpp:105] Iteration 65100, lr = 0.01
I1001 16:00:30.024276  5332 solver.cpp:218] Iteration 65200 (19.0252 iter/s, 5.25618s/100 iters), loss = 0.0339974
I1001 16:00:30.024317  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0339984 (* 1 = 0.0339984 loss)
I1001 16:00:30.024322  5332 sgd_solver.cpp:105] Iteration 65200, lr = 0.01
I1001 16:00:35.285040  5332 solver.cpp:218] Iteration 65300 (19.0089 iter/s, 5.2607s/100 iters), loss = 0.015232
I1001 16:00:35.285070  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0152329 (* 1 = 0.0152329 loss)
I1001 16:00:35.285076  5332 sgd_solver.cpp:105] Iteration 65300, lr = 0.01
I1001 16:00:40.538702  5332 solver.cpp:218] Iteration 65400 (19.0345 iter/s, 5.25361s/100 iters), loss = 0.012394
I1001 16:00:40.538872  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.012395 (* 1 = 0.012395 loss)
I1001 16:00:40.538882  5332 sgd_solver.cpp:105] Iteration 65400, lr = 0.01
I1001 16:00:45.532214  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:00:45.743309  5332 solver.cpp:330] Iteration 65500, Testing net (#0)
I1001 16:00:46.940294  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:00:46.990738  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8928
I1001 16:00:46.990773  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.405928 (* 1 = 0.405928 loss)
I1001 16:00:47.044215  5332 solver.cpp:218] Iteration 65500 (15.372 iter/s, 6.50532s/100 iters), loss = 0.0210305
I1001 16:00:47.044246  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0210315 (* 1 = 0.0210315 loss)
I1001 16:00:47.044253  5332 sgd_solver.cpp:105] Iteration 65500, lr = 0.01
I1001 16:00:52.298084  5332 solver.cpp:218] Iteration 65600 (19.0338 iter/s, 5.25381s/100 iters), loss = 0.0110043
I1001 16:00:52.298115  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110052 (* 1 = 0.0110052 loss)
I1001 16:00:52.298122  5332 sgd_solver.cpp:105] Iteration 65600, lr = 0.01
I1001 16:00:57.563262  5332 solver.cpp:218] Iteration 65700 (18.9929 iter/s, 5.26513s/100 iters), loss = 0.0379509
I1001 16:00:57.563302  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0379519 (* 1 = 0.0379519 loss)
I1001 16:00:57.563308  5332 sgd_solver.cpp:105] Iteration 65700, lr = 0.01
I1001 16:01:02.825624  5332 solver.cpp:218] Iteration 65800 (19.0031 iter/s, 5.2623s/100 iters), loss = 0.0484891
I1001 16:01:02.825657  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.04849 (* 1 = 0.04849 loss)
I1001 16:01:02.825666  5332 sgd_solver.cpp:105] Iteration 65800, lr = 0.01
I1001 16:01:08.093628  5332 solver.cpp:218] Iteration 65900 (18.9827 iter/s, 5.26795s/100 iters), loss = 0.00609324
I1001 16:01:08.093662  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00609421 (* 1 = 0.00609421 loss)
I1001 16:01:08.093672  5332 sgd_solver.cpp:105] Iteration 65900, lr = 0.01
I1001 16:01:13.082705  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:01:13.293052  5332 solver.cpp:330] Iteration 66000, Testing net (#0)
I1001 16:01:14.491292  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:01:14.541499  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8889
I1001 16:01:14.541534  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.410425 (* 1 = 0.410425 loss)
I1001 16:01:14.594715  5332 solver.cpp:218] Iteration 66000 (15.3822 iter/s, 6.50104s/100 iters), loss = 0.058414
I1001 16:01:14.594739  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.058415 (* 1 = 0.058415 loss)
I1001 16:01:14.594746  5332 sgd_solver.cpp:105] Iteration 66000, lr = 0.01
I1001 16:01:19.847357  5332 solver.cpp:218] Iteration 66100 (19.0382 iter/s, 5.25259s/100 iters), loss = 0.0249873
I1001 16:01:19.847390  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0249883 (* 1 = 0.0249883 loss)
I1001 16:01:19.847398  5332 sgd_solver.cpp:105] Iteration 66100, lr = 0.01
I1001 16:01:25.098512  5332 solver.cpp:218] Iteration 66200 (19.0436 iter/s, 5.2511s/100 iters), loss = 0.0266588
I1001 16:01:25.098554  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0266598 (* 1 = 0.0266598 loss)
I1001 16:01:25.098561  5332 sgd_solver.cpp:105] Iteration 66200, lr = 0.01
I1001 16:01:30.358031  5332 solver.cpp:218] Iteration 66300 (19.0134 iter/s, 5.25946s/100 iters), loss = 0.0505487
I1001 16:01:30.358072  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0505496 (* 1 = 0.0505496 loss)
I1001 16:01:30.358078  5332 sgd_solver.cpp:105] Iteration 66300, lr = 0.01
I1001 16:01:35.616194  5332 solver.cpp:218] Iteration 66400 (19.0183 iter/s, 5.2581s/100 iters), loss = 0.0108982
I1001 16:01:35.616235  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0108991 (* 1 = 0.0108991 loss)
I1001 16:01:35.616241  5332 sgd_solver.cpp:105] Iteration 66400, lr = 0.01
I1001 16:01:40.603499  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:01:40.817416  5332 solver.cpp:330] Iteration 66500, Testing net (#0)
I1001 16:01:42.015535  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:01:42.065950  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8992
I1001 16:01:42.065985  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.388803 (* 1 = 0.388803 loss)
I1001 16:01:42.118755  5332 solver.cpp:218] Iteration 66500 (15.3787 iter/s, 6.5025s/100 iters), loss = 0.0620402
I1001 16:01:42.118778  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0620411 (* 1 = 0.0620411 loss)
I1001 16:01:42.118785  5332 sgd_solver.cpp:105] Iteration 66500, lr = 0.01
I1001 16:01:47.383879  5332 solver.cpp:218] Iteration 66600 (18.9931 iter/s, 5.26508s/100 iters), loss = 0.0169765
I1001 16:01:47.383981  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0169774 (* 1 = 0.0169774 loss)
I1001 16:01:47.383988  5332 sgd_solver.cpp:105] Iteration 66600, lr = 0.01
I1001 16:01:52.634258  5332 solver.cpp:218] Iteration 66700 (19.0467 iter/s, 5.25026s/100 iters), loss = 0.0223876
I1001 16:01:52.634287  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0223886 (* 1 = 0.0223886 loss)
I1001 16:01:52.634294  5332 sgd_solver.cpp:105] Iteration 66700, lr = 0.01
I1001 16:01:57.897657  5332 solver.cpp:218] Iteration 66800 (18.9993 iter/s, 5.26335s/100 iters), loss = 0.0299942
I1001 16:01:57.897697  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0299951 (* 1 = 0.0299951 loss)
I1001 16:01:57.897703  5332 sgd_solver.cpp:105] Iteration 66800, lr = 0.01
I1001 16:02:03.160832  5332 solver.cpp:218] Iteration 66900 (19.0002 iter/s, 5.26311s/100 iters), loss = 0.00676164
I1001 16:02:03.160873  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00676254 (* 1 = 0.00676254 loss)
I1001 16:02:03.160879  5332 sgd_solver.cpp:105] Iteration 66900, lr = 0.01
I1001 16:02:08.159709  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:02:08.370177  5332 solver.cpp:330] Iteration 67000, Testing net (#0)
I1001 16:02:09.561012  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:02:09.611227  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8882
I1001 16:02:09.611249  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.433437 (* 1 = 0.433437 loss)
I1001 16:02:09.664039  5332 solver.cpp:218] Iteration 67000 (15.3772 iter/s, 6.50315s/100 iters), loss = 0.0218204
I1001 16:02:09.664072  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0218213 (* 1 = 0.0218213 loss)
I1001 16:02:09.664078  5332 sgd_solver.cpp:105] Iteration 67000, lr = 0.01
I1001 16:02:14.928433  5332 solver.cpp:218] Iteration 67100 (18.9957 iter/s, 5.26434s/100 iters), loss = 0.0170293
I1001 16:02:14.928465  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0170302 (* 1 = 0.0170302 loss)
I1001 16:02:14.928474  5332 sgd_solver.cpp:105] Iteration 67100, lr = 0.01
I1001 16:02:20.186908  5332 solver.cpp:218] Iteration 67200 (19.0171 iter/s, 5.25842s/100 iters), loss = 0.0129266
I1001 16:02:20.187062  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129276 (* 1 = 0.0129276 loss)
I1001 16:02:20.187072  5332 sgd_solver.cpp:105] Iteration 67200, lr = 0.01
I1001 16:02:25.440160  5332 solver.cpp:218] Iteration 67300 (19.0364 iter/s, 5.25309s/100 iters), loss = 0.0189953
I1001 16:02:25.440201  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0189962 (* 1 = 0.0189962 loss)
I1001 16:02:25.440207  5332 sgd_solver.cpp:105] Iteration 67300, lr = 0.01
I1001 16:02:30.703743  5332 solver.cpp:218] Iteration 67400 (18.9987 iter/s, 5.26352s/100 iters), loss = 0.0468233
I1001 16:02:30.703784  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0468243 (* 1 = 0.0468243 loss)
I1001 16:02:30.703789  5332 sgd_solver.cpp:105] Iteration 67400, lr = 0.01
I1001 16:02:35.701750  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:02:35.911870  5332 solver.cpp:330] Iteration 67500, Testing net (#0)
I1001 16:02:37.104779  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:02:37.155289  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8875
I1001 16:02:37.155324  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.455441 (* 1 = 0.455441 loss)
I1001 16:02:37.208118  5332 solver.cpp:218] Iteration 67500 (15.3744 iter/s, 6.50431s/100 iters), loss = 0.00981055
I1001 16:02:37.208143  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00981149 (* 1 = 0.00981149 loss)
I1001 16:02:37.208150  5332 sgd_solver.cpp:105] Iteration 67500, lr = 0.01
I1001 16:02:42.474830  5332 solver.cpp:218] Iteration 67600 (18.9874 iter/s, 5.26666s/100 iters), loss = 0.0307809
I1001 16:02:42.474870  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0307818 (* 1 = 0.0307818 loss)
I1001 16:02:42.474876  5332 sgd_solver.cpp:105] Iteration 67600, lr = 0.01
I1001 16:02:47.741426  5332 solver.cpp:218] Iteration 67700 (18.9878 iter/s, 5.26653s/100 iters), loss = 0.0333378
I1001 16:02:47.741466  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0333388 (* 1 = 0.0333388 loss)
I1001 16:02:47.741472  5332 sgd_solver.cpp:105] Iteration 67700, lr = 0.01
I1001 16:02:52.999752  5332 solver.cpp:218] Iteration 67800 (19.0177 iter/s, 5.25826s/100 iters), loss = 0.0147303
I1001 16:02:52.999891  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0147312 (* 1 = 0.0147312 loss)
I1001 16:02:52.999900  5332 sgd_solver.cpp:105] Iteration 67800, lr = 0.01
I1001 16:02:58.257238  5332 solver.cpp:218] Iteration 67900 (19.0211 iter/s, 5.25733s/100 iters), loss = 0.0284143
I1001 16:02:58.257268  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0284152 (* 1 = 0.0284152 loss)
I1001 16:02:58.257274  5332 sgd_solver.cpp:105] Iteration 67900, lr = 0.01
I1001 16:03:03.255383  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:03:03.465473  5332 solver.cpp:330] Iteration 68000, Testing net (#0)
I1001 16:03:04.659253  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:03:04.709561  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8879
I1001 16:03:04.709588  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.461335 (* 1 = 0.461335 loss)
I1001 16:03:04.763217  5332 solver.cpp:218] Iteration 68000 (15.3706 iter/s, 6.50593s/100 iters), loss = 0.0328108
I1001 16:03:04.763262  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0328117 (* 1 = 0.0328117 loss)
I1001 16:03:04.763269  5332 sgd_solver.cpp:105] Iteration 68000, lr = 0.01
I1001 16:03:10.027285  5332 solver.cpp:218] Iteration 68100 (18.9971 iter/s, 5.26397s/100 iters), loss = 0.019088
I1001 16:03:10.027314  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.019089 (* 1 = 0.019089 loss)
I1001 16:03:10.027319  5332 sgd_solver.cpp:105] Iteration 68100, lr = 0.01
I1001 16:03:15.292933  5332 solver.cpp:218] Iteration 68200 (18.9912 iter/s, 5.2656s/100 iters), loss = 0.0241662
I1001 16:03:15.292965  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0241672 (* 1 = 0.0241672 loss)
I1001 16:03:15.292971  5332 sgd_solver.cpp:105] Iteration 68200, lr = 0.01
I1001 16:03:20.560842  5332 solver.cpp:218] Iteration 68300 (18.9831 iter/s, 5.26786s/100 iters), loss = 0.0228098
I1001 16:03:20.560870  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0228108 (* 1 = 0.0228108 loss)
I1001 16:03:20.560876  5332 sgd_solver.cpp:105] Iteration 68300, lr = 0.01
I1001 16:03:25.820399  5332 solver.cpp:218] Iteration 68400 (19.0132 iter/s, 5.2595s/100 iters), loss = 0.0306148
I1001 16:03:25.820520  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0306158 (* 1 = 0.0306158 loss)
I1001 16:03:25.820539  5332 sgd_solver.cpp:105] Iteration 68400, lr = 0.01
I1001 16:03:30.821677  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:03:31.031592  5332 solver.cpp:330] Iteration 68500, Testing net (#0)
I1001 16:03:32.231017  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:03:32.281250  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8931
I1001 16:03:32.281286  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.425051 (* 1 = 0.425051 loss)
I1001 16:03:32.334079  5332 solver.cpp:218] Iteration 68500 (15.3527 iter/s, 6.5135s/100 iters), loss = 0.0121811
I1001 16:03:32.334120  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.012182 (* 1 = 0.012182 loss)
I1001 16:03:32.334127  5332 sgd_solver.cpp:105] Iteration 68500, lr = 0.01
I1001 16:03:37.583504  5332 solver.cpp:218] Iteration 68600 (19.0499 iter/s, 5.24937s/100 iters), loss = 0.0269821
I1001 16:03:37.583534  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.026983 (* 1 = 0.026983 loss)
I1001 16:03:37.583539  5332 sgd_solver.cpp:105] Iteration 68600, lr = 0.01
I1001 16:03:42.840847  5332 solver.cpp:218] Iteration 68700 (19.0212 iter/s, 5.25729s/100 iters), loss = 0.043791
I1001 16:03:42.840886  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.043792 (* 1 = 0.043792 loss)
I1001 16:03:42.840891  5332 sgd_solver.cpp:105] Iteration 68700, lr = 0.01
I1001 16:03:48.098729  5332 solver.cpp:218] Iteration 68800 (19.0193 iter/s, 5.25782s/100 iters), loss = 0.0092938
I1001 16:03:48.098759  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00929478 (* 1 = 0.00929478 loss)
I1001 16:03:48.098775  5332 sgd_solver.cpp:105] Iteration 68800, lr = 0.01
I1001 16:03:53.359405  5332 solver.cpp:218] Iteration 68900 (19.0091 iter/s, 5.26062s/100 iters), loss = 0.0125127
I1001 16:03:53.359434  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125137 (* 1 = 0.0125137 loss)
I1001 16:03:53.359441  5332 sgd_solver.cpp:105] Iteration 68900, lr = 0.01
I1001 16:03:58.347710  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:03:58.558971  5332 solver.cpp:330] Iteration 69000, Testing net (#0)
I1001 16:03:59.758857  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:03:59.809123  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8992
I1001 16:03:59.809156  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.398305 (* 1 = 0.398305 loss)
I1001 16:03:59.861821  5332 solver.cpp:218] Iteration 69000 (15.379 iter/s, 6.50236s/100 iters), loss = 0.0477309
I1001 16:03:59.861846  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0477319 (* 1 = 0.0477319 loss)
I1001 16:03:59.861852  5332 sgd_solver.cpp:105] Iteration 69000, lr = 0.01
I1001 16:04:05.124438  5332 solver.cpp:218] Iteration 69100 (19.0021 iter/s, 5.26257s/100 iters), loss = 0.0476435
I1001 16:04:05.124467  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0476444 (* 1 = 0.0476444 loss)
I1001 16:04:05.124474  5332 sgd_solver.cpp:105] Iteration 69100, lr = 0.01
I1001 16:04:10.377604  5332 solver.cpp:218] Iteration 69200 (19.0364 iter/s, 5.25311s/100 iters), loss = 0.0693722
I1001 16:04:10.377634  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0693732 (* 1 = 0.0693732 loss)
I1001 16:04:10.377650  5332 sgd_solver.cpp:105] Iteration 69200, lr = 0.01
I1001 16:04:15.640004  5332 solver.cpp:218] Iteration 69300 (19.0029 iter/s, 5.26235s/100 iters), loss = 0.058047
I1001 16:04:15.640034  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.058048 (* 1 = 0.058048 loss)
I1001 16:04:15.640049  5332 sgd_solver.cpp:105] Iteration 69300, lr = 0.01
I1001 16:04:20.902436  5332 solver.cpp:218] Iteration 69400 (19.0028 iter/s, 5.26238s/100 iters), loss = 0.0211158
I1001 16:04:20.902477  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0211168 (* 1 = 0.0211168 loss)
I1001 16:04:20.902483  5332 sgd_solver.cpp:105] Iteration 69400, lr = 0.01
I1001 16:04:25.901103  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:04:26.114346  5332 solver.cpp:330] Iteration 69500, Testing net (#0)
I1001 16:04:27.307585  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:04:27.357939  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8906
I1001 16:04:27.357962  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.434033 (* 1 = 0.434033 loss)
I1001 16:04:27.410567  5332 solver.cpp:218] Iteration 69500 (15.3655 iter/s, 6.50807s/100 iters), loss = 0.0456216
I1001 16:04:27.410593  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0456225 (* 1 = 0.0456225 loss)
I1001 16:04:27.410598  5332 sgd_solver.cpp:105] Iteration 69500, lr = 0.01
I1001 16:04:32.670513  5332 solver.cpp:218] Iteration 69600 (19.0118 iter/s, 5.2599s/100 iters), loss = 0.0247248
I1001 16:04:32.670641  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0247258 (* 1 = 0.0247258 loss)
I1001 16:04:32.670648  5332 sgd_solver.cpp:105] Iteration 69600, lr = 0.01
I1001 16:04:37.925099  5332 solver.cpp:218] Iteration 69700 (19.0315 iter/s, 5.25444s/100 iters), loss = 0.0196163
I1001 16:04:37.925159  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0196173 (* 1 = 0.0196173 loss)
I1001 16:04:37.925168  5332 sgd_solver.cpp:105] Iteration 69700, lr = 0.01
I1001 16:04:43.181586  5332 solver.cpp:218] Iteration 69800 (19.0246 iter/s, 5.25637s/100 iters), loss = 0.0315296
I1001 16:04:43.181624  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0315306 (* 1 = 0.0315306 loss)
I1001 16:04:43.181630  5332 sgd_solver.cpp:105] Iteration 69800, lr = 0.01
I1001 16:04:48.439337  5332 solver.cpp:218] Iteration 69900 (19.0197 iter/s, 5.25769s/100 iters), loss = 0.0395612
I1001 16:04:48.439375  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0395622 (* 1 = 0.0395622 loss)
I1001 16:04:48.439381  5332 sgd_solver.cpp:105] Iteration 69900, lr = 0.01
I1001 16:04:53.441964  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:04:53.652510  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_70000.caffemodel
I1001 16:04:53.657660  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_70000.solverstate
I1001 16:04:53.659080  5332 solver.cpp:330] Iteration 70000, Testing net (#0)
I1001 16:04:54.849700  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:04:54.900480  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8978
I1001 16:04:54.900504  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.403637 (* 1 = 0.403637 loss)
I1001 16:04:54.953155  5332 solver.cpp:218] Iteration 70000 (15.3521 iter/s, 6.51376s/100 iters), loss = 0.0386978
I1001 16:04:54.953181  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0386988 (* 1 = 0.0386988 loss)
I1001 16:04:54.953188  5332 sgd_solver.cpp:105] Iteration 70000, lr = 0.01
I1001 16:05:00.219288  5332 solver.cpp:218] Iteration 70100 (18.9894 iter/s, 5.26609s/100 iters), loss = 0.00731898
I1001 16:05:00.219329  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00731996 (* 1 = 0.00731996 loss)
I1001 16:05:00.219336  5332 sgd_solver.cpp:105] Iteration 70100, lr = 0.01
I1001 16:05:05.483916  5332 solver.cpp:218] Iteration 70200 (18.9949 iter/s, 5.26457s/100 iters), loss = 0.0250234
I1001 16:05:05.484097  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250244 (* 1 = 0.0250244 loss)
I1001 16:05:05.484107  5332 sgd_solver.cpp:105] Iteration 70200, lr = 0.01
I1001 16:05:10.739372  5332 solver.cpp:218] Iteration 70300 (19.0285 iter/s, 5.25527s/100 iters), loss = 0.0503048
I1001 16:05:10.739413  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0503059 (* 1 = 0.0503059 loss)
I1001 16:05:10.739418  5332 sgd_solver.cpp:105] Iteration 70300, lr = 0.01
I1001 16:05:16.003214  5332 solver.cpp:218] Iteration 70400 (18.9977 iter/s, 5.26378s/100 iters), loss = 0.0146091
I1001 16:05:16.003253  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0146101 (* 1 = 0.0146101 loss)
I1001 16:05:16.003259  5332 sgd_solver.cpp:105] Iteration 70400, lr = 0.01
I1001 16:05:21.000023  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:05:21.210875  5332 solver.cpp:330] Iteration 70500, Testing net (#0)
I1001 16:05:22.405495  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:05:22.455703  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.899
I1001 16:05:22.455739  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.381455 (* 1 = 0.381455 loss)
I1001 16:05:22.508257  5332 solver.cpp:218] Iteration 70500 (15.3728 iter/s, 6.50499s/100 iters), loss = 0.0261039
I1001 16:05:22.508283  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0261049 (* 1 = 0.0261049 loss)
I1001 16:05:22.508291  5332 sgd_solver.cpp:105] Iteration 70500, lr = 0.01
I1001 16:05:27.772337  5332 solver.cpp:218] Iteration 70600 (18.9968 iter/s, 5.26404s/100 iters), loss = 0.0176797
I1001 16:05:27.772377  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0176807 (* 1 = 0.0176807 loss)
I1001 16:05:27.772383  5332 sgd_solver.cpp:105] Iteration 70600, lr = 0.01
I1001 16:05:33.034299  5332 solver.cpp:218] Iteration 70700 (19.0045 iter/s, 5.26191s/100 iters), loss = 0.0251485
I1001 16:05:33.034338  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0251495 (* 1 = 0.0251495 loss)
I1001 16:05:33.034344  5332 sgd_solver.cpp:105] Iteration 70700, lr = 0.01
I1001 16:05:38.296875  5332 solver.cpp:218] Iteration 70800 (19.0023 iter/s, 5.26252s/100 iters), loss = 0.0159752
I1001 16:05:38.297014  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159762 (* 1 = 0.0159762 loss)
I1001 16:05:38.297024  5332 sgd_solver.cpp:105] Iteration 70800, lr = 0.01
I1001 16:05:43.554329  5332 solver.cpp:218] Iteration 70900 (19.0212 iter/s, 5.2573s/100 iters), loss = 0.0088128
I1001 16:05:43.554368  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00881381 (* 1 = 0.00881381 loss)
I1001 16:05:43.554383  5332 sgd_solver.cpp:105] Iteration 70900, lr = 0.01
I1001 16:05:48.555629  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:05:48.765725  5332 solver.cpp:330] Iteration 71000, Testing net (#0)
I1001 16:05:49.962911  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:05:50.013972  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.891
I1001 16:05:50.013998  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.434318 (* 1 = 0.434318 loss)
I1001 16:05:50.067939  5332 solver.cpp:218] Iteration 71000 (15.3526 iter/s, 6.51355s/100 iters), loss = 0.004461
I1001 16:05:50.067981  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00446201 (* 1 = 0.00446201 loss)
I1001 16:05:50.067989  5332 sgd_solver.cpp:105] Iteration 71000, lr = 0.01
I1001 16:05:55.322721  5332 solver.cpp:218] Iteration 71100 (19.0305 iter/s, 5.25472s/100 iters), loss = 0.0809061
I1001 16:05:55.322762  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0809071 (* 1 = 0.0809071 loss)
I1001 16:05:55.322767  5332 sgd_solver.cpp:105] Iteration 71100, lr = 0.01
I1001 16:06:00.584765  5332 solver.cpp:218] Iteration 71200 (19.0042 iter/s, 5.26199s/100 iters), loss = 0.0273508
I1001 16:06:00.584803  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0273518 (* 1 = 0.0273518 loss)
I1001 16:06:00.584810  5332 sgd_solver.cpp:105] Iteration 71200, lr = 0.01
I1001 16:06:05.844127  5332 solver.cpp:218] Iteration 71300 (19.0139 iter/s, 5.25931s/100 iters), loss = 0.0233438
I1001 16:06:05.844168  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0233448 (* 1 = 0.0233448 loss)
I1001 16:06:05.844175  5332 sgd_solver.cpp:105] Iteration 71300, lr = 0.01
I1001 16:06:11.103925  5332 solver.cpp:218] Iteration 71400 (19.0124 iter/s, 5.25973s/100 iters), loss = 0.0170897
I1001 16:06:11.104096  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0170907 (* 1 = 0.0170907 loss)
I1001 16:06:11.104106  5332 sgd_solver.cpp:105] Iteration 71400, lr = 0.01
I1001 16:06:16.088986  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:06:16.299022  5332 solver.cpp:330] Iteration 71500, Testing net (#0)
I1001 16:06:17.498755  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:06:17.549259  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8981
I1001 16:06:17.549284  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.400437 (* 1 = 0.400437 loss)
I1001 16:06:17.602020  5332 solver.cpp:218] Iteration 71500 (15.3896 iter/s, 6.49791s/100 iters), loss = 0.0056251
I1001 16:06:17.602046  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0056261 (* 1 = 0.0056261 loss)
I1001 16:06:17.602052  5332 sgd_solver.cpp:105] Iteration 71500, lr = 0.01
I1001 16:06:22.854789  5332 solver.cpp:218] Iteration 71600 (19.0378 iter/s, 5.25272s/100 iters), loss = 0.0218747
I1001 16:06:22.854835  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0218757 (* 1 = 0.0218757 loss)
I1001 16:06:22.854852  5332 sgd_solver.cpp:105] Iteration 71600, lr = 0.01
I1001 16:06:28.112076  5332 solver.cpp:218] Iteration 71700 (19.0216 iter/s, 5.25719s/100 iters), loss = 0.0216349
I1001 16:06:28.112104  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216359 (* 1 = 0.0216359 loss)
I1001 16:06:28.112110  5332 sgd_solver.cpp:105] Iteration 71700, lr = 0.01
I1001 16:06:33.372730  5332 solver.cpp:218] Iteration 71800 (19.0092 iter/s, 5.2606s/100 iters), loss = 0.0127773
I1001 16:06:33.372759  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0127783 (* 1 = 0.0127783 loss)
I1001 16:06:33.372766  5332 sgd_solver.cpp:105] Iteration 71800, lr = 0.01
I1001 16:06:38.633952  5332 solver.cpp:218] Iteration 71900 (19.0072 iter/s, 5.26117s/100 iters), loss = 0.0178192
I1001 16:06:38.633981  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0178202 (* 1 = 0.0178202 loss)
I1001 16:06:38.633987  5332 sgd_solver.cpp:105] Iteration 71900, lr = 0.01
I1001 16:06:43.623121  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:06:43.835976  5332 solver.cpp:330] Iteration 72000, Testing net (#0)
I1001 16:06:45.033332  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:06:45.083802  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8987
I1001 16:06:45.083837  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.387973 (* 1 = 0.387973 loss)
I1001 16:06:45.136360  5332 solver.cpp:218] Iteration 72000 (15.379 iter/s, 6.50236s/100 iters), loss = 0.0185169
I1001 16:06:45.136384  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0185179 (* 1 = 0.0185179 loss)
I1001 16:06:45.136391  5332 sgd_solver.cpp:105] Iteration 72000, lr = 0.01
I1001 16:06:50.402770  5332 solver.cpp:218] Iteration 72100 (18.9884 iter/s, 5.26636s/100 iters), loss = 0.00868096
I1001 16:06:50.402801  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00868195 (* 1 = 0.00868195 loss)
I1001 16:06:50.402806  5332 sgd_solver.cpp:105] Iteration 72100, lr = 0.01
I1001 16:06:55.657250  5332 solver.cpp:218] Iteration 72200 (19.0316 iter/s, 5.25443s/100 iters), loss = 0.0118003
I1001 16:06:55.657289  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0118013 (* 1 = 0.0118013 loss)
I1001 16:06:55.657295  5332 sgd_solver.cpp:105] Iteration 72200, lr = 0.01
I1001 16:07:00.916891  5332 solver.cpp:218] Iteration 72300 (19.0129 iter/s, 5.25958s/100 iters), loss = 0.0359829
I1001 16:07:00.916919  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0359839 (* 1 = 0.0359839 loss)
I1001 16:07:00.916925  5332 sgd_solver.cpp:105] Iteration 72300, lr = 0.01
I1001 16:07:06.180516  5332 solver.cpp:218] Iteration 72400 (18.9985 iter/s, 5.26357s/100 iters), loss = 0.0129655
I1001 16:07:06.180546  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129665 (* 1 = 0.0129665 loss)
I1001 16:07:06.180552  5332 sgd_solver.cpp:105] Iteration 72400, lr = 0.01
I1001 16:07:11.175485  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:07:11.384788  5332 solver.cpp:330] Iteration 72500, Testing net (#0)
I1001 16:07:12.579809  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:07:12.629992  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8876
I1001 16:07:12.630028  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.467603 (* 1 = 0.467603 loss)
I1001 16:07:12.682610  5332 solver.cpp:218] Iteration 72500 (15.3798 iter/s, 6.50205s/100 iters), loss = 0.0507867
I1001 16:07:12.682633  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0507878 (* 1 = 0.0507878 loss)
I1001 16:07:12.682639  5332 sgd_solver.cpp:105] Iteration 72500, lr = 0.01
I1001 16:07:17.941802  5332 solver.cpp:218] Iteration 72600 (19.0145 iter/s, 5.25915s/100 iters), loss = 0.0600283
I1001 16:07:17.941900  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0600293 (* 1 = 0.0600293 loss)
I1001 16:07:17.941920  5332 sgd_solver.cpp:105] Iteration 72600, lr = 0.01
I1001 16:07:23.204824  5332 solver.cpp:218] Iteration 72700 (19.0009 iter/s, 5.2629s/100 iters), loss = 0.0641245
I1001 16:07:23.204854  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0641255 (* 1 = 0.0641255 loss)
I1001 16:07:23.204859  5332 sgd_solver.cpp:105] Iteration 72700, lr = 0.01
I1001 16:07:28.461251  5332 solver.cpp:218] Iteration 72800 (19.0245 iter/s, 5.25637s/100 iters), loss = 0.00899384
I1001 16:07:28.461280  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00899487 (* 1 = 0.00899487 loss)
I1001 16:07:28.461297  5332 sgd_solver.cpp:105] Iteration 72800, lr = 0.01
I1001 16:07:33.729656  5332 solver.cpp:218] Iteration 72900 (18.9813 iter/s, 5.26835s/100 iters), loss = 0.0332686
I1001 16:07:33.729686  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0332696 (* 1 = 0.0332696 loss)
I1001 16:07:33.729691  5332 sgd_solver.cpp:105] Iteration 72900, lr = 0.01
I1001 16:07:38.730126  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:07:38.940949  5332 solver.cpp:330] Iteration 73000, Testing net (#0)
I1001 16:07:40.133522  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:07:40.184132  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9001
I1001 16:07:40.184166  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.396138 (* 1 = 0.396138 loss)
I1001 16:07:40.236804  5332 solver.cpp:218] Iteration 73000 (15.3678 iter/s, 6.50709s/100 iters), loss = 0.0250798
I1001 16:07:40.236846  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250809 (* 1 = 0.0250809 loss)
I1001 16:07:40.236853  5332 sgd_solver.cpp:105] Iteration 73000, lr = 0.01
I1001 16:07:45.494137  5332 solver.cpp:218] Iteration 73100 (19.0213 iter/s, 5.25727s/100 iters), loss = 0.00617167
I1001 16:07:45.494166  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0061727 (* 1 = 0.0061727 loss)
I1001 16:07:45.494171  5332 sgd_solver.cpp:105] Iteration 73100, lr = 0.01
I1001 16:07:50.748246  5332 solver.cpp:218] Iteration 73200 (19.0329 iter/s, 5.25405s/100 iters), loss = 0.00585733
I1001 16:07:50.748374  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00585835 (* 1 = 0.00585835 loss)
I1001 16:07:50.748392  5332 sgd_solver.cpp:105] Iteration 73200, lr = 0.01
I1001 16:07:56.005391  5332 solver.cpp:218] Iteration 73300 (19.0222 iter/s, 5.257s/100 iters), loss = 0.0182952
I1001 16:07:56.005429  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0182962 (* 1 = 0.0182962 loss)
I1001 16:07:56.005448  5332 sgd_solver.cpp:105] Iteration 73300, lr = 0.01
I1001 16:08:01.258595  5332 solver.cpp:218] Iteration 73400 (19.0363 iter/s, 5.25311s/100 iters), loss = 0.0435438
I1001 16:08:01.258631  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0435448 (* 1 = 0.0435448 loss)
I1001 16:08:01.258638  5332 sgd_solver.cpp:105] Iteration 73400, lr = 0.01
I1001 16:08:06.257390  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:08:06.467718  5332 solver.cpp:330] Iteration 73500, Testing net (#0)
I1001 16:08:07.661589  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:08:07.712141  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8997
I1001 16:08:07.712165  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38096 (* 1 = 0.38096 loss)
I1001 16:08:07.764262  5332 solver.cpp:218] Iteration 73500 (15.3713 iter/s, 6.50561s/100 iters), loss = 0.0106498
I1001 16:08:07.764300  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0106508 (* 1 = 0.0106508 loss)
I1001 16:08:07.764307  5332 sgd_solver.cpp:105] Iteration 73500, lr = 0.01
I1001 16:08:13.024757  5332 solver.cpp:218] Iteration 73600 (19.0098 iter/s, 5.26043s/100 iters), loss = 0.0363396
I1001 16:08:13.024797  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0363406 (* 1 = 0.0363406 loss)
I1001 16:08:13.024804  5332 sgd_solver.cpp:105] Iteration 73600, lr = 0.01
I1001 16:08:18.285459  5332 solver.cpp:218] Iteration 73700 (19.0091 iter/s, 5.26064s/100 iters), loss = 0.0450757
I1001 16:08:18.285487  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0450767 (* 1 = 0.0450767 loss)
I1001 16:08:18.285492  5332 sgd_solver.cpp:105] Iteration 73700, lr = 0.01
I1001 16:08:23.543011  5332 solver.cpp:218] Iteration 73800 (19.0204 iter/s, 5.2575s/100 iters), loss = 0.0260923
I1001 16:08:23.543130  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0260933 (* 1 = 0.0260933 loss)
I1001 16:08:23.543148  5332 sgd_solver.cpp:105] Iteration 73800, lr = 0.01
I1001 16:08:28.799504  5332 solver.cpp:218] Iteration 73900 (19.0246 iter/s, 5.25636s/100 iters), loss = 0.0172017
I1001 16:08:28.799546  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0172027 (* 1 = 0.0172027 loss)
I1001 16:08:28.799552  5332 sgd_solver.cpp:105] Iteration 73900, lr = 0.01
I1001 16:08:33.794353  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:08:34.004308  5332 solver.cpp:330] Iteration 74000, Testing net (#0)
I1001 16:08:35.204890  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:08:35.255571  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.899
I1001 16:08:35.255606  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383293 (* 1 = 0.383293 loss)
I1001 16:08:35.308251  5332 solver.cpp:218] Iteration 74000 (15.3642 iter/s, 6.50865s/100 iters), loss = 0.0147726
I1001 16:08:35.308280  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0147736 (* 1 = 0.0147736 loss)
I1001 16:08:35.308287  5332 sgd_solver.cpp:105] Iteration 74000, lr = 0.01
I1001 16:08:40.559274  5332 solver.cpp:218] Iteration 74100 (19.0441 iter/s, 5.25097s/100 iters), loss = 0.0123176
I1001 16:08:40.559314  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123186 (* 1 = 0.0123186 loss)
I1001 16:08:40.559320  5332 sgd_solver.cpp:105] Iteration 74100, lr = 0.01
I1001 16:08:45.812973  5332 solver.cpp:218] Iteration 74200 (19.0344 iter/s, 5.25364s/100 iters), loss = 0.0187034
I1001 16:08:45.813004  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0187044 (* 1 = 0.0187044 loss)
I1001 16:08:45.813022  5332 sgd_solver.cpp:105] Iteration 74200, lr = 0.01
I1001 16:08:51.075569  5332 solver.cpp:218] Iteration 74300 (19.0022 iter/s, 5.26255s/100 iters), loss = 0.0488061
I1001 16:08:51.075603  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0488071 (* 1 = 0.0488071 loss)
I1001 16:08:51.075610  5332 sgd_solver.cpp:105] Iteration 74300, lr = 0.01
I1001 16:08:56.345026  5332 solver.cpp:218] Iteration 74400 (18.9775 iter/s, 5.2694s/100 iters), loss = 0.0352051
I1001 16:08:56.345227  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0352061 (* 1 = 0.0352061 loss)
I1001 16:08:56.345238  5332 sgd_solver.cpp:105] Iteration 74400, lr = 0.01
I1001 16:09:01.337918  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:09:01.548916  5332 solver.cpp:330] Iteration 74500, Testing net (#0)
I1001 16:09:02.752334  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:09:02.802705  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8972
I1001 16:09:02.802731  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.393808 (* 1 = 0.393808 loss)
I1001 16:09:02.855132  5332 solver.cpp:218] Iteration 74500 (15.3611 iter/s, 6.50993s/100 iters), loss = 0.0269701
I1001 16:09:02.855159  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0269711 (* 1 = 0.0269711 loss)
I1001 16:09:02.855167  5332 sgd_solver.cpp:105] Iteration 74500, lr = 0.01
I1001 16:09:08.117116  5332 solver.cpp:218] Iteration 74600 (19.0044 iter/s, 5.26194s/100 iters), loss = 0.0122791
I1001 16:09:08.117146  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0122801 (* 1 = 0.0122801 loss)
I1001 16:09:08.117153  5332 sgd_solver.cpp:105] Iteration 74600, lr = 0.01
I1001 16:09:13.370039  5332 solver.cpp:218] Iteration 74700 (19.0372 iter/s, 5.25287s/100 iters), loss = 0.0131867
I1001 16:09:13.370069  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0131876 (* 1 = 0.0131876 loss)
I1001 16:09:13.370075  5332 sgd_solver.cpp:105] Iteration 74700, lr = 0.01
I1001 16:09:18.630597  5332 solver.cpp:218] Iteration 74800 (19.0096 iter/s, 5.26051s/100 iters), loss = 0.0130498
I1001 16:09:18.630628  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130507 (* 1 = 0.0130507 loss)
I1001 16:09:18.630635  5332 sgd_solver.cpp:105] Iteration 74800, lr = 0.01
I1001 16:09:23.891661  5332 solver.cpp:218] Iteration 74900 (19.0078 iter/s, 5.26101s/100 iters), loss = 0.0166389
I1001 16:09:23.891690  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0166398 (* 1 = 0.0166398 loss)
I1001 16:09:23.891696  5332 sgd_solver.cpp:105] Iteration 74900, lr = 0.01
I1001 16:09:28.882335  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:09:29.097645  5332 solver.cpp:330] Iteration 75000, Testing net (#0)
I1001 16:09:30.290052  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:09:30.340436  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8917
I1001 16:09:30.340462  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.427558 (* 1 = 0.427558 loss)
I1001 16:09:30.393070  5332 solver.cpp:218] Iteration 75000 (15.3814 iter/s, 6.50136s/100 iters), loss = 0.0225757
I1001 16:09:30.393103  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0225767 (* 1 = 0.0225767 loss)
I1001 16:09:30.393111  5332 sgd_solver.cpp:105] Iteration 75000, lr = 0.01
I1001 16:09:35.653996  5332 solver.cpp:218] Iteration 75100 (19.0083 iter/s, 5.26087s/100 iters), loss = 0.0334914
I1001 16:09:35.654026  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0334924 (* 1 = 0.0334924 loss)
I1001 16:09:35.654032  5332 sgd_solver.cpp:105] Iteration 75100, lr = 0.01
I1001 16:09:40.910022  5332 solver.cpp:218] Iteration 75200 (19.026 iter/s, 5.25597s/100 iters), loss = 0.0203159
I1001 16:09:40.910069  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0203169 (* 1 = 0.0203169 loss)
I1001 16:09:40.910076  5332 sgd_solver.cpp:105] Iteration 75200, lr = 0.01
I1001 16:09:46.168521  5332 solver.cpp:218] Iteration 75300 (19.0172 iter/s, 5.2584s/100 iters), loss = 0.0483751
I1001 16:09:46.168560  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0483761 (* 1 = 0.0483761 loss)
I1001 16:09:46.168566  5332 sgd_solver.cpp:105] Iteration 75300, lr = 0.01
I1001 16:09:51.429162  5332 solver.cpp:218] Iteration 75400 (19.0093 iter/s, 5.26058s/100 iters), loss = 0.00383456
I1001 16:09:51.429203  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00383547 (* 1 = 0.00383547 loss)
I1001 16:09:51.429208  5332 sgd_solver.cpp:105] Iteration 75400, lr = 0.01
I1001 16:09:56.433224  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:09:56.644465  5332 solver.cpp:330] Iteration 75500, Testing net (#0)
I1001 16:09:57.837445  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:09:57.887627  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8922
I1001 16:09:57.887662  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.424825 (* 1 = 0.424825 loss)
I1001 16:09:57.940255  5332 solver.cpp:218] Iteration 75500 (15.3586 iter/s, 6.51103s/100 iters), loss = 0.0117146
I1001 16:09:57.940285  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117155 (* 1 = 0.0117155 loss)
I1001 16:09:57.940291  5332 sgd_solver.cpp:105] Iteration 75500, lr = 0.01
I1001 16:10:03.203313  5332 solver.cpp:218] Iteration 75600 (19.0006 iter/s, 5.263s/100 iters), loss = 0.0348579
I1001 16:10:03.203415  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0348588 (* 1 = 0.0348588 loss)
I1001 16:10:03.203421  5332 sgd_solver.cpp:105] Iteration 75600, lr = 0.01
I1001 16:10:08.459870  5332 solver.cpp:218] Iteration 75700 (19.0243 iter/s, 5.25644s/100 iters), loss = 0.0324454
I1001 16:10:08.459900  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0324463 (* 1 = 0.0324463 loss)
I1001 16:10:08.459905  5332 sgd_solver.cpp:105] Iteration 75700, lr = 0.01
I1001 16:10:13.708243  5332 solver.cpp:218] Iteration 75800 (19.0537 iter/s, 5.24832s/100 iters), loss = 0.0316989
I1001 16:10:13.708279  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0316998 (* 1 = 0.0316998 loss)
I1001 16:10:13.708297  5332 sgd_solver.cpp:105] Iteration 75800, lr = 0.01
I1001 16:10:18.965888  5332 solver.cpp:218] Iteration 75900 (19.0201 iter/s, 5.25759s/100 iters), loss = 0.0367646
I1001 16:10:18.965919  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0367655 (* 1 = 0.0367655 loss)
I1001 16:10:18.965937  5332 sgd_solver.cpp:105] Iteration 75900, lr = 0.01
I1001 16:10:23.961081  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:10:24.170660  5332 solver.cpp:330] Iteration 76000, Testing net (#0)
I1001 16:10:25.361335  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:10:25.411667  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8977
I1001 16:10:25.411695  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.402795 (* 1 = 0.402795 loss)
I1001 16:10:25.464241  5332 solver.cpp:218] Iteration 76000 (15.3886 iter/s, 6.4983s/100 iters), loss = 0.0212862
I1001 16:10:25.464270  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212871 (* 1 = 0.0212871 loss)
I1001 16:10:25.464279  5332 sgd_solver.cpp:105] Iteration 76000, lr = 0.01
I1001 16:10:30.729249  5332 solver.cpp:218] Iteration 76100 (18.9935 iter/s, 5.26496s/100 iters), loss = 0.0491869
I1001 16:10:30.729279  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0491877 (* 1 = 0.0491877 loss)
I1001 16:10:30.729285  5332 sgd_solver.cpp:105] Iteration 76100, lr = 0.01
I1001 16:10:35.990660  5332 solver.cpp:218] Iteration 76200 (19.0065 iter/s, 5.26136s/100 iters), loss = 0.0612684
I1001 16:10:35.990795  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0612693 (* 1 = 0.0612693 loss)
I1001 16:10:35.990803  5332 sgd_solver.cpp:105] Iteration 76200, lr = 0.01
I1001 16:10:41.251297  5332 solver.cpp:218] Iteration 76300 (19.0096 iter/s, 5.26049s/100 iters), loss = 0.0214302
I1001 16:10:41.251339  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.021431 (* 1 = 0.021431 loss)
I1001 16:10:41.251345  5332 sgd_solver.cpp:105] Iteration 76300, lr = 0.01
I1001 16:10:46.503906  5332 solver.cpp:218] Iteration 76400 (19.0384 iter/s, 5.25255s/100 iters), loss = 0.0128207
I1001 16:10:46.503945  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0128215 (* 1 = 0.0128215 loss)
I1001 16:10:46.503952  5332 sgd_solver.cpp:105] Iteration 76400, lr = 0.01
I1001 16:10:51.502568  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:10:51.712630  5332 solver.cpp:330] Iteration 76500, Testing net (#0)
I1001 16:10:52.907785  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:10:52.958377  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8916
I1001 16:10:52.958405  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.425174 (* 1 = 0.425174 loss)
I1001 16:10:53.012245  5332 solver.cpp:218] Iteration 76500 (15.3651 iter/s, 6.50828s/100 iters), loss = 0.0192368
I1001 16:10:53.012282  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0192376 (* 1 = 0.0192376 loss)
I1001 16:10:53.012290  5332 sgd_solver.cpp:105] Iteration 76500, lr = 0.01
I1001 16:10:58.268462  5332 solver.cpp:218] Iteration 76600 (19.0253 iter/s, 5.25616s/100 iters), loss = 0.0302949
I1001 16:10:58.268493  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0302958 (* 1 = 0.0302958 loss)
I1001 16:10:58.268499  5332 sgd_solver.cpp:105] Iteration 76600, lr = 0.01
I1001 16:11:03.524986  5332 solver.cpp:218] Iteration 76700 (19.0242 iter/s, 5.25647s/100 iters), loss = 0.0466435
I1001 16:11:03.525014  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0466444 (* 1 = 0.0466444 loss)
I1001 16:11:03.525020  5332 sgd_solver.cpp:105] Iteration 76700, lr = 0.01
I1001 16:11:08.782629  5332 solver.cpp:218] Iteration 76800 (19.0201 iter/s, 5.25759s/100 iters), loss = 0.0222226
I1001 16:11:08.782753  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0222235 (* 1 = 0.0222235 loss)
I1001 16:11:08.782770  5332 sgd_solver.cpp:105] Iteration 76800, lr = 0.01
I1001 16:11:14.045243  5332 solver.cpp:218] Iteration 76900 (19.0025 iter/s, 5.26247s/100 iters), loss = 0.0426329
I1001 16:11:14.045276  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0426338 (* 1 = 0.0426338 loss)
I1001 16:11:14.045284  5332 sgd_solver.cpp:105] Iteration 76900, lr = 0.01
I1001 16:11:19.041605  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:11:19.251227  5332 solver.cpp:330] Iteration 77000, Testing net (#0)
I1001 16:11:20.453138  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:11:20.503603  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8865
I1001 16:11:20.503638  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.464043 (* 1 = 0.464043 loss)
I1001 16:11:20.556215  5332 solver.cpp:218] Iteration 77000 (15.3588 iter/s, 6.51092s/100 iters), loss = 0.0716278
I1001 16:11:20.556244  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0716287 (* 1 = 0.0716287 loss)
I1001 16:11:20.556252  5332 sgd_solver.cpp:105] Iteration 77000, lr = 0.01
I1001 16:11:25.808926  5332 solver.cpp:218] Iteration 77100 (19.038 iter/s, 5.25266s/100 iters), loss = 0.0126109
I1001 16:11:25.808970  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126117 (* 1 = 0.0126117 loss)
I1001 16:11:25.808977  5332 sgd_solver.cpp:105] Iteration 77100, lr = 0.01
I1001 16:11:31.069728  5332 solver.cpp:218] Iteration 77200 (19.0089 iter/s, 5.26071s/100 iters), loss = 0.0166433
I1001 16:11:31.069768  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0166442 (* 1 = 0.0166442 loss)
I1001 16:11:31.069774  5332 sgd_solver.cpp:105] Iteration 77200, lr = 0.01
I1001 16:11:36.330699  5332 solver.cpp:218] Iteration 77300 (19.0081 iter/s, 5.26091s/100 iters), loss = 0.0380007
I1001 16:11:36.330740  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380016 (* 1 = 0.0380016 loss)
I1001 16:11:36.330746  5332 sgd_solver.cpp:105] Iteration 77300, lr = 0.01
I1001 16:11:41.592403  5332 solver.cpp:218] Iteration 77400 (19.0055 iter/s, 5.26164s/100 iters), loss = 0.0471381
I1001 16:11:41.592530  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0471389 (* 1 = 0.0471389 loss)
I1001 16:11:41.592537  5332 sgd_solver.cpp:105] Iteration 77400, lr = 0.01
I1001 16:11:46.585044  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:11:46.796290  5332 solver.cpp:330] Iteration 77500, Testing net (#0)
I1001 16:11:47.992880  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:11:48.042840  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8951
I1001 16:11:48.042874  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416227 (* 1 = 0.416227 loss)
I1001 16:11:48.095499  5332 solver.cpp:218] Iteration 77500 (15.3776 iter/s, 6.50295s/100 iters), loss = 0.0147842
I1001 16:11:48.095520  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0147851 (* 1 = 0.0147851 loss)
I1001 16:11:48.095527  5332 sgd_solver.cpp:105] Iteration 77500, lr = 0.01
I1001 16:11:53.361596  5332 solver.cpp:218] Iteration 77600 (18.9896 iter/s, 5.26605s/100 iters), loss = 0.0089976
I1001 16:11:53.361637  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00899847 (* 1 = 0.00899847 loss)
I1001 16:11:53.361644  5332 sgd_solver.cpp:105] Iteration 77600, lr = 0.01
I1001 16:11:58.622009  5332 solver.cpp:218] Iteration 77700 (19.0101 iter/s, 5.26035s/100 iters), loss = 0.0334461
I1001 16:11:58.622050  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.033447 (* 1 = 0.033447 loss)
I1001 16:11:58.622056  5332 sgd_solver.cpp:105] Iteration 77700, lr = 0.01
I1001 16:12:03.892602  5332 solver.cpp:218] Iteration 77800 (18.9734 iter/s, 5.27053s/100 iters), loss = 0.0190555
I1001 16:12:03.892642  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0190564 (* 1 = 0.0190564 loss)
I1001 16:12:03.892648  5332 sgd_solver.cpp:105] Iteration 77800, lr = 0.01
I1001 16:12:09.163718  5332 solver.cpp:218] Iteration 77900 (18.9715 iter/s, 5.27105s/100 iters), loss = 0.0119038
I1001 16:12:09.163759  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119046 (* 1 = 0.0119046 loss)
I1001 16:12:09.163765  5332 sgd_solver.cpp:105] Iteration 77900, lr = 0.01
I1001 16:12:14.171553  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:12:14.383016  5332 solver.cpp:330] Iteration 78000, Testing net (#0)
I1001 16:12:15.577102  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:12:15.627605  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897
I1001 16:12:15.627640  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.410141 (* 1 = 0.410141 loss)
I1001 16:12:15.680431  5332 solver.cpp:218] Iteration 78000 (15.3453 iter/s, 6.51665s/100 iters), loss = 0.0105204
I1001 16:12:15.680459  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0105212 (* 1 = 0.0105212 loss)
I1001 16:12:15.680466  5332 sgd_solver.cpp:105] Iteration 78000, lr = 0.01
I1001 16:12:20.938985  5332 solver.cpp:218] Iteration 78100 (19.0168 iter/s, 5.25851s/100 iters), loss = 0.0117085
I1001 16:12:20.939014  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117094 (* 1 = 0.0117094 loss)
I1001 16:12:20.939020  5332 sgd_solver.cpp:105] Iteration 78100, lr = 0.01
I1001 16:12:26.196321  5332 solver.cpp:218] Iteration 78200 (19.0212 iter/s, 5.25728s/100 iters), loss = 0.0160248
I1001 16:12:26.196352  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0160257 (* 1 = 0.0160257 loss)
I1001 16:12:26.196359  5332 sgd_solver.cpp:105] Iteration 78200, lr = 0.01
I1001 16:12:31.447499  5332 solver.cpp:218] Iteration 78300 (19.0435 iter/s, 5.25112s/100 iters), loss = 0.0280901
I1001 16:12:31.447540  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0280909 (* 1 = 0.0280909 loss)
I1001 16:12:31.447546  5332 sgd_solver.cpp:105] Iteration 78300, lr = 0.01
I1001 16:12:36.707012  5332 solver.cpp:218] Iteration 78400 (19.0134 iter/s, 5.25945s/100 iters), loss = 0.0254148
I1001 16:12:36.707056  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0254156 (* 1 = 0.0254156 loss)
I1001 16:12:36.707062  5332 sgd_solver.cpp:105] Iteration 78400, lr = 0.01
I1001 16:12:41.704316  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:12:41.914772  5332 solver.cpp:330] Iteration 78500, Testing net (#0)
I1001 16:12:43.108289  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:12:43.158638  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8945
I1001 16:12:43.158661  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.410674 (* 1 = 0.410674 loss)
I1001 16:12:43.211344  5332 solver.cpp:218] Iteration 78500 (15.3745 iter/s, 6.50427s/100 iters), loss = 0.0799908
I1001 16:12:43.211369  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0799916 (* 1 = 0.0799916 loss)
I1001 16:12:43.211374  5332 sgd_solver.cpp:105] Iteration 78500, lr = 0.01
I1001 16:12:48.474647  5332 solver.cpp:218] Iteration 78600 (18.9996 iter/s, 5.26326s/100 iters), loss = 0.0145865
I1001 16:12:48.474791  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0145873 (* 1 = 0.0145873 loss)
I1001 16:12:48.474798  5332 sgd_solver.cpp:105] Iteration 78600, lr = 0.01
I1001 16:12:53.738150  5332 solver.cpp:218] Iteration 78700 (18.9993 iter/s, 5.26334s/100 iters), loss = 0.0141065
I1001 16:12:53.738189  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0141073 (* 1 = 0.0141073 loss)
I1001 16:12:53.738195  5332 sgd_solver.cpp:105] Iteration 78700, lr = 0.01
I1001 16:12:59.003376  5332 solver.cpp:218] Iteration 78800 (18.9928 iter/s, 5.26516s/100 iters), loss = 0.0409212
I1001 16:12:59.003413  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.040922 (* 1 = 0.040922 loss)
I1001 16:12:59.003422  5332 sgd_solver.cpp:105] Iteration 78800, lr = 0.01
I1001 16:13:04.261431  5332 solver.cpp:218] Iteration 78900 (19.0188 iter/s, 5.25796s/100 iters), loss = 0.0103935
I1001 16:13:04.261461  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0103943 (* 1 = 0.0103943 loss)
I1001 16:13:04.261466  5332 sgd_solver.cpp:105] Iteration 78900, lr = 0.01
I1001 16:13:09.262768  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:13:09.473860  5332 solver.cpp:330] Iteration 79000, Testing net (#0)
I1001 16:13:10.666954  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:13:10.717485  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8972
I1001 16:13:10.717519  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.420889 (* 1 = 0.420889 loss)
I1001 16:13:10.770097  5332 solver.cpp:218] Iteration 79000 (15.3642 iter/s, 6.50862s/100 iters), loss = 0.0235247
I1001 16:13:10.770120  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0235255 (* 1 = 0.0235255 loss)
I1001 16:13:10.770128  5332 sgd_solver.cpp:105] Iteration 79000, lr = 0.01
I1001 16:13:16.027971  5332 solver.cpp:218] Iteration 79100 (19.0193 iter/s, 5.25783s/100 iters), loss = 0.0302956
I1001 16:13:16.028000  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0302964 (* 1 = 0.0302964 loss)
I1001 16:13:16.028007  5332 sgd_solver.cpp:105] Iteration 79100, lr = 0.01
I1001 16:13:21.285091  5332 solver.cpp:218] Iteration 79200 (19.022 iter/s, 5.25707s/100 iters), loss = 0.0347216
I1001 16:13:21.285267  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0347224 (* 1 = 0.0347224 loss)
I1001 16:13:21.285276  5332 sgd_solver.cpp:105] Iteration 79200, lr = 0.01
I1001 16:13:26.542767  5332 solver.cpp:218] Iteration 79300 (19.0205 iter/s, 5.25749s/100 iters), loss = 0.0316313
I1001 16:13:26.542796  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0316321 (* 1 = 0.0316321 loss)
I1001 16:13:26.542814  5332 sgd_solver.cpp:105] Iteration 79300, lr = 0.01
I1001 16:13:31.797972  5332 solver.cpp:218] Iteration 79400 (19.0289 iter/s, 5.25516s/100 iters), loss = 0.00489011
I1001 16:13:31.798002  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00489091 (* 1 = 0.00489091 loss)
I1001 16:13:31.798018  5332 sgd_solver.cpp:105] Iteration 79400, lr = 0.01
I1001 16:13:36.795619  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:13:37.006567  5332 solver.cpp:330] Iteration 79500, Testing net (#0)
I1001 16:13:38.208052  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:13:38.258338  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896
I1001 16:13:38.258374  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415131 (* 1 = 0.415131 loss)
I1001 16:13:38.311491  5332 solver.cpp:218] Iteration 79500 (15.3528 iter/s, 6.51346s/100 iters), loss = 0.0240124
I1001 16:13:38.311519  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0240132 (* 1 = 0.0240132 loss)
I1001 16:13:38.311527  5332 sgd_solver.cpp:105] Iteration 79500, lr = 0.01
I1001 16:13:43.564754  5332 solver.cpp:218] Iteration 79600 (19.036 iter/s, 5.25321s/100 iters), loss = 0.00953569
I1001 16:13:43.564784  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00953648 (* 1 = 0.00953648 loss)
I1001 16:13:43.564788  5332 sgd_solver.cpp:105] Iteration 79600, lr = 0.01
I1001 16:13:48.829236  5332 solver.cpp:218] Iteration 79700 (18.9954 iter/s, 5.26443s/100 iters), loss = 0.0251518
I1001 16:13:48.829264  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0251526 (* 1 = 0.0251526 loss)
I1001 16:13:48.829270  5332 sgd_solver.cpp:105] Iteration 79700, lr = 0.01
I1001 16:13:54.094724  5332 solver.cpp:218] Iteration 79800 (18.9918 iter/s, 5.26544s/100 iters), loss = 0.00734873
I1001 16:13:54.094836  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0073495 (* 1 = 0.0073495 loss)
I1001 16:13:54.094853  5332 sgd_solver.cpp:105] Iteration 79800, lr = 0.01
I1001 16:13:59.362318  5332 solver.cpp:218] Iteration 79900 (18.9844 iter/s, 5.26747s/100 iters), loss = 0.0296609
I1001 16:13:59.362349  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0296617 (* 1 = 0.0296617 loss)
I1001 16:13:59.362366  5332 sgd_solver.cpp:105] Iteration 79900, lr = 0.01
I1001 16:14:04.353560  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:14:04.564733  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_80000.caffemodel
I1001 16:14:04.569922  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_80000.solverstate
I1001 16:14:04.571321  5332 solver.cpp:330] Iteration 80000, Testing net (#0)
I1001 16:14:05.769707  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:14:05.820056  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8864
I1001 16:14:05.820080  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.493185 (* 1 = 0.493185 loss)
I1001 16:14:05.872974  5332 solver.cpp:218] Iteration 80000 (15.3596 iter/s, 6.51061s/100 iters), loss = 0.0183486
I1001 16:14:05.872998  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0183493 (* 1 = 0.0183493 loss)
I1001 16:14:05.873004  5332 sgd_solver.cpp:46] MultiStep Status: Iteration 80000, step = 2
I1001 16:14:05.873008  5332 sgd_solver.cpp:105] Iteration 80000, lr = 0.001
I1001 16:14:11.132555  5332 solver.cpp:218] Iteration 80100 (19.0131 iter/s, 5.25954s/100 iters), loss = 0.0579375
I1001 16:14:11.132587  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0579382 (* 1 = 0.0579382 loss)
I1001 16:14:11.132594  5332 sgd_solver.cpp:105] Iteration 80100, lr = 0.001
I1001 16:14:16.384349  5332 solver.cpp:218] Iteration 80200 (19.0413 iter/s, 5.25174s/100 iters), loss = 0.00924213
I1001 16:14:16.384390  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00924289 (* 1 = 0.00924289 loss)
I1001 16:14:16.384397  5332 sgd_solver.cpp:105] Iteration 80200, lr = 0.001
I1001 16:14:21.648387  5332 solver.cpp:218] Iteration 80300 (18.997 iter/s, 5.26398s/100 iters), loss = 0.0112097
I1001 16:14:21.648419  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112104 (* 1 = 0.0112104 loss)
I1001 16:14:21.648437  5332 sgd_solver.cpp:105] Iteration 80300, lr = 0.001
I1001 16:14:26.914302  5332 solver.cpp:218] Iteration 80400 (18.9902 iter/s, 5.26587s/100 iters), loss = 0.0155416
I1001 16:14:26.914446  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0155423 (* 1 = 0.0155423 loss)
I1001 16:14:26.914508  5332 sgd_solver.cpp:105] Iteration 80400, lr = 0.001
I1001 16:14:31.910724  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:14:32.127461  5332 solver.cpp:330] Iteration 80500, Testing net (#0)
I1001 16:14:33.322965  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:14:33.372841  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.904
I1001 16:14:33.372869  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.372523 (* 1 = 0.372523 loss)
I1001 16:14:33.425025  5332 solver.cpp:218] Iteration 80500 (15.3597 iter/s, 6.51056s/100 iters), loss = 0.00691591
I1001 16:14:33.425052  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00691667 (* 1 = 0.00691667 loss)
I1001 16:14:33.425062  5332 sgd_solver.cpp:105] Iteration 80500, lr = 0.001
I1001 16:14:38.685401  5332 solver.cpp:218] Iteration 80600 (19.0102 iter/s, 5.26033s/100 iters), loss = 0.0215584
I1001 16:14:38.685432  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0215591 (* 1 = 0.0215591 loss)
I1001 16:14:38.685451  5332 sgd_solver.cpp:105] Iteration 80600, lr = 0.001
I1001 16:14:43.944142  5332 solver.cpp:218] Iteration 80700 (19.0161 iter/s, 5.25869s/100 iters), loss = 0.0103287
I1001 16:14:43.944180  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0103295 (* 1 = 0.0103295 loss)
I1001 16:14:43.944200  5332 sgd_solver.cpp:105] Iteration 80700, lr = 0.001
I1001 16:14:49.200834  5332 solver.cpp:218] Iteration 80800 (19.0237 iter/s, 5.25661s/100 iters), loss = 0.0219275
I1001 16:14:49.200863  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0219282 (* 1 = 0.0219282 loss)
I1001 16:14:49.200871  5332 sgd_solver.cpp:105] Iteration 80800, lr = 0.001
I1001 16:14:54.459574  5332 solver.cpp:218] Iteration 80900 (19.0161 iter/s, 5.25869s/100 iters), loss = 0.0031188
I1001 16:14:54.459606  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00311955 (* 1 = 0.00311955 loss)
I1001 16:14:54.459614  5332 sgd_solver.cpp:105] Iteration 80900, lr = 0.001
I1001 16:14:59.454582  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:14:59.664816  5332 solver.cpp:330] Iteration 81000, Testing net (#0)
I1001 16:15:00.856922  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:15:00.907142  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9088
I1001 16:15:00.907166  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358315 (* 1 = 0.358315 loss)
I1001 16:15:00.959656  5332 solver.cpp:218] Iteration 81000 (15.3845 iter/s, 6.50003s/100 iters), loss = 0.0116793
I1001 16:15:00.959692  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.01168 (* 1 = 0.01168 loss)
I1001 16:15:00.959702  5332 sgd_solver.cpp:105] Iteration 81000, lr = 0.001
I1001 16:15:06.214960  5332 solver.cpp:218] Iteration 81100 (19.0286 iter/s, 5.25525s/100 iters), loss = 0.0188514
I1001 16:15:06.214990  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0188522 (* 1 = 0.0188522 loss)
I1001 16:15:06.214996  5332 sgd_solver.cpp:105] Iteration 81100, lr = 0.001
I1001 16:15:11.470166  5332 solver.cpp:218] Iteration 81200 (19.0289 iter/s, 5.25516s/100 iters), loss = 0.00429735
I1001 16:15:11.470197  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0042981 (* 1 = 0.0042981 loss)
I1001 16:15:11.470203  5332 sgd_solver.cpp:105] Iteration 81200, lr = 0.001
I1001 16:15:16.724521  5332 solver.cpp:218] Iteration 81300 (19.032 iter/s, 5.2543s/100 iters), loss = 0.00391187
I1001 16:15:16.724561  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00391264 (* 1 = 0.00391264 loss)
I1001 16:15:16.724567  5332 sgd_solver.cpp:105] Iteration 81300, lr = 0.001
I1001 16:15:21.989094  5332 solver.cpp:218] Iteration 81400 (18.9951 iter/s, 5.26451s/100 iters), loss = 0.00497866
I1001 16:15:21.989135  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00497942 (* 1 = 0.00497942 loss)
I1001 16:15:21.989140  5332 sgd_solver.cpp:105] Iteration 81400, lr = 0.001
I1001 16:15:26.989105  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:15:27.199461  5332 solver.cpp:330] Iteration 81500, Testing net (#0)
I1001 16:15:28.393667  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:15:28.443776  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9076
I1001 16:15:28.443801  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.354517 (* 1 = 0.354517 loss)
I1001 16:15:28.496428  5332 solver.cpp:218] Iteration 81500 (15.3674 iter/s, 6.50727s/100 iters), loss = 0.00558354
I1001 16:15:28.496454  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0055843 (* 1 = 0.0055843 loss)
I1001 16:15:28.496461  5332 sgd_solver.cpp:105] Iteration 81500, lr = 0.001
I1001 16:15:33.759146  5332 solver.cpp:218] Iteration 81600 (19.0018 iter/s, 5.26267s/100 iters), loss = 0.00817072
I1001 16:15:33.759289  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00817149 (* 1 = 0.00817149 loss)
I1001 16:15:33.759297  5332 sgd_solver.cpp:105] Iteration 81600, lr = 0.001
I1001 16:15:39.022132  5332 solver.cpp:218] Iteration 81700 (19.0012 iter/s, 5.26282s/100 iters), loss = 0.011914
I1001 16:15:39.022173  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119148 (* 1 = 0.0119148 loss)
I1001 16:15:39.022179  5332 sgd_solver.cpp:105] Iteration 81700, lr = 0.001
I1001 16:15:44.280954  5332 solver.cpp:218] Iteration 81800 (19.0159 iter/s, 5.25876s/100 iters), loss = 0.0422288
I1001 16:15:44.280987  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0422296 (* 1 = 0.0422296 loss)
I1001 16:15:44.280994  5332 sgd_solver.cpp:105] Iteration 81800, lr = 0.001
I1001 16:15:49.534273  5332 solver.cpp:218] Iteration 81900 (19.0358 iter/s, 5.25327s/100 iters), loss = 0.0232318
I1001 16:15:49.534303  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0232325 (* 1 = 0.0232325 loss)
I1001 16:15:49.534310  5332 sgd_solver.cpp:105] Iteration 81900, lr = 0.001
I1001 16:15:54.528987  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:15:54.740084  5332 solver.cpp:330] Iteration 82000, Testing net (#0)
I1001 16:15:55.936507  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:15:55.987697  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9088
I1001 16:15:55.987733  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.354113 (* 1 = 0.354113 loss)
I1001 16:15:56.041680  5332 solver.cpp:218] Iteration 82000 (15.3672 iter/s, 6.50735s/100 iters), loss = 0.0206679
I1001 16:15:56.041725  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0206686 (* 1 = 0.0206686 loss)
I1001 16:15:56.041733  5332 sgd_solver.cpp:105] Iteration 82000, lr = 0.001
I1001 16:16:01.296041  5332 solver.cpp:218] Iteration 82100 (19.0322 iter/s, 5.25426s/100 iters), loss = 0.0182119
I1001 16:16:01.296087  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0182126 (* 1 = 0.0182126 loss)
I1001 16:16:01.296105  5332 sgd_solver.cpp:105] Iteration 82100, lr = 0.001
I1001 16:16:06.556280  5332 solver.cpp:218] Iteration 82200 (19.0108 iter/s, 5.26017s/100 iters), loss = 0.00687928
I1001 16:16:06.556406  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00688004 (* 1 = 0.00688004 loss)
I1001 16:16:06.556428  5332 sgd_solver.cpp:105] Iteration 82200, lr = 0.001
I1001 16:16:11.815894  5332 solver.cpp:218] Iteration 82300 (19.0133 iter/s, 5.25947s/100 iters), loss = 0.00604276
I1001 16:16:11.815924  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00604352 (* 1 = 0.00604352 loss)
I1001 16:16:11.815932  5332 sgd_solver.cpp:105] Iteration 82300, lr = 0.001
I1001 16:16:17.074805  5332 solver.cpp:218] Iteration 82400 (19.0155 iter/s, 5.25886s/100 iters), loss = 0.00433352
I1001 16:16:17.074841  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00433428 (* 1 = 0.00433428 loss)
I1001 16:16:17.074851  5332 sgd_solver.cpp:105] Iteration 82400, lr = 0.001
I1001 16:16:22.064321  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:16:22.274978  5332 solver.cpp:330] Iteration 82500, Testing net (#0)
I1001 16:16:23.479240  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:16:23.529790  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9095
I1001 16:16:23.529825  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.353281 (* 1 = 0.353281 loss)
I1001 16:16:23.582649  5332 solver.cpp:218] Iteration 82500 (15.3662 iter/s, 6.50779s/100 iters), loss = 0.00727852
I1001 16:16:23.582674  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00727928 (* 1 = 0.00727928 loss)
I1001 16:16:23.582680  5332 sgd_solver.cpp:105] Iteration 82500, lr = 0.001
I1001 16:16:28.838093  5332 solver.cpp:218] Iteration 82600 (19.0281 iter/s, 5.25539s/100 iters), loss = 0.00279556
I1001 16:16:28.838136  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00279633 (* 1 = 0.00279633 loss)
I1001 16:16:28.838143  5332 sgd_solver.cpp:105] Iteration 82600, lr = 0.001
I1001 16:16:34.097141  5332 solver.cpp:218] Iteration 82700 (19.0152 iter/s, 5.25895s/100 iters), loss = 0.0141841
I1001 16:16:34.097170  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0141849 (* 1 = 0.0141849 loss)
I1001 16:16:34.097177  5332 sgd_solver.cpp:105] Iteration 82700, lr = 0.001
I1001 16:16:39.356531  5332 solver.cpp:218] Iteration 82800 (19.0138 iter/s, 5.25934s/100 iters), loss = 0.00720194
I1001 16:16:39.356642  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0072027 (* 1 = 0.0072027 loss)
I1001 16:16:39.356648  5332 sgd_solver.cpp:105] Iteration 82800, lr = 0.001
I1001 16:16:44.614470  5332 solver.cpp:218] Iteration 82900 (19.0193 iter/s, 5.25781s/100 iters), loss = 0.00522228
I1001 16:16:44.614500  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00522304 (* 1 = 0.00522304 loss)
I1001 16:16:44.614507  5332 sgd_solver.cpp:105] Iteration 82900, lr = 0.001
I1001 16:16:49.604012  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:16:49.813354  5332 solver.cpp:330] Iteration 83000, Testing net (#0)
I1001 16:16:51.012354  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:16:51.062391  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9105
I1001 16:16:51.062425  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.351778 (* 1 = 0.351778 loss)
I1001 16:16:51.115087  5332 solver.cpp:218] Iteration 83000 (15.3833 iter/s, 6.50056s/100 iters), loss = 0.00385668
I1001 16:16:51.115113  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00385743 (* 1 = 0.00385743 loss)
I1001 16:16:51.115119  5332 sgd_solver.cpp:105] Iteration 83000, lr = 0.001
I1001 16:16:56.381002  5332 solver.cpp:218] Iteration 83100 (18.9902 iter/s, 5.26587s/100 iters), loss = 0.00482513
I1001 16:16:56.381033  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00482588 (* 1 = 0.00482588 loss)
I1001 16:16:56.381041  5332 sgd_solver.cpp:105] Iteration 83100, lr = 0.001
I1001 16:17:01.631153  5332 solver.cpp:218] Iteration 83200 (19.0473 iter/s, 5.2501s/100 iters), loss = 0.00625236
I1001 16:17:01.631193  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00625311 (* 1 = 0.00625311 loss)
I1001 16:17:01.631199  5332 sgd_solver.cpp:105] Iteration 83200, lr = 0.001
I1001 16:17:06.893303  5332 solver.cpp:218] Iteration 83300 (19.0039 iter/s, 5.26208s/100 iters), loss = 0.00577397
I1001 16:17:06.893342  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00577472 (* 1 = 0.00577472 loss)
I1001 16:17:06.893349  5332 sgd_solver.cpp:105] Iteration 83300, lr = 0.001
I1001 16:17:12.156496  5332 solver.cpp:218] Iteration 83400 (19.0001 iter/s, 5.26313s/100 iters), loss = 0.00502762
I1001 16:17:12.156611  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00502838 (* 1 = 0.00502838 loss)
I1001 16:17:12.156620  5332 sgd_solver.cpp:105] Iteration 83400, lr = 0.001
I1001 16:17:17.150455  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:17:17.360950  5332 solver.cpp:330] Iteration 83500, Testing net (#0)
I1001 16:17:18.552510  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:17:18.602479  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9095
I1001 16:17:18.602514  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.350201 (* 1 = 0.350201 loss)
I1001 16:17:18.655052  5332 solver.cpp:218] Iteration 83500 (15.3883 iter/s, 6.49843s/100 iters), loss = 0.0181596
I1001 16:17:18.655076  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0181604 (* 1 = 0.0181604 loss)
I1001 16:17:18.655082  5332 sgd_solver.cpp:105] Iteration 83500, lr = 0.001
I1001 16:17:23.913082  5332 solver.cpp:218] Iteration 83600 (19.0187 iter/s, 5.25798s/100 iters), loss = 0.0255029
I1001 16:17:23.913122  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0255036 (* 1 = 0.0255036 loss)
I1001 16:17:23.913128  5332 sgd_solver.cpp:105] Iteration 83600, lr = 0.001
I1001 16:17:29.164945  5332 solver.cpp:218] Iteration 83700 (19.0411 iter/s, 5.25179s/100 iters), loss = 0.0212174
I1001 16:17:29.164978  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212181 (* 1 = 0.0212181 loss)
I1001 16:17:29.164984  5332 sgd_solver.cpp:105] Iteration 83700, lr = 0.001
I1001 16:17:34.416424  5332 solver.cpp:218] Iteration 83800 (19.0424 iter/s, 5.25143s/100 iters), loss = 0.00776233
I1001 16:17:34.416453  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00776308 (* 1 = 0.00776308 loss)
I1001 16:17:34.416460  5332 sgd_solver.cpp:105] Iteration 83800, lr = 0.001
I1001 16:17:39.676033  5332 solver.cpp:218] Iteration 83900 (19.013 iter/s, 5.25956s/100 iters), loss = 0.00704353
I1001 16:17:39.676074  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00704428 (* 1 = 0.00704428 loss)
I1001 16:17:39.676079  5332 sgd_solver.cpp:105] Iteration 83900, lr = 0.001
I1001 16:17:44.675742  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:17:44.884856  5332 solver.cpp:330] Iteration 84000, Testing net (#0)
I1001 16:17:46.076695  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:17:46.126947  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9097
I1001 16:17:46.126981  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.353007 (* 1 = 0.353007 loss)
I1001 16:17:46.179361  5332 solver.cpp:218] Iteration 84000 (15.3769 iter/s, 6.50327s/100 iters), loss = 0.0154819
I1001 16:17:46.179395  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0154827 (* 1 = 0.0154827 loss)
I1001 16:17:46.179402  5332 sgd_solver.cpp:105] Iteration 84000, lr = 0.001
I1001 16:17:51.445827  5332 solver.cpp:218] Iteration 84100 (18.9883 iter/s, 5.26641s/100 iters), loss = 0.0164158
I1001 16:17:51.445865  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0164166 (* 1 = 0.0164166 loss)
I1001 16:17:51.445871  5332 sgd_solver.cpp:105] Iteration 84100, lr = 0.001
I1001 16:17:56.707247  5332 solver.cpp:218] Iteration 84200 (19.0065 iter/s, 5.26136s/100 iters), loss = 0.00231282
I1001 16:17:56.707278  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00231358 (* 1 = 0.00231358 loss)
I1001 16:17:56.707285  5332 sgd_solver.cpp:105] Iteration 84200, lr = 0.001
I1001 16:18:01.963662  5332 solver.cpp:218] Iteration 84300 (19.0246 iter/s, 5.25636s/100 iters), loss = 0.00906359
I1001 16:18:01.963696  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00906435 (* 1 = 0.00906435 loss)
I1001 16:18:01.963703  5332 sgd_solver.cpp:105] Iteration 84300, lr = 0.001
I1001 16:18:07.221834  5332 solver.cpp:218] Iteration 84400 (19.0182 iter/s, 5.25812s/100 iters), loss = 0.00180797
I1001 16:18:07.221875  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00180873 (* 1 = 0.00180873 loss)
I1001 16:18:07.221881  5332 sgd_solver.cpp:105] Iteration 84400, lr = 0.001
I1001 16:18:12.219292  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:18:12.429553  5332 solver.cpp:330] Iteration 84500, Testing net (#0)
I1001 16:18:13.620453  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:18:13.670743  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9093
I1001 16:18:13.670768  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.354894 (* 1 = 0.354894 loss)
I1001 16:18:13.723515  5332 solver.cpp:218] Iteration 84500 (15.3808 iter/s, 6.50162s/100 iters), loss = 0.00721958
I1001 16:18:13.723538  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00722034 (* 1 = 0.00722034 loss)
I1001 16:18:13.723544  5332 sgd_solver.cpp:105] Iteration 84500, lr = 0.001
I1001 16:18:18.988651  5332 solver.cpp:218] Iteration 84600 (18.993 iter/s, 5.26509s/100 iters), loss = 0.00726456
I1001 16:18:18.988754  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00726532 (* 1 = 0.00726532 loss)
I1001 16:18:18.988760  5332 sgd_solver.cpp:105] Iteration 84600, lr = 0.001
I1001 16:18:24.251744  5332 solver.cpp:218] Iteration 84700 (19.0007 iter/s, 5.26297s/100 iters), loss = 0.021933
I1001 16:18:24.251773  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0219338 (* 1 = 0.0219338 loss)
I1001 16:18:24.251790  5332 sgd_solver.cpp:105] Iteration 84700, lr = 0.001
I1001 16:18:29.519590  5332 solver.cpp:218] Iteration 84800 (18.9833 iter/s, 5.2678s/100 iters), loss = 0.00584282
I1001 16:18:29.519619  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00584358 (* 1 = 0.00584358 loss)
I1001 16:18:29.519625  5332 sgd_solver.cpp:105] Iteration 84800, lr = 0.001
I1001 16:18:34.776850  5332 solver.cpp:218] Iteration 84900 (19.0215 iter/s, 5.25721s/100 iters), loss = 0.00258556
I1001 16:18:34.776880  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00258633 (* 1 = 0.00258633 loss)
I1001 16:18:34.776885  5332 sgd_solver.cpp:105] Iteration 84900, lr = 0.001
I1001 16:18:39.781128  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:18:39.991788  5332 solver.cpp:330] Iteration 85000, Testing net (#0)
I1001 16:18:41.192956  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:18:41.243194  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9093
I1001 16:18:41.243221  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.351818 (* 1 = 0.351818 loss)
I1001 16:18:41.296034  5332 solver.cpp:218] Iteration 85000 (15.3395 iter/s, 6.51913s/100 iters), loss = 0.00591547
I1001 16:18:41.296061  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00591623 (* 1 = 0.00591623 loss)
I1001 16:18:41.296068  5332 sgd_solver.cpp:105] Iteration 85000, lr = 0.001
I1001 16:18:46.554364  5332 solver.cpp:218] Iteration 85100 (19.0176 iter/s, 5.25828s/100 iters), loss = 0.00559437
I1001 16:18:46.554394  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00559514 (* 1 = 0.00559514 loss)
I1001 16:18:46.554399  5332 sgd_solver.cpp:105] Iteration 85100, lr = 0.001
I1001 16:18:51.815901  5332 solver.cpp:218] Iteration 85200 (19.006 iter/s, 5.26149s/100 iters), loss = 0.0106989
I1001 16:18:51.816040  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0106997 (* 1 = 0.0106997 loss)
I1001 16:18:51.816058  5332 sgd_solver.cpp:105] Iteration 85200, lr = 0.001
I1001 16:18:57.075701  5332 solver.cpp:218] Iteration 85300 (19.0127 iter/s, 5.25964s/100 iters), loss = 0.0185377
I1001 16:18:57.075742  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0185385 (* 1 = 0.0185385 loss)
I1001 16:18:57.075747  5332 sgd_solver.cpp:105] Iteration 85300, lr = 0.001
I1001 16:19:02.330896  5332 solver.cpp:218] Iteration 85400 (19.029 iter/s, 5.25513s/100 iters), loss = 0.00533713
I1001 16:19:02.330925  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00533791 (* 1 = 0.00533791 loss)
I1001 16:19:02.330931  5332 sgd_solver.cpp:105] Iteration 85400, lr = 0.001
I1001 16:19:07.317847  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:19:07.527524  5332 solver.cpp:330] Iteration 85500, Testing net (#0)
I1001 16:19:08.727296  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:19:08.777525  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9088
I1001 16:19:08.777560  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35534 (* 1 = 0.35534 loss)
I1001 16:19:08.829988  5332 solver.cpp:218] Iteration 85500 (15.3869 iter/s, 6.49904s/100 iters), loss = 0.00558422
I1001 16:19:08.830011  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00558499 (* 1 = 0.00558499 loss)
I1001 16:19:08.830018  5332 sgd_solver.cpp:105] Iteration 85500, lr = 0.001
I1001 16:19:14.088593  5332 solver.cpp:218] Iteration 85600 (19.0166 iter/s, 5.25855s/100 iters), loss = 0.00746073
I1001 16:19:14.088634  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0074615 (* 1 = 0.0074615 loss)
I1001 16:19:14.088644  5332 sgd_solver.cpp:105] Iteration 85600, lr = 0.001
I1001 16:19:19.342442  5332 solver.cpp:218] Iteration 85700 (19.0339 iter/s, 5.25379s/100 iters), loss = 0.0121725
I1001 16:19:19.342476  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121733 (* 1 = 0.0121733 loss)
I1001 16:19:19.342484  5332 sgd_solver.cpp:105] Iteration 85700, lr = 0.001
I1001 16:19:24.608552  5332 solver.cpp:218] Iteration 85800 (18.9895 iter/s, 5.26606s/100 iters), loss = 0.00560634
I1001 16:19:24.608687  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00560711 (* 1 = 0.00560711 loss)
I1001 16:19:24.608706  5332 sgd_solver.cpp:105] Iteration 85800, lr = 0.001
I1001 16:19:29.876888  5332 solver.cpp:218] Iteration 85900 (18.9819 iter/s, 5.26818s/100 iters), loss = 0.0125349
I1001 16:19:29.876919  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125357 (* 1 = 0.0125357 loss)
I1001 16:19:29.876927  5332 sgd_solver.cpp:105] Iteration 85900, lr = 0.001
I1001 16:19:34.872243  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:19:35.089304  5332 solver.cpp:330] Iteration 86000, Testing net (#0)
I1001 16:19:36.282896  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:19:36.333101  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9091
I1001 16:19:36.333127  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.353972 (* 1 = 0.353972 loss)
I1001 16:19:36.385427  5332 solver.cpp:218] Iteration 86000 (15.3645 iter/s, 6.50849s/100 iters), loss = 0.00562084
I1001 16:19:36.385452  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00562161 (* 1 = 0.00562161 loss)
I1001 16:19:36.385469  5332 sgd_solver.cpp:105] Iteration 86000, lr = 0.001
I1001 16:19:41.645542  5332 solver.cpp:218] Iteration 86100 (19.0112 iter/s, 5.26006s/100 iters), loss = 0.014953
I1001 16:19:41.645582  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0149538 (* 1 = 0.0149538 loss)
I1001 16:19:41.645588  5332 sgd_solver.cpp:105] Iteration 86100, lr = 0.001
I1001 16:19:46.902271  5332 solver.cpp:218] Iteration 86200 (19.0235 iter/s, 5.25667s/100 iters), loss = 0.0158565
I1001 16:19:46.902300  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158573 (* 1 = 0.0158573 loss)
I1001 16:19:46.902307  5332 sgd_solver.cpp:105] Iteration 86200, lr = 0.001
I1001 16:19:52.159268  5332 solver.cpp:218] Iteration 86300 (19.0224 iter/s, 5.25695s/100 iters), loss = 0.00891278
I1001 16:19:52.159308  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00891355 (* 1 = 0.00891355 loss)
I1001 16:19:52.159314  5332 sgd_solver.cpp:105] Iteration 86300, lr = 0.001
I1001 16:19:57.426447  5332 solver.cpp:218] Iteration 86400 (18.9857 iter/s, 5.26712s/100 iters), loss = 0.013746
I1001 16:19:57.426540  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0137468 (* 1 = 0.0137468 loss)
I1001 16:19:57.426549  5332 sgd_solver.cpp:105] Iteration 86400, lr = 0.001
I1001 16:20:02.427047  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:20:02.638010  5332 solver.cpp:330] Iteration 86500, Testing net (#0)
I1001 16:20:03.829473  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:20:03.879822  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.909
I1001 16:20:03.879847  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.352856 (* 1 = 0.352856 loss)
I1001 16:20:03.932375  5332 solver.cpp:218] Iteration 86500 (15.3709 iter/s, 6.50582s/100 iters), loss = 0.00767005
I1001 16:20:03.932400  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00767082 (* 1 = 0.00767082 loss)
I1001 16:20:03.932407  5332 sgd_solver.cpp:105] Iteration 86500, lr = 0.001
I1001 16:20:09.198281  5332 solver.cpp:218] Iteration 86600 (18.9903 iter/s, 5.26586s/100 iters), loss = 0.00779859
I1001 16:20:09.198310  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00779936 (* 1 = 0.00779936 loss)
I1001 16:20:09.198315  5332 sgd_solver.cpp:105] Iteration 86600, lr = 0.001
I1001 16:20:14.461639  5332 solver.cpp:218] Iteration 86700 (18.9995 iter/s, 5.26331s/100 iters), loss = 0.0242792
I1001 16:20:14.461669  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0242799 (* 1 = 0.0242799 loss)
I1001 16:20:14.461676  5332 sgd_solver.cpp:105] Iteration 86700, lr = 0.001
I1001 16:20:19.714962  5332 solver.cpp:218] Iteration 86800 (19.0358 iter/s, 5.25327s/100 iters), loss = 0.00279453
I1001 16:20:19.714993  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00279529 (* 1 = 0.00279529 loss)
I1001 16:20:19.715008  5332 sgd_solver.cpp:105] Iteration 86800, lr = 0.001
I1001 16:20:24.975126  5332 solver.cpp:218] Iteration 86900 (19.011 iter/s, 5.26011s/100 iters), loss = 0.0120936
I1001 16:20:24.975167  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0120944 (* 1 = 0.0120944 loss)
I1001 16:20:24.975173  5332 sgd_solver.cpp:105] Iteration 86900, lr = 0.001
I1001 16:20:29.972470  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:20:30.182785  5332 solver.cpp:330] Iteration 87000, Testing net (#0)
I1001 16:20:31.373962  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:20:31.424250  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9094
I1001 16:20:31.424285  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.353762 (* 1 = 0.353762 loss)
I1001 16:20:31.476552  5332 solver.cpp:218] Iteration 87000 (15.3814 iter/s, 6.50136s/100 iters), loss = 0.00325977
I1001 16:20:31.476578  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00326052 (* 1 = 0.00326052 loss)
I1001 16:20:31.476584  5332 sgd_solver.cpp:105] Iteration 87000, lr = 0.001
I1001 16:20:36.736166  5332 solver.cpp:218] Iteration 87100 (19.013 iter/s, 5.25957s/100 iters), loss = 0.00426941
I1001 16:20:36.736196  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00427016 (* 1 = 0.00427016 loss)
I1001 16:20:36.736202  5332 sgd_solver.cpp:105] Iteration 87100, lr = 0.001
I1001 16:20:41.993080  5332 solver.cpp:218] Iteration 87200 (19.0228 iter/s, 5.25686s/100 iters), loss = 0.00299613
I1001 16:20:41.993109  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00299688 (* 1 = 0.00299688 loss)
I1001 16:20:41.993115  5332 sgd_solver.cpp:105] Iteration 87200, lr = 0.001
I1001 16:20:47.250087  5332 solver.cpp:218] Iteration 87300 (19.0224 iter/s, 5.25696s/100 iters), loss = 0.0116484
I1001 16:20:47.250118  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116491 (* 1 = 0.0116491 loss)
I1001 16:20:47.250124  5332 sgd_solver.cpp:105] Iteration 87300, lr = 0.001
I1001 16:20:52.506266  5332 solver.cpp:218] Iteration 87400 (19.0254 iter/s, 5.25613s/100 iters), loss = 0.00232145
I1001 16:20:52.506295  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00232219 (* 1 = 0.00232219 loss)
I1001 16:20:52.506301  5332 sgd_solver.cpp:105] Iteration 87400, lr = 0.001
I1001 16:20:57.505826  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:20:57.716645  5332 solver.cpp:330] Iteration 87500, Testing net (#0)
I1001 16:20:58.913642  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:20:58.964782  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9095
I1001 16:20:58.964809  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.352386 (* 1 = 0.352386 loss)
I1001 16:20:59.018740  5332 solver.cpp:218] Iteration 87500 (15.3553 iter/s, 6.51242s/100 iters), loss = 0.0048684
I1001 16:20:59.018781  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00486914 (* 1 = 0.00486914 loss)
I1001 16:20:59.018800  5332 sgd_solver.cpp:105] Iteration 87500, lr = 0.001
I1001 16:21:04.273773  5332 solver.cpp:218] Iteration 87600 (19.0296 iter/s, 5.25497s/100 iters), loss = 0.00421568
I1001 16:21:04.273938  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00421643 (* 1 = 0.00421643 loss)
I1001 16:21:04.273948  5332 sgd_solver.cpp:105] Iteration 87600, lr = 0.001
I1001 16:21:09.537611  5332 solver.cpp:218] Iteration 87700 (18.9982 iter/s, 5.26365s/100 iters), loss = 0.0416283
I1001 16:21:09.537652  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.041629 (* 1 = 0.041629 loss)
I1001 16:21:09.537657  5332 sgd_solver.cpp:105] Iteration 87700, lr = 0.001
I1001 16:21:14.800000  5332 solver.cpp:218] Iteration 87800 (19.003 iter/s, 5.26233s/100 iters), loss = 0.00484373
I1001 16:21:14.800040  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00484447 (* 1 = 0.00484447 loss)
I1001 16:21:14.800045  5332 sgd_solver.cpp:105] Iteration 87800, lr = 0.001
I1001 16:21:20.056880  5332 solver.cpp:218] Iteration 87900 (19.0229 iter/s, 5.25681s/100 iters), loss = 0.00244127
I1001 16:21:20.056921  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00244201 (* 1 = 0.00244201 loss)
I1001 16:21:20.056929  5332 sgd_solver.cpp:105] Iteration 87900, lr = 0.001
I1001 16:21:25.049657  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:21:25.259474  5332 solver.cpp:330] Iteration 88000, Testing net (#0)
I1001 16:21:26.456648  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:21:26.506726  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9103
I1001 16:21:26.506760  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.353024 (* 1 = 0.353024 loss)
I1001 16:21:26.559001  5332 solver.cpp:218] Iteration 88000 (15.3797 iter/s, 6.50206s/100 iters), loss = 0.00926406
I1001 16:21:26.559032  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0092648 (* 1 = 0.0092648 loss)
I1001 16:21:26.559039  5332 sgd_solver.cpp:105] Iteration 88000, lr = 0.001
I1001 16:21:31.810901  5332 solver.cpp:218] Iteration 88100 (19.0409 iter/s, 5.25184s/100 iters), loss = 0.0101189
I1001 16:21:31.810941  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0101196 (* 1 = 0.0101196 loss)
I1001 16:21:31.810947  5332 sgd_solver.cpp:105] Iteration 88100, lr = 0.001
I1001 16:21:37.072608  5332 solver.cpp:218] Iteration 88200 (19.0055 iter/s, 5.26165s/100 iters), loss = 0.0022988
I1001 16:21:37.072762  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00229954 (* 1 = 0.00229954 loss)
I1001 16:21:37.072770  5332 sgd_solver.cpp:105] Iteration 88200, lr = 0.001
I1001 16:21:42.335023  5332 solver.cpp:218] Iteration 88300 (19.0033 iter/s, 5.26225s/100 iters), loss = 0.00772759
I1001 16:21:42.335064  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00772832 (* 1 = 0.00772832 loss)
I1001 16:21:42.335069  5332 sgd_solver.cpp:105] Iteration 88300, lr = 0.001
I1001 16:21:47.603384  5332 solver.cpp:218] Iteration 88400 (18.9815 iter/s, 5.2683s/100 iters), loss = 0.00465314
I1001 16:21:47.603422  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00465388 (* 1 = 0.00465388 loss)
I1001 16:21:47.603430  5332 sgd_solver.cpp:105] Iteration 88400, lr = 0.001
I1001 16:21:52.600646  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:21:52.811012  5332 solver.cpp:330] Iteration 88500, Testing net (#0)
I1001 16:21:54.013478  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:21:54.063735  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9096
I1001 16:21:54.063769  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358336 (* 1 = 0.358336 loss)
I1001 16:21:54.116289  5332 solver.cpp:218] Iteration 88500 (15.3543 iter/s, 6.51284s/100 iters), loss = 0.00293488
I1001 16:21:54.116314  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00293562 (* 1 = 0.00293562 loss)
I1001 16:21:54.116322  5332 sgd_solver.cpp:105] Iteration 88500, lr = 0.001
I1001 16:21:59.386004  5332 solver.cpp:218] Iteration 88600 (18.9765 iter/s, 5.26967s/100 iters), loss = 0.00463426
I1001 16:21:59.386044  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.004635 (* 1 = 0.004635 loss)
I1001 16:21:59.386050  5332 sgd_solver.cpp:105] Iteration 88600, lr = 0.001
I1001 16:22:04.645892  5332 solver.cpp:218] Iteration 88700 (19.012 iter/s, 5.25983s/100 iters), loss = 0.0103069
I1001 16:22:04.645933  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0103077 (* 1 = 0.0103077 loss)
I1001 16:22:04.645939  5332 sgd_solver.cpp:105] Iteration 88700, lr = 0.001
I1001 16:22:09.904894  5332 solver.cpp:218] Iteration 88800 (19.0152 iter/s, 5.25894s/100 iters), loss = 0.0156129
I1001 16:22:09.905004  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0156136 (* 1 = 0.0156136 loss)
I1001 16:22:09.905011  5332 sgd_solver.cpp:105] Iteration 88800, lr = 0.001
I1001 16:22:15.167068  5332 solver.cpp:218] Iteration 88900 (19.004 iter/s, 5.26205s/100 iters), loss = 0.0150223
I1001 16:22:15.167109  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015023 (* 1 = 0.015023 loss)
I1001 16:22:15.167114  5332 sgd_solver.cpp:105] Iteration 88900, lr = 0.001
I1001 16:22:20.158319  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:22:20.368860  5332 solver.cpp:330] Iteration 89000, Testing net (#0)
I1001 16:22:21.560166  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:22:21.610347  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9092
I1001 16:22:21.610371  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358971 (* 1 = 0.358971 loss)
I1001 16:22:21.663069  5332 solver.cpp:218] Iteration 89000 (15.3942 iter/s, 6.49594s/100 iters), loss = 0.00323824
I1001 16:22:21.663091  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00323899 (* 1 = 0.00323899 loss)
I1001 16:22:21.663099  5332 sgd_solver.cpp:105] Iteration 89000, lr = 0.001
I1001 16:22:26.930004  5332 solver.cpp:218] Iteration 89100 (18.9865 iter/s, 5.26689s/100 iters), loss = 0.00698441
I1001 16:22:26.930044  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00698515 (* 1 = 0.00698515 loss)
I1001 16:22:26.930050  5332 sgd_solver.cpp:105] Iteration 89100, lr = 0.001
I1001 16:22:32.194542  5332 solver.cpp:218] Iteration 89200 (18.9953 iter/s, 5.26447s/100 iters), loss = 0.00311835
I1001 16:22:32.194586  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0031191 (* 1 = 0.0031191 loss)
I1001 16:22:32.194593  5332 sgd_solver.cpp:105] Iteration 89200, lr = 0.001
I1001 16:22:37.448822  5332 solver.cpp:218] Iteration 89300 (19.0323 iter/s, 5.25422s/100 iters), loss = 0.00852412
I1001 16:22:37.448850  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00852486 (* 1 = 0.00852486 loss)
I1001 16:22:37.448856  5332 sgd_solver.cpp:105] Iteration 89300, lr = 0.001
I1001 16:22:42.709375  5332 solver.cpp:218] Iteration 89400 (19.0096 iter/s, 5.2605s/100 iters), loss = 0.00130077
I1001 16:22:42.709487  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00130152 (* 1 = 0.00130152 loss)
I1001 16:22:42.709506  5332 sgd_solver.cpp:105] Iteration 89400, lr = 0.001
I1001 16:22:47.706387  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:22:47.917286  5332 solver.cpp:330] Iteration 89500, Testing net (#0)
I1001 16:22:49.111429  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:22:49.161810  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9088
I1001 16:22:49.161845  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357582 (* 1 = 0.357582 loss)
I1001 16:22:49.214272  5332 solver.cpp:218] Iteration 89500 (15.3733 iter/s, 6.50477s/100 iters), loss = 0.00523341
I1001 16:22:49.214298  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00523417 (* 1 = 0.00523417 loss)
I1001 16:22:49.214304  5332 sgd_solver.cpp:105] Iteration 89500, lr = 0.001
I1001 16:22:54.474689  5332 solver.cpp:218] Iteration 89600 (19.0101 iter/s, 5.26037s/100 iters), loss = 0.0020545
I1001 16:22:54.474719  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00205525 (* 1 = 0.00205525 loss)
I1001 16:22:54.474723  5332 sgd_solver.cpp:105] Iteration 89600, lr = 0.001
I1001 16:22:59.738147  5332 solver.cpp:218] Iteration 89700 (18.9991 iter/s, 5.2634s/100 iters), loss = 0.00487154
I1001 16:22:59.738188  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0048723 (* 1 = 0.0048723 loss)
I1001 16:22:59.738194  5332 sgd_solver.cpp:105] Iteration 89700, lr = 0.001
I1001 16:23:04.993839  5332 solver.cpp:218] Iteration 89800 (19.0272 iter/s, 5.25563s/100 iters), loss = 0.00471638
I1001 16:23:04.993883  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00471714 (* 1 = 0.00471714 loss)
I1001 16:23:04.993891  5332 sgd_solver.cpp:105] Iteration 89800, lr = 0.001
I1001 16:23:10.250435  5332 solver.cpp:218] Iteration 89900 (19.0241 iter/s, 5.2565s/100 iters), loss = 0.00420439
I1001 16:23:10.250465  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00420515 (* 1 = 0.00420515 loss)
I1001 16:23:10.250471  5332 sgd_solver.cpp:105] Iteration 89900, lr = 0.001
I1001 16:23:15.255213  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:23:15.465283  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_90000.caffemodel
I1001 16:23:15.470222  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_90000.solverstate
I1001 16:23:15.471611  5332 solver.cpp:330] Iteration 90000, Testing net (#0)
I1001 16:23:16.662686  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:23:16.712692  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9092
I1001 16:23:16.712718  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358965 (* 1 = 0.358965 loss)
I1001 16:23:16.765559  5332 solver.cpp:218] Iteration 90000 (15.349 iter/s, 6.51508s/100 iters), loss = 0.0151884
I1001 16:23:16.765585  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0151892 (* 1 = 0.0151892 loss)
I1001 16:23:16.765592  5332 sgd_solver.cpp:105] Iteration 90000, lr = 0.001
I1001 16:23:22.022645  5332 solver.cpp:218] Iteration 90100 (19.0221 iter/s, 5.25704s/100 iters), loss = 0.00331301
I1001 16:23:22.022685  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00331378 (* 1 = 0.00331378 loss)
I1001 16:23:22.022691  5332 sgd_solver.cpp:105] Iteration 90100, lr = 0.001
I1001 16:23:27.280412  5332 solver.cpp:218] Iteration 90200 (19.0197 iter/s, 5.25771s/100 iters), loss = 0.0167124
I1001 16:23:27.280441  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0167132 (* 1 = 0.0167132 loss)
I1001 16:23:27.280447  5332 sgd_solver.cpp:105] Iteration 90200, lr = 0.001
I1001 16:23:32.540477  5332 solver.cpp:218] Iteration 90300 (19.0114 iter/s, 5.26002s/100 iters), loss = 0.00349261
I1001 16:23:32.540505  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00349337 (* 1 = 0.00349337 loss)
I1001 16:23:32.540513  5332 sgd_solver.cpp:105] Iteration 90300, lr = 0.001
I1001 16:23:37.796219  5332 solver.cpp:218] Iteration 90400 (19.027 iter/s, 5.2557s/100 iters), loss = 0.00284592
I1001 16:23:37.796258  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00284668 (* 1 = 0.00284668 loss)
I1001 16:23:37.796264  5332 sgd_solver.cpp:105] Iteration 90400, lr = 0.001
I1001 16:23:42.793181  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:23:43.002718  5332 solver.cpp:330] Iteration 90500, Testing net (#0)
I1001 16:23:44.203971  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:23:44.254253  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9084
I1001 16:23:44.254278  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357037 (* 1 = 0.357037 loss)
I1001 16:23:44.306941  5332 solver.cpp:218] Iteration 90500 (15.3594 iter/s, 6.51066s/100 iters), loss = 0.00464081
I1001 16:23:44.306970  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00464157 (* 1 = 0.00464157 loss)
I1001 16:23:44.306977  5332 sgd_solver.cpp:105] Iteration 90500, lr = 0.001
I1001 16:23:49.559752  5332 solver.cpp:218] Iteration 90600 (19.0376 iter/s, 5.25276s/100 iters), loss = 0.00825042
I1001 16:23:49.559890  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00825118 (* 1 = 0.00825118 loss)
I1001 16:23:49.559896  5332 sgd_solver.cpp:105] Iteration 90600, lr = 0.001
I1001 16:23:54.819458  5332 solver.cpp:218] Iteration 90700 (19.013 iter/s, 5.25956s/100 iters), loss = 0.00329024
I1001 16:23:54.819499  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.003291 (* 1 = 0.003291 loss)
I1001 16:23:54.819504  5332 sgd_solver.cpp:105] Iteration 90700, lr = 0.001
I1001 16:24:00.080298  5332 solver.cpp:218] Iteration 90800 (19.0086 iter/s, 5.26078s/100 iters), loss = 0.00435682
I1001 16:24:00.080327  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00435757 (* 1 = 0.00435757 loss)
I1001 16:24:00.080333  5332 sgd_solver.cpp:105] Iteration 90800, lr = 0.001
I1001 16:24:05.346019  5332 solver.cpp:218] Iteration 90900 (18.9909 iter/s, 5.26567s/100 iters), loss = 0.00386158
I1001 16:24:05.346050  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00386234 (* 1 = 0.00386234 loss)
I1001 16:24:05.346056  5332 sgd_solver.cpp:105] Iteration 90900, lr = 0.001
I1001 16:24:10.340772  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:24:10.550616  5332 solver.cpp:330] Iteration 91000, Testing net (#0)
I1001 16:24:11.751091  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:24:11.801409  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9083
I1001 16:24:11.801443  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356035 (* 1 = 0.356035 loss)
I1001 16:24:11.853626  5332 solver.cpp:218] Iteration 91000 (15.3668 iter/s, 6.50756s/100 iters), loss = 0.0136635
I1001 16:24:11.853658  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0136642 (* 1 = 0.0136642 loss)
I1001 16:24:11.853665  5332 sgd_solver.cpp:105] Iteration 91000, lr = 0.001
I1001 16:24:17.111583  5332 solver.cpp:218] Iteration 91100 (19.019 iter/s, 5.25791s/100 iters), loss = 0.00713853
I1001 16:24:17.111618  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00713928 (* 1 = 0.00713928 loss)
I1001 16:24:17.111626  5332 sgd_solver.cpp:105] Iteration 91100, lr = 0.001
I1001 16:24:22.363281  5332 solver.cpp:218] Iteration 91200 (19.0417 iter/s, 5.25164s/100 iters), loss = 0.0137457
I1001 16:24:22.363390  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0137464 (* 1 = 0.0137464 loss)
I1001 16:24:22.363409  5332 sgd_solver.cpp:105] Iteration 91200, lr = 0.001
I1001 16:24:27.625735  5332 solver.cpp:218] Iteration 91300 (19.003 iter/s, 5.26233s/100 iters), loss = 0.00292825
I1001 16:24:27.625766  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.002929 (* 1 = 0.002929 loss)
I1001 16:24:27.625784  5332 sgd_solver.cpp:105] Iteration 91300, lr = 0.001
I1001 16:24:32.876641  5332 solver.cpp:218] Iteration 91400 (19.0445 iter/s, 5.25085s/100 iters), loss = 0.00404077
I1001 16:24:32.876673  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00404152 (* 1 = 0.00404152 loss)
I1001 16:24:32.876693  5332 sgd_solver.cpp:105] Iteration 91400, lr = 0.001
I1001 16:24:37.860204  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:24:38.076572  5332 solver.cpp:330] Iteration 91500, Testing net (#0)
I1001 16:24:39.273594  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:24:39.324126  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9074
I1001 16:24:39.324153  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358591 (* 1 = 0.358591 loss)
I1001 16:24:39.376718  5332 solver.cpp:218] Iteration 91500 (15.3846 iter/s, 6.50003s/100 iters), loss = 0.00839171
I1001 16:24:39.376744  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00839246 (* 1 = 0.00839246 loss)
I1001 16:24:39.376754  5332 sgd_solver.cpp:105] Iteration 91500, lr = 0.001
I1001 16:24:44.634069  5332 solver.cpp:218] Iteration 91600 (19.0212 iter/s, 5.2573s/100 iters), loss = 0.0150634
I1001 16:24:44.634102  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0150641 (* 1 = 0.0150641 loss)
I1001 16:24:44.634111  5332 sgd_solver.cpp:105] Iteration 91600, lr = 0.001
I1001 16:24:49.887696  5332 solver.cpp:218] Iteration 91700 (19.0347 iter/s, 5.25357s/100 iters), loss = 0.00809557
I1001 16:24:49.887732  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00809631 (* 1 = 0.00809631 loss)
I1001 16:24:49.887753  5332 sgd_solver.cpp:105] Iteration 91700, lr = 0.001
I1001 16:24:55.156265  5332 solver.cpp:218] Iteration 91800 (18.9808 iter/s, 5.26847s/100 iters), loss = 0.0067556
I1001 16:24:55.156342  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00675634 (* 1 = 0.00675634 loss)
I1001 16:24:55.156363  5332 sgd_solver.cpp:105] Iteration 91800, lr = 0.001
I1001 16:25:00.430049  5332 solver.cpp:218] Iteration 91900 (18.962 iter/s, 5.2737s/100 iters), loss = 0.00223412
I1001 16:25:00.430088  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00223486 (* 1 = 0.00223486 loss)
I1001 16:25:00.430095  5332 sgd_solver.cpp:105] Iteration 91900, lr = 0.001
I1001 16:25:05.432826  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:25:05.643093  5332 solver.cpp:330] Iteration 92000, Testing net (#0)
I1001 16:25:06.835566  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:25:06.885901  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9075
I1001 16:25:06.885926  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358814 (* 1 = 0.358814 loss)
I1001 16:25:06.938275  5332 solver.cpp:218] Iteration 92000 (15.3653 iter/s, 6.50817s/100 iters), loss = 0.00710221
I1001 16:25:06.938297  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00710295 (* 1 = 0.00710295 loss)
I1001 16:25:06.938304  5332 sgd_solver.cpp:105] Iteration 92000, lr = 0.001
I1001 16:25:12.199707  5332 solver.cpp:218] Iteration 92100 (19.0064 iter/s, 5.26139s/100 iters), loss = 0.00511048
I1001 16:25:12.199746  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00511123 (* 1 = 0.00511123 loss)
I1001 16:25:12.199753  5332 sgd_solver.cpp:105] Iteration 92100, lr = 0.001
I1001 16:25:17.455128  5332 solver.cpp:218] Iteration 92200 (19.0282 iter/s, 5.25536s/100 iters), loss = 0.00913983
I1001 16:25:17.455159  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00914059 (* 1 = 0.00914059 loss)
I1001 16:25:17.455165  5332 sgd_solver.cpp:105] Iteration 92200, lr = 0.001
I1001 16:25:22.702873  5332 solver.cpp:218] Iteration 92300 (19.056 iter/s, 5.24769s/100 iters), loss = 0.00375303
I1001 16:25:22.702913  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00375379 (* 1 = 0.00375379 loss)
I1001 16:25:22.702919  5332 sgd_solver.cpp:105] Iteration 92300, lr = 0.001
I1001 16:25:27.959771  5332 solver.cpp:218] Iteration 92400 (19.0228 iter/s, 5.25684s/100 iters), loss = 0.00269186
I1001 16:25:27.959899  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00269262 (* 1 = 0.00269262 loss)
I1001 16:25:27.959918  5332 sgd_solver.cpp:105] Iteration 92400, lr = 0.001
I1001 16:25:32.953327  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:25:33.163285  5332 solver.cpp:330] Iteration 92500, Testing net (#0)
I1001 16:25:34.356681  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:25:34.406972  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9087
I1001 16:25:34.407006  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35881 (* 1 = 0.35881 loss)
I1001 16:25:34.459280  5332 solver.cpp:218] Iteration 92500 (15.3861 iter/s, 6.49936s/100 iters), loss = 0.00497922
I1001 16:25:34.459307  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00497998 (* 1 = 0.00497998 loss)
I1001 16:25:34.459314  5332 sgd_solver.cpp:105] Iteration 92500, lr = 0.001
I1001 16:25:39.721249  5332 solver.cpp:218] Iteration 92600 (19.0045 iter/s, 5.26192s/100 iters), loss = 0.00315957
I1001 16:25:39.721289  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00316033 (* 1 = 0.00316033 loss)
I1001 16:25:39.721295  5332 sgd_solver.cpp:105] Iteration 92600, lr = 0.001
I1001 16:25:44.980782  5332 solver.cpp:218] Iteration 92700 (19.0133 iter/s, 5.25947s/100 iters), loss = 0.00397691
I1001 16:25:44.980823  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00397767 (* 1 = 0.00397767 loss)
I1001 16:25:44.980829  5332 sgd_solver.cpp:105] Iteration 92700, lr = 0.001
I1001 16:25:50.242532  5332 solver.cpp:218] Iteration 92800 (19.0053 iter/s, 5.26168s/100 iters), loss = 0.00403484
I1001 16:25:50.242575  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0040356 (* 1 = 0.0040356 loss)
I1001 16:25:50.242583  5332 sgd_solver.cpp:105] Iteration 92800, lr = 0.001
I1001 16:25:55.499635  5332 solver.cpp:218] Iteration 92900 (19.0221 iter/s, 5.25704s/100 iters), loss = 0.00494583
I1001 16:25:55.499676  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00494659 (* 1 = 0.00494659 loss)
I1001 16:25:55.499682  5332 sgd_solver.cpp:105] Iteration 92900, lr = 0.001
I1001 16:26:00.504364  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:26:00.714359  5332 solver.cpp:330] Iteration 93000, Testing net (#0)
I1001 16:26:01.905764  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:26:01.956466  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9091
I1001 16:26:01.956493  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357407 (* 1 = 0.357407 loss)
I1001 16:26:02.010572  5332 solver.cpp:218] Iteration 93000 (15.3589 iter/s, 6.51086s/100 iters), loss = 0.00236956
I1001 16:26:02.010637  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00237032 (* 1 = 0.00237032 loss)
I1001 16:26:02.010644  5332 sgd_solver.cpp:105] Iteration 93000, lr = 0.001
I1001 16:26:07.267741  5332 solver.cpp:218] Iteration 93100 (19.0221 iter/s, 5.25704s/100 iters), loss = 0.02143
I1001 16:26:07.267778  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0214308 (* 1 = 0.0214308 loss)
I1001 16:26:07.267796  5332 sgd_solver.cpp:105] Iteration 93100, lr = 0.001
I1001 16:26:12.526393  5332 solver.cpp:218] Iteration 93200 (19.0165 iter/s, 5.25859s/100 iters), loss = 0.00898987
I1001 16:26:12.526424  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00899063 (* 1 = 0.00899063 loss)
I1001 16:26:12.526442  5332 sgd_solver.cpp:105] Iteration 93200, lr = 0.001
I1001 16:26:17.784952  5332 solver.cpp:218] Iteration 93300 (19.0168 iter/s, 5.25851s/100 iters), loss = 0.00686886
I1001 16:26:17.784981  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00686962 (* 1 = 0.00686962 loss)
I1001 16:26:17.784987  5332 sgd_solver.cpp:105] Iteration 93300, lr = 0.001
I1001 16:26:23.042290  5332 solver.cpp:218] Iteration 93400 (19.0212 iter/s, 5.25729s/100 iters), loss = 0.00387244
I1001 16:26:23.042325  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0038732 (* 1 = 0.0038732 loss)
I1001 16:26:23.042331  5332 sgd_solver.cpp:105] Iteration 93400, lr = 0.001
I1001 16:26:28.036948  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:26:28.246105  5332 solver.cpp:330] Iteration 93500, Testing net (#0)
I1001 16:26:29.451267  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:26:29.501382  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9086
I1001 16:26:29.501407  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35835 (* 1 = 0.35835 loss)
I1001 16:26:29.553757  5332 solver.cpp:218] Iteration 93500 (15.3577 iter/s, 6.51141s/100 iters), loss = 0.00571734
I1001 16:26:29.553786  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00571811 (* 1 = 0.00571811 loss)
I1001 16:26:29.553793  5332 sgd_solver.cpp:105] Iteration 93500, lr = 0.001
I1001 16:26:34.814272  5332 solver.cpp:218] Iteration 93600 (19.0097 iter/s, 5.26046s/100 iters), loss = 0.00210843
I1001 16:26:34.814401  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0021092 (* 1 = 0.0021092 loss)
I1001 16:26:34.814410  5332 sgd_solver.cpp:105] Iteration 93600, lr = 0.001
I1001 16:26:40.084560  5332 solver.cpp:218] Iteration 93700 (18.9748 iter/s, 5.27015s/100 iters), loss = 0.00287738
I1001 16:26:40.084591  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00287814 (* 1 = 0.00287814 loss)
I1001 16:26:40.084607  5332 sgd_solver.cpp:105] Iteration 93700, lr = 0.001
I1001 16:26:45.350149  5332 solver.cpp:218] Iteration 93800 (18.9914 iter/s, 5.26554s/100 iters), loss = 0.00536398
I1001 16:26:45.350190  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00536474 (* 1 = 0.00536474 loss)
I1001 16:26:45.350196  5332 sgd_solver.cpp:105] Iteration 93800, lr = 0.001
I1001 16:26:50.614842  5332 solver.cpp:218] Iteration 93900 (18.9947 iter/s, 5.26463s/100 iters), loss = 0.00283967
I1001 16:26:50.614871  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00284043 (* 1 = 0.00284043 loss)
I1001 16:26:50.614877  5332 sgd_solver.cpp:105] Iteration 93900, lr = 0.001
I1001 16:26:55.601289  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:26:55.811780  5332 solver.cpp:330] Iteration 94000, Testing net (#0)
I1001 16:26:57.011603  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:26:57.061765  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9085
I1001 16:26:57.061800  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357354 (* 1 = 0.357354 loss)
I1001 16:26:57.114362  5332 solver.cpp:218] Iteration 94000 (15.3859 iter/s, 6.49947s/100 iters), loss = 0.00352544
I1001 16:26:57.114387  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0035262 (* 1 = 0.0035262 loss)
I1001 16:26:57.114393  5332 sgd_solver.cpp:105] Iteration 94000, lr = 0.001
I1001 16:27:02.374379  5332 solver.cpp:218] Iteration 94100 (19.0115 iter/s, 5.25997s/100 iters), loss = 0.0110675
I1001 16:27:02.374409  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110683 (* 1 = 0.0110683 loss)
I1001 16:27:02.374415  5332 sgd_solver.cpp:105] Iteration 94100, lr = 0.001
I1001 16:27:07.629729  5332 solver.cpp:218] Iteration 94200 (19.0284 iter/s, 5.2553s/100 iters), loss = 0.0019218
I1001 16:27:07.629858  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00192255 (* 1 = 0.00192255 loss)
I1001 16:27:07.629875  5332 sgd_solver.cpp:105] Iteration 94200, lr = 0.001
I1001 16:27:12.890070  5332 solver.cpp:218] Iteration 94300 (19.0107 iter/s, 5.2602s/100 iters), loss = 0.00858295
I1001 16:27:12.890100  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00858371 (* 1 = 0.00858371 loss)
I1001 16:27:12.890105  5332 sgd_solver.cpp:105] Iteration 94300, lr = 0.001
I1001 16:27:18.150712  5332 solver.cpp:218] Iteration 94400 (19.0093 iter/s, 5.26059s/100 iters), loss = 0.0089976
I1001 16:27:18.150741  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00899836 (* 1 = 0.00899836 loss)
I1001 16:27:18.150758  5332 sgd_solver.cpp:105] Iteration 94400, lr = 0.001
I1001 16:27:23.149024  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:27:23.360649  5332 solver.cpp:330] Iteration 94500, Testing net (#0)
I1001 16:27:24.554381  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:27:24.604954  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9091
I1001 16:27:24.604988  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.359235 (* 1 = 0.359235 loss)
I1001 16:27:24.657716  5332 solver.cpp:218] Iteration 94500 (15.3682 iter/s, 6.50695s/100 iters), loss = 0.0122627
I1001 16:27:24.657742  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0122634 (* 1 = 0.0122634 loss)
I1001 16:27:24.657747  5332 sgd_solver.cpp:105] Iteration 94500, lr = 0.001
I1001 16:27:29.921658  5332 solver.cpp:218] Iteration 94600 (18.9973 iter/s, 5.26389s/100 iters), loss = 0.00955345
I1001 16:27:29.921697  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00955421 (* 1 = 0.00955421 loss)
I1001 16:27:29.921702  5332 sgd_solver.cpp:105] Iteration 94600, lr = 0.001
I1001 16:27:35.183923  5332 solver.cpp:218] Iteration 94700 (19.0035 iter/s, 5.2622s/100 iters), loss = 0.00482208
I1001 16:27:35.183956  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00482284 (* 1 = 0.00482284 loss)
I1001 16:27:35.183974  5332 sgd_solver.cpp:105] Iteration 94700, lr = 0.001
I1001 16:27:40.438128  5332 solver.cpp:218] Iteration 94800 (19.0327 iter/s, 5.25412s/100 iters), loss = 0.00299993
I1001 16:27:40.438220  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00300069 (* 1 = 0.00300069 loss)
I1001 16:27:40.438232  5332 sgd_solver.cpp:105] Iteration 94800, lr = 0.001
I1001 16:27:45.702144  5332 solver.cpp:218] Iteration 94900 (18.9973 iter/s, 5.26392s/100 iters), loss = 0.0249832
I1001 16:27:45.702175  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0249839 (* 1 = 0.0249839 loss)
I1001 16:27:45.702191  5332 sgd_solver.cpp:105] Iteration 94900, lr = 0.001
I1001 16:27:50.704089  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:27:50.913655  5332 solver.cpp:330] Iteration 95000, Testing net (#0)
I1001 16:27:52.105087  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:27:52.155405  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9088
I1001 16:27:52.155439  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358071 (* 1 = 0.358071 loss)
I1001 16:27:52.207859  5332 solver.cpp:218] Iteration 95000 (15.3712 iter/s, 6.50566s/100 iters), loss = 0.00787387
I1001 16:27:52.207897  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00787463 (* 1 = 0.00787463 loss)
I1001 16:27:52.207904  5332 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1001 16:27:57.471036  5332 solver.cpp:218] Iteration 95100 (19.0001 iter/s, 5.26312s/100 iters), loss = 0.00942832
I1001 16:27:57.471065  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00942909 (* 1 = 0.00942909 loss)
I1001 16:27:57.471072  5332 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1001 16:28:02.740105  5332 solver.cpp:218] Iteration 95200 (18.9789 iter/s, 5.26901s/100 iters), loss = 0.00553387
I1001 16:28:02.740136  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00553464 (* 1 = 0.00553464 loss)
I1001 16:28:02.740154  5332 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1001 16:28:08.002918  5332 solver.cpp:218] Iteration 95300 (19.0014 iter/s, 5.26276s/100 iters), loss = 0.00142952
I1001 16:28:08.002955  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00143028 (* 1 = 0.00143028 loss)
I1001 16:28:08.002964  5332 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1001 16:28:13.269995  5332 solver.cpp:218] Iteration 95400 (18.9861 iter/s, 5.26702s/100 iters), loss = 0.00276928
I1001 16:28:13.270136  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00277006 (* 1 = 0.00277006 loss)
I1001 16:28:13.270155  5332 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1001 16:28:18.275499  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:28:18.486019  5332 solver.cpp:330] Iteration 95500, Testing net (#0)
I1001 16:28:19.679427  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:28:19.729640  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9084
I1001 16:28:19.729665  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358672 (* 1 = 0.358672 loss)
I1001 16:28:19.782196  5332 solver.cpp:218] Iteration 95500 (15.3562 iter/s, 6.51204s/100 iters), loss = 0.0126221
I1001 16:28:19.782218  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126229 (* 1 = 0.0126229 loss)
I1001 16:28:19.782224  5332 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1001 16:28:25.044266  5332 solver.cpp:218] Iteration 95600 (19.0041 iter/s, 5.26202s/100 iters), loss = 0.0246544
I1001 16:28:25.044297  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0246552 (* 1 = 0.0246552 loss)
I1001 16:28:25.044304  5332 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1001 16:28:30.301493  5332 solver.cpp:218] Iteration 95700 (19.0216 iter/s, 5.25718s/100 iters), loss = 0.00632463
I1001 16:28:30.301533  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0063254 (* 1 = 0.0063254 loss)
I1001 16:28:30.301539  5332 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1001 16:28:35.558054  5332 solver.cpp:218] Iteration 95800 (19.0241 iter/s, 5.2565s/100 iters), loss = 0.00335423
I1001 16:28:35.558095  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.003355 (* 1 = 0.003355 loss)
I1001 16:28:35.558101  5332 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1001 16:28:40.810741  5332 solver.cpp:218] Iteration 95900 (19.0381 iter/s, 5.25262s/100 iters), loss = 0.00264363
I1001 16:28:40.810789  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00264439 (* 1 = 0.00264439 loss)
I1001 16:28:40.810796  5332 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1001 16:28:45.804867  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:28:46.015156  5332 solver.cpp:330] Iteration 96000, Testing net (#0)
I1001 16:28:47.215348  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:28:47.265465  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9102
I1001 16:28:47.265494  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355719 (* 1 = 0.355719 loss)
I1001 16:28:47.317994  5332 solver.cpp:218] Iteration 96000 (15.3676 iter/s, 6.50718s/100 iters), loss = 0.0106083
I1001 16:28:47.318032  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0106091 (* 1 = 0.0106091 loss)
I1001 16:28:47.318042  5332 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1001 16:28:52.575292  5332 solver.cpp:218] Iteration 96100 (19.0214 iter/s, 5.25725s/100 iters), loss = 0.0049398
I1001 16:28:52.575325  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00494058 (* 1 = 0.00494058 loss)
I1001 16:28:52.575343  5332 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1001 16:28:57.838815  5332 solver.cpp:218] Iteration 96200 (18.9989 iter/s, 5.26347s/100 iters), loss = 0.0146572
I1001 16:28:57.838847  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0146579 (* 1 = 0.0146579 loss)
I1001 16:28:57.838865  5332 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1001 16:29:03.104307  5332 solver.cpp:218] Iteration 96300 (18.9918 iter/s, 5.26544s/100 iters), loss = 0.00581214
I1001 16:29:03.104339  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00581291 (* 1 = 0.00581291 loss)
I1001 16:29:03.104357  5332 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1001 16:29:08.368868  5332 solver.cpp:218] Iteration 96400 (18.9951 iter/s, 5.26451s/100 iters), loss = 0.00487483
I1001 16:29:08.368901  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0048756 (* 1 = 0.0048756 loss)
I1001 16:29:08.368909  5332 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1001 16:29:13.364007  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:29:13.574949  5332 solver.cpp:330] Iteration 96500, Testing net (#0)
I1001 16:29:14.776808  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:29:14.826942  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9099
I1001 16:29:14.826977  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355235 (* 1 = 0.355235 loss)
I1001 16:29:14.879582  5332 solver.cpp:218] Iteration 96500 (15.3594 iter/s, 6.51066s/100 iters), loss = 0.0151718
I1001 16:29:14.879611  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0151726 (* 1 = 0.0151726 loss)
I1001 16:29:14.879618  5332 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1001 16:29:20.135799  5332 solver.cpp:218] Iteration 96600 (19.0253 iter/s, 5.25616s/100 iters), loss = 0.0050885
I1001 16:29:20.135994  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00508926 (* 1 = 0.00508926 loss)
I1001 16:29:20.136006  5332 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1001 16:29:25.390259  5332 solver.cpp:218] Iteration 96700 (19.0322 iter/s, 5.25426s/100 iters), loss = 0.00452074
I1001 16:29:25.390291  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00452151 (* 1 = 0.00452151 loss)
I1001 16:29:25.390311  5332 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1001 16:29:30.649719  5332 solver.cpp:218] Iteration 96800 (19.0135 iter/s, 5.25941s/100 iters), loss = 0.00375061
I1001 16:29:30.649752  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00375138 (* 1 = 0.00375138 loss)
I1001 16:29:30.649770  5332 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1001 16:29:35.906419  5332 solver.cpp:218] Iteration 96900 (19.0235 iter/s, 5.25665s/100 iters), loss = 0.00783721
I1001 16:29:35.906450  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00783798 (* 1 = 0.00783798 loss)
I1001 16:29:35.906458  5332 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1001 16:29:40.896163  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:29:41.112020  5332 solver.cpp:330] Iteration 97000, Testing net (#0)
I1001 16:29:42.307059  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:29:42.357450  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9096
I1001 16:29:42.357477  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35839 (* 1 = 0.35839 loss)
I1001 16:29:42.410221  5332 solver.cpp:218] Iteration 97000 (15.3757 iter/s, 6.50375s/100 iters), loss = 0.00504522
I1001 16:29:42.410249  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00504599 (* 1 = 0.00504599 loss)
I1001 16:29:42.410259  5332 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1001 16:29:47.676823  5332 solver.cpp:218] Iteration 97100 (18.9877 iter/s, 5.26655s/100 iters), loss = 0.00442078
I1001 16:29:47.676863  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00442155 (* 1 = 0.00442155 loss)
I1001 16:29:47.676870  5332 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1001 16:29:52.933614  5332 solver.cpp:218] Iteration 97200 (19.0232 iter/s, 5.25673s/100 iters), loss = 0.00964792
I1001 16:29:52.933765  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00964868 (* 1 = 0.00964868 loss)
I1001 16:29:52.933785  5332 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1001 16:29:58.197346  5332 solver.cpp:218] Iteration 97300 (18.9987 iter/s, 5.26353s/100 iters), loss = 0.00510439
I1001 16:29:58.197386  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00510516 (* 1 = 0.00510516 loss)
I1001 16:29:58.197391  5332 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1001 16:30:03.457526  5332 solver.cpp:218] Iteration 97400 (19.011 iter/s, 5.26012s/100 iters), loss = 0.0113195
I1001 16:30:03.457556  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0113202 (* 1 = 0.0113202 loss)
I1001 16:30:03.457562  5332 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1001 16:30:08.452369  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:30:08.661978  5332 solver.cpp:330] Iteration 97500, Testing net (#0)
I1001 16:30:09.855484  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:30:09.905380  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9084
I1001 16:30:09.905414  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35723 (* 1 = 0.35723 loss)
I1001 16:30:09.958246  5332 solver.cpp:218] Iteration 97500 (15.383 iter/s, 6.50067s/100 iters), loss = 0.00348432
I1001 16:30:09.958269  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00348509 (* 1 = 0.00348509 loss)
I1001 16:30:09.958276  5332 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1001 16:30:15.215876  5332 solver.cpp:218] Iteration 97600 (19.0201 iter/s, 5.25758s/100 iters), loss = 0.00402185
I1001 16:30:15.215916  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00402262 (* 1 = 0.00402262 loss)
I1001 16:30:15.215922  5332 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1001 16:30:20.475750  5332 solver.cpp:218] Iteration 97700 (19.0121 iter/s, 5.25981s/100 iters), loss = 0.00452797
I1001 16:30:20.475780  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00452875 (* 1 = 0.00452875 loss)
I1001 16:30:20.475786  5332 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1001 16:30:25.731215  5332 solver.cpp:218] Iteration 97800 (19.028 iter/s, 5.25541s/100 iters), loss = 0.00820819
I1001 16:30:25.731330  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00820896 (* 1 = 0.00820896 loss)
I1001 16:30:25.731345  5332 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1001 16:30:30.998910  5332 solver.cpp:218] Iteration 97900 (18.9841 iter/s, 5.26756s/100 iters), loss = 0.0121499
I1001 16:30:30.998950  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121507 (* 1 = 0.0121507 loss)
I1001 16:30:30.998955  5332 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1001 16:30:36.000376  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:30:36.210700  5332 solver.cpp:330] Iteration 98000, Testing net (#0)
I1001 16:30:37.401861  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:30:37.451969  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9096
I1001 16:30:37.451993  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358057 (* 1 = 0.358057 loss)
I1001 16:30:37.504525  5332 solver.cpp:218] Iteration 98000 (15.3715 iter/s, 6.50555s/100 iters), loss = 0.00536009
I1001 16:30:37.504554  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00536086 (* 1 = 0.00536086 loss)
I1001 16:30:37.504560  5332 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1001 16:30:42.771440  5332 solver.cpp:218] Iteration 98100 (18.9866 iter/s, 5.26686s/100 iters), loss = 0.00458565
I1001 16:30:42.771468  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00458642 (* 1 = 0.00458642 loss)
I1001 16:30:42.771484  5332 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1001 16:30:48.030426  5332 solver.cpp:218] Iteration 98200 (19.0153 iter/s, 5.25894s/100 iters), loss = 0.0229063
I1001 16:30:48.030455  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0229071 (* 1 = 0.0229071 loss)
I1001 16:30:48.030462  5332 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1001 16:30:53.286564  5332 solver.cpp:218] Iteration 98300 (19.0256 iter/s, 5.25608s/100 iters), loss = 0.00840803
I1001 16:30:53.286597  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00840881 (* 1 = 0.00840881 loss)
I1001 16:30:53.286603  5332 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1001 16:30:58.538380  5332 solver.cpp:218] Iteration 98400 (19.0412 iter/s, 5.25176s/100 iters), loss = 0.00481684
I1001 16:30:58.538528  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00481762 (* 1 = 0.00481762 loss)
I1001 16:30:58.538548  5332 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1001 16:31:03.534935  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:31:03.744462  5332 solver.cpp:330] Iteration 98500, Testing net (#0)
I1001 16:31:04.936888  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:31:04.987267  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9085
I1001 16:31:04.987293  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.360009 (* 1 = 0.360009 loss)
I1001 16:31:05.040999  5332 solver.cpp:218] Iteration 98500 (15.3788 iter/s, 6.50247s/100 iters), loss = 0.00974424
I1001 16:31:05.041043  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00974501 (* 1 = 0.00974501 loss)
I1001 16:31:05.041050  5332 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1001 16:31:10.291355  5332 solver.cpp:218] Iteration 98600 (19.0466 iter/s, 5.25029s/100 iters), loss = 0.00975841
I1001 16:31:10.291394  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00975918 (* 1 = 0.00975918 loss)
I1001 16:31:10.291401  5332 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1001 16:31:15.553884  5332 solver.cpp:218] Iteration 98700 (19.0025 iter/s, 5.26247s/100 iters), loss = 0.00495539
I1001 16:31:15.553925  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00495616 (* 1 = 0.00495616 loss)
I1001 16:31:15.553930  5332 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1001 16:31:20.816786  5332 solver.cpp:218] Iteration 98800 (19.0011 iter/s, 5.26284s/100 iters), loss = 0.00626805
I1001 16:31:20.816817  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00626882 (* 1 = 0.00626882 loss)
I1001 16:31:20.816835  5332 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1001 16:31:26.073791  5332 solver.cpp:218] Iteration 98900 (19.0224 iter/s, 5.25695s/100 iters), loss = 0.00233686
I1001 16:31:26.073829  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00233763 (* 1 = 0.00233763 loss)
I1001 16:31:26.073837  5332 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1001 16:31:31.071621  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:31:31.281836  5332 solver.cpp:330] Iteration 99000, Testing net (#0)
I1001 16:31:32.482777  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:31:32.533072  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9087
I1001 16:31:32.533108  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.360302 (* 1 = 0.360302 loss)
I1001 16:31:32.585901  5332 solver.cpp:218] Iteration 99000 (15.3561 iter/s, 6.51206s/100 iters), loss = 0.0107117
I1001 16:31:32.585929  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107125 (* 1 = 0.0107125 loss)
I1001 16:31:32.585937  5332 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1001 16:31:37.837241  5332 solver.cpp:218] Iteration 99100 (19.0429 iter/s, 5.25129s/100 iters), loss = 0.00786302
I1001 16:31:37.837271  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0078638 (* 1 = 0.0078638 loss)
I1001 16:31:37.837287  5332 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1001 16:31:43.095734  5332 solver.cpp:218] Iteration 99200 (19.017 iter/s, 5.25844s/100 iters), loss = 0.00954328
I1001 16:31:43.095764  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00954405 (* 1 = 0.00954405 loss)
I1001 16:31:43.095780  5332 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1001 16:31:48.352581  5332 solver.cpp:218] Iteration 99300 (19.023 iter/s, 5.25679s/100 iters), loss = 0.00237117
I1001 16:31:48.352609  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00237194 (* 1 = 0.00237194 loss)
I1001 16:31:48.352614  5332 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1001 16:31:53.611455  5332 solver.cpp:218] Iteration 99400 (19.0157 iter/s, 5.25882s/100 iters), loss = 0.00338832
I1001 16:31:53.611496  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00338909 (* 1 = 0.00338909 loss)
I1001 16:31:53.611502  5332 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1001 16:31:58.601188  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:31:58.811445  5332 solver.cpp:330] Iteration 99500, Testing net (#0)
I1001 16:32:00.011821  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:32:00.062227  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9086
I1001 16:32:00.062261  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.359718 (* 1 = 0.359718 loss)
I1001 16:32:00.115084  5332 solver.cpp:218] Iteration 99500 (15.3762 iter/s, 6.50357s/100 iters), loss = 0.00520529
I1001 16:32:00.115118  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00520605 (* 1 = 0.00520605 loss)
I1001 16:32:00.115125  5332 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1001 16:32:05.377696  5332 solver.cpp:218] Iteration 99600 (19.0022 iter/s, 5.26255s/100 iters), loss = 0.00512736
I1001 16:32:05.377815  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00512812 (* 1 = 0.00512812 loss)
I1001 16:32:05.377835  5332 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1001 16:32:10.633489  5332 solver.cpp:218] Iteration 99700 (19.0271 iter/s, 5.25566s/100 iters), loss = 0.00517772
I1001 16:32:10.633522  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00517849 (* 1 = 0.00517849 loss)
I1001 16:32:10.633539  5332 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1001 16:32:15.898751  5332 solver.cpp:218] Iteration 99800 (18.9926 iter/s, 5.26521s/100 iters), loss = 0.00194411
I1001 16:32:15.898784  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00194488 (* 1 = 0.00194488 loss)
I1001 16:32:15.898802  5332 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1001 16:32:21.171464  5332 solver.cpp:218] Iteration 99900 (18.9658 iter/s, 5.27266s/100 iters), loss = 0.00225387
I1001 16:32:21.171496  5332 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00225464 (* 1 = 0.00225464 loss)
I1001 16:32:21.171515  5332 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1001 16:32:26.175853  5340 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:32:26.387464  5332 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_100000.caffemodel
I1001 16:32:26.392621  5332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res20/res20_penlu_alpha1_beta1_etanostudy_2study_2decay_gauss_iter_100000.solverstate
I1001 16:32:26.406656  5332 solver.cpp:310] Iteration 100000, loss = 0.00544205
I1001 16:32:26.406680  5332 solver.cpp:330] Iteration 100000, Testing net (#0)
I1001 16:32:27.598220  5341 data_layer.cpp:73] Restarting data prefetching from start.
I1001 16:32:27.648648  5332 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9088
I1001 16:32:27.648675  5332 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361633 (* 1 = 0.361633 loss)
I1001 16:32:27.648681  5332 solver.cpp:315] Optimization Done.
I1001 16:32:27.648684  5332 caffe.cpp:259] Optimization Done.
