I1013 11:27:17.540714 12690 caffe.cpp:218] Using GPUs 0
I1013 11:27:17.564878 12690 caffe.cpp:223] GPU 0: GeForce GTX 1080
I1013 11:27:17.791599 12690 solver.cpp:44] Initializing solver from parameters: 
test_iter: 200
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 100000
snapshot_prefix: "xn/PENLU/snapshot/WRN/WRN_penlu_0.25_2study_2decay"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/PENLU/neural/WRN/WRN_penlu_msra.prototxt"
train_state {
  level: 0
  stage: ""
}
test_initialization: false
stepvalue: 50000
stepvalue: 80000
type: "Nesterov"
I1013 11:27:17.791720 12690 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/PENLU/neural/WRN/WRN_penlu_msra.prototxt
I1013 11:27:17.793293 12690 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/WRN/WRN_penlu_msra.prototxt
I1013 11:27:17.793303 12690 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1013 11:27:17.793431 12690 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1013 11:27:17.793501 12690 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1013 11:27:17.793965 12690 net.cpp:51] Initializing net from parameters: 
name: "wrn_28_10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar100/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar100/cifar100_train_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution4"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution4"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Eltwise1"
  top: "Eltwise1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution5"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Convolution5"
  top: "Convolution6"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution6"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Eltwise2"
  top: "Eltwise2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution7"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Convolution7"
  top: "Convolution8"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Eltwise3"
  top: "Eltwise3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution10"
  bottom: "Eltwise3"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution13"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution13"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Eltwise5"
  top: "Eltwise5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution14"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Eltwise6"
  top: "Eltwise6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution17"
  bottom: "Eltwise6"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Eltwise7"
  top: "Eltwise7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution22"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Convolution22"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Eltwise9"
  top: "Eltwise9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution23"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution24"
  bottom: "Eltwise9"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Eltwise10"
  top: "Eltwise10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution25"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution26"
  bottom: "Eltwise10"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Eltwise11"
  top: "Eltwise11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Eltwise11"
  top: "Eltwise11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution27"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu22"
  type: "PENLU"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Convolution28"
  bottom: "Eltwise11"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise12"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 100
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "SoftmaxWithLoss1"
}
I1013 11:27:17.794490 12690 layer_factory.hpp:77] Creating layer cifar
I1013 11:27:17.794584 12690 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar100/cifar100_train_lmdb
I1013 11:27:17.794610 12690 net.cpp:84] Creating Layer cifar
I1013 11:27:17.794616 12690 net.cpp:380] cifar -> data
I1013 11:27:17.794638 12690 net.cpp:380] cifar -> label
I1013 11:27:17.794649 12690 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar100/mean.binaryproto
I1013 11:27:17.796033 12690 data_layer.cpp:45] output data size: 50,3,28,28
I1013 11:27:17.797256 12690 net.cpp:122] Setting up cifar
I1013 11:27:17.797267 12690 net.cpp:129] Top shape: 50 3 28 28 (117600)
I1013 11:27:17.797271 12690 net.cpp:129] Top shape: 50 (50)
I1013 11:27:17.797273 12690 net.cpp:137] Memory required for data: 470600
I1013 11:27:17.797279 12690 layer_factory.hpp:77] Creating layer Convolution1
I1013 11:27:17.797294 12690 net.cpp:84] Creating Layer Convolution1
I1013 11:27:17.797297 12690 net.cpp:406] Convolution1 <- data
I1013 11:27:17.797305 12690 net.cpp:380] Convolution1 -> Convolution1
I1013 11:27:17.942636 12690 net.cpp:122] Setting up Convolution1
I1013 11:27:17.942658 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:17.942662 12690 net.cpp:137] Memory required for data: 2979400
I1013 11:27:17.942677 12690 layer_factory.hpp:77] Creating layer BatchNorm1
I1013 11:27:17.942698 12690 net.cpp:84] Creating Layer BatchNorm1
I1013 11:27:17.942713 12690 net.cpp:406] BatchNorm1 <- Convolution1
I1013 11:27:17.942718 12690 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1013 11:27:17.942886 12690 net.cpp:122] Setting up BatchNorm1
I1013 11:27:17.942893 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:17.942894 12690 net.cpp:137] Memory required for data: 5488200
I1013 11:27:17.942901 12690 layer_factory.hpp:77] Creating layer Scale1
I1013 11:27:17.942910 12690 net.cpp:84] Creating Layer Scale1
I1013 11:27:17.942924 12690 net.cpp:406] Scale1 <- Convolution1
I1013 11:27:17.942929 12690 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1013 11:27:17.942986 12690 layer_factory.hpp:77] Creating layer Scale1
I1013 11:27:17.943123 12690 net.cpp:122] Setting up Scale1
I1013 11:27:17.943128 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:17.943130 12690 net.cpp:137] Memory required for data: 7997000
I1013 11:27:17.943135 12690 layer_factory.hpp:77] Creating layer penlu1
I1013 11:27:17.943143 12690 net.cpp:84] Creating Layer penlu1
I1013 11:27:17.943146 12690 net.cpp:406] penlu1 <- Convolution1
I1013 11:27:17.943159 12690 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1013 11:27:17.943780 12690 net.cpp:122] Setting up penlu1
I1013 11:27:17.943799 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:17.943801 12690 net.cpp:137] Memory required for data: 10505800
I1013 11:27:17.943809 12690 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1013 11:27:17.943826 12690 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1013 11:27:17.943830 12690 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1013 11:27:17.943835 12690 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1013 11:27:17.943840 12690 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1013 11:27:17.943884 12690 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1013 11:27:17.943900 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:17.943903 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:17.943905 12690 net.cpp:137] Memory required for data: 15523400
I1013 11:27:17.943907 12690 layer_factory.hpp:77] Creating layer Convolution2
I1013 11:27:17.943914 12690 net.cpp:84] Creating Layer Convolution2
I1013 11:27:17.943917 12690 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1013 11:27:17.943919 12690 net.cpp:380] Convolution2 -> Convolution2
I1013 11:27:17.945982 12690 net.cpp:122] Setting up Convolution2
I1013 11:27:17.945992 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.945994 12690 net.cpp:137] Memory required for data: 40611400
I1013 11:27:17.945998 12690 layer_factory.hpp:77] Creating layer BatchNorm2
I1013 11:27:17.946004 12690 net.cpp:84] Creating Layer BatchNorm2
I1013 11:27:17.946007 12690 net.cpp:406] BatchNorm2 <- Convolution2
I1013 11:27:17.946010 12690 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1013 11:27:17.946167 12690 net.cpp:122] Setting up BatchNorm2
I1013 11:27:17.946172 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.946174 12690 net.cpp:137] Memory required for data: 65699400
I1013 11:27:17.946179 12690 layer_factory.hpp:77] Creating layer Scale2
I1013 11:27:17.946184 12690 net.cpp:84] Creating Layer Scale2
I1013 11:27:17.946187 12690 net.cpp:406] Scale2 <- Convolution2
I1013 11:27:17.946189 12690 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1013 11:27:17.946249 12690 layer_factory.hpp:77] Creating layer Scale2
I1013 11:27:17.946367 12690 net.cpp:122] Setting up Scale2
I1013 11:27:17.946372 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.946373 12690 net.cpp:137] Memory required for data: 90787400
I1013 11:27:17.946377 12690 layer_factory.hpp:77] Creating layer penlu2
I1013 11:27:17.946382 12690 net.cpp:84] Creating Layer penlu2
I1013 11:27:17.946385 12690 net.cpp:406] penlu2 <- Convolution2
I1013 11:27:17.946389 12690 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1013 11:27:17.947046 12690 net.cpp:122] Setting up penlu2
I1013 11:27:17.947054 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.947057 12690 net.cpp:137] Memory required for data: 115875400
I1013 11:27:17.947063 12690 layer_factory.hpp:77] Creating layer Convolution3
I1013 11:27:17.947070 12690 net.cpp:84] Creating Layer Convolution3
I1013 11:27:17.947083 12690 net.cpp:406] Convolution3 <- Convolution2
I1013 11:27:17.947088 12690 net.cpp:380] Convolution3 -> Convolution3
I1013 11:27:17.953128 12690 net.cpp:122] Setting up Convolution3
I1013 11:27:17.953138 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.953141 12690 net.cpp:137] Memory required for data: 140963400
I1013 11:27:17.953145 12690 layer_factory.hpp:77] Creating layer Convolution4
I1013 11:27:17.953151 12690 net.cpp:84] Creating Layer Convolution4
I1013 11:27:17.953155 12690 net.cpp:406] Convolution4 <- Convolution1_penlu1_0_split_1
I1013 11:27:17.953169 12690 net.cpp:380] Convolution4 -> Convolution4
I1013 11:27:17.954010 12690 net.cpp:122] Setting up Convolution4
I1013 11:27:17.954020 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.954033 12690 net.cpp:137] Memory required for data: 166051400
I1013 11:27:17.954037 12690 layer_factory.hpp:77] Creating layer Eltwise1
I1013 11:27:17.954061 12690 net.cpp:84] Creating Layer Eltwise1
I1013 11:27:17.954064 12690 net.cpp:406] Eltwise1 <- Convolution3
I1013 11:27:17.954067 12690 net.cpp:406] Eltwise1 <- Convolution4
I1013 11:27:17.954072 12690 net.cpp:380] Eltwise1 -> Eltwise1
I1013 11:27:17.954104 12690 net.cpp:122] Setting up Eltwise1
I1013 11:27:17.954118 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.954121 12690 net.cpp:137] Memory required for data: 191139400
I1013 11:27:17.954123 12690 layer_factory.hpp:77] Creating layer BatchNorm3
I1013 11:27:17.954138 12690 net.cpp:84] Creating Layer BatchNorm3
I1013 11:27:17.954140 12690 net.cpp:406] BatchNorm3 <- Eltwise1
I1013 11:27:17.954144 12690 net.cpp:367] BatchNorm3 -> Eltwise1 (in-place)
I1013 11:27:17.954301 12690 net.cpp:122] Setting up BatchNorm3
I1013 11:27:17.954308 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.954319 12690 net.cpp:137] Memory required for data: 216227400
I1013 11:27:17.954325 12690 layer_factory.hpp:77] Creating layer Scale3
I1013 11:27:17.954329 12690 net.cpp:84] Creating Layer Scale3
I1013 11:27:17.954332 12690 net.cpp:406] Scale3 <- Eltwise1
I1013 11:27:17.954335 12690 net.cpp:367] Scale3 -> Eltwise1 (in-place)
I1013 11:27:17.954371 12690 layer_factory.hpp:77] Creating layer Scale3
I1013 11:27:17.954474 12690 net.cpp:122] Setting up Scale3
I1013 11:27:17.954481 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.954483 12690 net.cpp:137] Memory required for data: 241315400
I1013 11:27:17.954496 12690 layer_factory.hpp:77] Creating layer penlu3
I1013 11:27:17.954502 12690 net.cpp:84] Creating Layer penlu3
I1013 11:27:17.954515 12690 net.cpp:406] penlu3 <- Eltwise1
I1013 11:27:17.954519 12690 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1013 11:27:17.954699 12690 net.cpp:122] Setting up penlu3
I1013 11:27:17.954707 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.954710 12690 net.cpp:137] Memory required for data: 266403400
I1013 11:27:17.954718 12690 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1013 11:27:17.954727 12690 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1013 11:27:17.954731 12690 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1013 11:27:17.954738 12690 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1013 11:27:17.954752 12690 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1013 11:27:17.954783 12690 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1013 11:27:17.954790 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.954797 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.954813 12690 net.cpp:137] Memory required for data: 316579400
I1013 11:27:17.954819 12690 layer_factory.hpp:77] Creating layer Convolution5
I1013 11:27:17.954828 12690 net.cpp:84] Creating Layer Convolution5
I1013 11:27:17.954843 12690 net.cpp:406] Convolution5 <- Eltwise1_penlu3_0_split_0
I1013 11:27:17.954852 12690 net.cpp:380] Convolution5 -> Convolution5
I1013 11:27:17.961666 12690 net.cpp:122] Setting up Convolution5
I1013 11:27:17.961679 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.961684 12690 net.cpp:137] Memory required for data: 341667400
I1013 11:27:17.961691 12690 layer_factory.hpp:77] Creating layer BatchNorm4
I1013 11:27:17.961704 12690 net.cpp:84] Creating Layer BatchNorm4
I1013 11:27:17.961709 12690 net.cpp:406] BatchNorm4 <- Convolution5
I1013 11:27:17.961715 12690 net.cpp:367] BatchNorm4 -> Convolution5 (in-place)
I1013 11:27:17.961855 12690 net.cpp:122] Setting up BatchNorm4
I1013 11:27:17.961863 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.961868 12690 net.cpp:137] Memory required for data: 366755400
I1013 11:27:17.961880 12690 layer_factory.hpp:77] Creating layer Scale4
I1013 11:27:17.961889 12690 net.cpp:84] Creating Layer Scale4
I1013 11:27:17.961894 12690 net.cpp:406] Scale4 <- Convolution5
I1013 11:27:17.961912 12690 net.cpp:367] Scale4 -> Convolution5 (in-place)
I1013 11:27:17.961961 12690 layer_factory.hpp:77] Creating layer Scale4
I1013 11:27:17.962074 12690 net.cpp:122] Setting up Scale4
I1013 11:27:17.962081 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.962085 12690 net.cpp:137] Memory required for data: 391843400
I1013 11:27:17.962105 12690 layer_factory.hpp:77] Creating layer penlu4
I1013 11:27:17.962115 12690 net.cpp:84] Creating Layer penlu4
I1013 11:27:17.962119 12690 net.cpp:406] penlu4 <- Convolution5
I1013 11:27:17.962126 12690 net.cpp:367] penlu4 -> Convolution5 (in-place)
I1013 11:27:17.962297 12690 net.cpp:122] Setting up penlu4
I1013 11:27:17.962306 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.962321 12690 net.cpp:137] Memory required for data: 416931400
I1013 11:27:17.962329 12690 layer_factory.hpp:77] Creating layer Convolution6
I1013 11:27:17.962339 12690 net.cpp:84] Creating Layer Convolution6
I1013 11:27:17.962343 12690 net.cpp:406] Convolution6 <- Convolution5
I1013 11:27:17.962352 12690 net.cpp:380] Convolution6 -> Convolution6
I1013 11:27:17.969179 12690 net.cpp:122] Setting up Convolution6
I1013 11:27:17.969192 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.969197 12690 net.cpp:137] Memory required for data: 442019400
I1013 11:27:17.969202 12690 layer_factory.hpp:77] Creating layer Eltwise2
I1013 11:27:17.969213 12690 net.cpp:84] Creating Layer Eltwise2
I1013 11:27:17.969218 12690 net.cpp:406] Eltwise2 <- Convolution6
I1013 11:27:17.969223 12690 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1013 11:27:17.969231 12690 net.cpp:380] Eltwise2 -> Eltwise2
I1013 11:27:17.969257 12690 net.cpp:122] Setting up Eltwise2
I1013 11:27:17.969264 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.969270 12690 net.cpp:137] Memory required for data: 467107400
I1013 11:27:17.969275 12690 layer_factory.hpp:77] Creating layer BatchNorm5
I1013 11:27:17.969283 12690 net.cpp:84] Creating Layer BatchNorm5
I1013 11:27:17.969288 12690 net.cpp:406] BatchNorm5 <- Eltwise2
I1013 11:27:17.969295 12690 net.cpp:367] BatchNorm5 -> Eltwise2 (in-place)
I1013 11:27:17.969430 12690 net.cpp:122] Setting up BatchNorm5
I1013 11:27:17.969437 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.969444 12690 net.cpp:137] Memory required for data: 492195400
I1013 11:27:17.969452 12690 layer_factory.hpp:77] Creating layer Scale5
I1013 11:27:17.969458 12690 net.cpp:84] Creating Layer Scale5
I1013 11:27:17.969465 12690 net.cpp:406] Scale5 <- Eltwise2
I1013 11:27:17.969472 12690 net.cpp:367] Scale5 -> Eltwise2 (in-place)
I1013 11:27:17.969504 12690 layer_factory.hpp:77] Creating layer Scale5
I1013 11:27:17.969597 12690 net.cpp:122] Setting up Scale5
I1013 11:27:17.969604 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.969619 12690 net.cpp:137] Memory required for data: 517283400
I1013 11:27:17.969626 12690 layer_factory.hpp:77] Creating layer penlu5
I1013 11:27:17.969636 12690 net.cpp:84] Creating Layer penlu5
I1013 11:27:17.969641 12690 net.cpp:406] penlu5 <- Eltwise2
I1013 11:27:17.969648 12690 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1013 11:27:17.969820 12690 net.cpp:122] Setting up penlu5
I1013 11:27:17.969830 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.969835 12690 net.cpp:137] Memory required for data: 542371400
I1013 11:27:17.969842 12690 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1013 11:27:17.969849 12690 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1013 11:27:17.969854 12690 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1013 11:27:17.969861 12690 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1013 11:27:17.969868 12690 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1013 11:27:17.969897 12690 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1013 11:27:17.969904 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.969910 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.969920 12690 net.cpp:137] Memory required for data: 592547400
I1013 11:27:17.969925 12690 layer_factory.hpp:77] Creating layer Convolution7
I1013 11:27:17.969944 12690 net.cpp:84] Creating Layer Convolution7
I1013 11:27:17.969950 12690 net.cpp:406] Convolution7 <- Eltwise2_penlu5_0_split_0
I1013 11:27:17.969957 12690 net.cpp:380] Convolution7 -> Convolution7
I1013 11:27:17.976423 12690 net.cpp:122] Setting up Convolution7
I1013 11:27:17.976438 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.976442 12690 net.cpp:137] Memory required for data: 617635400
I1013 11:27:17.976459 12690 layer_factory.hpp:77] Creating layer BatchNorm6
I1013 11:27:17.976478 12690 net.cpp:84] Creating Layer BatchNorm6
I1013 11:27:17.976492 12690 net.cpp:406] BatchNorm6 <- Convolution7
I1013 11:27:17.976511 12690 net.cpp:367] BatchNorm6 -> Convolution7 (in-place)
I1013 11:27:17.976691 12690 net.cpp:122] Setting up BatchNorm6
I1013 11:27:17.976697 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.976701 12690 net.cpp:137] Memory required for data: 642723400
I1013 11:27:17.976722 12690 layer_factory.hpp:77] Creating layer Scale6
I1013 11:27:17.976743 12690 net.cpp:84] Creating Layer Scale6
I1013 11:27:17.976763 12690 net.cpp:406] Scale6 <- Convolution7
I1013 11:27:17.976785 12690 net.cpp:367] Scale6 -> Convolution7 (in-place)
I1013 11:27:17.976840 12690 layer_factory.hpp:77] Creating layer Scale6
I1013 11:27:17.976924 12690 net.cpp:122] Setting up Scale6
I1013 11:27:17.976932 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.976935 12690 net.cpp:137] Memory required for data: 667811400
I1013 11:27:17.976943 12690 layer_factory.hpp:77] Creating layer penlu6
I1013 11:27:17.976953 12690 net.cpp:84] Creating Layer penlu6
I1013 11:27:17.976956 12690 net.cpp:406] penlu6 <- Convolution7
I1013 11:27:17.976964 12690 net.cpp:367] penlu6 -> Convolution7 (in-place)
I1013 11:27:17.977128 12690 net.cpp:122] Setting up penlu6
I1013 11:27:17.977134 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.977138 12690 net.cpp:137] Memory required for data: 692899400
I1013 11:27:17.977147 12690 layer_factory.hpp:77] Creating layer Convolution8
I1013 11:27:17.977159 12690 net.cpp:84] Creating Layer Convolution8
I1013 11:27:17.977162 12690 net.cpp:406] Convolution8 <- Convolution7
I1013 11:27:17.977169 12690 net.cpp:380] Convolution8 -> Convolution8
I1013 11:27:17.983688 12690 net.cpp:122] Setting up Convolution8
I1013 11:27:17.983700 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.983705 12690 net.cpp:137] Memory required for data: 717987400
I1013 11:27:17.983711 12690 layer_factory.hpp:77] Creating layer Eltwise3
I1013 11:27:17.983724 12690 net.cpp:84] Creating Layer Eltwise3
I1013 11:27:17.983729 12690 net.cpp:406] Eltwise3 <- Convolution8
I1013 11:27:17.983736 12690 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1013 11:27:17.983742 12690 net.cpp:380] Eltwise3 -> Eltwise3
I1013 11:27:17.983767 12690 net.cpp:122] Setting up Eltwise3
I1013 11:27:17.983774 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.983779 12690 net.cpp:137] Memory required for data: 743075400
I1013 11:27:17.983783 12690 layer_factory.hpp:77] Creating layer BatchNorm7
I1013 11:27:17.983791 12690 net.cpp:84] Creating Layer BatchNorm7
I1013 11:27:17.983796 12690 net.cpp:406] BatchNorm7 <- Eltwise3
I1013 11:27:17.983801 12690 net.cpp:367] BatchNorm7 -> Eltwise3 (in-place)
I1013 11:27:17.983933 12690 net.cpp:122] Setting up BatchNorm7
I1013 11:27:17.983940 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.983944 12690 net.cpp:137] Memory required for data: 768163400
I1013 11:27:17.983953 12690 layer_factory.hpp:77] Creating layer Scale7
I1013 11:27:17.983960 12690 net.cpp:84] Creating Layer Scale7
I1013 11:27:17.983965 12690 net.cpp:406] Scale7 <- Eltwise3
I1013 11:27:17.983970 12690 net.cpp:367] Scale7 -> Eltwise3 (in-place)
I1013 11:27:17.984002 12690 layer_factory.hpp:77] Creating layer Scale7
I1013 11:27:17.984084 12690 net.cpp:122] Setting up Scale7
I1013 11:27:17.984092 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.984097 12690 net.cpp:137] Memory required for data: 793251400
I1013 11:27:17.984114 12690 layer_factory.hpp:77] Creating layer penlu7
I1013 11:27:17.984125 12690 net.cpp:84] Creating Layer penlu7
I1013 11:27:17.984129 12690 net.cpp:406] penlu7 <- Eltwise3
I1013 11:27:17.984136 12690 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1013 11:27:17.984293 12690 net.cpp:122] Setting up penlu7
I1013 11:27:17.984300 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.984304 12690 net.cpp:137] Memory required for data: 818339400
I1013 11:27:17.984321 12690 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1013 11:27:17.984329 12690 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1013 11:27:17.984334 12690 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1013 11:27:17.984340 12690 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1013 11:27:17.984349 12690 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1013 11:27:17.984376 12690 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1013 11:27:17.984383 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.984390 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.984395 12690 net.cpp:137] Memory required for data: 868515400
I1013 11:27:17.984398 12690 layer_factory.hpp:77] Creating layer Convolution9
I1013 11:27:17.984407 12690 net.cpp:84] Creating Layer Convolution9
I1013 11:27:17.984411 12690 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_0
I1013 11:27:17.984417 12690 net.cpp:380] Convolution9 -> Convolution9
I1013 11:27:17.990947 12690 net.cpp:122] Setting up Convolution9
I1013 11:27:17.990958 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.990963 12690 net.cpp:137] Memory required for data: 893603400
I1013 11:27:17.990969 12690 layer_factory.hpp:77] Creating layer BatchNorm8
I1013 11:27:17.990978 12690 net.cpp:84] Creating Layer BatchNorm8
I1013 11:27:17.990983 12690 net.cpp:406] BatchNorm8 <- Convolution9
I1013 11:27:17.990990 12690 net.cpp:367] BatchNorm8 -> Convolution9 (in-place)
I1013 11:27:17.991125 12690 net.cpp:122] Setting up BatchNorm8
I1013 11:27:17.991132 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.991137 12690 net.cpp:137] Memory required for data: 918691400
I1013 11:27:17.991145 12690 layer_factory.hpp:77] Creating layer Scale8
I1013 11:27:17.991153 12690 net.cpp:84] Creating Layer Scale8
I1013 11:27:17.991158 12690 net.cpp:406] Scale8 <- Convolution9
I1013 11:27:17.991165 12690 net.cpp:367] Scale8 -> Convolution9 (in-place)
I1013 11:27:17.991195 12690 layer_factory.hpp:77] Creating layer Scale8
I1013 11:27:17.991277 12690 net.cpp:122] Setting up Scale8
I1013 11:27:17.991284 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.991288 12690 net.cpp:137] Memory required for data: 943779400
I1013 11:27:17.991297 12690 layer_factory.hpp:77] Creating layer penlu8
I1013 11:27:17.991305 12690 net.cpp:84] Creating Layer penlu8
I1013 11:27:17.991309 12690 net.cpp:406] penlu8 <- Convolution9
I1013 11:27:17.991317 12690 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1013 11:27:17.991472 12690 net.cpp:122] Setting up penlu8
I1013 11:27:17.991479 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.991483 12690 net.cpp:137] Memory required for data: 968867400
I1013 11:27:17.991492 12690 layer_factory.hpp:77] Creating layer Convolution10
I1013 11:27:17.991502 12690 net.cpp:84] Creating Layer Convolution10
I1013 11:27:17.991505 12690 net.cpp:406] Convolution10 <- Convolution9
I1013 11:27:17.991513 12690 net.cpp:380] Convolution10 -> Convolution10
I1013 11:27:17.998041 12690 net.cpp:122] Setting up Convolution10
I1013 11:27:17.998054 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.998057 12690 net.cpp:137] Memory required for data: 993955400
I1013 11:27:17.998064 12690 layer_factory.hpp:77] Creating layer Eltwise4
I1013 11:27:17.998075 12690 net.cpp:84] Creating Layer Eltwise4
I1013 11:27:17.998080 12690 net.cpp:406] Eltwise4 <- Convolution10
I1013 11:27:17.998085 12690 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I1013 11:27:17.998092 12690 net.cpp:380] Eltwise4 -> Eltwise4
I1013 11:27:17.998126 12690 net.cpp:122] Setting up Eltwise4
I1013 11:27:17.998133 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.998137 12690 net.cpp:137] Memory required for data: 1019043400
I1013 11:27:17.998142 12690 layer_factory.hpp:77] Creating layer Eltwise4_Eltwise4_0_split
I1013 11:27:17.998150 12690 net.cpp:84] Creating Layer Eltwise4_Eltwise4_0_split
I1013 11:27:17.998154 12690 net.cpp:406] Eltwise4_Eltwise4_0_split <- Eltwise4
I1013 11:27:17.998162 12690 net.cpp:380] Eltwise4_Eltwise4_0_split -> Eltwise4_Eltwise4_0_split_0
I1013 11:27:17.998168 12690 net.cpp:380] Eltwise4_Eltwise4_0_split -> Eltwise4_Eltwise4_0_split_1
I1013 11:27:17.998198 12690 net.cpp:122] Setting up Eltwise4_Eltwise4_0_split
I1013 11:27:17.998204 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.998211 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:17.998216 12690 net.cpp:137] Memory required for data: 1069219400
I1013 11:27:17.998220 12690 layer_factory.hpp:77] Creating layer Convolution11
I1013 11:27:17.998230 12690 net.cpp:84] Creating Layer Convolution11
I1013 11:27:17.998235 12690 net.cpp:406] Convolution11 <- Eltwise4_Eltwise4_0_split_0
I1013 11:27:17.998244 12690 net.cpp:380] Convolution11 -> Convolution11
I1013 11:27:18.010120 12690 net.cpp:122] Setting up Convolution11
I1013 11:27:18.010140 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.010145 12690 net.cpp:137] Memory required for data: 1081763400
I1013 11:27:18.010152 12690 layer_factory.hpp:77] Creating layer BatchNorm9
I1013 11:27:18.010164 12690 net.cpp:84] Creating Layer BatchNorm9
I1013 11:27:18.010169 12690 net.cpp:406] BatchNorm9 <- Convolution11
I1013 11:27:18.010177 12690 net.cpp:367] BatchNorm9 -> Convolution11 (in-place)
I1013 11:27:18.010339 12690 net.cpp:122] Setting up BatchNorm9
I1013 11:27:18.010346 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.010350 12690 net.cpp:137] Memory required for data: 1094307400
I1013 11:27:18.010359 12690 layer_factory.hpp:77] Creating layer Scale9
I1013 11:27:18.010366 12690 net.cpp:84] Creating Layer Scale9
I1013 11:27:18.010372 12690 net.cpp:406] Scale9 <- Convolution11
I1013 11:27:18.010377 12690 net.cpp:367] Scale9 -> Convolution11 (in-place)
I1013 11:27:18.010416 12690 layer_factory.hpp:77] Creating layer Scale9
I1013 11:27:18.010500 12690 net.cpp:122] Setting up Scale9
I1013 11:27:18.010507 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.010511 12690 net.cpp:137] Memory required for data: 1106851400
I1013 11:27:18.010519 12690 layer_factory.hpp:77] Creating layer penlu9
I1013 11:27:18.010529 12690 net.cpp:84] Creating Layer penlu9
I1013 11:27:18.010532 12690 net.cpp:406] penlu9 <- Convolution11
I1013 11:27:18.010540 12690 net.cpp:367] penlu9 -> Convolution11 (in-place)
I1013 11:27:18.010695 12690 net.cpp:122] Setting up penlu9
I1013 11:27:18.010702 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.010707 12690 net.cpp:137] Memory required for data: 1119395400
I1013 11:27:18.010716 12690 layer_factory.hpp:77] Creating layer Convolution12
I1013 11:27:18.010726 12690 net.cpp:84] Creating Layer Convolution12
I1013 11:27:18.010730 12690 net.cpp:406] Convolution12 <- Convolution11
I1013 11:27:18.010738 12690 net.cpp:380] Convolution12 -> Convolution12
I1013 11:27:18.032356 12690 net.cpp:122] Setting up Convolution12
I1013 11:27:18.032369 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.032373 12690 net.cpp:137] Memory required for data: 1131939400
I1013 11:27:18.032380 12690 layer_factory.hpp:77] Creating layer Convolution13
I1013 11:27:18.032393 12690 net.cpp:84] Creating Layer Convolution13
I1013 11:27:18.032399 12690 net.cpp:406] Convolution13 <- Eltwise4_Eltwise4_0_split_1
I1013 11:27:18.032407 12690 net.cpp:380] Convolution13 -> Convolution13
I1013 11:27:18.034505 12690 net.cpp:122] Setting up Convolution13
I1013 11:27:18.034518 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.034523 12690 net.cpp:137] Memory required for data: 1144483400
I1013 11:27:18.034549 12690 layer_factory.hpp:77] Creating layer Eltwise5
I1013 11:27:18.034561 12690 net.cpp:84] Creating Layer Eltwise5
I1013 11:27:18.034566 12690 net.cpp:406] Eltwise5 <- Convolution12
I1013 11:27:18.034574 12690 net.cpp:406] Eltwise5 <- Convolution13
I1013 11:27:18.034592 12690 net.cpp:380] Eltwise5 -> Eltwise5
I1013 11:27:18.034626 12690 net.cpp:122] Setting up Eltwise5
I1013 11:27:18.034633 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.034637 12690 net.cpp:137] Memory required for data: 1157027400
I1013 11:27:18.034652 12690 layer_factory.hpp:77] Creating layer BatchNorm10
I1013 11:27:18.034662 12690 net.cpp:84] Creating Layer BatchNorm10
I1013 11:27:18.034667 12690 net.cpp:406] BatchNorm10 <- Eltwise5
I1013 11:27:18.034683 12690 net.cpp:367] BatchNorm10 -> Eltwise5 (in-place)
I1013 11:27:18.034858 12690 net.cpp:122] Setting up BatchNorm10
I1013 11:27:18.034865 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.034869 12690 net.cpp:137] Memory required for data: 1169571400
I1013 11:27:18.034889 12690 layer_factory.hpp:77] Creating layer Scale10
I1013 11:27:18.034900 12690 net.cpp:84] Creating Layer Scale10
I1013 11:27:18.034905 12690 net.cpp:406] Scale10 <- Eltwise5
I1013 11:27:18.034911 12690 net.cpp:367] Scale10 -> Eltwise5 (in-place)
I1013 11:27:18.034960 12690 layer_factory.hpp:77] Creating layer Scale10
I1013 11:27:18.035068 12690 net.cpp:122] Setting up Scale10
I1013 11:27:18.035075 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.035079 12690 net.cpp:137] Memory required for data: 1182115400
I1013 11:27:18.035097 12690 layer_factory.hpp:77] Creating layer penlu10
I1013 11:27:18.035107 12690 net.cpp:84] Creating Layer penlu10
I1013 11:27:18.035114 12690 net.cpp:406] penlu10 <- Eltwise5
I1013 11:27:18.035130 12690 net.cpp:367] penlu10 -> Eltwise5 (in-place)
I1013 11:27:18.035323 12690 net.cpp:122] Setting up penlu10
I1013 11:27:18.035331 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.035336 12690 net.cpp:137] Memory required for data: 1194659400
I1013 11:27:18.035344 12690 layer_factory.hpp:77] Creating layer Eltwise5_penlu10_0_split
I1013 11:27:18.035351 12690 net.cpp:84] Creating Layer Eltwise5_penlu10_0_split
I1013 11:27:18.035356 12690 net.cpp:406] Eltwise5_penlu10_0_split <- Eltwise5
I1013 11:27:18.035363 12690 net.cpp:380] Eltwise5_penlu10_0_split -> Eltwise5_penlu10_0_split_0
I1013 11:27:18.035372 12690 net.cpp:380] Eltwise5_penlu10_0_split -> Eltwise5_penlu10_0_split_1
I1013 11:27:18.035401 12690 net.cpp:122] Setting up Eltwise5_penlu10_0_split
I1013 11:27:18.035408 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.035414 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.035419 12690 net.cpp:137] Memory required for data: 1219747400
I1013 11:27:18.035424 12690 layer_factory.hpp:77] Creating layer Convolution14
I1013 11:27:18.035435 12690 net.cpp:84] Creating Layer Convolution14
I1013 11:27:18.035439 12690 net.cpp:406] Convolution14 <- Eltwise5_penlu10_0_split_0
I1013 11:27:18.035446 12690 net.cpp:380] Convolution14 -> Convolution14
I1013 11:27:18.057699 12690 net.cpp:122] Setting up Convolution14
I1013 11:27:18.057713 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.057718 12690 net.cpp:137] Memory required for data: 1232291400
I1013 11:27:18.057724 12690 layer_factory.hpp:77] Creating layer BatchNorm11
I1013 11:27:18.057734 12690 net.cpp:84] Creating Layer BatchNorm11
I1013 11:27:18.057739 12690 net.cpp:406] BatchNorm11 <- Convolution14
I1013 11:27:18.057749 12690 net.cpp:367] BatchNorm11 -> Convolution14 (in-place)
I1013 11:27:18.057902 12690 net.cpp:122] Setting up BatchNorm11
I1013 11:27:18.057910 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.057914 12690 net.cpp:137] Memory required for data: 1244835400
I1013 11:27:18.057937 12690 layer_factory.hpp:77] Creating layer Scale11
I1013 11:27:18.057945 12690 net.cpp:84] Creating Layer Scale11
I1013 11:27:18.057960 12690 net.cpp:406] Scale11 <- Convolution14
I1013 11:27:18.057977 12690 net.cpp:367] Scale11 -> Convolution14 (in-place)
I1013 11:27:18.058014 12690 layer_factory.hpp:77] Creating layer Scale11
I1013 11:27:18.058102 12690 net.cpp:122] Setting up Scale11
I1013 11:27:18.058110 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.058115 12690 net.cpp:137] Memory required for data: 1257379400
I1013 11:27:18.058123 12690 layer_factory.hpp:77] Creating layer penlu11
I1013 11:27:18.058133 12690 net.cpp:84] Creating Layer penlu11
I1013 11:27:18.058138 12690 net.cpp:406] penlu11 <- Convolution14
I1013 11:27:18.058145 12690 net.cpp:367] penlu11 -> Convolution14 (in-place)
I1013 11:27:18.058300 12690 net.cpp:122] Setting up penlu11
I1013 11:27:18.058310 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.058315 12690 net.cpp:137] Memory required for data: 1269923400
I1013 11:27:18.058322 12690 layer_factory.hpp:77] Creating layer Convolution15
I1013 11:27:18.058331 12690 net.cpp:84] Creating Layer Convolution15
I1013 11:27:18.058336 12690 net.cpp:406] Convolution15 <- Convolution14
I1013 11:27:18.058342 12690 net.cpp:380] Convolution15 -> Convolution15
I1013 11:27:18.080369 12690 net.cpp:122] Setting up Convolution15
I1013 11:27:18.080389 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.080394 12690 net.cpp:137] Memory required for data: 1282467400
I1013 11:27:18.080401 12690 layer_factory.hpp:77] Creating layer Eltwise6
I1013 11:27:18.080413 12690 net.cpp:84] Creating Layer Eltwise6
I1013 11:27:18.080420 12690 net.cpp:406] Eltwise6 <- Convolution15
I1013 11:27:18.080428 12690 net.cpp:406] Eltwise6 <- Eltwise5_penlu10_0_split_1
I1013 11:27:18.080435 12690 net.cpp:380] Eltwise6 -> Eltwise6
I1013 11:27:18.080464 12690 net.cpp:122] Setting up Eltwise6
I1013 11:27:18.080471 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.080476 12690 net.cpp:137] Memory required for data: 1295011400
I1013 11:27:18.080479 12690 layer_factory.hpp:77] Creating layer BatchNorm12
I1013 11:27:18.080488 12690 net.cpp:84] Creating Layer BatchNorm12
I1013 11:27:18.080503 12690 net.cpp:406] BatchNorm12 <- Eltwise6
I1013 11:27:18.080509 12690 net.cpp:367] BatchNorm12 -> Eltwise6 (in-place)
I1013 11:27:18.080677 12690 net.cpp:122] Setting up BatchNorm12
I1013 11:27:18.080683 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.080687 12690 net.cpp:137] Memory required for data: 1307555400
I1013 11:27:18.080698 12690 layer_factory.hpp:77] Creating layer Scale12
I1013 11:27:18.080704 12690 net.cpp:84] Creating Layer Scale12
I1013 11:27:18.080708 12690 net.cpp:406] Scale12 <- Eltwise6
I1013 11:27:18.080715 12690 net.cpp:367] Scale12 -> Eltwise6 (in-place)
I1013 11:27:18.080754 12690 layer_factory.hpp:77] Creating layer Scale12
I1013 11:27:18.080842 12690 net.cpp:122] Setting up Scale12
I1013 11:27:18.080849 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.080853 12690 net.cpp:137] Memory required for data: 1320099400
I1013 11:27:18.080862 12690 layer_factory.hpp:77] Creating layer penlu12
I1013 11:27:18.080870 12690 net.cpp:84] Creating Layer penlu12
I1013 11:27:18.080875 12690 net.cpp:406] penlu12 <- Eltwise6
I1013 11:27:18.080883 12690 net.cpp:367] penlu12 -> Eltwise6 (in-place)
I1013 11:27:18.081037 12690 net.cpp:122] Setting up penlu12
I1013 11:27:18.081044 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.081048 12690 net.cpp:137] Memory required for data: 1332643400
I1013 11:27:18.081058 12690 layer_factory.hpp:77] Creating layer Eltwise6_penlu12_0_split
I1013 11:27:18.081070 12690 net.cpp:84] Creating Layer Eltwise6_penlu12_0_split
I1013 11:27:18.081074 12690 net.cpp:406] Eltwise6_penlu12_0_split <- Eltwise6
I1013 11:27:18.081080 12690 net.cpp:380] Eltwise6_penlu12_0_split -> Eltwise6_penlu12_0_split_0
I1013 11:27:18.081089 12690 net.cpp:380] Eltwise6_penlu12_0_split -> Eltwise6_penlu12_0_split_1
I1013 11:27:18.081118 12690 net.cpp:122] Setting up Eltwise6_penlu12_0_split
I1013 11:27:18.081125 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.081131 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.081146 12690 net.cpp:137] Memory required for data: 1357731400
I1013 11:27:18.081151 12690 layer_factory.hpp:77] Creating layer Convolution16
I1013 11:27:18.081161 12690 net.cpp:84] Creating Layer Convolution16
I1013 11:27:18.081166 12690 net.cpp:406] Convolution16 <- Eltwise6_penlu12_0_split_0
I1013 11:27:18.081171 12690 net.cpp:380] Convolution16 -> Convolution16
I1013 11:27:18.103363 12690 net.cpp:122] Setting up Convolution16
I1013 11:27:18.103384 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.103389 12690 net.cpp:137] Memory required for data: 1370275400
I1013 11:27:18.103405 12690 layer_factory.hpp:77] Creating layer BatchNorm13
I1013 11:27:18.103415 12690 net.cpp:84] Creating Layer BatchNorm13
I1013 11:27:18.103423 12690 net.cpp:406] BatchNorm13 <- Convolution16
I1013 11:27:18.103433 12690 net.cpp:367] BatchNorm13 -> Convolution16 (in-place)
I1013 11:27:18.103607 12690 net.cpp:122] Setting up BatchNorm13
I1013 11:27:18.103615 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.103618 12690 net.cpp:137] Memory required for data: 1382819400
I1013 11:27:18.103638 12690 layer_factory.hpp:77] Creating layer Scale13
I1013 11:27:18.103657 12690 net.cpp:84] Creating Layer Scale13
I1013 11:27:18.103662 12690 net.cpp:406] Scale13 <- Convolution16
I1013 11:27:18.103672 12690 net.cpp:367] Scale13 -> Convolution16 (in-place)
I1013 11:27:18.103730 12690 layer_factory.hpp:77] Creating layer Scale13
I1013 11:27:18.103863 12690 net.cpp:122] Setting up Scale13
I1013 11:27:18.103870 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.103874 12690 net.cpp:137] Memory required for data: 1395363400
I1013 11:27:18.103893 12690 layer_factory.hpp:77] Creating layer penlu13
I1013 11:27:18.103917 12690 net.cpp:84] Creating Layer penlu13
I1013 11:27:18.103930 12690 net.cpp:406] penlu13 <- Convolution16
I1013 11:27:18.103948 12690 net.cpp:367] penlu13 -> Convolution16 (in-place)
I1013 11:27:18.104118 12690 net.cpp:122] Setting up penlu13
I1013 11:27:18.104126 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.104130 12690 net.cpp:137] Memory required for data: 1407907400
I1013 11:27:18.104148 12690 layer_factory.hpp:77] Creating layer Convolution17
I1013 11:27:18.104159 12690 net.cpp:84] Creating Layer Convolution17
I1013 11:27:18.104163 12690 net.cpp:406] Convolution17 <- Convolution16
I1013 11:27:18.104171 12690 net.cpp:380] Convolution17 -> Convolution17
I1013 11:27:18.126296 12690 net.cpp:122] Setting up Convolution17
I1013 11:27:18.126313 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.126317 12690 net.cpp:137] Memory required for data: 1420451400
I1013 11:27:18.126334 12690 layer_factory.hpp:77] Creating layer Eltwise7
I1013 11:27:18.126345 12690 net.cpp:84] Creating Layer Eltwise7
I1013 11:27:18.126351 12690 net.cpp:406] Eltwise7 <- Convolution17
I1013 11:27:18.126368 12690 net.cpp:406] Eltwise7 <- Eltwise6_penlu12_0_split_1
I1013 11:27:18.126375 12690 net.cpp:380] Eltwise7 -> Eltwise7
I1013 11:27:18.126404 12690 net.cpp:122] Setting up Eltwise7
I1013 11:27:18.126410 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.126415 12690 net.cpp:137] Memory required for data: 1432995400
I1013 11:27:18.126420 12690 layer_factory.hpp:77] Creating layer BatchNorm14
I1013 11:27:18.126428 12690 net.cpp:84] Creating Layer BatchNorm14
I1013 11:27:18.126432 12690 net.cpp:406] BatchNorm14 <- Eltwise7
I1013 11:27:18.126438 12690 net.cpp:367] BatchNorm14 -> Eltwise7 (in-place)
I1013 11:27:18.126597 12690 net.cpp:122] Setting up BatchNorm14
I1013 11:27:18.126605 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.126608 12690 net.cpp:137] Memory required for data: 1445539400
I1013 11:27:18.126628 12690 layer_factory.hpp:77] Creating layer Scale14
I1013 11:27:18.126636 12690 net.cpp:84] Creating Layer Scale14
I1013 11:27:18.126641 12690 net.cpp:406] Scale14 <- Eltwise7
I1013 11:27:18.126646 12690 net.cpp:367] Scale14 -> Eltwise7 (in-place)
I1013 11:27:18.126695 12690 layer_factory.hpp:77] Creating layer Scale14
I1013 11:27:18.126823 12690 net.cpp:122] Setting up Scale14
I1013 11:27:18.126830 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.126835 12690 net.cpp:137] Memory required for data: 1458083400
I1013 11:27:18.126853 12690 layer_factory.hpp:77] Creating layer penlu14
I1013 11:27:18.126863 12690 net.cpp:84] Creating Layer penlu14
I1013 11:27:18.126868 12690 net.cpp:406] penlu14 <- Eltwise7
I1013 11:27:18.126876 12690 net.cpp:367] penlu14 -> Eltwise7 (in-place)
I1013 11:27:18.127074 12690 net.cpp:122] Setting up penlu14
I1013 11:27:18.127081 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.127084 12690 net.cpp:137] Memory required for data: 1470627400
I1013 11:27:18.127121 12690 layer_factory.hpp:77] Creating layer Eltwise7_penlu14_0_split
I1013 11:27:18.127128 12690 net.cpp:84] Creating Layer Eltwise7_penlu14_0_split
I1013 11:27:18.127135 12690 net.cpp:406] Eltwise7_penlu14_0_split <- Eltwise7
I1013 11:27:18.127140 12690 net.cpp:380] Eltwise7_penlu14_0_split -> Eltwise7_penlu14_0_split_0
I1013 11:27:18.127148 12690 net.cpp:380] Eltwise7_penlu14_0_split -> Eltwise7_penlu14_0_split_1
I1013 11:27:18.127179 12690 net.cpp:122] Setting up Eltwise7_penlu14_0_split
I1013 11:27:18.127187 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.127193 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.127198 12690 net.cpp:137] Memory required for data: 1495715400
I1013 11:27:18.127202 12690 layer_factory.hpp:77] Creating layer Convolution18
I1013 11:27:18.127215 12690 net.cpp:84] Creating Layer Convolution18
I1013 11:27:18.127220 12690 net.cpp:406] Convolution18 <- Eltwise7_penlu14_0_split_0
I1013 11:27:18.127228 12690 net.cpp:380] Convolution18 -> Convolution18
I1013 11:27:18.149374 12690 net.cpp:122] Setting up Convolution18
I1013 11:27:18.149390 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.149395 12690 net.cpp:137] Memory required for data: 1508259400
I1013 11:27:18.149401 12690 layer_factory.hpp:77] Creating layer BatchNorm15
I1013 11:27:18.149410 12690 net.cpp:84] Creating Layer BatchNorm15
I1013 11:27:18.149416 12690 net.cpp:406] BatchNorm15 <- Convolution18
I1013 11:27:18.149425 12690 net.cpp:367] BatchNorm15 -> Convolution18 (in-place)
I1013 11:27:18.149591 12690 net.cpp:122] Setting up BatchNorm15
I1013 11:27:18.149600 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.149603 12690 net.cpp:137] Memory required for data: 1520803400
I1013 11:27:18.149622 12690 layer_factory.hpp:77] Creating layer Scale15
I1013 11:27:18.149631 12690 net.cpp:84] Creating Layer Scale15
I1013 11:27:18.149636 12690 net.cpp:406] Scale15 <- Convolution18
I1013 11:27:18.149643 12690 net.cpp:367] Scale15 -> Convolution18 (in-place)
I1013 11:27:18.149689 12690 layer_factory.hpp:77] Creating layer Scale15
I1013 11:27:18.149780 12690 net.cpp:122] Setting up Scale15
I1013 11:27:18.149786 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.149791 12690 net.cpp:137] Memory required for data: 1533347400
I1013 11:27:18.149798 12690 layer_factory.hpp:77] Creating layer penlu15
I1013 11:27:18.149808 12690 net.cpp:84] Creating Layer penlu15
I1013 11:27:18.149812 12690 net.cpp:406] penlu15 <- Convolution18
I1013 11:27:18.149818 12690 net.cpp:367] penlu15 -> Convolution18 (in-place)
I1013 11:27:18.149981 12690 net.cpp:122] Setting up penlu15
I1013 11:27:18.149988 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.149992 12690 net.cpp:137] Memory required for data: 1545891400
I1013 11:27:18.150001 12690 layer_factory.hpp:77] Creating layer Convolution19
I1013 11:27:18.150012 12690 net.cpp:84] Creating Layer Convolution19
I1013 11:27:18.150015 12690 net.cpp:406] Convolution19 <- Convolution18
I1013 11:27:18.150022 12690 net.cpp:380] Convolution19 -> Convolution19
I1013 11:27:18.171943 12690 net.cpp:122] Setting up Convolution19
I1013 11:27:18.171963 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.171967 12690 net.cpp:137] Memory required for data: 1558435400
I1013 11:27:18.171974 12690 layer_factory.hpp:77] Creating layer Eltwise8
I1013 11:27:18.172009 12690 net.cpp:84] Creating Layer Eltwise8
I1013 11:27:18.172016 12690 net.cpp:406] Eltwise8 <- Convolution19
I1013 11:27:18.172034 12690 net.cpp:406] Eltwise8 <- Eltwise7_penlu14_0_split_1
I1013 11:27:18.172049 12690 net.cpp:380] Eltwise8 -> Eltwise8
I1013 11:27:18.172106 12690 net.cpp:122] Setting up Eltwise8
I1013 11:27:18.172112 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.172116 12690 net.cpp:137] Memory required for data: 1570979400
I1013 11:27:18.172132 12690 layer_factory.hpp:77] Creating layer Eltwise8_Eltwise8_0_split
I1013 11:27:18.172147 12690 net.cpp:84] Creating Layer Eltwise8_Eltwise8_0_split
I1013 11:27:18.172152 12690 net.cpp:406] Eltwise8_Eltwise8_0_split <- Eltwise8
I1013 11:27:18.172171 12690 net.cpp:380] Eltwise8_Eltwise8_0_split -> Eltwise8_Eltwise8_0_split_0
I1013 11:27:18.172179 12690 net.cpp:380] Eltwise8_Eltwise8_0_split -> Eltwise8_Eltwise8_0_split_1
I1013 11:27:18.172220 12690 net.cpp:122] Setting up Eltwise8_Eltwise8_0_split
I1013 11:27:18.172227 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.172233 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.172238 12690 net.cpp:137] Memory required for data: 1596067400
I1013 11:27:18.172242 12690 layer_factory.hpp:77] Creating layer Convolution20
I1013 11:27:18.172264 12690 net.cpp:84] Creating Layer Convolution20
I1013 11:27:18.172269 12690 net.cpp:406] Convolution20 <- Eltwise8_Eltwise8_0_split_0
I1013 11:27:18.172286 12690 net.cpp:380] Convolution20 -> Convolution20
I1013 11:27:18.214609 12690 net.cpp:122] Setting up Convolution20
I1013 11:27:18.214632 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.214637 12690 net.cpp:137] Memory required for data: 1602339400
I1013 11:27:18.214643 12690 layer_factory.hpp:77] Creating layer BatchNorm16
I1013 11:27:18.214656 12690 net.cpp:84] Creating Layer BatchNorm16
I1013 11:27:18.214675 12690 net.cpp:406] BatchNorm16 <- Convolution20
I1013 11:27:18.214685 12690 net.cpp:367] BatchNorm16 -> Convolution20 (in-place)
I1013 11:27:18.214898 12690 net.cpp:122] Setting up BatchNorm16
I1013 11:27:18.214906 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.214910 12690 net.cpp:137] Memory required for data: 1608611400
I1013 11:27:18.214920 12690 layer_factory.hpp:77] Creating layer Scale16
I1013 11:27:18.214937 12690 net.cpp:84] Creating Layer Scale16
I1013 11:27:18.214943 12690 net.cpp:406] Scale16 <- Convolution20
I1013 11:27:18.214951 12690 net.cpp:367] Scale16 -> Convolution20 (in-place)
I1013 11:27:18.214998 12690 layer_factory.hpp:77] Creating layer Scale16
I1013 11:27:18.215106 12690 net.cpp:122] Setting up Scale16
I1013 11:27:18.215112 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.215116 12690 net.cpp:137] Memory required for data: 1614883400
I1013 11:27:18.215134 12690 layer_factory.hpp:77] Creating layer penlu16
I1013 11:27:18.215143 12690 net.cpp:84] Creating Layer penlu16
I1013 11:27:18.215147 12690 net.cpp:406] penlu16 <- Convolution20
I1013 11:27:18.215155 12690 net.cpp:367] penlu16 -> Convolution20 (in-place)
I1013 11:27:18.215801 12690 net.cpp:122] Setting up penlu16
I1013 11:27:18.215811 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.215816 12690 net.cpp:137] Memory required for data: 1621155400
I1013 11:27:18.215823 12690 layer_factory.hpp:77] Creating layer Convolution21
I1013 11:27:18.215837 12690 net.cpp:84] Creating Layer Convolution21
I1013 11:27:18.215842 12690 net.cpp:406] Convolution21 <- Convolution20
I1013 11:27:18.215849 12690 net.cpp:380] Convolution21 -> Convolution21
I1013 11:27:18.299715 12690 net.cpp:122] Setting up Convolution21
I1013 11:27:18.299738 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.299743 12690 net.cpp:137] Memory required for data: 1627427400
I1013 11:27:18.299751 12690 layer_factory.hpp:77] Creating layer Convolution22
I1013 11:27:18.299777 12690 net.cpp:84] Creating Layer Convolution22
I1013 11:27:18.299784 12690 net.cpp:406] Convolution22 <- Eltwise8_Eltwise8_0_split_1
I1013 11:27:18.299839 12690 net.cpp:380] Convolution22 -> Convolution22
I1013 11:27:18.305155 12690 net.cpp:122] Setting up Convolution22
I1013 11:27:18.305166 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.305171 12690 net.cpp:137] Memory required for data: 1633699400
I1013 11:27:18.305177 12690 layer_factory.hpp:77] Creating layer Eltwise9
I1013 11:27:18.305187 12690 net.cpp:84] Creating Layer Eltwise9
I1013 11:27:18.305207 12690 net.cpp:406] Eltwise9 <- Convolution21
I1013 11:27:18.305212 12690 net.cpp:406] Eltwise9 <- Convolution22
I1013 11:27:18.305228 12690 net.cpp:380] Eltwise9 -> Eltwise9
I1013 11:27:18.305287 12690 net.cpp:122] Setting up Eltwise9
I1013 11:27:18.305302 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.305306 12690 net.cpp:137] Memory required for data: 1639971400
I1013 11:27:18.305321 12690 layer_factory.hpp:77] Creating layer BatchNorm17
I1013 11:27:18.305330 12690 net.cpp:84] Creating Layer BatchNorm17
I1013 11:27:18.305335 12690 net.cpp:406] BatchNorm17 <- Eltwise9
I1013 11:27:18.305351 12690 net.cpp:367] BatchNorm17 -> Eltwise9 (in-place)
I1013 11:27:18.305521 12690 net.cpp:122] Setting up BatchNorm17
I1013 11:27:18.305528 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.305532 12690 net.cpp:137] Memory required for data: 1646243400
I1013 11:27:18.305541 12690 layer_factory.hpp:77] Creating layer Scale17
I1013 11:27:18.305562 12690 net.cpp:84] Creating Layer Scale17
I1013 11:27:18.305577 12690 net.cpp:406] Scale17 <- Eltwise9
I1013 11:27:18.305583 12690 net.cpp:367] Scale17 -> Eltwise9 (in-place)
I1013 11:27:18.305647 12690 layer_factory.hpp:77] Creating layer Scale17
I1013 11:27:18.305745 12690 net.cpp:122] Setting up Scale17
I1013 11:27:18.305752 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.305757 12690 net.cpp:137] Memory required for data: 1652515400
I1013 11:27:18.305775 12690 layer_factory.hpp:77] Creating layer penlu17
I1013 11:27:18.305785 12690 net.cpp:84] Creating Layer penlu17
I1013 11:27:18.305789 12690 net.cpp:406] penlu17 <- Eltwise9
I1013 11:27:18.305805 12690 net.cpp:367] penlu17 -> Eltwise9 (in-place)
I1013 11:27:18.306020 12690 net.cpp:122] Setting up penlu17
I1013 11:27:18.306028 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.306031 12690 net.cpp:137] Memory required for data: 1658787400
I1013 11:27:18.306049 12690 layer_factory.hpp:77] Creating layer Eltwise9_penlu17_0_split
I1013 11:27:18.306058 12690 net.cpp:84] Creating Layer Eltwise9_penlu17_0_split
I1013 11:27:18.306062 12690 net.cpp:406] Eltwise9_penlu17_0_split <- Eltwise9
I1013 11:27:18.306069 12690 net.cpp:380] Eltwise9_penlu17_0_split -> Eltwise9_penlu17_0_split_0
I1013 11:27:18.306077 12690 net.cpp:380] Eltwise9_penlu17_0_split -> Eltwise9_penlu17_0_split_1
I1013 11:27:18.306120 12690 net.cpp:122] Setting up Eltwise9_penlu17_0_split
I1013 11:27:18.306128 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.306143 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.306149 12690 net.cpp:137] Memory required for data: 1671331400
I1013 11:27:18.306152 12690 layer_factory.hpp:77] Creating layer Convolution23
I1013 11:27:18.306161 12690 net.cpp:84] Creating Layer Convolution23
I1013 11:27:18.306165 12690 net.cpp:406] Convolution23 <- Eltwise9_penlu17_0_split_0
I1013 11:27:18.306172 12690 net.cpp:380] Convolution23 -> Convolution23
I1013 11:27:18.389292 12690 net.cpp:122] Setting up Convolution23
I1013 11:27:18.389315 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.389320 12690 net.cpp:137] Memory required for data: 1677603400
I1013 11:27:18.389328 12690 layer_factory.hpp:77] Creating layer BatchNorm18
I1013 11:27:18.389340 12690 net.cpp:84] Creating Layer BatchNorm18
I1013 11:27:18.389359 12690 net.cpp:406] BatchNorm18 <- Convolution23
I1013 11:27:18.389367 12690 net.cpp:367] BatchNorm18 -> Convolution23 (in-place)
I1013 11:27:18.389539 12690 net.cpp:122] Setting up BatchNorm18
I1013 11:27:18.389546 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.389550 12690 net.cpp:137] Memory required for data: 1683875400
I1013 11:27:18.389580 12690 layer_factory.hpp:77] Creating layer Scale18
I1013 11:27:18.389591 12690 net.cpp:84] Creating Layer Scale18
I1013 11:27:18.389596 12690 net.cpp:406] Scale18 <- Convolution23
I1013 11:27:18.389602 12690 net.cpp:367] Scale18 -> Convolution23 (in-place)
I1013 11:27:18.389652 12690 layer_factory.hpp:77] Creating layer Scale18
I1013 11:27:18.389761 12690 net.cpp:122] Setting up Scale18
I1013 11:27:18.389768 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.389772 12690 net.cpp:137] Memory required for data: 1690147400
I1013 11:27:18.389791 12690 layer_factory.hpp:77] Creating layer penlu18
I1013 11:27:18.389801 12690 net.cpp:84] Creating Layer penlu18
I1013 11:27:18.389806 12690 net.cpp:406] penlu18 <- Convolution23
I1013 11:27:18.389812 12690 net.cpp:367] penlu18 -> Convolution23 (in-place)
I1013 11:27:18.389962 12690 net.cpp:122] Setting up penlu18
I1013 11:27:18.389971 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.389974 12690 net.cpp:137] Memory required for data: 1696419400
I1013 11:27:18.389991 12690 layer_factory.hpp:77] Creating layer Convolution24
I1013 11:27:18.390003 12690 net.cpp:84] Creating Layer Convolution24
I1013 11:27:18.390008 12690 net.cpp:406] Convolution24 <- Convolution23
I1013 11:27:18.390014 12690 net.cpp:380] Convolution24 -> Convolution24
I1013 11:27:18.473671 12690 net.cpp:122] Setting up Convolution24
I1013 11:27:18.473695 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.473701 12690 net.cpp:137] Memory required for data: 1702691400
I1013 11:27:18.473708 12690 layer_factory.hpp:77] Creating layer Eltwise10
I1013 11:27:18.473731 12690 net.cpp:84] Creating Layer Eltwise10
I1013 11:27:18.473749 12690 net.cpp:406] Eltwise10 <- Convolution24
I1013 11:27:18.473767 12690 net.cpp:406] Eltwise10 <- Eltwise9_penlu17_0_split_1
I1013 11:27:18.473776 12690 net.cpp:380] Eltwise10 -> Eltwise10
I1013 11:27:18.473845 12690 net.cpp:122] Setting up Eltwise10
I1013 11:27:18.473851 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.473855 12690 net.cpp:137] Memory required for data: 1708963400
I1013 11:27:18.473860 12690 layer_factory.hpp:77] Creating layer BatchNorm19
I1013 11:27:18.473883 12690 net.cpp:84] Creating Layer BatchNorm19
I1013 11:27:18.473887 12690 net.cpp:406] BatchNorm19 <- Eltwise10
I1013 11:27:18.473903 12690 net.cpp:367] BatchNorm19 -> Eltwise10 (in-place)
I1013 11:27:18.474108 12690 net.cpp:122] Setting up BatchNorm19
I1013 11:27:18.474117 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.474120 12690 net.cpp:137] Memory required for data: 1715235400
I1013 11:27:18.474130 12690 layer_factory.hpp:77] Creating layer Scale19
I1013 11:27:18.474139 12690 net.cpp:84] Creating Layer Scale19
I1013 11:27:18.474143 12690 net.cpp:406] Scale19 <- Eltwise10
I1013 11:27:18.474153 12690 net.cpp:367] Scale19 -> Eltwise10 (in-place)
I1013 11:27:18.474210 12690 layer_factory.hpp:77] Creating layer Scale19
I1013 11:27:18.474354 12690 net.cpp:122] Setting up Scale19
I1013 11:27:18.474362 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.474376 12690 net.cpp:137] Memory required for data: 1721507400
I1013 11:27:18.474385 12690 layer_factory.hpp:77] Creating layer penlu19
I1013 11:27:18.474405 12690 net.cpp:84] Creating Layer penlu19
I1013 11:27:18.474418 12690 net.cpp:406] penlu19 <- Eltwise10
I1013 11:27:18.474439 12690 net.cpp:367] penlu19 -> Eltwise10 (in-place)
I1013 11:27:18.474616 12690 net.cpp:122] Setting up penlu19
I1013 11:27:18.474623 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.474638 12690 net.cpp:137] Memory required for data: 1727779400
I1013 11:27:18.474647 12690 layer_factory.hpp:77] Creating layer Eltwise10_penlu19_0_split
I1013 11:27:18.474664 12690 net.cpp:84] Creating Layer Eltwise10_penlu19_0_split
I1013 11:27:18.474678 12690 net.cpp:406] Eltwise10_penlu19_0_split <- Eltwise10
I1013 11:27:18.474696 12690 net.cpp:380] Eltwise10_penlu19_0_split -> Eltwise10_penlu19_0_split_0
I1013 11:27:18.474704 12690 net.cpp:380] Eltwise10_penlu19_0_split -> Eltwise10_penlu19_0_split_1
I1013 11:27:18.474759 12690 net.cpp:122] Setting up Eltwise10_penlu19_0_split
I1013 11:27:18.474767 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.474774 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.474779 12690 net.cpp:137] Memory required for data: 1740323400
I1013 11:27:18.474783 12690 layer_factory.hpp:77] Creating layer Convolution25
I1013 11:27:18.474795 12690 net.cpp:84] Creating Layer Convolution25
I1013 11:27:18.474800 12690 net.cpp:406] Convolution25 <- Eltwise10_penlu19_0_split_0
I1013 11:27:18.474807 12690 net.cpp:380] Convolution25 -> Convolution25
I1013 11:27:18.558008 12690 net.cpp:122] Setting up Convolution25
I1013 11:27:18.558032 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.558037 12690 net.cpp:137] Memory required for data: 1746595400
I1013 11:27:18.558045 12690 layer_factory.hpp:77] Creating layer BatchNorm20
I1013 11:27:18.558058 12690 net.cpp:84] Creating Layer BatchNorm20
I1013 11:27:18.558076 12690 net.cpp:406] BatchNorm20 <- Convolution25
I1013 11:27:18.558095 12690 net.cpp:367] BatchNorm20 -> Convolution25 (in-place)
I1013 11:27:18.558333 12690 net.cpp:122] Setting up BatchNorm20
I1013 11:27:18.558341 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.558346 12690 net.cpp:137] Memory required for data: 1752867400
I1013 11:27:18.558354 12690 layer_factory.hpp:77] Creating layer Scale20
I1013 11:27:18.558362 12690 net.cpp:84] Creating Layer Scale20
I1013 11:27:18.558379 12690 net.cpp:406] Scale20 <- Convolution25
I1013 11:27:18.558395 12690 net.cpp:367] Scale20 -> Convolution25 (in-place)
I1013 11:27:18.558455 12690 layer_factory.hpp:77] Creating layer Scale20
I1013 11:27:18.558568 12690 net.cpp:122] Setting up Scale20
I1013 11:27:18.558576 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.558591 12690 net.cpp:137] Memory required for data: 1759139400
I1013 11:27:18.558599 12690 layer_factory.hpp:77] Creating layer penlu20
I1013 11:27:18.558609 12690 net.cpp:84] Creating Layer penlu20
I1013 11:27:18.558612 12690 net.cpp:406] penlu20 <- Convolution25
I1013 11:27:18.558619 12690 net.cpp:367] penlu20 -> Convolution25 (in-place)
I1013 11:27:18.558761 12690 net.cpp:122] Setting up penlu20
I1013 11:27:18.558768 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.558773 12690 net.cpp:137] Memory required for data: 1765411400
I1013 11:27:18.558781 12690 layer_factory.hpp:77] Creating layer Convolution26
I1013 11:27:18.558792 12690 net.cpp:84] Creating Layer Convolution26
I1013 11:27:18.558797 12690 net.cpp:406] Convolution26 <- Convolution25
I1013 11:27:18.558805 12690 net.cpp:380] Convolution26 -> Convolution26
I1013 11:27:18.642400 12690 net.cpp:122] Setting up Convolution26
I1013 11:27:18.642423 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.642428 12690 net.cpp:137] Memory required for data: 1771683400
I1013 11:27:18.642437 12690 layer_factory.hpp:77] Creating layer Eltwise11
I1013 11:27:18.642448 12690 net.cpp:84] Creating Layer Eltwise11
I1013 11:27:18.642467 12690 net.cpp:406] Eltwise11 <- Convolution26
I1013 11:27:18.642474 12690 net.cpp:406] Eltwise11 <- Eltwise10_penlu19_0_split_1
I1013 11:27:18.642483 12690 net.cpp:380] Eltwise11 -> Eltwise11
I1013 11:27:18.642529 12690 net.cpp:122] Setting up Eltwise11
I1013 11:27:18.642535 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.642540 12690 net.cpp:137] Memory required for data: 1777955400
I1013 11:27:18.642545 12690 layer_factory.hpp:77] Creating layer BatchNorm21
I1013 11:27:18.642554 12690 net.cpp:84] Creating Layer BatchNorm21
I1013 11:27:18.642570 12690 net.cpp:406] BatchNorm21 <- Eltwise11
I1013 11:27:18.642586 12690 net.cpp:367] BatchNorm21 -> Eltwise11 (in-place)
I1013 11:27:18.642778 12690 net.cpp:122] Setting up BatchNorm21
I1013 11:27:18.642786 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.642791 12690 net.cpp:137] Memory required for data: 1784227400
I1013 11:27:18.642809 12690 layer_factory.hpp:77] Creating layer Scale21
I1013 11:27:18.642850 12690 net.cpp:84] Creating Layer Scale21
I1013 11:27:18.642866 12690 net.cpp:406] Scale21 <- Eltwise11
I1013 11:27:18.642873 12690 net.cpp:367] Scale21 -> Eltwise11 (in-place)
I1013 11:27:18.642933 12690 layer_factory.hpp:77] Creating layer Scale21
I1013 11:27:18.643071 12690 net.cpp:122] Setting up Scale21
I1013 11:27:18.643079 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.643082 12690 net.cpp:137] Memory required for data: 1790499400
I1013 11:27:18.643100 12690 layer_factory.hpp:77] Creating layer penlu21
I1013 11:27:18.643126 12690 net.cpp:84] Creating Layer penlu21
I1013 11:27:18.643139 12690 net.cpp:406] penlu21 <- Eltwise11
I1013 11:27:18.643146 12690 net.cpp:367] penlu21 -> Eltwise11 (in-place)
I1013 11:27:18.643292 12690 net.cpp:122] Setting up penlu21
I1013 11:27:18.643301 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.643306 12690 net.cpp:137] Memory required for data: 1796771400
I1013 11:27:18.643324 12690 layer_factory.hpp:77] Creating layer Eltwise11_penlu21_0_split
I1013 11:27:18.643331 12690 net.cpp:84] Creating Layer Eltwise11_penlu21_0_split
I1013 11:27:18.643335 12690 net.cpp:406] Eltwise11_penlu21_0_split <- Eltwise11
I1013 11:27:18.643342 12690 net.cpp:380] Eltwise11_penlu21_0_split -> Eltwise11_penlu21_0_split_0
I1013 11:27:18.643359 12690 net.cpp:380] Eltwise11_penlu21_0_split -> Eltwise11_penlu21_0_split_1
I1013 11:27:18.643402 12690 net.cpp:122] Setting up Eltwise11_penlu21_0_split
I1013 11:27:18.643409 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.643424 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.643427 12690 net.cpp:137] Memory required for data: 1809315400
I1013 11:27:18.643442 12690 layer_factory.hpp:77] Creating layer Convolution27
I1013 11:27:18.643470 12690 net.cpp:84] Creating Layer Convolution27
I1013 11:27:18.643473 12690 net.cpp:406] Convolution27 <- Eltwise11_penlu21_0_split_0
I1013 11:27:18.643491 12690 net.cpp:380] Convolution27 -> Convolution27
I1013 11:27:18.726487 12690 net.cpp:122] Setting up Convolution27
I1013 11:27:18.726516 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.726521 12690 net.cpp:137] Memory required for data: 1815587400
I1013 11:27:18.726539 12690 layer_factory.hpp:77] Creating layer BatchNorm22
I1013 11:27:18.726563 12690 net.cpp:84] Creating Layer BatchNorm22
I1013 11:27:18.726572 12690 net.cpp:406] BatchNorm22 <- Convolution27
I1013 11:27:18.726583 12690 net.cpp:367] BatchNorm22 -> Convolution27 (in-place)
I1013 11:27:18.726789 12690 net.cpp:122] Setting up BatchNorm22
I1013 11:27:18.726799 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.726802 12690 net.cpp:137] Memory required for data: 1821859400
I1013 11:27:18.726822 12690 layer_factory.hpp:77] Creating layer Scale22
I1013 11:27:18.726842 12690 net.cpp:84] Creating Layer Scale22
I1013 11:27:18.726858 12690 net.cpp:406] Scale22 <- Convolution27
I1013 11:27:18.726876 12690 net.cpp:367] Scale22 -> Convolution27 (in-place)
I1013 11:27:18.726941 12690 layer_factory.hpp:77] Creating layer Scale22
I1013 11:27:18.727084 12690 net.cpp:122] Setting up Scale22
I1013 11:27:18.727093 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.727097 12690 net.cpp:137] Memory required for data: 1828131400
I1013 11:27:18.727115 12690 layer_factory.hpp:77] Creating layer penlu22
I1013 11:27:18.727126 12690 net.cpp:84] Creating Layer penlu22
I1013 11:27:18.727131 12690 net.cpp:406] penlu22 <- Convolution27
I1013 11:27:18.727140 12690 net.cpp:367] penlu22 -> Convolution27 (in-place)
I1013 11:27:18.727290 12690 net.cpp:122] Setting up penlu22
I1013 11:27:18.727298 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.727301 12690 net.cpp:137] Memory required for data: 1834403400
I1013 11:27:18.727319 12690 layer_factory.hpp:77] Creating layer Convolution28
I1013 11:27:18.727331 12690 net.cpp:84] Creating Layer Convolution28
I1013 11:27:18.727336 12690 net.cpp:406] Convolution28 <- Convolution27
I1013 11:27:18.727344 12690 net.cpp:380] Convolution28 -> Convolution28
I1013 11:27:18.810971 12690 net.cpp:122] Setting up Convolution28
I1013 11:27:18.810997 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.811002 12690 net.cpp:137] Memory required for data: 1840675400
I1013 11:27:18.811020 12690 layer_factory.hpp:77] Creating layer Eltwise12
I1013 11:27:18.811044 12690 net.cpp:84] Creating Layer Eltwise12
I1013 11:27:18.811053 12690 net.cpp:406] Eltwise12 <- Convolution28
I1013 11:27:18.811069 12690 net.cpp:406] Eltwise12 <- Eltwise11_penlu21_0_split_1
I1013 11:27:18.811087 12690 net.cpp:380] Eltwise12 -> Eltwise12
I1013 11:27:18.811136 12690 net.cpp:122] Setting up Eltwise12
I1013 11:27:18.811152 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:18.811156 12690 net.cpp:137] Memory required for data: 1846947400
I1013 11:27:18.811172 12690 layer_factory.hpp:77] Creating layer Pooling1
I1013 11:27:18.811195 12690 net.cpp:84] Creating Layer Pooling1
I1013 11:27:18.811210 12690 net.cpp:406] Pooling1 <- Eltwise12
I1013 11:27:18.811233 12690 net.cpp:380] Pooling1 -> Pooling1
I1013 11:27:18.811821 12690 net.cpp:122] Setting up Pooling1
I1013 11:27:18.811833 12690 net.cpp:129] Top shape: 50 640 1 1 (32000)
I1013 11:27:18.811837 12690 net.cpp:137] Memory required for data: 1847075400
I1013 11:27:18.811854 12690 layer_factory.hpp:77] Creating layer InnerProduct1
I1013 11:27:18.811878 12690 net.cpp:84] Creating Layer InnerProduct1
I1013 11:27:18.811884 12690 net.cpp:406] InnerProduct1 <- Pooling1
I1013 11:27:18.811900 12690 net.cpp:380] InnerProduct1 -> InnerProduct1
I1013 11:27:18.812099 12690 net.cpp:122] Setting up InnerProduct1
I1013 11:27:18.812108 12690 net.cpp:129] Top shape: 50 100 (5000)
I1013 11:27:18.812110 12690 net.cpp:137] Memory required for data: 1847095400
I1013 11:27:18.812129 12690 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1013 11:27:18.812147 12690 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1013 11:27:18.812152 12690 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1013 11:27:18.812158 12690 net.cpp:406] SoftmaxWithLoss1 <- label
I1013 11:27:18.812165 12690 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1013 11:27:18.812176 12690 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1013 11:27:18.812398 12690 net.cpp:122] Setting up SoftmaxWithLoss1
I1013 11:27:18.812408 12690 net.cpp:129] Top shape: (1)
I1013 11:27:18.812413 12690 net.cpp:132]     with loss weight 1
I1013 11:27:18.812440 12690 net.cpp:137] Memory required for data: 1847095404
I1013 11:27:18.812445 12690 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1013 11:27:18.812451 12690 net.cpp:198] InnerProduct1 needs backward computation.
I1013 11:27:18.812456 12690 net.cpp:198] Pooling1 needs backward computation.
I1013 11:27:18.812461 12690 net.cpp:198] Eltwise12 needs backward computation.
I1013 11:27:18.812467 12690 net.cpp:198] Convolution28 needs backward computation.
I1013 11:27:18.812472 12690 net.cpp:198] penlu22 needs backward computation.
I1013 11:27:18.812477 12690 net.cpp:198] Scale22 needs backward computation.
I1013 11:27:18.812481 12690 net.cpp:198] BatchNorm22 needs backward computation.
I1013 11:27:18.812486 12690 net.cpp:198] Convolution27 needs backward computation.
I1013 11:27:18.812491 12690 net.cpp:198] Eltwise11_penlu21_0_split needs backward computation.
I1013 11:27:18.812496 12690 net.cpp:198] penlu21 needs backward computation.
I1013 11:27:18.812501 12690 net.cpp:198] Scale21 needs backward computation.
I1013 11:27:18.812506 12690 net.cpp:198] BatchNorm21 needs backward computation.
I1013 11:27:18.812510 12690 net.cpp:198] Eltwise11 needs backward computation.
I1013 11:27:18.812516 12690 net.cpp:198] Convolution26 needs backward computation.
I1013 11:27:18.812521 12690 net.cpp:198] penlu20 needs backward computation.
I1013 11:27:18.812527 12690 net.cpp:198] Scale20 needs backward computation.
I1013 11:27:18.812532 12690 net.cpp:198] BatchNorm20 needs backward computation.
I1013 11:27:18.812537 12690 net.cpp:198] Convolution25 needs backward computation.
I1013 11:27:18.812541 12690 net.cpp:198] Eltwise10_penlu19_0_split needs backward computation.
I1013 11:27:18.812558 12690 net.cpp:198] penlu19 needs backward computation.
I1013 11:27:18.812564 12690 net.cpp:198] Scale19 needs backward computation.
I1013 11:27:18.812569 12690 net.cpp:198] BatchNorm19 needs backward computation.
I1013 11:27:18.812574 12690 net.cpp:198] Eltwise10 needs backward computation.
I1013 11:27:18.812580 12690 net.cpp:198] Convolution24 needs backward computation.
I1013 11:27:18.812585 12690 net.cpp:198] penlu18 needs backward computation.
I1013 11:27:18.812589 12690 net.cpp:198] Scale18 needs backward computation.
I1013 11:27:18.812594 12690 net.cpp:198] BatchNorm18 needs backward computation.
I1013 11:27:18.812598 12690 net.cpp:198] Convolution23 needs backward computation.
I1013 11:27:18.812603 12690 net.cpp:198] Eltwise9_penlu17_0_split needs backward computation.
I1013 11:27:18.812609 12690 net.cpp:198] penlu17 needs backward computation.
I1013 11:27:18.812614 12690 net.cpp:198] Scale17 needs backward computation.
I1013 11:27:18.812619 12690 net.cpp:198] BatchNorm17 needs backward computation.
I1013 11:27:18.812624 12690 net.cpp:198] Eltwise9 needs backward computation.
I1013 11:27:18.812629 12690 net.cpp:198] Convolution22 needs backward computation.
I1013 11:27:18.812635 12690 net.cpp:198] Convolution21 needs backward computation.
I1013 11:27:18.812640 12690 net.cpp:198] penlu16 needs backward computation.
I1013 11:27:18.812646 12690 net.cpp:198] Scale16 needs backward computation.
I1013 11:27:18.812651 12690 net.cpp:198] BatchNorm16 needs backward computation.
I1013 11:27:18.812655 12690 net.cpp:198] Convolution20 needs backward computation.
I1013 11:27:18.812660 12690 net.cpp:198] Eltwise8_Eltwise8_0_split needs backward computation.
I1013 11:27:18.812666 12690 net.cpp:198] Eltwise8 needs backward computation.
I1013 11:27:18.812671 12690 net.cpp:198] Convolution19 needs backward computation.
I1013 11:27:18.812677 12690 net.cpp:198] penlu15 needs backward computation.
I1013 11:27:18.812681 12690 net.cpp:198] Scale15 needs backward computation.
I1013 11:27:18.812686 12690 net.cpp:198] BatchNorm15 needs backward computation.
I1013 11:27:18.812690 12690 net.cpp:198] Convolution18 needs backward computation.
I1013 11:27:18.812695 12690 net.cpp:198] Eltwise7_penlu14_0_split needs backward computation.
I1013 11:27:18.812701 12690 net.cpp:198] penlu14 needs backward computation.
I1013 11:27:18.812706 12690 net.cpp:198] Scale14 needs backward computation.
I1013 11:27:18.812711 12690 net.cpp:198] BatchNorm14 needs backward computation.
I1013 11:27:18.812716 12690 net.cpp:198] Eltwise7 needs backward computation.
I1013 11:27:18.812722 12690 net.cpp:198] Convolution17 needs backward computation.
I1013 11:27:18.812727 12690 net.cpp:198] penlu13 needs backward computation.
I1013 11:27:18.812732 12690 net.cpp:198] Scale13 needs backward computation.
I1013 11:27:18.812737 12690 net.cpp:198] BatchNorm13 needs backward computation.
I1013 11:27:18.812742 12690 net.cpp:198] Convolution16 needs backward computation.
I1013 11:27:18.812748 12690 net.cpp:198] Eltwise6_penlu12_0_split needs backward computation.
I1013 11:27:18.812753 12690 net.cpp:198] penlu12 needs backward computation.
I1013 11:27:18.812758 12690 net.cpp:198] Scale12 needs backward computation.
I1013 11:27:18.812763 12690 net.cpp:198] BatchNorm12 needs backward computation.
I1013 11:27:18.812767 12690 net.cpp:198] Eltwise6 needs backward computation.
I1013 11:27:18.812772 12690 net.cpp:198] Convolution15 needs backward computation.
I1013 11:27:18.812778 12690 net.cpp:198] penlu11 needs backward computation.
I1013 11:27:18.812783 12690 net.cpp:198] Scale11 needs backward computation.
I1013 11:27:18.812788 12690 net.cpp:198] BatchNorm11 needs backward computation.
I1013 11:27:18.812793 12690 net.cpp:198] Convolution14 needs backward computation.
I1013 11:27:18.812798 12690 net.cpp:198] Eltwise5_penlu10_0_split needs backward computation.
I1013 11:27:18.812803 12690 net.cpp:198] penlu10 needs backward computation.
I1013 11:27:18.812808 12690 net.cpp:198] Scale10 needs backward computation.
I1013 11:27:18.812813 12690 net.cpp:198] BatchNorm10 needs backward computation.
I1013 11:27:18.812822 12690 net.cpp:198] Eltwise5 needs backward computation.
I1013 11:27:18.812829 12690 net.cpp:198] Convolution13 needs backward computation.
I1013 11:27:18.812834 12690 net.cpp:198] Convolution12 needs backward computation.
I1013 11:27:18.812840 12690 net.cpp:198] penlu9 needs backward computation.
I1013 11:27:18.812845 12690 net.cpp:198] Scale9 needs backward computation.
I1013 11:27:18.812850 12690 net.cpp:198] BatchNorm9 needs backward computation.
I1013 11:27:18.812855 12690 net.cpp:198] Convolution11 needs backward computation.
I1013 11:27:18.812860 12690 net.cpp:198] Eltwise4_Eltwise4_0_split needs backward computation.
I1013 11:27:18.812865 12690 net.cpp:198] Eltwise4 needs backward computation.
I1013 11:27:18.812871 12690 net.cpp:198] Convolution10 needs backward computation.
I1013 11:27:18.812876 12690 net.cpp:198] penlu8 needs backward computation.
I1013 11:27:18.812882 12690 net.cpp:198] Scale8 needs backward computation.
I1013 11:27:18.812886 12690 net.cpp:198] BatchNorm8 needs backward computation.
I1013 11:27:18.812891 12690 net.cpp:198] Convolution9 needs backward computation.
I1013 11:27:18.812896 12690 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1013 11:27:18.812901 12690 net.cpp:198] penlu7 needs backward computation.
I1013 11:27:18.812906 12690 net.cpp:198] Scale7 needs backward computation.
I1013 11:27:18.812911 12690 net.cpp:198] BatchNorm7 needs backward computation.
I1013 11:27:18.812916 12690 net.cpp:198] Eltwise3 needs backward computation.
I1013 11:27:18.812922 12690 net.cpp:198] Convolution8 needs backward computation.
I1013 11:27:18.812928 12690 net.cpp:198] penlu6 needs backward computation.
I1013 11:27:18.812933 12690 net.cpp:198] Scale6 needs backward computation.
I1013 11:27:18.812938 12690 net.cpp:198] BatchNorm6 needs backward computation.
I1013 11:27:18.812942 12690 net.cpp:198] Convolution7 needs backward computation.
I1013 11:27:18.812948 12690 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1013 11:27:18.812953 12690 net.cpp:198] penlu5 needs backward computation.
I1013 11:27:18.812958 12690 net.cpp:198] Scale5 needs backward computation.
I1013 11:27:18.812963 12690 net.cpp:198] BatchNorm5 needs backward computation.
I1013 11:27:18.812966 12690 net.cpp:198] Eltwise2 needs backward computation.
I1013 11:27:18.812974 12690 net.cpp:198] Convolution6 needs backward computation.
I1013 11:27:18.812979 12690 net.cpp:198] penlu4 needs backward computation.
I1013 11:27:18.812984 12690 net.cpp:198] Scale4 needs backward computation.
I1013 11:27:18.812989 12690 net.cpp:198] BatchNorm4 needs backward computation.
I1013 11:27:18.812994 12690 net.cpp:198] Convolution5 needs backward computation.
I1013 11:27:18.812999 12690 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1013 11:27:18.813005 12690 net.cpp:198] penlu3 needs backward computation.
I1013 11:27:18.813010 12690 net.cpp:198] Scale3 needs backward computation.
I1013 11:27:18.813015 12690 net.cpp:198] BatchNorm3 needs backward computation.
I1013 11:27:18.813020 12690 net.cpp:198] Eltwise1 needs backward computation.
I1013 11:27:18.813025 12690 net.cpp:198] Convolution4 needs backward computation.
I1013 11:27:18.813031 12690 net.cpp:198] Convolution3 needs backward computation.
I1013 11:27:18.813036 12690 net.cpp:198] penlu2 needs backward computation.
I1013 11:27:18.813041 12690 net.cpp:198] Scale2 needs backward computation.
I1013 11:27:18.813046 12690 net.cpp:198] BatchNorm2 needs backward computation.
I1013 11:27:18.813051 12690 net.cpp:198] Convolution2 needs backward computation.
I1013 11:27:18.813057 12690 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1013 11:27:18.813063 12690 net.cpp:198] penlu1 needs backward computation.
I1013 11:27:18.813068 12690 net.cpp:198] Scale1 needs backward computation.
I1013 11:27:18.813072 12690 net.cpp:198] BatchNorm1 needs backward computation.
I1013 11:27:18.813076 12690 net.cpp:198] Convolution1 needs backward computation.
I1013 11:27:18.813081 12690 net.cpp:200] cifar does not need backward computation.
I1013 11:27:18.813091 12690 net.cpp:242] This network produces output SoftmaxWithLoss1
I1013 11:27:18.813141 12690 net.cpp:255] Network initialization done.
I1013 11:27:18.815055 12690 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/WRN/WRN_penlu_msra.prototxt
I1013 11:27:18.815068 12690 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1013 11:27:18.815075 12690 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/PENLU/neural/WRN/WRN_penlu_msra.prototxt
I1013 11:27:18.815167 12690 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1013 11:27:18.815655 12690 net.cpp:51] Initializing net from parameters: 
name: "wrn_28_10"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar100/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar100/cifar100_test_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "data"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution4"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution4"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Eltwise1"
  top: "Eltwise1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution5"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Convolution5"
  top: "Convolution6"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution6"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Eltwise2"
  top: "Eltwise2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution7"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Convolution7"
  top: "Convolution8"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Eltwise3"
  top: "Eltwise3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  convolution_param {
    num_output: 160
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution10"
  bottom: "Eltwise3"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution13"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution13"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Eltwise5"
  top: "Eltwise5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution14"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Eltwise6"
  top: "Eltwise6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution17"
  bottom: "Eltwise6"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Eltwise7"
  top: "Eltwise7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution22"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Convolution22"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Eltwise9"
  top: "Eltwise9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution23"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution24"
  bottom: "Eltwise9"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Eltwise10"
  top: "Eltwise10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution25"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution26"
  bottom: "Eltwise10"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Eltwise11"
  top: "Eltwise11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Eltwise11"
  top: "Eltwise11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution27"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu22"
  type: "PENLU"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  convolution_param {
    num_output: 640
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Convolution28"
  bottom: "Eltwise11"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise12"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 100
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
I1013 11:27:18.816124 12690 layer_factory.hpp:77] Creating layer cifar
I1013 11:27:18.816180 12690 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar100/cifar100_test_lmdb
I1013 11:27:18.816195 12690 net.cpp:84] Creating Layer cifar
I1013 11:27:18.816202 12690 net.cpp:380] cifar -> data
I1013 11:27:18.816210 12690 net.cpp:380] cifar -> label
I1013 11:27:18.816216 12690 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar100/mean.binaryproto
I1013 11:27:18.816345 12690 data_layer.cpp:45] output data size: 50,3,28,28
I1013 11:27:18.816990 12690 net.cpp:122] Setting up cifar
I1013 11:27:18.816998 12690 net.cpp:129] Top shape: 50 3 28 28 (117600)
I1013 11:27:18.817003 12690 net.cpp:129] Top shape: 50 (50)
I1013 11:27:18.817004 12690 net.cpp:137] Memory required for data: 470600
I1013 11:27:18.817008 12690 layer_factory.hpp:77] Creating layer label_cifar_1_split
I1013 11:27:18.817011 12690 net.cpp:84] Creating Layer label_cifar_1_split
I1013 11:27:18.817014 12690 net.cpp:406] label_cifar_1_split <- label
I1013 11:27:18.817018 12690 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1013 11:27:18.817023 12690 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1013 11:27:18.817085 12690 net.cpp:122] Setting up label_cifar_1_split
I1013 11:27:18.817090 12690 net.cpp:129] Top shape: 50 (50)
I1013 11:27:18.817092 12690 net.cpp:129] Top shape: 50 (50)
I1013 11:27:18.817095 12690 net.cpp:137] Memory required for data: 471000
I1013 11:27:18.817096 12690 layer_factory.hpp:77] Creating layer Convolution1
I1013 11:27:18.817104 12690 net.cpp:84] Creating Layer Convolution1
I1013 11:27:18.817107 12690 net.cpp:406] Convolution1 <- data
I1013 11:27:18.817111 12690 net.cpp:380] Convolution1 -> Convolution1
I1013 11:27:18.818235 12690 net.cpp:122] Setting up Convolution1
I1013 11:27:18.818245 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:18.818248 12690 net.cpp:137] Memory required for data: 2979800
I1013 11:27:18.818255 12690 layer_factory.hpp:77] Creating layer BatchNorm1
I1013 11:27:18.818261 12690 net.cpp:84] Creating Layer BatchNorm1
I1013 11:27:18.818265 12690 net.cpp:406] BatchNorm1 <- Convolution1
I1013 11:27:18.818270 12690 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1013 11:27:18.818437 12690 net.cpp:122] Setting up BatchNorm1
I1013 11:27:18.818441 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:18.818445 12690 net.cpp:137] Memory required for data: 5488600
I1013 11:27:18.818454 12690 layer_factory.hpp:77] Creating layer Scale1
I1013 11:27:18.818459 12690 net.cpp:84] Creating Layer Scale1
I1013 11:27:18.818461 12690 net.cpp:406] Scale1 <- Convolution1
I1013 11:27:18.818464 12690 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1013 11:27:18.818498 12690 layer_factory.hpp:77] Creating layer Scale1
I1013 11:27:18.818606 12690 net.cpp:122] Setting up Scale1
I1013 11:27:18.818612 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:18.818614 12690 net.cpp:137] Memory required for data: 7997400
I1013 11:27:18.818626 12690 layer_factory.hpp:77] Creating layer penlu1
I1013 11:27:18.818631 12690 net.cpp:84] Creating Layer penlu1
I1013 11:27:18.818634 12690 net.cpp:406] penlu1 <- Convolution1
I1013 11:27:18.818639 12690 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1013 11:27:18.818794 12690 net.cpp:122] Setting up penlu1
I1013 11:27:18.818800 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:18.818802 12690 net.cpp:137] Memory required for data: 10506200
I1013 11:27:18.818809 12690 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1013 11:27:18.818814 12690 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1013 11:27:18.818815 12690 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1013 11:27:18.818819 12690 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1013 11:27:18.818823 12690 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1013 11:27:18.818850 12690 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1013 11:27:18.818855 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:18.818857 12690 net.cpp:129] Top shape: 50 16 28 28 (627200)
I1013 11:27:18.818859 12690 net.cpp:137] Memory required for data: 15523800
I1013 11:27:18.818861 12690 layer_factory.hpp:77] Creating layer Convolution2
I1013 11:27:18.818867 12690 net.cpp:84] Creating Layer Convolution2
I1013 11:27:18.818869 12690 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1013 11:27:18.818876 12690 net.cpp:380] Convolution2 -> Convolution2
I1013 11:27:18.820417 12690 net.cpp:122] Setting up Convolution2
I1013 11:27:18.820427 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.820430 12690 net.cpp:137] Memory required for data: 40611800
I1013 11:27:18.820435 12690 layer_factory.hpp:77] Creating layer BatchNorm2
I1013 11:27:18.820441 12690 net.cpp:84] Creating Layer BatchNorm2
I1013 11:27:18.820443 12690 net.cpp:406] BatchNorm2 <- Convolution2
I1013 11:27:18.820449 12690 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1013 11:27:18.820611 12690 net.cpp:122] Setting up BatchNorm2
I1013 11:27:18.820618 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.820621 12690 net.cpp:137] Memory required for data: 65699800
I1013 11:27:18.820626 12690 layer_factory.hpp:77] Creating layer Scale2
I1013 11:27:18.820631 12690 net.cpp:84] Creating Layer Scale2
I1013 11:27:18.820633 12690 net.cpp:406] Scale2 <- Convolution2
I1013 11:27:18.820636 12690 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1013 11:27:18.820667 12690 layer_factory.hpp:77] Creating layer Scale2
I1013 11:27:18.820760 12690 net.cpp:122] Setting up Scale2
I1013 11:27:18.820765 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.820766 12690 net.cpp:137] Memory required for data: 90787800
I1013 11:27:18.820770 12690 layer_factory.hpp:77] Creating layer penlu2
I1013 11:27:18.820777 12690 net.cpp:84] Creating Layer penlu2
I1013 11:27:18.820780 12690 net.cpp:406] penlu2 <- Convolution2
I1013 11:27:18.820785 12690 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1013 11:27:18.821005 12690 net.cpp:122] Setting up penlu2
I1013 11:27:18.821010 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.821013 12690 net.cpp:137] Memory required for data: 115875800
I1013 11:27:18.821019 12690 layer_factory.hpp:77] Creating layer Convolution3
I1013 11:27:18.821027 12690 net.cpp:84] Creating Layer Convolution3
I1013 11:27:18.821028 12690 net.cpp:406] Convolution3 <- Convolution2
I1013 11:27:18.821033 12690 net.cpp:380] Convolution3 -> Convolution3
I1013 11:27:18.828330 12690 net.cpp:122] Setting up Convolution3
I1013 11:27:18.828341 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.828344 12690 net.cpp:137] Memory required for data: 140963800
I1013 11:27:18.828348 12690 layer_factory.hpp:77] Creating layer Convolution4
I1013 11:27:18.828354 12690 net.cpp:84] Creating Layer Convolution4
I1013 11:27:18.828357 12690 net.cpp:406] Convolution4 <- Convolution1_penlu1_0_split_1
I1013 11:27:18.828363 12690 net.cpp:380] Convolution4 -> Convolution4
I1013 11:27:18.829357 12690 net.cpp:122] Setting up Convolution4
I1013 11:27:18.829366 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.829370 12690 net.cpp:137] Memory required for data: 166051800
I1013 11:27:18.829372 12690 layer_factory.hpp:77] Creating layer Eltwise1
I1013 11:27:18.829378 12690 net.cpp:84] Creating Layer Eltwise1
I1013 11:27:18.829381 12690 net.cpp:406] Eltwise1 <- Convolution3
I1013 11:27:18.829385 12690 net.cpp:406] Eltwise1 <- Convolution4
I1013 11:27:18.829387 12690 net.cpp:380] Eltwise1 -> Eltwise1
I1013 11:27:18.829409 12690 net.cpp:122] Setting up Eltwise1
I1013 11:27:18.829413 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.829416 12690 net.cpp:137] Memory required for data: 191139800
I1013 11:27:18.829417 12690 layer_factory.hpp:77] Creating layer BatchNorm3
I1013 11:27:18.829421 12690 net.cpp:84] Creating Layer BatchNorm3
I1013 11:27:18.829424 12690 net.cpp:406] BatchNorm3 <- Eltwise1
I1013 11:27:18.829428 12690 net.cpp:367] BatchNorm3 -> Eltwise1 (in-place)
I1013 11:27:18.829586 12690 net.cpp:122] Setting up BatchNorm3
I1013 11:27:18.829591 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.829593 12690 net.cpp:137] Memory required for data: 216227800
I1013 11:27:18.829598 12690 layer_factory.hpp:77] Creating layer Scale3
I1013 11:27:18.829602 12690 net.cpp:84] Creating Layer Scale3
I1013 11:27:18.829604 12690 net.cpp:406] Scale3 <- Eltwise1
I1013 11:27:18.829608 12690 net.cpp:367] Scale3 -> Eltwise1 (in-place)
I1013 11:27:18.829639 12690 layer_factory.hpp:77] Creating layer Scale3
I1013 11:27:18.829733 12690 net.cpp:122] Setting up Scale3
I1013 11:27:18.829737 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.829740 12690 net.cpp:137] Memory required for data: 241315800
I1013 11:27:18.829743 12690 layer_factory.hpp:77] Creating layer penlu3
I1013 11:27:18.829751 12690 net.cpp:84] Creating Layer penlu3
I1013 11:27:18.829752 12690 net.cpp:406] penlu3 <- Eltwise1
I1013 11:27:18.829756 12690 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1013 11:27:18.829994 12690 net.cpp:122] Setting up penlu3
I1013 11:27:18.830000 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.830003 12690 net.cpp:137] Memory required for data: 266403800
I1013 11:27:18.830008 12690 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1013 11:27:18.830011 12690 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1013 11:27:18.830013 12690 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1013 11:27:18.830016 12690 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1013 11:27:18.830020 12690 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1013 11:27:18.830049 12690 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1013 11:27:18.830052 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.830055 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.830057 12690 net.cpp:137] Memory required for data: 316579800
I1013 11:27:18.830060 12690 layer_factory.hpp:77] Creating layer Convolution5
I1013 11:27:18.830065 12690 net.cpp:84] Creating Layer Convolution5
I1013 11:27:18.830067 12690 net.cpp:406] Convolution5 <- Eltwise1_penlu3_0_split_0
I1013 11:27:18.830071 12690 net.cpp:380] Convolution5 -> Convolution5
I1013 11:27:18.836747 12690 net.cpp:122] Setting up Convolution5
I1013 11:27:18.836756 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.836760 12690 net.cpp:137] Memory required for data: 341667800
I1013 11:27:18.836762 12690 layer_factory.hpp:77] Creating layer BatchNorm4
I1013 11:27:18.836768 12690 net.cpp:84] Creating Layer BatchNorm4
I1013 11:27:18.836771 12690 net.cpp:406] BatchNorm4 <- Convolution5
I1013 11:27:18.836776 12690 net.cpp:367] BatchNorm4 -> Convolution5 (in-place)
I1013 11:27:18.836963 12690 net.cpp:122] Setting up BatchNorm4
I1013 11:27:18.836971 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.836974 12690 net.cpp:137] Memory required for data: 366755800
I1013 11:27:18.836988 12690 layer_factory.hpp:77] Creating layer Scale4
I1013 11:27:18.837003 12690 net.cpp:84] Creating Layer Scale4
I1013 11:27:18.837007 12690 net.cpp:406] Scale4 <- Convolution5
I1013 11:27:18.837013 12690 net.cpp:367] Scale4 -> Convolution5 (in-place)
I1013 11:27:18.837059 12690 layer_factory.hpp:77] Creating layer Scale4
I1013 11:27:18.837191 12690 net.cpp:122] Setting up Scale4
I1013 11:27:18.837199 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.837203 12690 net.cpp:137] Memory required for data: 391843800
I1013 11:27:18.837210 12690 layer_factory.hpp:77] Creating layer penlu4
I1013 11:27:18.837218 12690 net.cpp:84] Creating Layer penlu4
I1013 11:27:18.837222 12690 net.cpp:406] penlu4 <- Convolution5
I1013 11:27:18.837229 12690 net.cpp:367] penlu4 -> Convolution5 (in-place)
I1013 11:27:18.837466 12690 net.cpp:122] Setting up penlu4
I1013 11:27:18.837471 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.837473 12690 net.cpp:137] Memory required for data: 416931800
I1013 11:27:18.837478 12690 layer_factory.hpp:77] Creating layer Convolution6
I1013 11:27:18.837486 12690 net.cpp:84] Creating Layer Convolution6
I1013 11:27:18.837488 12690 net.cpp:406] Convolution6 <- Convolution5
I1013 11:27:18.837491 12690 net.cpp:380] Convolution6 -> Convolution6
I1013 11:27:18.844586 12690 net.cpp:122] Setting up Convolution6
I1013 11:27:18.844597 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.844600 12690 net.cpp:137] Memory required for data: 442019800
I1013 11:27:18.844604 12690 layer_factory.hpp:77] Creating layer Eltwise2
I1013 11:27:18.844610 12690 net.cpp:84] Creating Layer Eltwise2
I1013 11:27:18.844614 12690 net.cpp:406] Eltwise2 <- Convolution6
I1013 11:27:18.844616 12690 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1013 11:27:18.844621 12690 net.cpp:380] Eltwise2 -> Eltwise2
I1013 11:27:18.844645 12690 net.cpp:122] Setting up Eltwise2
I1013 11:27:18.844650 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.844651 12690 net.cpp:137] Memory required for data: 467107800
I1013 11:27:18.844653 12690 layer_factory.hpp:77] Creating layer BatchNorm5
I1013 11:27:18.844658 12690 net.cpp:84] Creating Layer BatchNorm5
I1013 11:27:18.844660 12690 net.cpp:406] BatchNorm5 <- Eltwise2
I1013 11:27:18.844665 12690 net.cpp:367] BatchNorm5 -> Eltwise2 (in-place)
I1013 11:27:18.844823 12690 net.cpp:122] Setting up BatchNorm5
I1013 11:27:18.844827 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.844830 12690 net.cpp:137] Memory required for data: 492195800
I1013 11:27:18.844835 12690 layer_factory.hpp:77] Creating layer Scale5
I1013 11:27:18.844840 12690 net.cpp:84] Creating Layer Scale5
I1013 11:27:18.844841 12690 net.cpp:406] Scale5 <- Eltwise2
I1013 11:27:18.844844 12690 net.cpp:367] Scale5 -> Eltwise2 (in-place)
I1013 11:27:18.844877 12690 layer_factory.hpp:77] Creating layer Scale5
I1013 11:27:18.844969 12690 net.cpp:122] Setting up Scale5
I1013 11:27:18.844974 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.844975 12690 net.cpp:137] Memory required for data: 517283800
I1013 11:27:18.844980 12690 layer_factory.hpp:77] Creating layer penlu5
I1013 11:27:18.844983 12690 net.cpp:84] Creating Layer penlu5
I1013 11:27:18.844986 12690 net.cpp:406] penlu5 <- Eltwise2
I1013 11:27:18.844990 12690 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1013 11:27:18.845192 12690 net.cpp:122] Setting up penlu5
I1013 11:27:18.845197 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.845199 12690 net.cpp:137] Memory required for data: 542371800
I1013 11:27:18.845203 12690 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1013 11:27:18.845207 12690 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1013 11:27:18.845209 12690 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1013 11:27:18.845213 12690 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1013 11:27:18.845219 12690 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1013 11:27:18.845247 12690 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1013 11:27:18.845260 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.845263 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.845265 12690 net.cpp:137] Memory required for data: 592547800
I1013 11:27:18.845268 12690 layer_factory.hpp:77] Creating layer Convolution7
I1013 11:27:18.845274 12690 net.cpp:84] Creating Layer Convolution7
I1013 11:27:18.845276 12690 net.cpp:406] Convolution7 <- Eltwise2_penlu5_0_split_0
I1013 11:27:18.845280 12690 net.cpp:380] Convolution7 -> Convolution7
I1013 11:27:18.852129 12690 net.cpp:122] Setting up Convolution7
I1013 11:27:18.852138 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.852140 12690 net.cpp:137] Memory required for data: 617635800
I1013 11:27:18.852144 12690 layer_factory.hpp:77] Creating layer BatchNorm6
I1013 11:27:18.852150 12690 net.cpp:84] Creating Layer BatchNorm6
I1013 11:27:18.852152 12690 net.cpp:406] BatchNorm6 <- Convolution7
I1013 11:27:18.852157 12690 net.cpp:367] BatchNorm6 -> Convolution7 (in-place)
I1013 11:27:18.852314 12690 net.cpp:122] Setting up BatchNorm6
I1013 11:27:18.852319 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.852321 12690 net.cpp:137] Memory required for data: 642723800
I1013 11:27:18.852326 12690 layer_factory.hpp:77] Creating layer Scale6
I1013 11:27:18.852331 12690 net.cpp:84] Creating Layer Scale6
I1013 11:27:18.852334 12690 net.cpp:406] Scale6 <- Convolution7
I1013 11:27:18.852337 12690 net.cpp:367] Scale6 -> Convolution7 (in-place)
I1013 11:27:18.852367 12690 layer_factory.hpp:77] Creating layer Scale6
I1013 11:27:18.852459 12690 net.cpp:122] Setting up Scale6
I1013 11:27:18.852463 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.852465 12690 net.cpp:137] Memory required for data: 667811800
I1013 11:27:18.852469 12690 layer_factory.hpp:77] Creating layer penlu6
I1013 11:27:18.852474 12690 net.cpp:84] Creating Layer penlu6
I1013 11:27:18.852478 12690 net.cpp:406] penlu6 <- Convolution7
I1013 11:27:18.852480 12690 net.cpp:367] penlu6 -> Convolution7 (in-place)
I1013 11:27:18.852674 12690 net.cpp:122] Setting up penlu6
I1013 11:27:18.852679 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.852680 12690 net.cpp:137] Memory required for data: 692899800
I1013 11:27:18.852684 12690 layer_factory.hpp:77] Creating layer Convolution8
I1013 11:27:18.852694 12690 net.cpp:84] Creating Layer Convolution8
I1013 11:27:18.852695 12690 net.cpp:406] Convolution8 <- Convolution7
I1013 11:27:18.852699 12690 net.cpp:380] Convolution8 -> Convolution8
I1013 11:27:18.859483 12690 net.cpp:122] Setting up Convolution8
I1013 11:27:18.859493 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.859496 12690 net.cpp:137] Memory required for data: 717987800
I1013 11:27:18.859500 12690 layer_factory.hpp:77] Creating layer Eltwise3
I1013 11:27:18.859505 12690 net.cpp:84] Creating Layer Eltwise3
I1013 11:27:18.859508 12690 net.cpp:406] Eltwise3 <- Convolution8
I1013 11:27:18.859511 12690 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1013 11:27:18.859514 12690 net.cpp:380] Eltwise3 -> Eltwise3
I1013 11:27:18.859539 12690 net.cpp:122] Setting up Eltwise3
I1013 11:27:18.859542 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.859544 12690 net.cpp:137] Memory required for data: 743075800
I1013 11:27:18.859546 12690 layer_factory.hpp:77] Creating layer BatchNorm7
I1013 11:27:18.859551 12690 net.cpp:84] Creating Layer BatchNorm7
I1013 11:27:18.859553 12690 net.cpp:406] BatchNorm7 <- Eltwise3
I1013 11:27:18.859556 12690 net.cpp:367] BatchNorm7 -> Eltwise3 (in-place)
I1013 11:27:18.859711 12690 net.cpp:122] Setting up BatchNorm7
I1013 11:27:18.859715 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.859717 12690 net.cpp:137] Memory required for data: 768163800
I1013 11:27:18.859722 12690 layer_factory.hpp:77] Creating layer Scale7
I1013 11:27:18.859726 12690 net.cpp:84] Creating Layer Scale7
I1013 11:27:18.859728 12690 net.cpp:406] Scale7 <- Eltwise3
I1013 11:27:18.859731 12690 net.cpp:367] Scale7 -> Eltwise3 (in-place)
I1013 11:27:18.859774 12690 layer_factory.hpp:77] Creating layer Scale7
I1013 11:27:18.859865 12690 net.cpp:122] Setting up Scale7
I1013 11:27:18.859871 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.859874 12690 net.cpp:137] Memory required for data: 793251800
I1013 11:27:18.859877 12690 layer_factory.hpp:77] Creating layer penlu7
I1013 11:27:18.859882 12690 net.cpp:84] Creating Layer penlu7
I1013 11:27:18.859884 12690 net.cpp:406] penlu7 <- Eltwise3
I1013 11:27:18.859889 12690 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1013 11:27:18.860081 12690 net.cpp:122] Setting up penlu7
I1013 11:27:18.860085 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.860087 12690 net.cpp:137] Memory required for data: 818339800
I1013 11:27:18.860100 12690 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1013 11:27:18.860105 12690 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1013 11:27:18.860106 12690 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1013 11:27:18.860110 12690 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1013 11:27:18.860114 12690 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1013 11:27:18.860141 12690 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1013 11:27:18.860146 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.860148 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.860150 12690 net.cpp:137] Memory required for data: 868515800
I1013 11:27:18.860152 12690 layer_factory.hpp:77] Creating layer Convolution9
I1013 11:27:18.860157 12690 net.cpp:84] Creating Layer Convolution9
I1013 11:27:18.860160 12690 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_0
I1013 11:27:18.860164 12690 net.cpp:380] Convolution9 -> Convolution9
I1013 11:27:18.866864 12690 net.cpp:122] Setting up Convolution9
I1013 11:27:18.866873 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.866875 12690 net.cpp:137] Memory required for data: 893603800
I1013 11:27:18.866879 12690 layer_factory.hpp:77] Creating layer BatchNorm8
I1013 11:27:18.866884 12690 net.cpp:84] Creating Layer BatchNorm8
I1013 11:27:18.866888 12690 net.cpp:406] BatchNorm8 <- Convolution9
I1013 11:27:18.866891 12690 net.cpp:367] BatchNorm8 -> Convolution9 (in-place)
I1013 11:27:18.867050 12690 net.cpp:122] Setting up BatchNorm8
I1013 11:27:18.867055 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.867058 12690 net.cpp:137] Memory required for data: 918691800
I1013 11:27:18.867063 12690 layer_factory.hpp:77] Creating layer Scale8
I1013 11:27:18.867067 12690 net.cpp:84] Creating Layer Scale8
I1013 11:27:18.867069 12690 net.cpp:406] Scale8 <- Convolution9
I1013 11:27:18.867072 12690 net.cpp:367] Scale8 -> Convolution9 (in-place)
I1013 11:27:18.867103 12690 layer_factory.hpp:77] Creating layer Scale8
I1013 11:27:18.867193 12690 net.cpp:122] Setting up Scale8
I1013 11:27:18.867197 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.867199 12690 net.cpp:137] Memory required for data: 943779800
I1013 11:27:18.867203 12690 layer_factory.hpp:77] Creating layer penlu8
I1013 11:27:18.867209 12690 net.cpp:84] Creating Layer penlu8
I1013 11:27:18.867211 12690 net.cpp:406] penlu8 <- Convolution9
I1013 11:27:18.867215 12690 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1013 11:27:18.867413 12690 net.cpp:122] Setting up penlu8
I1013 11:27:18.867416 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.867419 12690 net.cpp:137] Memory required for data: 968867800
I1013 11:27:18.867422 12690 layer_factory.hpp:77] Creating layer Convolution10
I1013 11:27:18.867429 12690 net.cpp:84] Creating Layer Convolution10
I1013 11:27:18.867431 12690 net.cpp:406] Convolution10 <- Convolution9
I1013 11:27:18.867435 12690 net.cpp:380] Convolution10 -> Convolution10
I1013 11:27:18.874665 12690 net.cpp:122] Setting up Convolution10
I1013 11:27:18.874678 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.874682 12690 net.cpp:137] Memory required for data: 993955800
I1013 11:27:18.874697 12690 layer_factory.hpp:77] Creating layer Eltwise4
I1013 11:27:18.874706 12690 net.cpp:84] Creating Layer Eltwise4
I1013 11:27:18.874708 12690 net.cpp:406] Eltwise4 <- Convolution10
I1013 11:27:18.874712 12690 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I1013 11:27:18.874716 12690 net.cpp:380] Eltwise4 -> Eltwise4
I1013 11:27:18.874742 12690 net.cpp:122] Setting up Eltwise4
I1013 11:27:18.874747 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.874748 12690 net.cpp:137] Memory required for data: 1019043800
I1013 11:27:18.874750 12690 layer_factory.hpp:77] Creating layer Eltwise4_Eltwise4_0_split
I1013 11:27:18.874754 12690 net.cpp:84] Creating Layer Eltwise4_Eltwise4_0_split
I1013 11:27:18.874756 12690 net.cpp:406] Eltwise4_Eltwise4_0_split <- Eltwise4
I1013 11:27:18.874760 12690 net.cpp:380] Eltwise4_Eltwise4_0_split -> Eltwise4_Eltwise4_0_split_0
I1013 11:27:18.874765 12690 net.cpp:380] Eltwise4_Eltwise4_0_split -> Eltwise4_Eltwise4_0_split_1
I1013 11:27:18.874794 12690 net.cpp:122] Setting up Eltwise4_Eltwise4_0_split
I1013 11:27:18.874797 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.874800 12690 net.cpp:129] Top shape: 50 160 28 28 (6272000)
I1013 11:27:18.874802 12690 net.cpp:137] Memory required for data: 1069219800
I1013 11:27:18.874804 12690 layer_factory.hpp:77] Creating layer Convolution11
I1013 11:27:18.874810 12690 net.cpp:84] Creating Layer Convolution11
I1013 11:27:18.874814 12690 net.cpp:406] Convolution11 <- Eltwise4_Eltwise4_0_split_0
I1013 11:27:18.874816 12690 net.cpp:380] Convolution11 -> Convolution11
I1013 11:27:18.886365 12690 net.cpp:122] Setting up Convolution11
I1013 11:27:18.886374 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.886376 12690 net.cpp:137] Memory required for data: 1081763800
I1013 11:27:18.886380 12690 layer_factory.hpp:77] Creating layer BatchNorm9
I1013 11:27:18.886386 12690 net.cpp:84] Creating Layer BatchNorm9
I1013 11:27:18.886389 12690 net.cpp:406] BatchNorm9 <- Convolution11
I1013 11:27:18.886392 12690 net.cpp:367] BatchNorm9 -> Convolution11 (in-place)
I1013 11:27:18.886566 12690 net.cpp:122] Setting up BatchNorm9
I1013 11:27:18.886571 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.886574 12690 net.cpp:137] Memory required for data: 1094307800
I1013 11:27:18.886579 12690 layer_factory.hpp:77] Creating layer Scale9
I1013 11:27:18.886584 12690 net.cpp:84] Creating Layer Scale9
I1013 11:27:18.886585 12690 net.cpp:406] Scale9 <- Convolution11
I1013 11:27:18.886589 12690 net.cpp:367] Scale9 -> Convolution11 (in-place)
I1013 11:27:18.886626 12690 layer_factory.hpp:77] Creating layer Scale9
I1013 11:27:18.886718 12690 net.cpp:122] Setting up Scale9
I1013 11:27:18.886723 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.886724 12690 net.cpp:137] Memory required for data: 1106851800
I1013 11:27:18.886729 12690 layer_factory.hpp:77] Creating layer penlu9
I1013 11:27:18.886734 12690 net.cpp:84] Creating Layer penlu9
I1013 11:27:18.886736 12690 net.cpp:406] penlu9 <- Convolution11
I1013 11:27:18.886740 12690 net.cpp:367] penlu9 -> Convolution11 (in-place)
I1013 11:27:18.886906 12690 net.cpp:122] Setting up penlu9
I1013 11:27:18.886911 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.886914 12690 net.cpp:137] Memory required for data: 1119395800
I1013 11:27:18.886917 12690 layer_factory.hpp:77] Creating layer Convolution12
I1013 11:27:18.886924 12690 net.cpp:84] Creating Layer Convolution12
I1013 11:27:18.886926 12690 net.cpp:406] Convolution12 <- Convolution11
I1013 11:27:18.886930 12690 net.cpp:380] Convolution12 -> Convolution12
I1013 11:27:18.909471 12690 net.cpp:122] Setting up Convolution12
I1013 11:27:18.909492 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.909497 12690 net.cpp:137] Memory required for data: 1131939800
I1013 11:27:18.909504 12690 layer_factory.hpp:77] Creating layer Convolution13
I1013 11:27:18.909518 12690 net.cpp:84] Creating Layer Convolution13
I1013 11:27:18.909524 12690 net.cpp:406] Convolution13 <- Eltwise4_Eltwise4_0_split_1
I1013 11:27:18.909545 12690 net.cpp:380] Convolution13 -> Convolution13
I1013 11:27:18.911962 12690 net.cpp:122] Setting up Convolution13
I1013 11:27:18.911972 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.911978 12690 net.cpp:137] Memory required for data: 1144483800
I1013 11:27:18.911983 12690 layer_factory.hpp:77] Creating layer Eltwise5
I1013 11:27:18.911994 12690 net.cpp:84] Creating Layer Eltwise5
I1013 11:27:18.911999 12690 net.cpp:406] Eltwise5 <- Convolution12
I1013 11:27:18.912004 12690 net.cpp:406] Eltwise5 <- Convolution13
I1013 11:27:18.912011 12690 net.cpp:380] Eltwise5 -> Eltwise5
I1013 11:27:18.912039 12690 net.cpp:122] Setting up Eltwise5
I1013 11:27:18.912046 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.912050 12690 net.cpp:137] Memory required for data: 1157027800
I1013 11:27:18.912055 12690 layer_factory.hpp:77] Creating layer BatchNorm10
I1013 11:27:18.912063 12690 net.cpp:84] Creating Layer BatchNorm10
I1013 11:27:18.912067 12690 net.cpp:406] BatchNorm10 <- Eltwise5
I1013 11:27:18.912073 12690 net.cpp:367] BatchNorm10 -> Eltwise5 (in-place)
I1013 11:27:18.912253 12690 net.cpp:122] Setting up BatchNorm10
I1013 11:27:18.912261 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.912264 12690 net.cpp:137] Memory required for data: 1169571800
I1013 11:27:18.912274 12690 layer_factory.hpp:77] Creating layer Scale10
I1013 11:27:18.912281 12690 net.cpp:84] Creating Layer Scale10
I1013 11:27:18.912286 12690 net.cpp:406] Scale10 <- Eltwise5
I1013 11:27:18.912292 12690 net.cpp:367] Scale10 -> Eltwise5 (in-place)
I1013 11:27:18.912335 12690 layer_factory.hpp:77] Creating layer Scale10
I1013 11:27:18.912433 12690 net.cpp:122] Setting up Scale10
I1013 11:27:18.912441 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.912444 12690 net.cpp:137] Memory required for data: 1182115800
I1013 11:27:18.912452 12690 layer_factory.hpp:77] Creating layer penlu10
I1013 11:27:18.912462 12690 net.cpp:84] Creating Layer penlu10
I1013 11:27:18.912466 12690 net.cpp:406] penlu10 <- Eltwise5
I1013 11:27:18.912473 12690 net.cpp:367] penlu10 -> Eltwise5 (in-place)
I1013 11:27:18.912643 12690 net.cpp:122] Setting up penlu10
I1013 11:27:18.912650 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.912654 12690 net.cpp:137] Memory required for data: 1194659800
I1013 11:27:18.912662 12690 layer_factory.hpp:77] Creating layer Eltwise5_penlu10_0_split
I1013 11:27:18.912670 12690 net.cpp:84] Creating Layer Eltwise5_penlu10_0_split
I1013 11:27:18.912674 12690 net.cpp:406] Eltwise5_penlu10_0_split <- Eltwise5
I1013 11:27:18.912680 12690 net.cpp:380] Eltwise5_penlu10_0_split -> Eltwise5_penlu10_0_split_0
I1013 11:27:18.912691 12690 net.cpp:380] Eltwise5_penlu10_0_split -> Eltwise5_penlu10_0_split_1
I1013 11:27:18.912724 12690 net.cpp:122] Setting up Eltwise5_penlu10_0_split
I1013 11:27:18.912731 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.912737 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.912741 12690 net.cpp:137] Memory required for data: 1219747800
I1013 11:27:18.912745 12690 layer_factory.hpp:77] Creating layer Convolution14
I1013 11:27:18.912755 12690 net.cpp:84] Creating Layer Convolution14
I1013 11:27:18.912758 12690 net.cpp:406] Convolution14 <- Eltwise5_penlu10_0_split_0
I1013 11:27:18.912766 12690 net.cpp:380] Convolution14 -> Convolution14
I1013 11:27:18.935137 12690 net.cpp:122] Setting up Convolution14
I1013 11:27:18.935158 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.935163 12690 net.cpp:137] Memory required for data: 1232291800
I1013 11:27:18.935169 12690 layer_factory.hpp:77] Creating layer BatchNorm11
I1013 11:27:18.935181 12690 net.cpp:84] Creating Layer BatchNorm11
I1013 11:27:18.935201 12690 net.cpp:406] BatchNorm11 <- Convolution14
I1013 11:27:18.935220 12690 net.cpp:367] BatchNorm11 -> Convolution14 (in-place)
I1013 11:27:18.935446 12690 net.cpp:122] Setting up BatchNorm11
I1013 11:27:18.935454 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.935468 12690 net.cpp:137] Memory required for data: 1244835800
I1013 11:27:18.935493 12690 layer_factory.hpp:77] Creating layer Scale11
I1013 11:27:18.935503 12690 net.cpp:84] Creating Layer Scale11
I1013 11:27:18.935508 12690 net.cpp:406] Scale11 <- Convolution14
I1013 11:27:18.935513 12690 net.cpp:367] Scale11 -> Convolution14 (in-place)
I1013 11:27:18.935569 12690 layer_factory.hpp:77] Creating layer Scale11
I1013 11:27:18.935669 12690 net.cpp:122] Setting up Scale11
I1013 11:27:18.935678 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.935681 12690 net.cpp:137] Memory required for data: 1257379800
I1013 11:27:18.935688 12690 layer_factory.hpp:77] Creating layer penlu11
I1013 11:27:18.935698 12690 net.cpp:84] Creating Layer penlu11
I1013 11:27:18.935701 12690 net.cpp:406] penlu11 <- Convolution14
I1013 11:27:18.935708 12690 net.cpp:367] penlu11 -> Convolution14 (in-place)
I1013 11:27:18.936400 12690 net.cpp:122] Setting up penlu11
I1013 11:27:18.936410 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.936414 12690 net.cpp:137] Memory required for data: 1269923800
I1013 11:27:18.936424 12690 layer_factory.hpp:77] Creating layer Convolution15
I1013 11:27:18.936434 12690 net.cpp:84] Creating Layer Convolution15
I1013 11:27:18.936449 12690 net.cpp:406] Convolution15 <- Convolution14
I1013 11:27:18.936457 12690 net.cpp:380] Convolution15 -> Convolution15
I1013 11:27:18.957974 12690 net.cpp:122] Setting up Convolution15
I1013 11:27:18.957986 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.957991 12690 net.cpp:137] Memory required for data: 1282467800
I1013 11:27:18.957998 12690 layer_factory.hpp:77] Creating layer Eltwise6
I1013 11:27:18.958009 12690 net.cpp:84] Creating Layer Eltwise6
I1013 11:27:18.958014 12690 net.cpp:406] Eltwise6 <- Convolution15
I1013 11:27:18.958021 12690 net.cpp:406] Eltwise6 <- Eltwise5_penlu10_0_split_1
I1013 11:27:18.958027 12690 net.cpp:380] Eltwise6 -> Eltwise6
I1013 11:27:18.958058 12690 net.cpp:122] Setting up Eltwise6
I1013 11:27:18.958065 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.958070 12690 net.cpp:137] Memory required for data: 1295011800
I1013 11:27:18.958075 12690 layer_factory.hpp:77] Creating layer BatchNorm12
I1013 11:27:18.958082 12690 net.cpp:84] Creating Layer BatchNorm12
I1013 11:27:18.958086 12690 net.cpp:406] BatchNorm12 <- Eltwise6
I1013 11:27:18.958092 12690 net.cpp:367] BatchNorm12 -> Eltwise6 (in-place)
I1013 11:27:18.958268 12690 net.cpp:122] Setting up BatchNorm12
I1013 11:27:18.958276 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.958279 12690 net.cpp:137] Memory required for data: 1307555800
I1013 11:27:18.958289 12690 layer_factory.hpp:77] Creating layer Scale12
I1013 11:27:18.958295 12690 net.cpp:84] Creating Layer Scale12
I1013 11:27:18.958300 12690 net.cpp:406] Scale12 <- Eltwise6
I1013 11:27:18.958307 12690 net.cpp:367] Scale12 -> Eltwise6 (in-place)
I1013 11:27:18.958350 12690 layer_factory.hpp:77] Creating layer Scale12
I1013 11:27:18.958453 12690 net.cpp:122] Setting up Scale12
I1013 11:27:18.958461 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.958464 12690 net.cpp:137] Memory required for data: 1320099800
I1013 11:27:18.958472 12690 layer_factory.hpp:77] Creating layer penlu12
I1013 11:27:18.958487 12690 net.cpp:84] Creating Layer penlu12
I1013 11:27:18.958492 12690 net.cpp:406] penlu12 <- Eltwise6
I1013 11:27:18.958498 12690 net.cpp:367] penlu12 -> Eltwise6 (in-place)
I1013 11:27:18.958673 12690 net.cpp:122] Setting up penlu12
I1013 11:27:18.958679 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.958683 12690 net.cpp:137] Memory required for data: 1332643800
I1013 11:27:18.958693 12690 layer_factory.hpp:77] Creating layer Eltwise6_penlu12_0_split
I1013 11:27:18.958701 12690 net.cpp:84] Creating Layer Eltwise6_penlu12_0_split
I1013 11:27:18.958705 12690 net.cpp:406] Eltwise6_penlu12_0_split <- Eltwise6
I1013 11:27:18.958711 12690 net.cpp:380] Eltwise6_penlu12_0_split -> Eltwise6_penlu12_0_split_0
I1013 11:27:18.958727 12690 net.cpp:380] Eltwise6_penlu12_0_split -> Eltwise6_penlu12_0_split_1
I1013 11:27:18.958763 12690 net.cpp:122] Setting up Eltwise6_penlu12_0_split
I1013 11:27:18.958770 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.958776 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.958781 12690 net.cpp:137] Memory required for data: 1357731800
I1013 11:27:18.958784 12690 layer_factory.hpp:77] Creating layer Convolution16
I1013 11:27:18.958796 12690 net.cpp:84] Creating Layer Convolution16
I1013 11:27:18.958799 12690 net.cpp:406] Convolution16 <- Eltwise6_penlu12_0_split_0
I1013 11:27:18.958806 12690 net.cpp:380] Convolution16 -> Convolution16
I1013 11:27:18.981051 12690 net.cpp:122] Setting up Convolution16
I1013 11:27:18.981072 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.981076 12690 net.cpp:137] Memory required for data: 1370275800
I1013 11:27:18.981086 12690 layer_factory.hpp:77] Creating layer BatchNorm13
I1013 11:27:18.981096 12690 net.cpp:84] Creating Layer BatchNorm13
I1013 11:27:18.981103 12690 net.cpp:406] BatchNorm13 <- Convolution16
I1013 11:27:18.981112 12690 net.cpp:367] BatchNorm13 -> Convolution16 (in-place)
I1013 11:27:18.981304 12690 net.cpp:122] Setting up BatchNorm13
I1013 11:27:18.981312 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.981317 12690 net.cpp:137] Memory required for data: 1382819800
I1013 11:27:18.981325 12690 layer_factory.hpp:77] Creating layer Scale13
I1013 11:27:18.981333 12690 net.cpp:84] Creating Layer Scale13
I1013 11:27:18.981338 12690 net.cpp:406] Scale13 <- Convolution16
I1013 11:27:18.981344 12690 net.cpp:367] Scale13 -> Convolution16 (in-place)
I1013 11:27:18.981390 12690 layer_factory.hpp:77] Creating layer Scale13
I1013 11:27:18.981495 12690 net.cpp:122] Setting up Scale13
I1013 11:27:18.981503 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.981506 12690 net.cpp:137] Memory required for data: 1395363800
I1013 11:27:18.981514 12690 layer_factory.hpp:77] Creating layer penlu13
I1013 11:27:18.981525 12690 net.cpp:84] Creating Layer penlu13
I1013 11:27:18.981529 12690 net.cpp:406] penlu13 <- Convolution16
I1013 11:27:18.981536 12690 net.cpp:367] penlu13 -> Convolution16 (in-place)
I1013 11:27:18.981710 12690 net.cpp:122] Setting up penlu13
I1013 11:27:18.981717 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:18.981721 12690 net.cpp:137] Memory required for data: 1407907800
I1013 11:27:18.981730 12690 layer_factory.hpp:77] Creating layer Convolution17
I1013 11:27:18.981751 12690 net.cpp:84] Creating Layer Convolution17
I1013 11:27:18.981765 12690 net.cpp:406] Convolution17 <- Convolution16
I1013 11:27:18.981772 12690 net.cpp:380] Convolution17 -> Convolution17
I1013 11:27:19.004178 12690 net.cpp:122] Setting up Convolution17
I1013 11:27:19.004196 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.004200 12690 net.cpp:137] Memory required for data: 1420451800
I1013 11:27:19.004218 12690 layer_factory.hpp:77] Creating layer Eltwise7
I1013 11:27:19.004230 12690 net.cpp:84] Creating Layer Eltwise7
I1013 11:27:19.004236 12690 net.cpp:406] Eltwise7 <- Convolution17
I1013 11:27:19.004245 12690 net.cpp:406] Eltwise7 <- Eltwise6_penlu12_0_split_1
I1013 11:27:19.004252 12690 net.cpp:380] Eltwise7 -> Eltwise7
I1013 11:27:19.004302 12690 net.cpp:122] Setting up Eltwise7
I1013 11:27:19.004309 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.004313 12690 net.cpp:137] Memory required for data: 1432995800
I1013 11:27:19.004326 12690 layer_factory.hpp:77] Creating layer BatchNorm14
I1013 11:27:19.004345 12690 net.cpp:84] Creating Layer BatchNorm14
I1013 11:27:19.004350 12690 net.cpp:406] BatchNorm14 <- Eltwise7
I1013 11:27:19.004355 12690 net.cpp:367] BatchNorm14 -> Eltwise7 (in-place)
I1013 11:27:19.004554 12690 net.cpp:122] Setting up BatchNorm14
I1013 11:27:19.004561 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.004565 12690 net.cpp:137] Memory required for data: 1445539800
I1013 11:27:19.004585 12690 layer_factory.hpp:77] Creating layer Scale14
I1013 11:27:19.004602 12690 net.cpp:84] Creating Layer Scale14
I1013 11:27:19.004607 12690 net.cpp:406] Scale14 <- Eltwise7
I1013 11:27:19.004613 12690 net.cpp:367] Scale14 -> Eltwise7 (in-place)
I1013 11:27:19.004672 12690 layer_factory.hpp:77] Creating layer Scale14
I1013 11:27:19.004796 12690 net.cpp:122] Setting up Scale14
I1013 11:27:19.004803 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.004817 12690 net.cpp:137] Memory required for data: 1458083800
I1013 11:27:19.004825 12690 layer_factory.hpp:77] Creating layer penlu14
I1013 11:27:19.004835 12690 net.cpp:84] Creating Layer penlu14
I1013 11:27:19.004839 12690 net.cpp:406] penlu14 <- Eltwise7
I1013 11:27:19.004848 12690 net.cpp:367] penlu14 -> Eltwise7 (in-place)
I1013 11:27:19.005035 12690 net.cpp:122] Setting up penlu14
I1013 11:27:19.005043 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.005046 12690 net.cpp:137] Memory required for data: 1470627800
I1013 11:27:19.005077 12690 layer_factory.hpp:77] Creating layer Eltwise7_penlu14_0_split
I1013 11:27:19.005085 12690 net.cpp:84] Creating Layer Eltwise7_penlu14_0_split
I1013 11:27:19.005090 12690 net.cpp:406] Eltwise7_penlu14_0_split <- Eltwise7
I1013 11:27:19.005096 12690 net.cpp:380] Eltwise7_penlu14_0_split -> Eltwise7_penlu14_0_split_0
I1013 11:27:19.005105 12690 net.cpp:380] Eltwise7_penlu14_0_split -> Eltwise7_penlu14_0_split_1
I1013 11:27:19.005141 12690 net.cpp:122] Setting up Eltwise7_penlu14_0_split
I1013 11:27:19.005147 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.005154 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.005158 12690 net.cpp:137] Memory required for data: 1495715800
I1013 11:27:19.005162 12690 layer_factory.hpp:77] Creating layer Convolution18
I1013 11:27:19.005174 12690 net.cpp:84] Creating Layer Convolution18
I1013 11:27:19.005178 12690 net.cpp:406] Convolution18 <- Eltwise7_penlu14_0_split_0
I1013 11:27:19.005185 12690 net.cpp:380] Convolution18 -> Convolution18
I1013 11:27:19.027768 12690 net.cpp:122] Setting up Convolution18
I1013 11:27:19.027788 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.027792 12690 net.cpp:137] Memory required for data: 1508259800
I1013 11:27:19.027801 12690 layer_factory.hpp:77] Creating layer BatchNorm15
I1013 11:27:19.027811 12690 net.cpp:84] Creating Layer BatchNorm15
I1013 11:27:19.027827 12690 net.cpp:406] BatchNorm15 <- Convolution18
I1013 11:27:19.027837 12690 net.cpp:367] BatchNorm15 -> Convolution18 (in-place)
I1013 11:27:19.028041 12690 net.cpp:122] Setting up BatchNorm15
I1013 11:27:19.028049 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.028053 12690 net.cpp:137] Memory required for data: 1520803800
I1013 11:27:19.028076 12690 layer_factory.hpp:77] Creating layer Scale15
I1013 11:27:19.028084 12690 net.cpp:84] Creating Layer Scale15
I1013 11:27:19.028090 12690 net.cpp:406] Scale15 <- Convolution18
I1013 11:27:19.028095 12690 net.cpp:367] Scale15 -> Convolution18 (in-place)
I1013 11:27:19.028151 12690 layer_factory.hpp:77] Creating layer Scale15
I1013 11:27:19.028277 12690 net.cpp:122] Setting up Scale15
I1013 11:27:19.028285 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.028290 12690 net.cpp:137] Memory required for data: 1533347800
I1013 11:27:19.028307 12690 layer_factory.hpp:77] Creating layer penlu15
I1013 11:27:19.028326 12690 net.cpp:84] Creating Layer penlu15
I1013 11:27:19.028329 12690 net.cpp:406] penlu15 <- Convolution18
I1013 11:27:19.028347 12690 net.cpp:367] penlu15 -> Convolution18 (in-place)
I1013 11:27:19.028581 12690 net.cpp:122] Setting up penlu15
I1013 11:27:19.028589 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.028594 12690 net.cpp:137] Memory required for data: 1545891800
I1013 11:27:19.028615 12690 layer_factory.hpp:77] Creating layer Convolution19
I1013 11:27:19.028626 12690 net.cpp:84] Creating Layer Convolution19
I1013 11:27:19.028630 12690 net.cpp:406] Convolution19 <- Convolution18
I1013 11:27:19.028638 12690 net.cpp:380] Convolution19 -> Convolution19
I1013 11:27:19.050613 12690 net.cpp:122] Setting up Convolution19
I1013 11:27:19.050626 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.050631 12690 net.cpp:137] Memory required for data: 1558435800
I1013 11:27:19.050647 12690 layer_factory.hpp:77] Creating layer Eltwise8
I1013 11:27:19.050657 12690 net.cpp:84] Creating Layer Eltwise8
I1013 11:27:19.050662 12690 net.cpp:406] Eltwise8 <- Convolution19
I1013 11:27:19.050669 12690 net.cpp:406] Eltwise8 <- Eltwise7_penlu14_0_split_1
I1013 11:27:19.050678 12690 net.cpp:380] Eltwise8 -> Eltwise8
I1013 11:27:19.050719 12690 net.cpp:122] Setting up Eltwise8
I1013 11:27:19.050725 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.050729 12690 net.cpp:137] Memory required for data: 1570979800
I1013 11:27:19.050743 12690 layer_factory.hpp:77] Creating layer Eltwise8_Eltwise8_0_split
I1013 11:27:19.050751 12690 net.cpp:84] Creating Layer Eltwise8_Eltwise8_0_split
I1013 11:27:19.050755 12690 net.cpp:406] Eltwise8_Eltwise8_0_split <- Eltwise8
I1013 11:27:19.050761 12690 net.cpp:380] Eltwise8_Eltwise8_0_split -> Eltwise8_Eltwise8_0_split_0
I1013 11:27:19.050770 12690 net.cpp:380] Eltwise8_Eltwise8_0_split -> Eltwise8_Eltwise8_0_split_1
I1013 11:27:19.050817 12690 net.cpp:122] Setting up Eltwise8_Eltwise8_0_split
I1013 11:27:19.050822 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.050827 12690 net.cpp:129] Top shape: 50 320 14 14 (3136000)
I1013 11:27:19.050843 12690 net.cpp:137] Memory required for data: 1596067800
I1013 11:27:19.050846 12690 layer_factory.hpp:77] Creating layer Convolution20
I1013 11:27:19.050856 12690 net.cpp:84] Creating Layer Convolution20
I1013 11:27:19.050861 12690 net.cpp:406] Convolution20 <- Eltwise8_Eltwise8_0_split_0
I1013 11:27:19.050869 12690 net.cpp:380] Convolution20 -> Convolution20
I1013 11:27:19.093389 12690 net.cpp:122] Setting up Convolution20
I1013 11:27:19.093410 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.093415 12690 net.cpp:137] Memory required for data: 1602339800
I1013 11:27:19.093422 12690 layer_factory.hpp:77] Creating layer BatchNorm16
I1013 11:27:19.093435 12690 net.cpp:84] Creating Layer BatchNorm16
I1013 11:27:19.093441 12690 net.cpp:406] BatchNorm16 <- Convolution20
I1013 11:27:19.093451 12690 net.cpp:367] BatchNorm16 -> Convolution20 (in-place)
I1013 11:27:19.093639 12690 net.cpp:122] Setting up BatchNorm16
I1013 11:27:19.093647 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.093650 12690 net.cpp:137] Memory required for data: 1608611800
I1013 11:27:19.093659 12690 layer_factory.hpp:77] Creating layer Scale16
I1013 11:27:19.093668 12690 net.cpp:84] Creating Layer Scale16
I1013 11:27:19.093672 12690 net.cpp:406] Scale16 <- Convolution20
I1013 11:27:19.093678 12690 net.cpp:367] Scale16 -> Convolution20 (in-place)
I1013 11:27:19.093719 12690 layer_factory.hpp:77] Creating layer Scale16
I1013 11:27:19.093822 12690 net.cpp:122] Setting up Scale16
I1013 11:27:19.093829 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.093833 12690 net.cpp:137] Memory required for data: 1614883800
I1013 11:27:19.093842 12690 layer_factory.hpp:77] Creating layer penlu16
I1013 11:27:19.093850 12690 net.cpp:84] Creating Layer penlu16
I1013 11:27:19.093854 12690 net.cpp:406] penlu16 <- Convolution20
I1013 11:27:19.093863 12690 net.cpp:367] penlu16 -> Convolution20 (in-place)
I1013 11:27:19.094058 12690 net.cpp:122] Setting up penlu16
I1013 11:27:19.094066 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.094070 12690 net.cpp:137] Memory required for data: 1621155800
I1013 11:27:19.094089 12690 layer_factory.hpp:77] Creating layer Convolution21
I1013 11:27:19.094099 12690 net.cpp:84] Creating Layer Convolution21
I1013 11:27:19.094103 12690 net.cpp:406] Convolution21 <- Convolution20
I1013 11:27:19.094111 12690 net.cpp:380] Convolution21 -> Convolution21
I1013 11:27:19.178243 12690 net.cpp:122] Setting up Convolution21
I1013 11:27:19.178266 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.178313 12690 net.cpp:137] Memory required for data: 1627427800
I1013 11:27:19.178331 12690 layer_factory.hpp:77] Creating layer Convolution22
I1013 11:27:19.178350 12690 net.cpp:84] Creating Layer Convolution22
I1013 11:27:19.178366 12690 net.cpp:406] Convolution22 <- Eltwise8_Eltwise8_0_split_1
I1013 11:27:19.178377 12690 net.cpp:380] Convolution22 -> Convolution22
I1013 11:27:19.183732 12690 net.cpp:122] Setting up Convolution22
I1013 11:27:19.183743 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.183748 12690 net.cpp:137] Memory required for data: 1633699800
I1013 11:27:19.183753 12690 layer_factory.hpp:77] Creating layer Eltwise9
I1013 11:27:19.183763 12690 net.cpp:84] Creating Layer Eltwise9
I1013 11:27:19.183768 12690 net.cpp:406] Eltwise9 <- Convolution21
I1013 11:27:19.183774 12690 net.cpp:406] Eltwise9 <- Convolution22
I1013 11:27:19.183784 12690 net.cpp:380] Eltwise9 -> Eltwise9
I1013 11:27:19.183809 12690 net.cpp:122] Setting up Eltwise9
I1013 11:27:19.183816 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.183820 12690 net.cpp:137] Memory required for data: 1639971800
I1013 11:27:19.183825 12690 layer_factory.hpp:77] Creating layer BatchNorm17
I1013 11:27:19.183835 12690 net.cpp:84] Creating Layer BatchNorm17
I1013 11:27:19.183840 12690 net.cpp:406] BatchNorm17 <- Eltwise9
I1013 11:27:19.183846 12690 net.cpp:367] BatchNorm17 -> Eltwise9 (in-place)
I1013 11:27:19.184020 12690 net.cpp:122] Setting up BatchNorm17
I1013 11:27:19.184027 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.184031 12690 net.cpp:137] Memory required for data: 1646243800
I1013 11:27:19.184051 12690 layer_factory.hpp:77] Creating layer Scale17
I1013 11:27:19.184058 12690 net.cpp:84] Creating Layer Scale17
I1013 11:27:19.184063 12690 net.cpp:406] Scale17 <- Eltwise9
I1013 11:27:19.184072 12690 net.cpp:367] Scale17 -> Eltwise9 (in-place)
I1013 11:27:19.184121 12690 layer_factory.hpp:77] Creating layer Scale17
I1013 11:27:19.184253 12690 net.cpp:122] Setting up Scale17
I1013 11:27:19.184260 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.184264 12690 net.cpp:137] Memory required for data: 1652515800
I1013 11:27:19.184283 12690 layer_factory.hpp:77] Creating layer penlu17
I1013 11:27:19.184291 12690 net.cpp:84] Creating Layer penlu17
I1013 11:27:19.184296 12690 net.cpp:406] penlu17 <- Eltwise9
I1013 11:27:19.184303 12690 net.cpp:367] penlu17 -> Eltwise9 (in-place)
I1013 11:27:19.184475 12690 net.cpp:122] Setting up penlu17
I1013 11:27:19.184482 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.184486 12690 net.cpp:137] Memory required for data: 1658787800
I1013 11:27:19.184495 12690 layer_factory.hpp:77] Creating layer Eltwise9_penlu17_0_split
I1013 11:27:19.184502 12690 net.cpp:84] Creating Layer Eltwise9_penlu17_0_split
I1013 11:27:19.184506 12690 net.cpp:406] Eltwise9_penlu17_0_split <- Eltwise9
I1013 11:27:19.184514 12690 net.cpp:380] Eltwise9_penlu17_0_split -> Eltwise9_penlu17_0_split_0
I1013 11:27:19.184521 12690 net.cpp:380] Eltwise9_penlu17_0_split -> Eltwise9_penlu17_0_split_1
I1013 11:27:19.184556 12690 net.cpp:122] Setting up Eltwise9_penlu17_0_split
I1013 11:27:19.184563 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.184569 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.184574 12690 net.cpp:137] Memory required for data: 1671331800
I1013 11:27:19.184578 12690 layer_factory.hpp:77] Creating layer Convolution23
I1013 11:27:19.184587 12690 net.cpp:84] Creating Layer Convolution23
I1013 11:27:19.184592 12690 net.cpp:406] Convolution23 <- Eltwise9_penlu17_0_split_0
I1013 11:27:19.184598 12690 net.cpp:380] Convolution23 -> Convolution23
I1013 11:27:19.268002 12690 net.cpp:122] Setting up Convolution23
I1013 11:27:19.268023 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.268028 12690 net.cpp:137] Memory required for data: 1677603800
I1013 11:27:19.268035 12690 layer_factory.hpp:77] Creating layer BatchNorm18
I1013 11:27:19.268059 12690 net.cpp:84] Creating Layer BatchNorm18
I1013 11:27:19.268098 12690 net.cpp:406] BatchNorm18 <- Convolution23
I1013 11:27:19.268118 12690 net.cpp:367] BatchNorm18 -> Convolution23 (in-place)
I1013 11:27:19.268338 12690 net.cpp:122] Setting up BatchNorm18
I1013 11:27:19.268347 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.268350 12690 net.cpp:137] Memory required for data: 1683875800
I1013 11:27:19.268359 12690 layer_factory.hpp:77] Creating layer Scale18
I1013 11:27:19.268378 12690 net.cpp:84] Creating Layer Scale18
I1013 11:27:19.268383 12690 net.cpp:406] Scale18 <- Convolution23
I1013 11:27:19.268389 12690 net.cpp:367] Scale18 -> Convolution23 (in-place)
I1013 11:27:19.268440 12690 layer_factory.hpp:77] Creating layer Scale18
I1013 11:27:19.268565 12690 net.cpp:122] Setting up Scale18
I1013 11:27:19.268573 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.268577 12690 net.cpp:137] Memory required for data: 1690147800
I1013 11:27:19.268585 12690 layer_factory.hpp:77] Creating layer penlu18
I1013 11:27:19.268604 12690 net.cpp:84] Creating Layer penlu18
I1013 11:27:19.268607 12690 net.cpp:406] penlu18 <- Convolution23
I1013 11:27:19.268615 12690 net.cpp:367] penlu18 -> Convolution23 (in-place)
I1013 11:27:19.268777 12690 net.cpp:122] Setting up penlu18
I1013 11:27:19.268784 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.268788 12690 net.cpp:137] Memory required for data: 1696419800
I1013 11:27:19.268807 12690 layer_factory.hpp:77] Creating layer Convolution24
I1013 11:27:19.268817 12690 net.cpp:84] Creating Layer Convolution24
I1013 11:27:19.268822 12690 net.cpp:406] Convolution24 <- Convolution23
I1013 11:27:19.268829 12690 net.cpp:380] Convolution24 -> Convolution24
I1013 11:27:19.352967 12690 net.cpp:122] Setting up Convolution24
I1013 11:27:19.352989 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.352993 12690 net.cpp:137] Memory required for data: 1702691800
I1013 11:27:19.353011 12690 layer_factory.hpp:77] Creating layer Eltwise10
I1013 11:27:19.353039 12690 net.cpp:84] Creating Layer Eltwise10
I1013 11:27:19.353045 12690 net.cpp:406] Eltwise10 <- Convolution24
I1013 11:27:19.353063 12690 net.cpp:406] Eltwise10 <- Eltwise9_penlu17_0_split_1
I1013 11:27:19.353082 12690 net.cpp:380] Eltwise10 -> Eltwise10
I1013 11:27:19.353140 12690 net.cpp:122] Setting up Eltwise10
I1013 11:27:19.353147 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.353152 12690 net.cpp:137] Memory required for data: 1708963800
I1013 11:27:19.353157 12690 layer_factory.hpp:77] Creating layer BatchNorm19
I1013 11:27:19.353165 12690 net.cpp:84] Creating Layer BatchNorm19
I1013 11:27:19.353169 12690 net.cpp:406] BatchNorm19 <- Eltwise10
I1013 11:27:19.353178 12690 net.cpp:367] BatchNorm19 -> Eltwise10 (in-place)
I1013 11:27:19.353379 12690 net.cpp:122] Setting up BatchNorm19
I1013 11:27:19.353386 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.353390 12690 net.cpp:137] Memory required for data: 1715235800
I1013 11:27:19.353410 12690 layer_factory.hpp:77] Creating layer Scale19
I1013 11:27:19.353418 12690 net.cpp:84] Creating Layer Scale19
I1013 11:27:19.353423 12690 net.cpp:406] Scale19 <- Eltwise10
I1013 11:27:19.353430 12690 net.cpp:367] Scale19 -> Eltwise10 (in-place)
I1013 11:27:19.353482 12690 layer_factory.hpp:77] Creating layer Scale19
I1013 11:27:19.353611 12690 net.cpp:122] Setting up Scale19
I1013 11:27:19.353618 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.353623 12690 net.cpp:137] Memory required for data: 1721507800
I1013 11:27:19.353641 12690 layer_factory.hpp:77] Creating layer penlu19
I1013 11:27:19.353653 12690 net.cpp:84] Creating Layer penlu19
I1013 11:27:19.353658 12690 net.cpp:406] penlu19 <- Eltwise10
I1013 11:27:19.353664 12690 net.cpp:367] penlu19 -> Eltwise10 (in-place)
I1013 11:27:19.353832 12690 net.cpp:122] Setting up penlu19
I1013 11:27:19.353839 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.353843 12690 net.cpp:137] Memory required for data: 1727779800
I1013 11:27:19.353863 12690 layer_factory.hpp:77] Creating layer Eltwise10_penlu19_0_split
I1013 11:27:19.353880 12690 net.cpp:84] Creating Layer Eltwise10_penlu19_0_split
I1013 11:27:19.353885 12690 net.cpp:406] Eltwise10_penlu19_0_split <- Eltwise10
I1013 11:27:19.353893 12690 net.cpp:380] Eltwise10_penlu19_0_split -> Eltwise10_penlu19_0_split_0
I1013 11:27:19.353899 12690 net.cpp:380] Eltwise10_penlu19_0_split -> Eltwise10_penlu19_0_split_1
I1013 11:27:19.353946 12690 net.cpp:122] Setting up Eltwise10_penlu19_0_split
I1013 11:27:19.353955 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.353960 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.353965 12690 net.cpp:137] Memory required for data: 1740323800
I1013 11:27:19.353970 12690 layer_factory.hpp:77] Creating layer Convolution25
I1013 11:27:19.353981 12690 net.cpp:84] Creating Layer Convolution25
I1013 11:27:19.353986 12690 net.cpp:406] Convolution25 <- Eltwise10_penlu19_0_split_0
I1013 11:27:19.353993 12690 net.cpp:380] Convolution25 -> Convolution25
I1013 11:27:19.437906 12690 net.cpp:122] Setting up Convolution25
I1013 11:27:19.437947 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.437952 12690 net.cpp:137] Memory required for data: 1746595800
I1013 11:27:19.437970 12690 layer_factory.hpp:77] Creating layer BatchNorm20
I1013 11:27:19.438004 12690 net.cpp:84] Creating Layer BatchNorm20
I1013 11:27:19.438011 12690 net.cpp:406] BatchNorm20 <- Convolution25
I1013 11:27:19.438043 12690 net.cpp:367] BatchNorm20 -> Convolution25 (in-place)
I1013 11:27:19.438284 12690 net.cpp:122] Setting up BatchNorm20
I1013 11:27:19.438292 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.438297 12690 net.cpp:137] Memory required for data: 1752867800
I1013 11:27:19.438315 12690 layer_factory.hpp:77] Creating layer Scale20
I1013 11:27:19.438325 12690 net.cpp:84] Creating Layer Scale20
I1013 11:27:19.438329 12690 net.cpp:406] Scale20 <- Convolution25
I1013 11:27:19.438336 12690 net.cpp:367] Scale20 -> Convolution25 (in-place)
I1013 11:27:19.438401 12690 layer_factory.hpp:77] Creating layer Scale20
I1013 11:27:19.438552 12690 net.cpp:122] Setting up Scale20
I1013 11:27:19.438560 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.438565 12690 net.cpp:137] Memory required for data: 1759139800
I1013 11:27:19.438582 12690 layer_factory.hpp:77] Creating layer penlu20
I1013 11:27:19.438606 12690 net.cpp:84] Creating Layer penlu20
I1013 11:27:19.438621 12690 net.cpp:406] penlu20 <- Convolution25
I1013 11:27:19.438627 12690 net.cpp:367] penlu20 -> Convolution25 (in-place)
I1013 11:27:19.438791 12690 net.cpp:122] Setting up penlu20
I1013 11:27:19.438798 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.438802 12690 net.cpp:137] Memory required for data: 1765411800
I1013 11:27:19.438822 12690 layer_factory.hpp:77] Creating layer Convolution26
I1013 11:27:19.438833 12690 net.cpp:84] Creating Layer Convolution26
I1013 11:27:19.438838 12690 net.cpp:406] Convolution26 <- Convolution25
I1013 11:27:19.438844 12690 net.cpp:380] Convolution26 -> Convolution26
I1013 11:27:19.522743 12690 net.cpp:122] Setting up Convolution26
I1013 11:27:19.522766 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.522771 12690 net.cpp:137] Memory required for data: 1771683800
I1013 11:27:19.522779 12690 layer_factory.hpp:77] Creating layer Eltwise11
I1013 11:27:19.522802 12690 net.cpp:84] Creating Layer Eltwise11
I1013 11:27:19.522820 12690 net.cpp:406] Eltwise11 <- Convolution26
I1013 11:27:19.522840 12690 net.cpp:406] Eltwise11 <- Eltwise10_penlu19_0_split_1
I1013 11:27:19.522847 12690 net.cpp:380] Eltwise11 -> Eltwise11
I1013 11:27:19.522923 12690 net.cpp:122] Setting up Eltwise11
I1013 11:27:19.522930 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.522934 12690 net.cpp:137] Memory required for data: 1777955800
I1013 11:27:19.522939 12690 layer_factory.hpp:77] Creating layer BatchNorm21
I1013 11:27:19.522948 12690 net.cpp:84] Creating Layer BatchNorm21
I1013 11:27:19.522967 12690 net.cpp:406] BatchNorm21 <- Eltwise11
I1013 11:27:19.522974 12690 net.cpp:367] BatchNorm21 -> Eltwise11 (in-place)
I1013 11:27:19.523226 12690 net.cpp:122] Setting up BatchNorm21
I1013 11:27:19.523233 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.523237 12690 net.cpp:137] Memory required for data: 1784227800
I1013 11:27:19.523248 12690 layer_factory.hpp:77] Creating layer Scale21
I1013 11:27:19.523257 12690 net.cpp:84] Creating Layer Scale21
I1013 11:27:19.523260 12690 net.cpp:406] Scale21 <- Eltwise11
I1013 11:27:19.523267 12690 net.cpp:367] Scale21 -> Eltwise11 (in-place)
I1013 11:27:19.523320 12690 layer_factory.hpp:77] Creating layer Scale21
I1013 11:27:19.523448 12690 net.cpp:122] Setting up Scale21
I1013 11:27:19.523455 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.523459 12690 net.cpp:137] Memory required for data: 1790499800
I1013 11:27:19.523478 12690 layer_factory.hpp:77] Creating layer penlu21
I1013 11:27:19.523488 12690 net.cpp:84] Creating Layer penlu21
I1013 11:27:19.523491 12690 net.cpp:406] penlu21 <- Eltwise11
I1013 11:27:19.523499 12690 net.cpp:367] penlu21 -> Eltwise11 (in-place)
I1013 11:27:19.523668 12690 net.cpp:122] Setting up penlu21
I1013 11:27:19.523674 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.523679 12690 net.cpp:137] Memory required for data: 1796771800
I1013 11:27:19.523697 12690 layer_factory.hpp:77] Creating layer Eltwise11_penlu21_0_split
I1013 11:27:19.523703 12690 net.cpp:84] Creating Layer Eltwise11_penlu21_0_split
I1013 11:27:19.523708 12690 net.cpp:406] Eltwise11_penlu21_0_split <- Eltwise11
I1013 11:27:19.523715 12690 net.cpp:380] Eltwise11_penlu21_0_split -> Eltwise11_penlu21_0_split_0
I1013 11:27:19.523723 12690 net.cpp:380] Eltwise11_penlu21_0_split -> Eltwise11_penlu21_0_split_1
I1013 11:27:19.523777 12690 net.cpp:122] Setting up Eltwise11_penlu21_0_split
I1013 11:27:19.523783 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.523798 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.523802 12690 net.cpp:137] Memory required for data: 1809315800
I1013 11:27:19.523807 12690 layer_factory.hpp:77] Creating layer Convolution27
I1013 11:27:19.523818 12690 net.cpp:84] Creating Layer Convolution27
I1013 11:27:19.523821 12690 net.cpp:406] Convolution27 <- Eltwise11_penlu21_0_split_0
I1013 11:27:19.523829 12690 net.cpp:380] Convolution27 -> Convolution27
I1013 11:27:19.607336 12690 net.cpp:122] Setting up Convolution27
I1013 11:27:19.607359 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.607364 12690 net.cpp:137] Memory required for data: 1815587800
I1013 11:27:19.607373 12690 layer_factory.hpp:77] Creating layer BatchNorm22
I1013 11:27:19.607398 12690 net.cpp:84] Creating Layer BatchNorm22
I1013 11:27:19.607416 12690 net.cpp:406] BatchNorm22 <- Convolution27
I1013 11:27:19.607434 12690 net.cpp:367] BatchNorm22 -> Convolution27 (in-place)
I1013 11:27:19.607702 12690 net.cpp:122] Setting up BatchNorm22
I1013 11:27:19.607709 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.607713 12690 net.cpp:137] Memory required for data: 1821859800
I1013 11:27:19.607733 12690 layer_factory.hpp:77] Creating layer Scale22
I1013 11:27:19.607743 12690 net.cpp:84] Creating Layer Scale22
I1013 11:27:19.607748 12690 net.cpp:406] Scale22 <- Convolution27
I1013 11:27:19.607754 12690 net.cpp:367] Scale22 -> Convolution27 (in-place)
I1013 11:27:19.607833 12690 layer_factory.hpp:77] Creating layer Scale22
I1013 11:27:19.607964 12690 net.cpp:122] Setting up Scale22
I1013 11:27:19.607971 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.607975 12690 net.cpp:137] Memory required for data: 1828131800
I1013 11:27:19.607995 12690 layer_factory.hpp:77] Creating layer penlu22
I1013 11:27:19.608016 12690 net.cpp:84] Creating Layer penlu22
I1013 11:27:19.608021 12690 net.cpp:406] penlu22 <- Convolution27
I1013 11:27:19.608037 12690 net.cpp:367] penlu22 -> Convolution27 (in-place)
I1013 11:27:19.608228 12690 net.cpp:122] Setting up penlu22
I1013 11:27:19.608237 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.608240 12690 net.cpp:137] Memory required for data: 1834403800
I1013 11:27:19.608283 12690 layer_factory.hpp:77] Creating layer Convolution28
I1013 11:27:19.608309 12690 net.cpp:84] Creating Layer Convolution28
I1013 11:27:19.608314 12690 net.cpp:406] Convolution28 <- Convolution27
I1013 11:27:19.608330 12690 net.cpp:380] Convolution28 -> Convolution28
I1013 11:27:19.691977 12690 net.cpp:122] Setting up Convolution28
I1013 11:27:19.692000 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.692005 12690 net.cpp:137] Memory required for data: 1840675800
I1013 11:27:19.692014 12690 layer_factory.hpp:77] Creating layer Eltwise12
I1013 11:27:19.692036 12690 net.cpp:84] Creating Layer Eltwise12
I1013 11:27:19.692044 12690 net.cpp:406] Eltwise12 <- Convolution28
I1013 11:27:19.692062 12690 net.cpp:406] Eltwise12 <- Eltwise11_penlu21_0_split_1
I1013 11:27:19.692082 12690 net.cpp:380] Eltwise12 -> Eltwise12
I1013 11:27:19.692168 12690 net.cpp:122] Setting up Eltwise12
I1013 11:27:19.692175 12690 net.cpp:129] Top shape: 50 640 7 7 (1568000)
I1013 11:27:19.692179 12690 net.cpp:137] Memory required for data: 1846947800
I1013 11:27:19.692184 12690 layer_factory.hpp:77] Creating layer Pooling1
I1013 11:27:19.692195 12690 net.cpp:84] Creating Layer Pooling1
I1013 11:27:19.692214 12690 net.cpp:406] Pooling1 <- Eltwise12
I1013 11:27:19.692230 12690 net.cpp:380] Pooling1 -> Pooling1
I1013 11:27:19.692432 12690 net.cpp:122] Setting up Pooling1
I1013 11:27:19.692441 12690 net.cpp:129] Top shape: 50 640 1 1 (32000)
I1013 11:27:19.692445 12690 net.cpp:137] Memory required for data: 1847075800
I1013 11:27:19.692450 12690 layer_factory.hpp:77] Creating layer InnerProduct1
I1013 11:27:19.692458 12690 net.cpp:84] Creating Layer InnerProduct1
I1013 11:27:19.692473 12690 net.cpp:406] InnerProduct1 <- Pooling1
I1013 11:27:19.692492 12690 net.cpp:380] InnerProduct1 -> InnerProduct1
I1013 11:27:19.693320 12690 net.cpp:122] Setting up InnerProduct1
I1013 11:27:19.693331 12690 net.cpp:129] Top shape: 50 100 (5000)
I1013 11:27:19.693334 12690 net.cpp:137] Memory required for data: 1847095800
I1013 11:27:19.693341 12690 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1013 11:27:19.693349 12690 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1013 11:27:19.693366 12690 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1013 11:27:19.693372 12690 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1013 11:27:19.693380 12690 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1013 11:27:19.693434 12690 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1013 11:27:19.693441 12690 net.cpp:129] Top shape: 50 100 (5000)
I1013 11:27:19.693446 12690 net.cpp:129] Top shape: 50 100 (5000)
I1013 11:27:19.693460 12690 net.cpp:137] Memory required for data: 1847135800
I1013 11:27:19.693465 12690 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1013 11:27:19.693475 12690 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1013 11:27:19.693480 12690 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I1013 11:27:19.693485 12690 net.cpp:406] SoftmaxWithLoss1 <- label_cifar_1_split_0
I1013 11:27:19.693490 12690 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1013 11:27:19.693500 12690 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1013 11:27:19.694186 12690 net.cpp:122] Setting up SoftmaxWithLoss1
I1013 11:27:19.694196 12690 net.cpp:129] Top shape: (1)
I1013 11:27:19.694201 12690 net.cpp:132]     with loss weight 1
I1013 11:27:19.694222 12690 net.cpp:137] Memory required for data: 1847135804
I1013 11:27:19.694242 12690 layer_factory.hpp:77] Creating layer accuracy
I1013 11:27:19.694263 12690 net.cpp:84] Creating Layer accuracy
I1013 11:27:19.694280 12690 net.cpp:406] accuracy <- InnerProduct1_InnerProduct1_0_split_1
I1013 11:27:19.694295 12690 net.cpp:406] accuracy <- label_cifar_1_split_1
I1013 11:27:19.694303 12690 net.cpp:380] accuracy -> accuracy
I1013 11:27:19.694332 12690 net.cpp:122] Setting up accuracy
I1013 11:27:19.694339 12690 net.cpp:129] Top shape: (1)
I1013 11:27:19.694353 12690 net.cpp:137] Memory required for data: 1847135808
I1013 11:27:19.694360 12690 net.cpp:200] accuracy does not need backward computation.
I1013 11:27:19.694365 12690 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1013 11:27:19.694370 12690 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1013 11:27:19.694375 12690 net.cpp:198] InnerProduct1 needs backward computation.
I1013 11:27:19.694391 12690 net.cpp:198] Pooling1 needs backward computation.
I1013 11:27:19.694396 12690 net.cpp:198] Eltwise12 needs backward computation.
I1013 11:27:19.694401 12690 net.cpp:198] Convolution28 needs backward computation.
I1013 11:27:19.694406 12690 net.cpp:198] penlu22 needs backward computation.
I1013 11:27:19.694422 12690 net.cpp:198] Scale22 needs backward computation.
I1013 11:27:19.694427 12690 net.cpp:198] BatchNorm22 needs backward computation.
I1013 11:27:19.694432 12690 net.cpp:198] Convolution27 needs backward computation.
I1013 11:27:19.694437 12690 net.cpp:198] Eltwise11_penlu21_0_split needs backward computation.
I1013 11:27:19.694442 12690 net.cpp:198] penlu21 needs backward computation.
I1013 11:27:19.694447 12690 net.cpp:198] Scale21 needs backward computation.
I1013 11:27:19.694450 12690 net.cpp:198] BatchNorm21 needs backward computation.
I1013 11:27:19.694455 12690 net.cpp:198] Eltwise11 needs backward computation.
I1013 11:27:19.694459 12690 net.cpp:198] Convolution26 needs backward computation.
I1013 11:27:19.694464 12690 net.cpp:198] penlu20 needs backward computation.
I1013 11:27:19.694469 12690 net.cpp:198] Scale20 needs backward computation.
I1013 11:27:19.694473 12690 net.cpp:198] BatchNorm20 needs backward computation.
I1013 11:27:19.694478 12690 net.cpp:198] Convolution25 needs backward computation.
I1013 11:27:19.694483 12690 net.cpp:198] Eltwise10_penlu19_0_split needs backward computation.
I1013 11:27:19.694488 12690 net.cpp:198] penlu19 needs backward computation.
I1013 11:27:19.694492 12690 net.cpp:198] Scale19 needs backward computation.
I1013 11:27:19.694497 12690 net.cpp:198] BatchNorm19 needs backward computation.
I1013 11:27:19.694501 12690 net.cpp:198] Eltwise10 needs backward computation.
I1013 11:27:19.694506 12690 net.cpp:198] Convolution24 needs backward computation.
I1013 11:27:19.694512 12690 net.cpp:198] penlu18 needs backward computation.
I1013 11:27:19.694516 12690 net.cpp:198] Scale18 needs backward computation.
I1013 11:27:19.694521 12690 net.cpp:198] BatchNorm18 needs backward computation.
I1013 11:27:19.694526 12690 net.cpp:198] Convolution23 needs backward computation.
I1013 11:27:19.694530 12690 net.cpp:198] Eltwise9_penlu17_0_split needs backward computation.
I1013 11:27:19.694536 12690 net.cpp:198] penlu17 needs backward computation.
I1013 11:27:19.694541 12690 net.cpp:198] Scale17 needs backward computation.
I1013 11:27:19.694545 12690 net.cpp:198] BatchNorm17 needs backward computation.
I1013 11:27:19.694550 12690 net.cpp:198] Eltwise9 needs backward computation.
I1013 11:27:19.694555 12690 net.cpp:198] Convolution22 needs backward computation.
I1013 11:27:19.694561 12690 net.cpp:198] Convolution21 needs backward computation.
I1013 11:27:19.694566 12690 net.cpp:198] penlu16 needs backward computation.
I1013 11:27:19.694571 12690 net.cpp:198] Scale16 needs backward computation.
I1013 11:27:19.694576 12690 net.cpp:198] BatchNorm16 needs backward computation.
I1013 11:27:19.694579 12690 net.cpp:198] Convolution20 needs backward computation.
I1013 11:27:19.694586 12690 net.cpp:198] Eltwise8_Eltwise8_0_split needs backward computation.
I1013 11:27:19.694591 12690 net.cpp:198] Eltwise8 needs backward computation.
I1013 11:27:19.694597 12690 net.cpp:198] Convolution19 needs backward computation.
I1013 11:27:19.694602 12690 net.cpp:198] penlu15 needs backward computation.
I1013 11:27:19.694607 12690 net.cpp:198] Scale15 needs backward computation.
I1013 11:27:19.694610 12690 net.cpp:198] BatchNorm15 needs backward computation.
I1013 11:27:19.694615 12690 net.cpp:198] Convolution18 needs backward computation.
I1013 11:27:19.694625 12690 net.cpp:198] Eltwise7_penlu14_0_split needs backward computation.
I1013 11:27:19.694631 12690 net.cpp:198] penlu14 needs backward computation.
I1013 11:27:19.694636 12690 net.cpp:198] Scale14 needs backward computation.
I1013 11:27:19.694641 12690 net.cpp:198] BatchNorm14 needs backward computation.
I1013 11:27:19.694646 12690 net.cpp:198] Eltwise7 needs backward computation.
I1013 11:27:19.694651 12690 net.cpp:198] Convolution17 needs backward computation.
I1013 11:27:19.694656 12690 net.cpp:198] penlu13 needs backward computation.
I1013 11:27:19.694661 12690 net.cpp:198] Scale13 needs backward computation.
I1013 11:27:19.694665 12690 net.cpp:198] BatchNorm13 needs backward computation.
I1013 11:27:19.694669 12690 net.cpp:198] Convolution16 needs backward computation.
I1013 11:27:19.694675 12690 net.cpp:198] Eltwise6_penlu12_0_split needs backward computation.
I1013 11:27:19.694681 12690 net.cpp:198] penlu12 needs backward computation.
I1013 11:27:19.694686 12690 net.cpp:198] Scale12 needs backward computation.
I1013 11:27:19.694690 12690 net.cpp:198] BatchNorm12 needs backward computation.
I1013 11:27:19.694695 12690 net.cpp:198] Eltwise6 needs backward computation.
I1013 11:27:19.694700 12690 net.cpp:198] Convolution15 needs backward computation.
I1013 11:27:19.694705 12690 net.cpp:198] penlu11 needs backward computation.
I1013 11:27:19.694710 12690 net.cpp:198] Scale11 needs backward computation.
I1013 11:27:19.694715 12690 net.cpp:198] BatchNorm11 needs backward computation.
I1013 11:27:19.694720 12690 net.cpp:198] Convolution14 needs backward computation.
I1013 11:27:19.694723 12690 net.cpp:198] Eltwise5_penlu10_0_split needs backward computation.
I1013 11:27:19.694728 12690 net.cpp:198] penlu10 needs backward computation.
I1013 11:27:19.694733 12690 net.cpp:198] Scale10 needs backward computation.
I1013 11:27:19.694738 12690 net.cpp:198] BatchNorm10 needs backward computation.
I1013 11:27:19.694742 12690 net.cpp:198] Eltwise5 needs backward computation.
I1013 11:27:19.694747 12690 net.cpp:198] Convolution13 needs backward computation.
I1013 11:27:19.694752 12690 net.cpp:198] Convolution12 needs backward computation.
I1013 11:27:19.694757 12690 net.cpp:198] penlu9 needs backward computation.
I1013 11:27:19.694762 12690 net.cpp:198] Scale9 needs backward computation.
I1013 11:27:19.694767 12690 net.cpp:198] BatchNorm9 needs backward computation.
I1013 11:27:19.694772 12690 net.cpp:198] Convolution11 needs backward computation.
I1013 11:27:19.694777 12690 net.cpp:198] Eltwise4_Eltwise4_0_split needs backward computation.
I1013 11:27:19.694782 12690 net.cpp:198] Eltwise4 needs backward computation.
I1013 11:27:19.694787 12690 net.cpp:198] Convolution10 needs backward computation.
I1013 11:27:19.694793 12690 net.cpp:198] penlu8 needs backward computation.
I1013 11:27:19.694797 12690 net.cpp:198] Scale8 needs backward computation.
I1013 11:27:19.694802 12690 net.cpp:198] BatchNorm8 needs backward computation.
I1013 11:27:19.694806 12690 net.cpp:198] Convolution9 needs backward computation.
I1013 11:27:19.694811 12690 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1013 11:27:19.694816 12690 net.cpp:198] penlu7 needs backward computation.
I1013 11:27:19.694821 12690 net.cpp:198] Scale7 needs backward computation.
I1013 11:27:19.694825 12690 net.cpp:198] BatchNorm7 needs backward computation.
I1013 11:27:19.694830 12690 net.cpp:198] Eltwise3 needs backward computation.
I1013 11:27:19.694836 12690 net.cpp:198] Convolution8 needs backward computation.
I1013 11:27:19.694841 12690 net.cpp:198] penlu6 needs backward computation.
I1013 11:27:19.694846 12690 net.cpp:198] Scale6 needs backward computation.
I1013 11:27:19.694851 12690 net.cpp:198] BatchNorm6 needs backward computation.
I1013 11:27:19.694856 12690 net.cpp:198] Convolution7 needs backward computation.
I1013 11:27:19.694861 12690 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1013 11:27:19.694866 12690 net.cpp:198] penlu5 needs backward computation.
I1013 11:27:19.694871 12690 net.cpp:198] Scale5 needs backward computation.
I1013 11:27:19.694881 12690 net.cpp:198] BatchNorm5 needs backward computation.
I1013 11:27:19.694886 12690 net.cpp:198] Eltwise2 needs backward computation.
I1013 11:27:19.694891 12690 net.cpp:198] Convolution6 needs backward computation.
I1013 11:27:19.694897 12690 net.cpp:198] penlu4 needs backward computation.
I1013 11:27:19.694901 12690 net.cpp:198] Scale4 needs backward computation.
I1013 11:27:19.694906 12690 net.cpp:198] BatchNorm4 needs backward computation.
I1013 11:27:19.694911 12690 net.cpp:198] Convolution5 needs backward computation.
I1013 11:27:19.694916 12690 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1013 11:27:19.694921 12690 net.cpp:198] penlu3 needs backward computation.
I1013 11:27:19.694926 12690 net.cpp:198] Scale3 needs backward computation.
I1013 11:27:19.694931 12690 net.cpp:198] BatchNorm3 needs backward computation.
I1013 11:27:19.694936 12690 net.cpp:198] Eltwise1 needs backward computation.
I1013 11:27:19.694941 12690 net.cpp:198] Convolution4 needs backward computation.
I1013 11:27:19.694947 12690 net.cpp:198] Convolution3 needs backward computation.
I1013 11:27:19.694952 12690 net.cpp:198] penlu2 needs backward computation.
I1013 11:27:19.694955 12690 net.cpp:198] Scale2 needs backward computation.
I1013 11:27:19.694959 12690 net.cpp:198] BatchNorm2 needs backward computation.
I1013 11:27:19.694964 12690 net.cpp:198] Convolution2 needs backward computation.
I1013 11:27:19.694969 12690 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1013 11:27:19.694974 12690 net.cpp:198] penlu1 needs backward computation.
I1013 11:27:19.694979 12690 net.cpp:198] Scale1 needs backward computation.
I1013 11:27:19.694984 12690 net.cpp:198] BatchNorm1 needs backward computation.
I1013 11:27:19.694989 12690 net.cpp:198] Convolution1 needs backward computation.
I1013 11:27:19.694995 12690 net.cpp:200] label_cifar_1_split does not need backward computation.
I1013 11:27:19.695003 12690 net.cpp:200] cifar does not need backward computation.
I1013 11:27:19.695006 12690 net.cpp:242] This network produces output SoftmaxWithLoss1
I1013 11:27:19.695011 12690 net.cpp:242] This network produces output accuracy
I1013 11:27:19.695062 12690 net.cpp:255] Network initialization done.
I1013 11:27:19.695415 12690 solver.cpp:56] Solver scaffolding done.
I1013 11:27:19.701647 12690 caffe.cpp:248] Starting Optimization
I1013 11:27:19.701653 12690 solver.cpp:272] Solving wrn_28_10
I1013 11:27:19.701658 12690 solver.cpp:273] Learning Rate Policy: multistep
I1013 11:27:20.059454 12690 solver.cpp:218] Iteration 0 (0 iter/s, 0.357774s/100 iters), loss = 4.60517
I1013 11:27:20.059490 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 4.60517 (* 1 = 4.60517 loss)
I1013 11:27:20.059509 12690 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1013 11:27:49.049590 12690 solver.cpp:218] Iteration 100 (3.44946 iter/s, 28.99s/100 iters), loss = 3.86373
I1013 11:27:49.049773 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 3.86373 (* 1 = 3.86373 loss)
I1013 11:27:49.049796 12690 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I1013 11:28:18.290298 12690 solver.cpp:218] Iteration 200 (3.41992 iter/s, 29.2405s/100 iters), loss = 3.95477
I1013 11:28:18.290331 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 3.95477 (* 1 = 3.95477 loss)
I1013 11:28:18.290340 12690 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I1013 11:28:48.076341 12690 solver.cpp:218] Iteration 300 (3.35729 iter/s, 29.786s/100 iters), loss = 3.90397
I1013 11:28:48.076409 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 3.90397 (* 1 = 3.90397 loss)
I1013 11:28:48.076417 12690 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I1013 11:29:18.100144 12690 solver.cpp:218] Iteration 400 (3.3307 iter/s, 30.0237s/100 iters), loss = 3.65351
I1013 11:29:18.100282 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 3.65351 (* 1 = 3.65351 loss)
I1013 11:29:18.100292 12690 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I1013 11:29:47.960405 12690 solver.cpp:330] Iteration 500, Testing net (#0)
I1013 11:30:04.349380 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:30:04.684474 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 3.37109 (* 1 = 3.37109 loss)
I1013 11:30:04.684489 12690 solver.cpp:397]     Test net output #1: accuracy = 0.1788
I1013 11:30:04.979640 12690 solver.cpp:218] Iteration 500 (2.13314 iter/s, 46.8793s/100 iters), loss = 3.21888
I1013 11:30:04.979672 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 3.21888 (* 1 = 3.21888 loss)
I1013 11:30:04.979679 12690 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I1013 11:30:35.194546 12690 solver.cpp:218] Iteration 600 (3.30963 iter/s, 30.2149s/100 iters), loss = 3.28754
I1013 11:30:35.194689 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 3.28754 (* 1 = 3.28754 loss)
I1013 11:30:35.194699 12690 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I1013 11:31:05.504844 12690 solver.cpp:218] Iteration 700 (3.29923 iter/s, 30.3101s/100 iters), loss = 2.81433
I1013 11:31:05.504945 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.81433 (* 1 = 2.81433 loss)
I1013 11:31:05.504952 12690 sgd_solver.cpp:105] Iteration 700, lr = 0.01
I1013 11:31:35.788643 12690 solver.cpp:218] Iteration 800 (3.30211 iter/s, 30.2837s/100 iters), loss = 2.65309
I1013 11:31:35.788785 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.65309 (* 1 = 2.65309 loss)
I1013 11:31:35.788794 12690 sgd_solver.cpp:105] Iteration 800, lr = 0.01
I1013 11:32:06.120430 12690 solver.cpp:218] Iteration 900 (3.29689 iter/s, 30.3316s/100 iters), loss = 2.83344
I1013 11:32:06.120564 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.83344 (* 1 = 2.83344 loss)
I1013 11:32:06.120573 12690 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I1013 11:32:34.965629 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:32:36.177821 12690 solver.cpp:330] Iteration 1000, Testing net (#0)
I1013 11:32:52.694802 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:32:53.035177 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 2.85017 (* 1 = 2.85017 loss)
I1013 11:32:53.035193 12690 solver.cpp:397]     Test net output #1: accuracy = 0.2734
I1013 11:32:53.333545 12690 solver.cpp:218] Iteration 1000 (2.11806 iter/s, 47.213s/100 iters), loss = 2.57503
I1013 11:32:53.333572 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.57503 (* 1 = 2.57503 loss)
I1013 11:32:53.333580 12690 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I1013 11:33:23.760888 12690 solver.cpp:218] Iteration 1100 (3.28652 iter/s, 30.4273s/100 iters), loss = 2.42515
I1013 11:33:23.761554 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.42515 (* 1 = 2.42515 loss)
I1013 11:33:23.761571 12690 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I1013 11:33:54.225394 12690 solver.cpp:218] Iteration 1200 (3.28258 iter/s, 30.4638s/100 iters), loss = 2.90413
I1013 11:33:54.225492 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.90413 (* 1 = 2.90413 loss)
I1013 11:33:54.225500 12690 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I1013 11:34:24.657171 12690 solver.cpp:218] Iteration 1300 (3.28605 iter/s, 30.4317s/100 iters), loss = 2.66437
I1013 11:34:24.657304 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.66437 (* 1 = 2.66437 loss)
I1013 11:34:24.657312 12690 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I1013 11:34:55.129858 12690 solver.cpp:218] Iteration 1400 (3.28164 iter/s, 30.4726s/100 iters), loss = 2.4732
I1013 11:34:55.130002 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.4732 (* 1 = 2.4732 loss)
I1013 11:34:55.130009 12690 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I1013 11:35:25.293431 12690 solver.cpp:330] Iteration 1500, Testing net (#0)
I1013 11:35:41.836998 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:35:42.177953 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 2.47922 (* 1 = 2.47922 loss)
I1013 11:35:42.177983 12690 solver.cpp:397]     Test net output #1: accuracy = 0.3481
I1013 11:35:42.485049 12690 solver.cpp:218] Iteration 1500 (2.11171 iter/s, 47.355s/100 iters), loss = 2.18866
I1013 11:35:42.485076 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.18866 (* 1 = 2.18866 loss)
I1013 11:35:42.485083 12690 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I1013 11:36:12.950423 12690 solver.cpp:218] Iteration 1600 (3.28242 iter/s, 30.4653s/100 iters), loss = 2.32203
I1013 11:36:12.950531 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.32203 (* 1 = 2.32203 loss)
I1013 11:36:12.950541 12690 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I1013 11:36:43.404783 12690 solver.cpp:218] Iteration 1700 (3.28361 iter/s, 30.4543s/100 iters), loss = 2.366
I1013 11:36:43.404922 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.366 (* 1 = 2.366 loss)
I1013 11:36:43.404932 12690 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I1013 11:37:13.888231 12690 solver.cpp:218] Iteration 1800 (3.28048 iter/s, 30.4833s/100 iters), loss = 2.11293
I1013 11:37:13.888344 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.11293 (* 1 = 2.11293 loss)
I1013 11:37:13.888362 12690 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I1013 11:37:44.390951 12690 solver.cpp:218] Iteration 1900 (3.27841 iter/s, 30.5026s/100 iters), loss = 2.29876
I1013 11:37:44.391243 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.29876 (* 1 = 2.29876 loss)
I1013 11:37:44.391252 12690 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I1013 11:38:13.395750 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:38:14.617103 12690 solver.cpp:330] Iteration 2000, Testing net (#0)
I1013 11:38:31.239439 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:38:31.578716 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 2.21549 (* 1 = 2.21549 loss)
I1013 11:38:31.578730 12690 solver.cpp:397]     Test net output #1: accuracy = 0.4087
I1013 11:38:31.878592 12690 solver.cpp:218] Iteration 2000 (2.10582 iter/s, 47.4874s/100 iters), loss = 1.92051
I1013 11:38:31.878620 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.92051 (* 1 = 1.92051 loss)
I1013 11:38:31.878628 12690 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I1013 11:39:02.400905 12690 solver.cpp:218] Iteration 2100 (3.27629 iter/s, 30.5223s/100 iters), loss = 2.12948
I1013 11:39:02.401001 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.12948 (* 1 = 2.12948 loss)
I1013 11:39:02.401017 12690 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I1013 11:39:32.942100 12690 solver.cpp:218] Iteration 2200 (3.27428 iter/s, 30.5411s/100 iters), loss = 2.20556
I1013 11:39:32.942564 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.20556 (* 1 = 2.20556 loss)
I1013 11:39:32.942572 12690 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I1013 11:40:03.539553 12690 solver.cpp:218] Iteration 2300 (3.26829 iter/s, 30.597s/100 iters), loss = 2.14908
I1013 11:40:03.539650 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.14908 (* 1 = 2.14908 loss)
I1013 11:40:03.539659 12690 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I1013 11:40:34.109392 12690 solver.cpp:218] Iteration 2400 (3.27121 iter/s, 30.5697s/100 iters), loss = 2.13636
I1013 11:40:34.109499 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.13636 (* 1 = 2.13636 loss)
I1013 11:40:34.109508 12690 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I1013 11:41:04.376538 12690 solver.cpp:330] Iteration 2500, Testing net (#0)
I1013 11:41:20.988679 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:41:21.327944 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 2.05951 (* 1 = 2.05951 loss)
I1013 11:41:21.327960 12690 solver.cpp:397]     Test net output #1: accuracy = 0.4466
I1013 11:41:21.628298 12690 solver.cpp:218] Iteration 2500 (2.10443 iter/s, 47.5188s/100 iters), loss = 1.73808
I1013 11:41:21.628329 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.73808 (* 1 = 1.73808 loss)
I1013 11:41:21.628335 12690 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I1013 11:41:52.152214 12690 solver.cpp:218] Iteration 2600 (3.27612 iter/s, 30.5239s/100 iters), loss = 1.87804
I1013 11:41:52.152387 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.87804 (* 1 = 1.87804 loss)
I1013 11:41:52.152396 12690 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I1013 11:42:22.745662 12690 solver.cpp:218] Iteration 2700 (3.26869 iter/s, 30.5933s/100 iters), loss = 1.80721
I1013 11:42:22.745782 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.80721 (* 1 = 1.80721 loss)
I1013 11:42:22.745800 12690 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I1013 11:42:53.346755 12690 solver.cpp:218] Iteration 2800 (3.26787 iter/s, 30.601s/100 iters), loss = 1.376
I1013 11:42:53.347337 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.376 (* 1 = 1.376 loss)
I1013 11:42:53.347354 12690 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I1013 11:43:23.939932 12690 solver.cpp:218] Iteration 2900 (3.26876 iter/s, 30.5926s/100 iters), loss = 1.82835
I1013 11:43:23.940042 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.82835 (* 1 = 1.82835 loss)
I1013 11:43:23.940059 12690 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I1013 11:43:53.035828 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:43:54.265348 12690 solver.cpp:330] Iteration 3000, Testing net (#0)
I1013 11:44:10.896559 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:44:11.235893 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.91166 (* 1 = 1.91166 loss)
I1013 11:44:11.235909 12690 solver.cpp:397]     Test net output #1: accuracy = 0.4795
I1013 11:44:11.534593 12690 solver.cpp:218] Iteration 3000 (2.10108 iter/s, 47.5946s/100 iters), loss = 1.42054
I1013 11:44:11.534622 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.42054 (* 1 = 1.42054 loss)
I1013 11:44:11.534628 12690 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I1013 11:44:42.132223 12690 solver.cpp:218] Iteration 3100 (3.26823 iter/s, 30.5976s/100 iters), loss = 2.04814
I1013 11:44:42.132335 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.04814 (* 1 = 2.04814 loss)
I1013 11:44:42.132354 12690 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I1013 11:45:12.683387 12690 solver.cpp:218] Iteration 3200 (3.27321 iter/s, 30.5511s/100 iters), loss = 1.79587
I1013 11:45:12.683511 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.79587 (* 1 = 1.79587 loss)
I1013 11:45:12.683518 12690 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I1013 11:45:43.266572 12690 solver.cpp:218] Iteration 3300 (3.26978 iter/s, 30.5831s/100 iters), loss = 1.81484
I1013 11:45:43.266712 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.81484 (* 1 = 1.81484 loss)
I1013 11:45:43.266721 12690 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I1013 11:46:13.824458 12690 solver.cpp:218] Iteration 3400 (3.27249 iter/s, 30.5578s/100 iters), loss = 1.7859
I1013 11:46:13.824571 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.7859 (* 1 = 1.7859 loss)
I1013 11:46:13.824579 12690 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I1013 11:46:44.059871 12690 solver.cpp:330] Iteration 3500, Testing net (#0)
I1013 11:47:00.645028 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:47:00.988785 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.83658 (* 1 = 1.83658 loss)
I1013 11:47:00.988801 12690 solver.cpp:397]     Test net output #1: accuracy = 0.4939
I1013 11:47:01.289877 12690 solver.cpp:218] Iteration 3500 (2.1068 iter/s, 47.4653s/100 iters), loss = 1.37197
I1013 11:47:01.289911 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.37197 (* 1 = 1.37197 loss)
I1013 11:47:01.289919 12690 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I1013 11:47:31.895921 12690 solver.cpp:218] Iteration 3600 (3.26733 iter/s, 30.606s/100 iters), loss = 1.64252
I1013 11:47:31.896100 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.64252 (* 1 = 1.64252 loss)
I1013 11:47:31.896111 12690 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I1013 11:48:02.479487 12690 solver.cpp:218] Iteration 3700 (3.26975 iter/s, 30.5834s/100 iters), loss = 1.59745
I1013 11:48:02.479635 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.59745 (* 1 = 1.59745 loss)
I1013 11:48:02.479642 12690 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I1013 11:48:33.071887 12690 solver.cpp:218] Iteration 3800 (3.2688 iter/s, 30.5923s/100 iters), loss = 1.23642
I1013 11:48:33.072005 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.23642 (* 1 = 1.23642 loss)
I1013 11:48:33.072013 12690 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I1013 11:49:03.662210 12690 solver.cpp:218] Iteration 3900 (3.26902 iter/s, 30.5902s/100 iters), loss = 1.6364
I1013 11:49:03.662340 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.6364 (* 1 = 1.6364 loss)
I1013 11:49:03.662349 12690 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I1013 11:49:32.707017 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:49:33.926892 12690 solver.cpp:330] Iteration 4000, Testing net (#0)
I1013 11:49:50.590059 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:49:50.928828 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.7451 (* 1 = 1.7451 loss)
I1013 11:49:50.928844 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5198
I1013 11:49:51.231346 12690 solver.cpp:218] Iteration 4000 (2.10221 iter/s, 47.569s/100 iters), loss = 1.298
I1013 11:49:51.231375 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.298 (* 1 = 1.298 loss)
I1013 11:49:51.231382 12690 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I1013 11:50:21.830528 12690 solver.cpp:218] Iteration 4100 (3.26806 iter/s, 30.5992s/100 iters), loss = 1.8342
I1013 11:50:21.830669 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.8342 (* 1 = 1.8342 loss)
I1013 11:50:21.830677 12690 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I1013 11:50:52.449306 12690 solver.cpp:218] Iteration 4200 (3.26598 iter/s, 30.6186s/100 iters), loss = 1.59864
I1013 11:50:52.449440 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.59864 (* 1 = 1.59864 loss)
I1013 11:50:52.449447 12690 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I1013 11:51:23.075002 12690 solver.cpp:218] Iteration 4300 (3.26525 iter/s, 30.6256s/100 iters), loss = 1.70284
I1013 11:51:23.075099 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.70284 (* 1 = 1.70284 loss)
I1013 11:51:23.075116 12690 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I1013 11:51:53.666712 12690 solver.cpp:218] Iteration 4400 (3.26887 iter/s, 30.5916s/100 iters), loss = 1.76293
I1013 11:51:53.666833 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.76293 (* 1 = 1.76293 loss)
I1013 11:51:53.666841 12690 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I1013 11:52:23.963966 12690 solver.cpp:330] Iteration 4500, Testing net (#0)
I1013 11:52:40.577337 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:52:40.918473 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.73212 (* 1 = 1.73212 loss)
I1013 11:52:40.918488 12690 solver.cpp:397]     Test net output #1: accuracy = 0.525
I1013 11:52:41.219060 12690 solver.cpp:218] Iteration 4500 (2.10295 iter/s, 47.5522s/100 iters), loss = 1.31862
I1013 11:52:41.219089 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.31862 (* 1 = 1.31862 loss)
I1013 11:52:41.219096 12690 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I1013 11:53:11.814187 12690 solver.cpp:218] Iteration 4600 (3.2685 iter/s, 30.5951s/100 iters), loss = 1.44398
I1013 11:53:11.815137 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.44398 (* 1 = 1.44398 loss)
I1013 11:53:11.815156 12690 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I1013 11:53:42.442816 12690 solver.cpp:218] Iteration 4700 (3.26502 iter/s, 30.6277s/100 iters), loss = 1.46395
I1013 11:53:42.442991 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.46395 (* 1 = 1.46395 loss)
I1013 11:53:42.443009 12690 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I1013 11:54:13.005090 12690 solver.cpp:218] Iteration 4800 (3.27202 iter/s, 30.5621s/100 iters), loss = 1.05857
I1013 11:54:13.005237 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.05857 (* 1 = 1.05857 loss)
I1013 11:54:13.005246 12690 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I1013 11:54:43.593565 12690 solver.cpp:218] Iteration 4900 (3.26922 iter/s, 30.5883s/100 iters), loss = 1.49086
I1013 11:54:43.593660 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.49086 (* 1 = 1.49086 loss)
I1013 11:54:43.593667 12690 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I1013 11:55:12.695703 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:55:13.915146 12690 solver.cpp:330] Iteration 5000, Testing net (#0)
I1013 11:55:30.502096 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:55:30.843365 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.66982 (* 1 = 1.66982 loss)
I1013 11:55:30.843384 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5427
I1013 11:55:31.146123 12690 solver.cpp:218] Iteration 5000 (2.10294 iter/s, 47.5525s/100 iters), loss = 1.25494
I1013 11:55:31.146150 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.25494 (* 1 = 1.25494 loss)
I1013 11:55:31.146157 12690 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1013 11:56:01.787900 12690 solver.cpp:218] Iteration 5100 (3.26352 iter/s, 30.6418s/100 iters), loss = 1.34001
I1013 11:56:01.788038 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.34001 (* 1 = 1.34001 loss)
I1013 11:56:01.788045 12690 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1013 11:56:32.435468 12690 solver.cpp:218] Iteration 5200 (3.26291 iter/s, 30.6474s/100 iters), loss = 1.41665
I1013 11:56:32.435611 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.41665 (* 1 = 1.41665 loss)
I1013 11:56:32.435618 12690 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1013 11:57:03.101300 12690 solver.cpp:218] Iteration 5300 (3.26097 iter/s, 30.6657s/100 iters), loss = 1.26753
I1013 11:57:03.101442 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.26753 (* 1 = 1.26753 loss)
I1013 11:57:03.101451 12690 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1013 11:57:33.716666 12690 solver.cpp:218] Iteration 5400 (3.26635 iter/s, 30.6152s/100 iters), loss = 1.1917
I1013 11:57:33.716778 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.1917 (* 1 = 1.1917 loss)
I1013 11:57:33.716785 12690 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1013 11:58:04.093353 12690 solver.cpp:330] Iteration 5500, Testing net (#0)
I1013 11:58:20.740572 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 11:58:21.082409 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.6589 (* 1 = 1.6589 loss)
I1013 11:58:21.082427 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5457
I1013 11:58:21.386359 12690 solver.cpp:218] Iteration 5500 (2.09777 iter/s, 47.6696s/100 iters), loss = 1.32099
I1013 11:58:21.386389 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.32099 (* 1 = 1.32099 loss)
I1013 11:58:21.386395 12690 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1013 11:58:51.982395 12690 solver.cpp:218] Iteration 5600 (3.2684 iter/s, 30.596s/100 iters), loss = 1.34061
I1013 11:58:51.982542 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.34061 (* 1 = 1.34061 loss)
I1013 11:58:51.982551 12690 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1013 11:59:22.554193 12690 solver.cpp:218] Iteration 5700 (3.271 iter/s, 30.5717s/100 iters), loss = 1.18556
I1013 11:59:22.554323 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.18556 (* 1 = 1.18556 loss)
I1013 11:59:22.554330 12690 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1013 11:59:53.177984 12690 solver.cpp:218] Iteration 5800 (3.26545 iter/s, 30.6237s/100 iters), loss = 0.928608
I1013 11:59:53.178104 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.928608 (* 1 = 0.928608 loss)
I1013 11:59:53.178122 12690 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1013 12:00:23.798225 12690 solver.cpp:218] Iteration 5900 (3.26583 iter/s, 30.6201s/100 iters), loss = 1.32175
I1013 12:00:23.798367 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.32175 (* 1 = 1.32175 loss)
I1013 12:00:23.798377 12690 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1013 12:00:52.860308 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:00:54.089166 12690 solver.cpp:330] Iteration 6000, Testing net (#0)
I1013 12:01:10.847132 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:01:11.193017 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56456 (* 1 = 1.56456 loss)
I1013 12:01:11.193033 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5717
I1013 12:01:11.493054 12690 solver.cpp:218] Iteration 6000 (2.09667 iter/s, 47.6947s/100 iters), loss = 0.975015
I1013 12:01:11.493085 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.975015 (* 1 = 0.975015 loss)
I1013 12:01:11.493091 12690 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1013 12:01:42.170981 12690 solver.cpp:218] Iteration 6100 (3.25968 iter/s, 30.6779s/100 iters), loss = 1.2721
I1013 12:01:42.171094 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.2721 (* 1 = 1.2721 loss)
I1013 12:01:42.171103 12690 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1013 12:02:12.820924 12690 solver.cpp:218] Iteration 6200 (3.26266 iter/s, 30.6498s/100 iters), loss = 1.22251
I1013 12:02:12.821030 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.22251 (* 1 = 1.22251 loss)
I1013 12:02:12.821039 12690 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1013 12:02:43.499625 12690 solver.cpp:218] Iteration 6300 (3.2596 iter/s, 30.6786s/100 iters), loss = 1.30107
I1013 12:02:43.499742 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.30107 (* 1 = 1.30107 loss)
I1013 12:02:43.499752 12690 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1013 12:03:14.117158 12690 solver.cpp:218] Iteration 6400 (3.26611 iter/s, 30.6174s/100 iters), loss = 1.22326
I1013 12:03:14.117305 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.22326 (* 1 = 1.22326 loss)
I1013 12:03:14.117316 12690 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1013 12:03:44.502276 12690 solver.cpp:330] Iteration 6500, Testing net (#0)
I1013 12:04:01.186969 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:04:01.530236 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.5856 (* 1 = 1.5856 loss)
I1013 12:04:01.530251 12690 solver.cpp:397]     Test net output #1: accuracy = 0.568
I1013 12:04:01.834326 12690 solver.cpp:218] Iteration 6500 (2.09569 iter/s, 47.717s/100 iters), loss = 1.11709
I1013 12:04:01.834354 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.11709 (* 1 = 1.11709 loss)
I1013 12:04:01.834362 12690 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1013 12:04:32.512876 12690 solver.cpp:218] Iteration 6600 (3.25961 iter/s, 30.6785s/100 iters), loss = 1.34237
I1013 12:04:32.513022 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.34237 (* 1 = 1.34237 loss)
I1013 12:04:32.513031 12690 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1013 12:05:03.125471 12690 solver.cpp:218] Iteration 6700 (3.26664 iter/s, 30.6125s/100 iters), loss = 1.06426
I1013 12:05:03.125614 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.06426 (* 1 = 1.06426 loss)
I1013 12:05:03.125623 12690 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1013 12:05:33.752477 12690 solver.cpp:218] Iteration 6800 (3.26511 iter/s, 30.6269s/100 iters), loss = 0.712402
I1013 12:05:33.752647 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.712402 (* 1 = 0.712402 loss)
I1013 12:05:33.752657 12690 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1013 12:06:04.373498 12690 solver.cpp:218] Iteration 6900 (3.26575 iter/s, 30.6209s/100 iters), loss = 1.38733
I1013 12:06:04.373819 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.38733 (* 1 = 1.38733 loss)
I1013 12:06:04.373826 12690 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1013 12:06:33.472345 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:06:34.693159 12690 solver.cpp:330] Iteration 7000, Testing net (#0)
I1013 12:06:51.334013 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:06:51.674007 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.52321 (* 1 = 1.52321 loss)
I1013 12:06:51.674022 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5842
I1013 12:06:51.973646 12690 solver.cpp:218] Iteration 7000 (2.10085 iter/s, 47.5998s/100 iters), loss = 0.780694
I1013 12:06:51.973678 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.780694 (* 1 = 0.780694 loss)
I1013 12:06:51.973685 12690 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1013 12:07:22.627223 12690 solver.cpp:218] Iteration 7100 (3.26226 iter/s, 30.6535s/100 iters), loss = 1.12416
I1013 12:07:22.627382 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.12416 (* 1 = 1.12416 loss)
I1013 12:07:22.627390 12690 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1013 12:07:53.299592 12690 solver.cpp:218] Iteration 7200 (3.26028 iter/s, 30.6722s/100 iters), loss = 1.37214
I1013 12:07:53.299690 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.37214 (* 1 = 1.37214 loss)
I1013 12:07:53.299697 12690 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1013 12:08:23.921550 12690 solver.cpp:218] Iteration 7300 (3.26564 iter/s, 30.6219s/100 iters), loss = 1.08383
I1013 12:08:23.921663 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.08383 (* 1 = 1.08383 loss)
I1013 12:08:23.921670 12690 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1013 12:08:54.586686 12690 solver.cpp:218] Iteration 7400 (3.26104 iter/s, 30.665s/100 iters), loss = 1.03305
I1013 12:08:54.586791 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03305 (* 1 = 1.03305 loss)
I1013 12:08:54.586799 12690 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1013 12:09:24.868057 12690 solver.cpp:330] Iteration 7500, Testing net (#0)
I1013 12:09:41.518869 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:09:41.858604 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.50203 (* 1 = 1.50203 loss)
I1013 12:09:41.858620 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5854
I1013 12:09:42.161808 12690 solver.cpp:218] Iteration 7500 (2.10194 iter/s, 47.575s/100 iters), loss = 1.19013
I1013 12:09:42.161839 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.19013 (* 1 = 1.19013 loss)
I1013 12:09:42.161846 12690 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1013 12:10:12.753651 12690 solver.cpp:218] Iteration 7600 (3.26885 iter/s, 30.5918s/100 iters), loss = 1.24558
I1013 12:10:12.753729 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.24558 (* 1 = 1.24558 loss)
I1013 12:10:12.753746 12690 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1013 12:10:43.344905 12690 solver.cpp:218] Iteration 7700 (3.26892 iter/s, 30.5912s/100 iters), loss = 0.95111
I1013 12:10:43.345041 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.95111 (* 1 = 0.95111 loss)
I1013 12:10:43.345048 12690 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1013 12:11:14.006480 12690 solver.cpp:218] Iteration 7800 (3.26142 iter/s, 30.6614s/100 iters), loss = 0.961374
I1013 12:11:14.006623 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.961374 (* 1 = 0.961374 loss)
I1013 12:11:14.006644 12690 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1013 12:11:44.616219 12690 solver.cpp:218] Iteration 7900 (3.26695 iter/s, 30.6096s/100 iters), loss = 1.21127
I1013 12:11:44.616338 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.21127 (* 1 = 1.21127 loss)
I1013 12:11:44.616345 12690 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1013 12:12:13.696929 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:12:14.922247 12690 solver.cpp:330] Iteration 8000, Testing net (#0)
I1013 12:12:31.610040 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:12:31.950295 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.46604 (* 1 = 1.46604 loss)
I1013 12:12:31.950310 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5949
I1013 12:12:32.250747 12690 solver.cpp:218] Iteration 8000 (2.09932 iter/s, 47.6344s/100 iters), loss = 0.674113
I1013 12:12:32.250778 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.674113 (* 1 = 0.674113 loss)
I1013 12:12:32.250784 12690 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1013 12:13:02.847669 12690 solver.cpp:218] Iteration 8100 (3.26831 iter/s, 30.5969s/100 iters), loss = 0.98288
I1013 12:13:02.847784 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.98288 (* 1 = 0.98288 loss)
I1013 12:13:02.847801 12690 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1013 12:13:33.491397 12690 solver.cpp:218] Iteration 8200 (3.26332 iter/s, 30.6436s/100 iters), loss = 1.09769
I1013 12:13:33.491489 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.09769 (* 1 = 1.09769 loss)
I1013 12:13:33.491497 12690 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1013 12:14:04.094571 12690 solver.cpp:218] Iteration 8300 (3.26764 iter/s, 30.6031s/100 iters), loss = 1.07914
I1013 12:14:04.094676 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.07914 (* 1 = 1.07914 loss)
I1013 12:14:04.094684 12690 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1013 12:14:34.712918 12690 solver.cpp:218] Iteration 8400 (3.26603 iter/s, 30.6182s/100 iters), loss = 1.1308
I1013 12:14:34.713037 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.1308 (* 1 = 1.1308 loss)
I1013 12:14:34.713043 12690 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1013 12:15:04.950310 12690 solver.cpp:330] Iteration 8500, Testing net (#0)
I1013 12:15:21.629115 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:15:21.968734 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.5147 (* 1 = 1.5147 loss)
I1013 12:15:21.968750 12690 solver.cpp:397]     Test net output #1: accuracy = 0.5903
I1013 12:15:22.269927 12690 solver.cpp:218] Iteration 8500 (2.10274 iter/s, 47.5569s/100 iters), loss = 1.03112
I1013 12:15:22.269958 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03112 (* 1 = 1.03112 loss)
I1013 12:15:22.269964 12690 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1013 12:15:52.863181 12690 solver.cpp:218] Iteration 8600 (3.2687 iter/s, 30.5932s/100 iters), loss = 1.06955
I1013 12:15:52.863319 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.06955 (* 1 = 1.06955 loss)
I1013 12:15:52.863327 12690 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1013 12:16:23.427018 12690 solver.cpp:218] Iteration 8700 (3.27185 iter/s, 30.5637s/100 iters), loss = 0.840112
I1013 12:16:23.427132 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.840112 (* 1 = 0.840112 loss)
I1013 12:16:23.427140 12690 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1013 12:16:53.993993 12690 solver.cpp:218] Iteration 8800 (3.27152 iter/s, 30.5669s/100 iters), loss = 0.768008
I1013 12:16:53.994102 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.768008 (* 1 = 0.768008 loss)
I1013 12:16:53.994110 12690 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1013 12:17:24.615345 12690 solver.cpp:218] Iteration 8900 (3.2657 iter/s, 30.6213s/100 iters), loss = 0.943278
I1013 12:17:24.615484 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.943278 (* 1 = 0.943278 loss)
I1013 12:17:24.615494 12690 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1013 12:17:53.698088 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:17:54.912317 12690 solver.cpp:330] Iteration 9000, Testing net (#0)
I1013 12:18:11.572715 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:18:11.912183 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.43797 (* 1 = 1.43797 loss)
I1013 12:18:11.912197 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6091
I1013 12:18:12.215529 12690 solver.cpp:218] Iteration 9000 (2.10084 iter/s, 47.6001s/100 iters), loss = 0.593868
I1013 12:18:12.215559 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.593868 (* 1 = 0.593868 loss)
I1013 12:18:12.215564 12690 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1013 12:18:42.860891 12690 solver.cpp:218] Iteration 9100 (3.26314 iter/s, 30.6453s/100 iters), loss = 0.94516
I1013 12:18:42.860987 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.94516 (* 1 = 0.94516 loss)
I1013 12:18:42.860996 12690 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1013 12:19:13.437561 12690 solver.cpp:218] Iteration 9200 (3.27048 iter/s, 30.5766s/100 iters), loss = 0.989109
I1013 12:19:13.437659 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.989109 (* 1 = 0.989109 loss)
I1013 12:19:13.437675 12690 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1013 12:19:44.000195 12690 solver.cpp:218] Iteration 9300 (3.27198 iter/s, 30.5625s/100 iters), loss = 0.933625
I1013 12:19:44.000339 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.933625 (* 1 = 0.933625 loss)
I1013 12:19:44.000349 12690 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1013 12:20:14.611996 12690 solver.cpp:218] Iteration 9400 (3.26673 iter/s, 30.6117s/100 iters), loss = 0.770269
I1013 12:20:14.612134 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.770269 (* 1 = 0.770269 loss)
I1013 12:20:14.612141 12690 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1013 12:20:44.945369 12690 solver.cpp:330] Iteration 9500, Testing net (#0)
I1013 12:21:01.608130 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:21:01.948086 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.47693 (* 1 = 1.47693 loss)
I1013 12:21:01.948102 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6028
I1013 12:21:02.249011 12690 solver.cpp:218] Iteration 9500 (2.09921 iter/s, 47.6369s/100 iters), loss = 0.836197
I1013 12:21:02.249039 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.836197 (* 1 = 0.836197 loss)
I1013 12:21:02.249047 12690 sgd_solver.cpp:105] Iteration 9500, lr = 0.01
I1013 12:21:32.855026 12690 solver.cpp:218] Iteration 9600 (3.26733 iter/s, 30.606s/100 iters), loss = 1.22317
I1013 12:21:32.855150 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.22317 (* 1 = 1.22317 loss)
I1013 12:21:32.855159 12690 sgd_solver.cpp:105] Iteration 9600, lr = 0.01
I1013 12:22:03.456547 12690 solver.cpp:218] Iteration 9700 (3.26782 iter/s, 30.6014s/100 iters), loss = 0.760811
I1013 12:22:03.456642 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.760811 (* 1 = 0.760811 loss)
I1013 12:22:03.456661 12690 sgd_solver.cpp:105] Iteration 9700, lr = 0.01
I1013 12:22:33.994638 12690 solver.cpp:218] Iteration 9800 (3.27461 iter/s, 30.538s/100 iters), loss = 0.687266
I1013 12:22:33.994740 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.687266 (* 1 = 0.687266 loss)
I1013 12:22:33.994750 12690 sgd_solver.cpp:105] Iteration 9800, lr = 0.01
I1013 12:23:04.490311 12690 solver.cpp:218] Iteration 9900 (3.27916 iter/s, 30.4956s/100 iters), loss = 0.955788
I1013 12:23:04.490438 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.955788 (* 1 = 0.955788 loss)
I1013 12:23:04.490447 12690 sgd_solver.cpp:105] Iteration 9900, lr = 0.01
I1013 12:23:33.446920 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:23:34.670464 12690 solver.cpp:330] Iteration 10000, Testing net (#0)
I1013 12:23:51.183169 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:23:51.521589 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.42125 (* 1 = 1.42125 loss)
I1013 12:23:51.521605 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6084
I1013 12:23:51.821022 12690 solver.cpp:218] Iteration 10000 (2.1128 iter/s, 47.3306s/100 iters), loss = 0.475817
I1013 12:23:51.821056 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.475817 (* 1 = 0.475817 loss)
I1013 12:23:51.821064 12690 sgd_solver.cpp:105] Iteration 10000, lr = 0.01
I1013 12:24:22.320081 12690 solver.cpp:218] Iteration 10100 (3.27879 iter/s, 30.499s/100 iters), loss = 0.831634
I1013 12:24:22.320212 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.831634 (* 1 = 0.831634 loss)
I1013 12:24:22.320230 12690 sgd_solver.cpp:105] Iteration 10100, lr = 0.01
I1013 12:24:52.820628 12690 solver.cpp:218] Iteration 10200 (3.27864 iter/s, 30.5004s/100 iters), loss = 0.798897
I1013 12:24:52.820770 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.798897 (* 1 = 0.798897 loss)
I1013 12:24:52.820780 12690 sgd_solver.cpp:105] Iteration 10200, lr = 0.01
I1013 12:25:23.312110 12690 solver.cpp:218] Iteration 10300 (3.27962 iter/s, 30.4914s/100 iters), loss = 0.976858
I1013 12:25:23.312206 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.976858 (* 1 = 0.976858 loss)
I1013 12:25:23.312222 12690 sgd_solver.cpp:105] Iteration 10300, lr = 0.01
I1013 12:25:53.809237 12690 solver.cpp:218] Iteration 10400 (3.27901 iter/s, 30.497s/100 iters), loss = 1.02332
I1013 12:25:53.809377 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.02332 (* 1 = 1.02332 loss)
I1013 12:25:53.809386 12690 sgd_solver.cpp:105] Iteration 10400, lr = 0.01
I1013 12:26:23.983696 12690 solver.cpp:330] Iteration 10500, Testing net (#0)
I1013 12:26:40.520787 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:26:40.857677 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.50225 (* 1 = 1.50225 loss)
I1013 12:26:40.857694 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6001
I1013 12:26:41.156929 12690 solver.cpp:218] Iteration 10500 (2.11204 iter/s, 47.3476s/100 iters), loss = 0.959873
I1013 12:26:41.156961 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.959873 (* 1 = 0.959873 loss)
I1013 12:26:41.156968 12690 sgd_solver.cpp:105] Iteration 10500, lr = 0.01
I1013 12:27:11.636812 12690 solver.cpp:218] Iteration 10600 (3.28085 iter/s, 30.4799s/100 iters), loss = 1.07038
I1013 12:27:11.636960 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.07038 (* 1 = 1.07038 loss)
I1013 12:27:11.636970 12690 sgd_solver.cpp:105] Iteration 10600, lr = 0.01
I1013 12:27:42.114639 12690 solver.cpp:218] Iteration 10700 (3.28109 iter/s, 30.4777s/100 iters), loss = 0.580712
I1013 12:27:42.114753 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.580712 (* 1 = 0.580712 loss)
I1013 12:27:42.114770 12690 sgd_solver.cpp:105] Iteration 10700, lr = 0.01
I1013 12:28:12.582511 12690 solver.cpp:218] Iteration 10800 (3.28216 iter/s, 30.4678s/100 iters), loss = 0.629196
I1013 12:28:12.582648 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.629196 (* 1 = 0.629196 loss)
I1013 12:28:12.582655 12690 sgd_solver.cpp:105] Iteration 10800, lr = 0.01
I1013 12:28:43.077936 12690 solver.cpp:218] Iteration 10900 (3.27919 iter/s, 30.4953s/100 iters), loss = 0.716914
I1013 12:28:43.078049 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.716914 (* 1 = 0.716914 loss)
I1013 12:28:43.078068 12690 sgd_solver.cpp:105] Iteration 10900, lr = 0.01
I1013 12:29:12.059747 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:29:13.278342 12690 solver.cpp:330] Iteration 11000, Testing net (#0)
I1013 12:29:29.800225 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:29:30.137784 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.43416 (* 1 = 1.43416 loss)
I1013 12:29:30.137799 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6118
I1013 12:29:30.437201 12690 solver.cpp:218] Iteration 11000 (2.11152 iter/s, 47.3592s/100 iters), loss = 0.673373
I1013 12:29:30.437239 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.673373 (* 1 = 0.673373 loss)
I1013 12:29:30.437247 12690 sgd_solver.cpp:105] Iteration 11000, lr = 0.01
I1013 12:30:00.915616 12690 solver.cpp:218] Iteration 11100 (3.28101 iter/s, 30.4784s/100 iters), loss = 0.88453
I1013 12:30:00.915776 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.88453 (* 1 = 0.88453 loss)
I1013 12:30:00.915786 12690 sgd_solver.cpp:105] Iteration 11100, lr = 0.01
I1013 12:30:31.411412 12690 solver.cpp:218] Iteration 11200 (3.27916 iter/s, 30.4956s/100 iters), loss = 0.814192
I1013 12:30:31.411556 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.814193 (* 1 = 0.814193 loss)
I1013 12:30:31.411564 12690 sgd_solver.cpp:105] Iteration 11200, lr = 0.01
I1013 12:31:01.895510 12690 solver.cpp:218] Iteration 11300 (3.28041 iter/s, 30.484s/100 iters), loss = 0.636731
I1013 12:31:01.895650 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.636732 (* 1 = 0.636732 loss)
I1013 12:31:01.895659 12690 sgd_solver.cpp:105] Iteration 11300, lr = 0.01
I1013 12:31:32.364706 12690 solver.cpp:218] Iteration 11400 (3.28202 iter/s, 30.4691s/100 iters), loss = 0.637841
I1013 12:31:32.364799 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.637841 (* 1 = 0.637841 loss)
I1013 12:31:32.364817 12690 sgd_solver.cpp:105] Iteration 11400, lr = 0.01
I1013 12:32:02.542166 12690 solver.cpp:330] Iteration 11500, Testing net (#0)
I1013 12:32:19.069042 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:32:19.406718 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.51853 (* 1 = 1.51853 loss)
I1013 12:32:19.406733 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6006
I1013 12:32:19.705420 12690 solver.cpp:218] Iteration 11500 (2.11235 iter/s, 47.3406s/100 iters), loss = 1.03736
I1013 12:32:19.705456 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03736 (* 1 = 1.03736 loss)
I1013 12:32:19.705463 12690 sgd_solver.cpp:105] Iteration 11500, lr = 0.01
I1013 12:32:50.207643 12690 solver.cpp:218] Iteration 11600 (3.27845 iter/s, 30.5022s/100 iters), loss = 0.943581
I1013 12:32:50.207787 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.943581 (* 1 = 0.943581 loss)
I1013 12:32:50.207795 12690 sgd_solver.cpp:105] Iteration 11600, lr = 0.01
I1013 12:33:20.722440 12690 solver.cpp:218] Iteration 11700 (3.27711 iter/s, 30.5147s/100 iters), loss = 0.549268
I1013 12:33:20.722548 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.549268 (* 1 = 0.549268 loss)
I1013 12:33:20.722556 12690 sgd_solver.cpp:105] Iteration 11700, lr = 0.01
I1013 12:33:51.213388 12690 solver.cpp:218] Iteration 11800 (3.27967 iter/s, 30.4909s/100 iters), loss = 0.535211
I1013 12:33:51.213528 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.535211 (* 1 = 0.535211 loss)
I1013 12:33:51.213537 12690 sgd_solver.cpp:105] Iteration 11800, lr = 0.01
I1013 12:34:21.695709 12690 solver.cpp:218] Iteration 11900 (3.2806 iter/s, 30.4822s/100 iters), loss = 0.783491
I1013 12:34:21.695802 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.783491 (* 1 = 0.783491 loss)
I1013 12:34:21.695819 12690 sgd_solver.cpp:105] Iteration 11900, lr = 0.01
I1013 12:34:50.675071 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:34:51.898366 12690 solver.cpp:330] Iteration 12000, Testing net (#0)
I1013 12:35:08.422175 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:35:08.758183 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.40568 (* 1 = 1.40568 loss)
I1013 12:35:08.758198 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6258
I1013 12:35:09.058878 12690 solver.cpp:218] Iteration 12000 (2.11135 iter/s, 47.3631s/100 iters), loss = 0.376826
I1013 12:35:09.058912 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376826 (* 1 = 0.376826 loss)
I1013 12:35:09.058920 12690 sgd_solver.cpp:105] Iteration 12000, lr = 0.01
I1013 12:35:39.518088 12690 solver.cpp:218] Iteration 12100 (3.28308 iter/s, 30.4592s/100 iters), loss = 0.715774
I1013 12:35:39.518254 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.715774 (* 1 = 0.715774 loss)
I1013 12:35:39.518261 12690 sgd_solver.cpp:105] Iteration 12100, lr = 0.01
I1013 12:36:09.974777 12690 solver.cpp:218] Iteration 12200 (3.28337 iter/s, 30.4565s/100 iters), loss = 0.792351
I1013 12:36:09.974903 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.792351 (* 1 = 0.792351 loss)
I1013 12:36:09.974913 12690 sgd_solver.cpp:105] Iteration 12200, lr = 0.01
I1013 12:36:40.439000 12690 solver.cpp:218] Iteration 12300 (3.28255 iter/s, 30.4641s/100 iters), loss = 0.708906
I1013 12:36:40.439144 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.708906 (* 1 = 0.708906 loss)
I1013 12:36:40.439153 12690 sgd_solver.cpp:105] Iteration 12300, lr = 0.01
I1013 12:37:10.913230 12690 solver.cpp:218] Iteration 12400 (3.28148 iter/s, 30.4741s/100 iters), loss = 0.651658
I1013 12:37:10.913343 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.651659 (* 1 = 0.651659 loss)
I1013 12:37:10.913352 12690 sgd_solver.cpp:105] Iteration 12400, lr = 0.01
I1013 12:37:41.082557 12690 solver.cpp:330] Iteration 12500, Testing net (#0)
I1013 12:37:57.614325 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:37:57.952782 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.4946 (* 1 = 1.4946 loss)
I1013 12:37:57.952798 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6148
I1013 12:37:58.251271 12690 solver.cpp:218] Iteration 12500 (2.11247 iter/s, 47.3379s/100 iters), loss = 0.974976
I1013 12:37:58.251299 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.974976 (* 1 = 0.974976 loss)
I1013 12:37:58.251307 12690 sgd_solver.cpp:105] Iteration 12500, lr = 0.01
I1013 12:38:28.732435 12690 solver.cpp:218] Iteration 12600 (3.28072 iter/s, 30.4811s/100 iters), loss = 0.813728
I1013 12:38:28.732555 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.813728 (* 1 = 0.813728 loss)
I1013 12:38:28.732573 12690 sgd_solver.cpp:105] Iteration 12600, lr = 0.01
I1013 12:38:59.220767 12690 solver.cpp:218] Iteration 12700 (3.27995 iter/s, 30.4882s/100 iters), loss = 0.747469
I1013 12:38:59.220906 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.747469 (* 1 = 0.747469 loss)
I1013 12:38:59.220913 12690 sgd_solver.cpp:105] Iteration 12700, lr = 0.01
I1013 12:39:29.716439 12690 solver.cpp:218] Iteration 12800 (3.27917 iter/s, 30.4955s/100 iters), loss = 0.417787
I1013 12:39:29.716538 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.417787 (* 1 = 0.417787 loss)
I1013 12:39:29.716555 12690 sgd_solver.cpp:105] Iteration 12800, lr = 0.01
I1013 12:40:00.204854 12690 solver.cpp:218] Iteration 12900 (3.27994 iter/s, 30.4883s/100 iters), loss = 0.602827
I1013 12:40:00.205000 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.602827 (* 1 = 0.602827 loss)
I1013 12:40:00.205009 12690 sgd_solver.cpp:105] Iteration 12900, lr = 0.01
I1013 12:40:29.180487 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:40:30.399684 12690 solver.cpp:330] Iteration 13000, Testing net (#0)
I1013 12:40:46.922564 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:40:47.261500 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.43707 (* 1 = 1.43707 loss)
I1013 12:40:47.261515 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6248
I1013 12:40:47.561368 12690 solver.cpp:218] Iteration 13000 (2.11165 iter/s, 47.3564s/100 iters), loss = 0.524732
I1013 12:40:47.561399 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.524733 (* 1 = 0.524733 loss)
I1013 12:40:47.561405 12690 sgd_solver.cpp:105] Iteration 13000, lr = 0.01
I1013 12:41:18.074112 12690 solver.cpp:218] Iteration 13100 (3.27732 iter/s, 30.5127s/100 iters), loss = 0.711301
I1013 12:41:18.074259 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.711301 (* 1 = 0.711301 loss)
I1013 12:41:18.074280 12690 sgd_solver.cpp:105] Iteration 13100, lr = 0.01
I1013 12:41:48.577560 12690 solver.cpp:218] Iteration 13200 (3.27833 iter/s, 30.5033s/100 iters), loss = 0.72864
I1013 12:41:48.577664 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.72864 (* 1 = 0.72864 loss)
I1013 12:41:48.577672 12690 sgd_solver.cpp:105] Iteration 13200, lr = 0.01
I1013 12:42:19.083178 12690 solver.cpp:218] Iteration 13300 (3.27809 iter/s, 30.5055s/100 iters), loss = 0.717714
I1013 12:42:19.083314 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.717714 (* 1 = 0.717714 loss)
I1013 12:42:19.083323 12690 sgd_solver.cpp:105] Iteration 13300, lr = 0.01
I1013 12:42:49.585546 12690 solver.cpp:218] Iteration 13400 (3.27845 iter/s, 30.5023s/100 iters), loss = 0.623543
I1013 12:42:49.585654 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.623543 (* 1 = 0.623543 loss)
I1013 12:42:49.585669 12690 sgd_solver.cpp:105] Iteration 13400, lr = 0.01
I1013 12:43:19.774044 12690 solver.cpp:330] Iteration 13500, Testing net (#0)
I1013 12:43:36.306731 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:43:36.645702 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.48535 (* 1 = 1.48535 loss)
I1013 12:43:36.645718 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6177
I1013 12:43:36.947353 12690 solver.cpp:218] Iteration 13500 (2.11141 iter/s, 47.3617s/100 iters), loss = 0.737972
I1013 12:43:36.947388 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.737973 (* 1 = 0.737973 loss)
I1013 12:43:36.947396 12690 sgd_solver.cpp:105] Iteration 13500, lr = 0.01
I1013 12:44:07.436681 12690 solver.cpp:218] Iteration 13600 (3.27984 iter/s, 30.4893s/100 iters), loss = 0.724065
I1013 12:44:07.436812 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.724065 (* 1 = 0.724065 loss)
I1013 12:44:07.436820 12690 sgd_solver.cpp:105] Iteration 13600, lr = 0.01
I1013 12:44:38.293586 12690 solver.cpp:218] Iteration 13700 (3.24078 iter/s, 30.8568s/100 iters), loss = 0.666804
I1013 12:44:38.293726 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.666805 (* 1 = 0.666805 loss)
I1013 12:44:38.293735 12690 sgd_solver.cpp:105] Iteration 13700, lr = 0.01
I1013 12:45:08.965579 12690 solver.cpp:218] Iteration 13800 (3.26032 iter/s, 30.6719s/100 iters), loss = 0.350132
I1013 12:45:08.965688 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.350132 (* 1 = 0.350132 loss)
I1013 12:45:08.965706 12690 sgd_solver.cpp:105] Iteration 13800, lr = 0.01
I1013 12:45:39.450037 12690 solver.cpp:218] Iteration 13900 (3.28037 iter/s, 30.4844s/100 iters), loss = 0.693987
I1013 12:45:39.450178 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.693987 (* 1 = 0.693987 loss)
I1013 12:45:39.450187 12690 sgd_solver.cpp:105] Iteration 13900, lr = 0.01
I1013 12:46:08.412403 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:46:09.631759 12690 solver.cpp:330] Iteration 14000, Testing net (#0)
I1013 12:46:26.163203 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:46:26.501255 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.42734 (* 1 = 1.42734 loss)
I1013 12:46:26.501269 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6296
I1013 12:46:26.799263 12690 solver.cpp:218] Iteration 14000 (2.11197 iter/s, 47.3491s/100 iters), loss = 0.519522
I1013 12:46:26.799291 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.519522 (* 1 = 0.519522 loss)
I1013 12:46:26.799299 12690 sgd_solver.cpp:105] Iteration 14000, lr = 0.01
I1013 12:46:57.263487 12690 solver.cpp:218] Iteration 14100 (3.28254 iter/s, 30.4642s/100 iters), loss = 0.523813
I1013 12:46:57.263624 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.523814 (* 1 = 0.523814 loss)
I1013 12:46:57.263633 12690 sgd_solver.cpp:105] Iteration 14100, lr = 0.01
I1013 12:47:27.722671 12690 solver.cpp:218] Iteration 14200 (3.2831 iter/s, 30.4591s/100 iters), loss = 0.657219
I1013 12:47:27.722833 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.657219 (* 1 = 0.657219 loss)
I1013 12:47:27.722841 12690 sgd_solver.cpp:105] Iteration 14200, lr = 0.01
I1013 12:47:58.186331 12690 solver.cpp:218] Iteration 14300 (3.28262 iter/s, 30.4635s/100 iters), loss = 0.741377
I1013 12:47:58.186836 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.741377 (* 1 = 0.741377 loss)
I1013 12:47:58.186844 12690 sgd_solver.cpp:105] Iteration 14300, lr = 0.01
I1013 12:48:28.644484 12690 solver.cpp:218] Iteration 14400 (3.28325 iter/s, 30.4577s/100 iters), loss = 0.544822
I1013 12:48:28.644624 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.544822 (* 1 = 0.544822 loss)
I1013 12:48:28.644632 12690 sgd_solver.cpp:105] Iteration 14400, lr = 0.01
I1013 12:48:58.815026 12690 solver.cpp:330] Iteration 14500, Testing net (#0)
I1013 12:49:15.325017 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:49:15.664136 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.51797 (* 1 = 1.51797 loss)
I1013 12:49:15.664152 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6191
I1013 12:49:15.962617 12690 solver.cpp:218] Iteration 14500 (2.11336 iter/s, 47.318s/100 iters), loss = 0.585286
I1013 12:49:15.962652 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.585286 (* 1 = 0.585286 loss)
I1013 12:49:15.962659 12690 sgd_solver.cpp:105] Iteration 14500, lr = 0.01
I1013 12:49:46.419646 12690 solver.cpp:218] Iteration 14600 (3.28332 iter/s, 30.457s/100 iters), loss = 0.720125
I1013 12:49:46.419744 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.720125 (* 1 = 0.720125 loss)
I1013 12:49:46.419750 12690 sgd_solver.cpp:105] Iteration 14600, lr = 0.01
I1013 12:50:16.889227 12690 solver.cpp:218] Iteration 14700 (3.28197 iter/s, 30.4695s/100 iters), loss = 0.475831
I1013 12:50:16.889369 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.475831 (* 1 = 0.475831 loss)
I1013 12:50:16.889379 12690 sgd_solver.cpp:105] Iteration 14700, lr = 0.01
I1013 12:50:47.369771 12690 solver.cpp:218] Iteration 14800 (3.2808 iter/s, 30.4804s/100 iters), loss = 0.414366
I1013 12:50:47.369899 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.414366 (* 1 = 0.414366 loss)
I1013 12:50:47.369906 12690 sgd_solver.cpp:105] Iteration 14800, lr = 0.01
I1013 12:51:17.841383 12690 solver.cpp:218] Iteration 14900 (3.28176 iter/s, 30.4715s/100 iters), loss = 0.539491
I1013 12:51:17.841521 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.539491 (* 1 = 0.539491 loss)
I1013 12:51:17.841529 12690 sgd_solver.cpp:105] Iteration 14900, lr = 0.01
I1013 12:51:46.808965 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:51:48.029639 12690 solver.cpp:330] Iteration 15000, Testing net (#0)
I1013 12:52:04.551698 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:52:04.889219 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.44064 (* 1 = 1.44064 loss)
I1013 12:52:04.889235 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6302
I1013 12:52:05.186885 12690 solver.cpp:218] Iteration 15000 (2.11214 iter/s, 47.3454s/100 iters), loss = 0.43468
I1013 12:52:05.186925 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.43468 (* 1 = 0.43468 loss)
I1013 12:52:05.186933 12690 sgd_solver.cpp:105] Iteration 15000, lr = 0.01
I1013 12:52:35.642204 12690 solver.cpp:218] Iteration 15100 (3.2835 iter/s, 30.4553s/100 iters), loss = 0.368597
I1013 12:52:35.642318 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.368597 (* 1 = 0.368597 loss)
I1013 12:52:35.642326 12690 sgd_solver.cpp:105] Iteration 15100, lr = 0.01
I1013 12:53:06.129561 12690 solver.cpp:218] Iteration 15200 (3.28006 iter/s, 30.4873s/100 iters), loss = 0.679241
I1013 12:53:06.129704 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.679241 (* 1 = 0.679241 loss)
I1013 12:53:06.129714 12690 sgd_solver.cpp:105] Iteration 15200, lr = 0.01
I1013 12:53:36.589439 12690 solver.cpp:218] Iteration 15300 (3.28302 iter/s, 30.4597s/100 iters), loss = 0.632141
I1013 12:53:36.589599 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.632141 (* 1 = 0.632141 loss)
I1013 12:53:36.589608 12690 sgd_solver.cpp:105] Iteration 15300, lr = 0.01
I1013 12:54:07.081262 12690 solver.cpp:218] Iteration 15400 (3.27958 iter/s, 30.4917s/100 iters), loss = 0.653618
I1013 12:54:07.081384 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.653618 (* 1 = 0.653618 loss)
I1013 12:54:07.081393 12690 sgd_solver.cpp:105] Iteration 15400, lr = 0.01
I1013 12:54:37.272899 12690 solver.cpp:330] Iteration 15500, Testing net (#0)
I1013 12:54:53.793823 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:54:54.132138 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.46908 (* 1 = 1.46908 loss)
I1013 12:54:54.132153 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6324
I1013 12:54:54.432072 12690 solver.cpp:218] Iteration 15500 (2.1119 iter/s, 47.3507s/100 iters), loss = 0.745827
I1013 12:54:54.432101 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.745827 (* 1 = 0.745827 loss)
I1013 12:54:54.432107 12690 sgd_solver.cpp:105] Iteration 15500, lr = 0.01
I1013 12:55:24.933734 12690 solver.cpp:218] Iteration 15600 (3.27851 iter/s, 30.5016s/100 iters), loss = 0.728102
I1013 12:55:24.933780 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.728103 (* 1 = 0.728103 loss)
I1013 12:55:24.933796 12690 sgd_solver.cpp:105] Iteration 15600, lr = 0.01
I1013 12:55:55.444231 12690 solver.cpp:218] Iteration 15700 (3.27756 iter/s, 30.5105s/100 iters), loss = 0.563893
I1013 12:55:55.444337 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.563893 (* 1 = 0.563893 loss)
I1013 12:55:55.444345 12690 sgd_solver.cpp:105] Iteration 15700, lr = 0.01
I1013 12:56:25.953847 12690 solver.cpp:218] Iteration 15800 (3.27766 iter/s, 30.5095s/100 iters), loss = 0.411538
I1013 12:56:25.953979 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.411538 (* 1 = 0.411538 loss)
I1013 12:56:25.953997 12690 sgd_solver.cpp:105] Iteration 15800, lr = 0.01
I1013 12:56:56.451992 12690 solver.cpp:218] Iteration 15900 (3.2789 iter/s, 30.498s/100 iters), loss = 0.816409
I1013 12:56:56.452101 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.816409 (* 1 = 0.816409 loss)
I1013 12:56:56.452108 12690 sgd_solver.cpp:105] Iteration 15900, lr = 0.01
I1013 12:57:25.470397 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:57:26.737707 12690 solver.cpp:330] Iteration 16000, Testing net (#0)
I1013 12:57:43.346011 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 12:57:43.683105 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.43904 (* 1 = 1.43904 loss)
I1013 12:57:43.683121 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6322
I1013 12:57:43.982334 12690 solver.cpp:218] Iteration 16000 (2.10392 iter/s, 47.5303s/100 iters), loss = 0.334005
I1013 12:57:43.982369 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334005 (* 1 = 0.334005 loss)
I1013 12:57:43.982375 12690 sgd_solver.cpp:105] Iteration 16000, lr = 0.01
I1013 12:58:14.663933 12690 solver.cpp:218] Iteration 16100 (3.25929 iter/s, 30.6816s/100 iters), loss = 0.430127
I1013 12:58:14.664055 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.430127 (* 1 = 0.430127 loss)
I1013 12:58:14.664077 12690 sgd_solver.cpp:105] Iteration 16100, lr = 0.01
I1013 12:58:45.188694 12690 solver.cpp:218] Iteration 16200 (3.27604 iter/s, 30.5247s/100 iters), loss = 0.636448
I1013 12:58:45.188792 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.636448 (* 1 = 0.636448 loss)
I1013 12:58:45.188799 12690 sgd_solver.cpp:105] Iteration 16200, lr = 0.01
I1013 12:59:15.694898 12690 solver.cpp:218] Iteration 16300 (3.27803 iter/s, 30.5061s/100 iters), loss = 0.412544
I1013 12:59:15.695019 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.412545 (* 1 = 0.412545 loss)
I1013 12:59:15.695027 12690 sgd_solver.cpp:105] Iteration 16300, lr = 0.01
I1013 12:59:46.203835 12690 solver.cpp:218] Iteration 16400 (3.27774 iter/s, 30.5088s/100 iters), loss = 0.467038
I1013 12:59:46.203985 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.467039 (* 1 = 0.467039 loss)
I1013 12:59:46.203995 12690 sgd_solver.cpp:105] Iteration 16400, lr = 0.01
I1013 13:00:16.414150 12690 solver.cpp:330] Iteration 16500, Testing net (#0)
I1013 13:00:32.936718 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:00:33.274258 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.48043 (* 1 = 1.48043 loss)
I1013 13:00:33.274273 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6312
I1013 13:00:33.573369 12690 solver.cpp:218] Iteration 16500 (2.11107 iter/s, 47.3694s/100 iters), loss = 0.622476
I1013 13:00:33.573398 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.622476 (* 1 = 0.622476 loss)
I1013 13:00:33.573405 12690 sgd_solver.cpp:105] Iteration 16500, lr = 0.01
I1013 13:01:04.070967 12690 solver.cpp:218] Iteration 16600 (3.27895 iter/s, 30.4976s/100 iters), loss = 0.625545
I1013 13:01:04.071110 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.625545 (* 1 = 0.625545 loss)
I1013 13:01:04.071120 12690 sgd_solver.cpp:105] Iteration 16600, lr = 0.01
I1013 13:01:34.574906 12690 solver.cpp:218] Iteration 16700 (3.27828 iter/s, 30.5038s/100 iters), loss = 0.606149
I1013 13:01:34.575038 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.606149 (* 1 = 0.606149 loss)
I1013 13:01:34.575045 12690 sgd_solver.cpp:105] Iteration 16700, lr = 0.01
I1013 13:02:05.078765 12690 solver.cpp:218] Iteration 16800 (3.27829 iter/s, 30.5037s/100 iters), loss = 0.430432
I1013 13:02:05.080353 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.430432 (* 1 = 0.430432 loss)
I1013 13:02:05.080369 12690 sgd_solver.cpp:105] Iteration 16800, lr = 0.01
I1013 13:02:35.583700 12690 solver.cpp:218] Iteration 16900 (3.27833 iter/s, 30.5034s/100 iters), loss = 0.592827
I1013 13:02:35.583813 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.592827 (* 1 = 0.592827 loss)
I1013 13:02:35.583822 12690 sgd_solver.cpp:105] Iteration 16900, lr = 0.01
I1013 13:03:04.579403 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:03:05.806242 12690 solver.cpp:330] Iteration 17000, Testing net (#0)
I1013 13:03:22.321560 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:03:22.659288 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.50761 (* 1 = 1.50761 loss)
I1013 13:03:22.659304 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6274
I1013 13:03:22.959980 12690 solver.cpp:218] Iteration 17000 (2.11076 iter/s, 47.3762s/100 iters), loss = 0.262575
I1013 13:03:22.960014 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262576 (* 1 = 0.262576 loss)
I1013 13:03:22.960021 12690 sgd_solver.cpp:105] Iteration 17000, lr = 0.01
I1013 13:03:53.427338 12690 solver.cpp:218] Iteration 17100 (3.2822 iter/s, 30.4673s/100 iters), loss = 0.411648
I1013 13:03:53.427449 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.411648 (* 1 = 0.411648 loss)
I1013 13:03:53.427458 12690 sgd_solver.cpp:105] Iteration 17100, lr = 0.01
I1013 13:04:23.892244 12690 solver.cpp:218] Iteration 17200 (3.28248 iter/s, 30.4648s/100 iters), loss = 0.572193
I1013 13:04:23.892380 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.572194 (* 1 = 0.572194 loss)
I1013 13:04:23.892387 12690 sgd_solver.cpp:105] Iteration 17200, lr = 0.01
I1013 13:04:54.364292 12690 solver.cpp:218] Iteration 17300 (3.28171 iter/s, 30.4719s/100 iters), loss = 0.467086
I1013 13:04:54.364400 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.467086 (* 1 = 0.467086 loss)
I1013 13:04:54.364408 12690 sgd_solver.cpp:105] Iteration 17300, lr = 0.01
I1013 13:05:24.833611 12690 solver.cpp:218] Iteration 17400 (3.282 iter/s, 30.4692s/100 iters), loss = 0.491333
I1013 13:05:24.833720 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.491334 (* 1 = 0.491334 loss)
I1013 13:05:24.833739 12690 sgd_solver.cpp:105] Iteration 17400, lr = 0.01
I1013 13:05:55.002825 12690 solver.cpp:330] Iteration 17500, Testing net (#0)
I1013 13:06:11.515558 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:06:11.852840 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.49999 (* 1 = 1.49999 loss)
I1013 13:06:11.852855 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6294
I1013 13:06:12.152066 12690 solver.cpp:218] Iteration 17500 (2.11334 iter/s, 47.3184s/100 iters), loss = 0.446136
I1013 13:06:12.152099 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.446136 (* 1 = 0.446136 loss)
I1013 13:06:12.152106 12690 sgd_solver.cpp:105] Iteration 17500, lr = 0.01
I1013 13:06:42.618175 12690 solver.cpp:218] Iteration 17600 (3.28234 iter/s, 30.4661s/100 iters), loss = 0.628735
I1013 13:06:42.618288 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.628736 (* 1 = 0.628736 loss)
I1013 13:06:42.618304 12690 sgd_solver.cpp:105] Iteration 17600, lr = 0.01
I1013 13:07:13.074935 12690 solver.cpp:218] Iteration 17700 (3.28335 iter/s, 30.4567s/100 iters), loss = 0.412828
I1013 13:07:13.075080 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.412829 (* 1 = 0.412829 loss)
I1013 13:07:13.075089 12690 sgd_solver.cpp:105] Iteration 17700, lr = 0.01
I1013 13:07:43.538784 12690 solver.cpp:218] Iteration 17800 (3.28259 iter/s, 30.4637s/100 iters), loss = 0.491035
I1013 13:07:43.538921 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.491035 (* 1 = 0.491035 loss)
I1013 13:07:43.538928 12690 sgd_solver.cpp:105] Iteration 17800, lr = 0.01
I1013 13:08:14.014376 12690 solver.cpp:218] Iteration 17900 (3.28133 iter/s, 30.4755s/100 iters), loss = 0.425223
I1013 13:08:14.014485 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.425223 (* 1 = 0.425223 loss)
I1013 13:08:14.014503 12690 sgd_solver.cpp:105] Iteration 17900, lr = 0.01
I1013 13:08:42.958963 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:08:44.176889 12690 solver.cpp:330] Iteration 18000, Testing net (#0)
I1013 13:09:00.693398 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:09:01.031317 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.50069 (* 1 = 1.50069 loss)
I1013 13:09:01.031333 12690 solver.cpp:397]     Test net output #1: accuracy = 0.633
I1013 13:09:01.330910 12690 solver.cpp:218] Iteration 18000 (2.11343 iter/s, 47.3165s/100 iters), loss = 0.23884
I1013 13:09:01.330938 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238841 (* 1 = 0.238841 loss)
I1013 13:09:01.330945 12690 sgd_solver.cpp:105] Iteration 18000, lr = 0.01
I1013 13:09:31.792280 12690 solver.cpp:218] Iteration 18100 (3.28285 iter/s, 30.4614s/100 iters), loss = 0.372123
I1013 13:09:31.792381 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372123 (* 1 = 0.372123 loss)
I1013 13:09:31.792398 12690 sgd_solver.cpp:105] Iteration 18100, lr = 0.01
I1013 13:10:02.260692 12690 solver.cpp:218] Iteration 18200 (3.2821 iter/s, 30.4683s/100 iters), loss = 0.254421
I1013 13:10:02.260831 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254422 (* 1 = 0.254422 loss)
I1013 13:10:02.260840 12690 sgd_solver.cpp:105] Iteration 18200, lr = 0.01
I1013 13:10:32.738750 12690 solver.cpp:218] Iteration 18300 (3.28106 iter/s, 30.4779s/100 iters), loss = 0.355887
I1013 13:10:32.738904 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.355888 (* 1 = 0.355888 loss)
I1013 13:10:32.738912 12690 sgd_solver.cpp:105] Iteration 18300, lr = 0.01
I1013 13:11:03.218727 12690 solver.cpp:218] Iteration 18400 (3.28086 iter/s, 30.4798s/100 iters), loss = 0.364782
I1013 13:11:03.218829 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364783 (* 1 = 0.364783 loss)
I1013 13:11:03.218845 12690 sgd_solver.cpp:105] Iteration 18400, lr = 0.01
I1013 13:11:33.378712 12690 solver.cpp:330] Iteration 18500, Testing net (#0)
I1013 13:11:49.890257 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:11:50.228081 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.54777 (* 1 = 1.54777 loss)
I1013 13:11:50.228096 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6286
I1013 13:11:50.528518 12690 solver.cpp:218] Iteration 18500 (2.11373 iter/s, 47.3097s/100 iters), loss = 0.528309
I1013 13:11:50.528547 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.52831 (* 1 = 0.52831 loss)
I1013 13:11:50.528553 12690 sgd_solver.cpp:105] Iteration 18500, lr = 0.01
I1013 13:12:20.976491 12690 solver.cpp:218] Iteration 18600 (3.28429 iter/s, 30.448s/100 iters), loss = 0.722629
I1013 13:12:20.976608 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.722629 (* 1 = 0.722629 loss)
I1013 13:12:20.976627 12690 sgd_solver.cpp:105] Iteration 18600, lr = 0.01
I1013 13:12:51.435963 12690 solver.cpp:218] Iteration 18700 (3.28306 iter/s, 30.4594s/100 iters), loss = 0.462021
I1013 13:12:51.436105 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.462021 (* 1 = 0.462021 loss)
I1013 13:12:51.436113 12690 sgd_solver.cpp:105] Iteration 18700, lr = 0.01
I1013 13:13:21.900061 12690 solver.cpp:218] Iteration 18800 (3.28257 iter/s, 30.464s/100 iters), loss = 0.337785
I1013 13:13:21.900182 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.337786 (* 1 = 0.337786 loss)
I1013 13:13:21.900189 12690 sgd_solver.cpp:105] Iteration 18800, lr = 0.01
I1013 13:13:52.352656 12690 solver.cpp:218] Iteration 18900 (3.2838 iter/s, 30.4525s/100 iters), loss = 0.480909
I1013 13:13:52.352793 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.480909 (* 1 = 0.480909 loss)
I1013 13:13:52.352802 12690 sgd_solver.cpp:105] Iteration 18900, lr = 0.01
I1013 13:14:21.319217 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:14:22.534981 12690 solver.cpp:330] Iteration 19000, Testing net (#0)
I1013 13:14:39.062981 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:14:39.401147 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.51828 (* 1 = 1.51828 loss)
I1013 13:14:39.401163 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6309
I1013 13:14:39.700582 12690 solver.cpp:218] Iteration 19000 (2.11203 iter/s, 47.3478s/100 iters), loss = 0.320802
I1013 13:14:39.700615 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320803 (* 1 = 0.320803 loss)
I1013 13:14:39.700621 12690 sgd_solver.cpp:105] Iteration 19000, lr = 0.01
I1013 13:15:10.182921 12690 solver.cpp:218] Iteration 19100 (3.28059 iter/s, 30.4823s/100 iters), loss = 0.480127
I1013 13:15:10.183007 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.480128 (* 1 = 0.480128 loss)
I1013 13:15:10.183024 12690 sgd_solver.cpp:105] Iteration 19100, lr = 0.01
I1013 13:15:40.673413 12690 solver.cpp:218] Iteration 19200 (3.27972 iter/s, 30.4904s/100 iters), loss = 0.682947
I1013 13:15:40.673553 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.682948 (* 1 = 0.682948 loss)
I1013 13:15:40.673563 12690 sgd_solver.cpp:105] Iteration 19200, lr = 0.01
I1013 13:16:11.167835 12690 solver.cpp:218] Iteration 19300 (3.2793 iter/s, 30.4943s/100 iters), loss = 0.31587
I1013 13:16:11.167943 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.31587 (* 1 = 0.31587 loss)
I1013 13:16:11.167960 12690 sgd_solver.cpp:105] Iteration 19300, lr = 0.01
I1013 13:16:41.651768 12690 solver.cpp:218] Iteration 19400 (3.28043 iter/s, 30.4838s/100 iters), loss = 0.658903
I1013 13:16:41.651875 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.658903 (* 1 = 0.658903 loss)
I1013 13:16:41.651883 12690 sgd_solver.cpp:105] Iteration 19400, lr = 0.01
I1013 13:17:11.827672 12690 solver.cpp:330] Iteration 19500, Testing net (#0)
I1013 13:17:28.350054 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:17:28.687404 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.51574 (* 1 = 1.51574 loss)
I1013 13:17:28.687419 12690 solver.cpp:397]     Test net output #1: accuracy = 0.636
I1013 13:17:28.988912 12690 solver.cpp:218] Iteration 19500 (2.11251 iter/s, 47.3371s/100 iters), loss = 0.388615
I1013 13:17:28.988947 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.388616 (* 1 = 0.388616 loss)
I1013 13:17:28.988955 12690 sgd_solver.cpp:105] Iteration 19500, lr = 0.01
I1013 13:17:59.470350 12690 solver.cpp:218] Iteration 19600 (3.28069 iter/s, 30.4814s/100 iters), loss = 0.439458
I1013 13:17:59.470468 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.439458 (* 1 = 0.439458 loss)
I1013 13:17:59.470476 12690 sgd_solver.cpp:105] Iteration 19600, lr = 0.01
I1013 13:18:29.952486 12690 solver.cpp:218] Iteration 19700 (3.28062 iter/s, 30.482s/100 iters), loss = 0.496438
I1013 13:18:29.952621 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.496439 (* 1 = 0.496439 loss)
I1013 13:18:29.952630 12690 sgd_solver.cpp:105] Iteration 19700, lr = 0.01
I1013 13:19:00.432497 12690 solver.cpp:218] Iteration 19800 (3.28085 iter/s, 30.4799s/100 iters), loss = 0.53137
I1013 13:19:00.432597 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.531371 (* 1 = 0.531371 loss)
I1013 13:19:00.432615 12690 sgd_solver.cpp:105] Iteration 19800, lr = 0.01
I1013 13:19:30.902597 12690 solver.cpp:218] Iteration 19900 (3.28192 iter/s, 30.47s/100 iters), loss = 0.679655
I1013 13:19:30.902743 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.679655 (* 1 = 0.679655 loss)
I1013 13:19:30.902752 12690 sgd_solver.cpp:105] Iteration 19900, lr = 0.01
I1013 13:19:59.857120 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:20:01.074306 12690 solver.cpp:330] Iteration 20000, Testing net (#0)
I1013 13:20:17.595839 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:20:17.932696 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.51172 (* 1 = 1.51172 loss)
I1013 13:20:17.932713 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6384
I1013 13:20:18.232031 12690 solver.cpp:218] Iteration 20000 (2.11286 iter/s, 47.3293s/100 iters), loss = 0.149032
I1013 13:20:18.232074 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149032 (* 1 = 0.149032 loss)
I1013 13:20:18.232080 12690 sgd_solver.cpp:105] Iteration 20000, lr = 0.01
I1013 13:20:48.719964 12690 solver.cpp:218] Iteration 20100 (3.27999 iter/s, 30.4879s/100 iters), loss = 0.212889
I1013 13:20:48.720463 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212889 (* 1 = 0.212889 loss)
I1013 13:20:48.720472 12690 sgd_solver.cpp:105] Iteration 20100, lr = 0.01
I1013 13:21:19.223069 12690 solver.cpp:218] Iteration 20200 (3.27841 iter/s, 30.5026s/100 iters), loss = 0.4014
I1013 13:21:19.223208 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.4014 (* 1 = 0.4014 loss)
I1013 13:21:19.223217 12690 sgd_solver.cpp:105] Iteration 20200, lr = 0.01
I1013 13:21:49.718665 12690 solver.cpp:218] Iteration 20300 (3.27918 iter/s, 30.4955s/100 iters), loss = 0.452678
I1013 13:21:49.718811 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.452679 (* 1 = 0.452679 loss)
I1013 13:21:49.718821 12690 sgd_solver.cpp:105] Iteration 20300, lr = 0.01
I1013 13:22:20.224028 12690 solver.cpp:218] Iteration 20400 (3.27813 iter/s, 30.5052s/100 iters), loss = 0.246876
I1013 13:22:20.224172 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246876 (* 1 = 0.246876 loss)
I1013 13:22:20.224181 12690 sgd_solver.cpp:105] Iteration 20400, lr = 0.01
I1013 13:22:50.415199 12690 solver.cpp:330] Iteration 20500, Testing net (#0)
I1013 13:23:06.939695 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:23:07.277407 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.5774 (* 1 = 1.5774 loss)
I1013 13:23:07.277423 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6309
I1013 13:23:07.575574 12690 solver.cpp:218] Iteration 20500 (2.11187 iter/s, 47.3514s/100 iters), loss = 0.487857
I1013 13:23:07.575606 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.487857 (* 1 = 0.487857 loss)
I1013 13:23:07.575616 12690 sgd_solver.cpp:105] Iteration 20500, lr = 0.01
I1013 13:23:38.032825 12690 solver.cpp:218] Iteration 20600 (3.28329 iter/s, 30.4572s/100 iters), loss = 0.337529
I1013 13:23:38.032985 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.337529 (* 1 = 0.337529 loss)
I1013 13:23:38.033007 12690 sgd_solver.cpp:105] Iteration 20600, lr = 0.01
I1013 13:24:08.487825 12690 solver.cpp:218] Iteration 20700 (3.28355 iter/s, 30.4549s/100 iters), loss = 0.293529
I1013 13:24:08.487980 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.29353 (* 1 = 0.29353 loss)
I1013 13:24:08.487989 12690 sgd_solver.cpp:105] Iteration 20700, lr = 0.01
I1013 13:24:38.952651 12690 solver.cpp:218] Iteration 20800 (3.28249 iter/s, 30.4647s/100 iters), loss = 0.224189
I1013 13:24:38.952761 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224189 (* 1 = 0.224189 loss)
I1013 13:24:38.952769 12690 sgd_solver.cpp:105] Iteration 20800, lr = 0.01
I1013 13:25:09.416378 12690 solver.cpp:218] Iteration 20900 (3.2826 iter/s, 30.4636s/100 iters), loss = 0.47816
I1013 13:25:09.416515 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.478161 (* 1 = 0.478161 loss)
I1013 13:25:09.416523 12690 sgd_solver.cpp:105] Iteration 20900, lr = 0.01
I1013 13:25:38.364799 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:25:39.581081 12690 solver.cpp:330] Iteration 21000, Testing net (#0)
I1013 13:25:56.105424 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:25:56.444479 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.51277 (* 1 = 1.51277 loss)
I1013 13:25:56.444494 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6403
I1013 13:25:56.746029 12690 solver.cpp:218] Iteration 21000 (2.11285 iter/s, 47.3295s/100 iters), loss = 0.248233
I1013 13:25:56.746059 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248234 (* 1 = 0.248234 loss)
I1013 13:25:56.746065 12690 sgd_solver.cpp:105] Iteration 21000, lr = 0.01
I1013 13:26:27.196357 12690 solver.cpp:218] Iteration 21100 (3.28404 iter/s, 30.4503s/100 iters), loss = 0.412295
I1013 13:26:27.196481 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.412296 (* 1 = 0.412296 loss)
I1013 13:26:27.196499 12690 sgd_solver.cpp:105] Iteration 21100, lr = 0.01
I1013 13:26:57.644256 12690 solver.cpp:218] Iteration 21200 (3.28431 iter/s, 30.4478s/100 iters), loss = 0.275441
I1013 13:26:57.644392 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.275442 (* 1 = 0.275442 loss)
I1013 13:26:57.644402 12690 sgd_solver.cpp:105] Iteration 21200, lr = 0.01
I1013 13:27:28.090941 12690 solver.cpp:218] Iteration 21300 (3.28444 iter/s, 30.4466s/100 iters), loss = 0.419381
I1013 13:27:28.091083 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419381 (* 1 = 0.419381 loss)
I1013 13:27:28.091090 12690 sgd_solver.cpp:105] Iteration 21300, lr = 0.01
I1013 13:27:58.542527 12690 solver.cpp:218] Iteration 21400 (3.28392 iter/s, 30.4515s/100 iters), loss = 0.174284
I1013 13:27:58.542667 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.174285 (* 1 = 0.174285 loss)
I1013 13:27:58.542675 12690 sgd_solver.cpp:105] Iteration 21400, lr = 0.01
I1013 13:28:28.716207 12690 solver.cpp:330] Iteration 21500, Testing net (#0)
I1013 13:28:45.235976 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:28:45.573302 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.578 (* 1 = 1.578 loss)
I1013 13:28:45.573318 12690 solver.cpp:397]     Test net output #1: accuracy = 0.635
I1013 13:28:45.872123 12690 solver.cpp:218] Iteration 21500 (2.11285 iter/s, 47.3295s/100 iters), loss = 0.497471
I1013 13:28:45.872158 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.497472 (* 1 = 0.497472 loss)
I1013 13:28:45.872164 12690 sgd_solver.cpp:105] Iteration 21500, lr = 0.01
I1013 13:29:16.320377 12690 solver.cpp:218] Iteration 21600 (3.28426 iter/s, 30.4482s/100 iters), loss = 0.485781
I1013 13:29:16.320534 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.485782 (* 1 = 0.485782 loss)
I1013 13:29:16.320554 12690 sgd_solver.cpp:105] Iteration 21600, lr = 0.01
I1013 13:29:46.756733 12690 solver.cpp:218] Iteration 21700 (3.28556 iter/s, 30.4362s/100 iters), loss = 0.303509
I1013 13:29:46.756872 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.30351 (* 1 = 0.30351 loss)
I1013 13:29:46.756880 12690 sgd_solver.cpp:105] Iteration 21700, lr = 0.01
I1013 13:30:17.240406 12690 solver.cpp:218] Iteration 21800 (3.28046 iter/s, 30.4835s/100 iters), loss = 0.208198
I1013 13:30:17.240540 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208199 (* 1 = 0.208199 loss)
I1013 13:30:17.240547 12690 sgd_solver.cpp:105] Iteration 21800, lr = 0.01
I1013 13:30:47.704717 12690 solver.cpp:218] Iteration 21900 (3.28254 iter/s, 30.4642s/100 iters), loss = 0.274281
I1013 13:30:47.704845 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.274282 (* 1 = 0.274282 loss)
I1013 13:30:47.704854 12690 sgd_solver.cpp:105] Iteration 21900, lr = 0.01
I1013 13:31:16.669147 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:31:17.890005 12690 solver.cpp:330] Iteration 22000, Testing net (#0)
I1013 13:31:34.417026 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:31:34.754818 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.49467 (* 1 = 1.49467 loss)
I1013 13:31:34.754834 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6506
I1013 13:31:35.054652 12690 solver.cpp:218] Iteration 22000 (2.11194 iter/s, 47.3498s/100 iters), loss = 0.235375
I1013 13:31:35.054687 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235375 (* 1 = 0.235375 loss)
I1013 13:31:35.054695 12690 sgd_solver.cpp:105] Iteration 22000, lr = 0.01
I1013 13:32:05.537490 12690 solver.cpp:218] Iteration 22100 (3.28054 iter/s, 30.4828s/100 iters), loss = 0.476947
I1013 13:32:05.537619 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.476947 (* 1 = 0.476947 loss)
I1013 13:32:05.537628 12690 sgd_solver.cpp:105] Iteration 22100, lr = 0.01
I1013 13:32:36.026561 12690 solver.cpp:218] Iteration 22200 (3.27988 iter/s, 30.489s/100 iters), loss = 0.348764
I1013 13:32:36.026702 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348764 (* 1 = 0.348764 loss)
I1013 13:32:36.026711 12690 sgd_solver.cpp:105] Iteration 22200, lr = 0.01
I1013 13:33:06.523604 12690 solver.cpp:218] Iteration 22300 (3.27902 iter/s, 30.4969s/100 iters), loss = 0.354246
I1013 13:33:06.523746 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.354247 (* 1 = 0.354247 loss)
I1013 13:33:06.523756 12690 sgd_solver.cpp:105] Iteration 22300, lr = 0.01
I1013 13:33:37.006510 12690 solver.cpp:218] Iteration 22400 (3.28054 iter/s, 30.4828s/100 iters), loss = 0.393265
I1013 13:33:37.006651 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.393265 (* 1 = 0.393265 loss)
I1013 13:33:37.006660 12690 sgd_solver.cpp:105] Iteration 22400, lr = 0.01
I1013 13:34:07.204032 12690 solver.cpp:330] Iteration 22500, Testing net (#0)
I1013 13:34:23.718637 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:34:24.055923 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.5686 (* 1 = 1.5686 loss)
I1013 13:34:24.055939 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6381
I1013 13:34:24.357476 12690 solver.cpp:218] Iteration 22500 (2.11189 iter/s, 47.3508s/100 iters), loss = 0.575066
I1013 13:34:24.357504 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.575066 (* 1 = 0.575066 loss)
I1013 13:34:24.357511 12690 sgd_solver.cpp:105] Iteration 22500, lr = 0.01
I1013 13:34:54.855180 12690 solver.cpp:218] Iteration 22600 (3.27894 iter/s, 30.4977s/100 iters), loss = 0.353344
I1013 13:34:54.855341 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353345 (* 1 = 0.353345 loss)
I1013 13:34:54.855351 12690 sgd_solver.cpp:105] Iteration 22600, lr = 0.01
I1013 13:35:25.366909 12690 solver.cpp:218] Iteration 22700 (3.27744 iter/s, 30.5116s/100 iters), loss = 0.412474
I1013 13:35:25.370473 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.412475 (* 1 = 0.412475 loss)
I1013 13:35:25.370491 12690 sgd_solver.cpp:105] Iteration 22700, lr = 0.01
I1013 13:35:56.052364 12690 solver.cpp:218] Iteration 22800 (3.25925 iter/s, 30.6819s/100 iters), loss = 0.459336
I1013 13:35:56.052510 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.459336 (* 1 = 0.459336 loss)
I1013 13:35:56.052518 12690 sgd_solver.cpp:105] Iteration 22800, lr = 0.01
I1013 13:36:26.763283 12690 solver.cpp:218] Iteration 22900 (3.25619 iter/s, 30.7108s/100 iters), loss = 0.360575
I1013 13:36:26.763419 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.360575 (* 1 = 0.360575 loss)
I1013 13:36:26.763428 12690 sgd_solver.cpp:105] Iteration 22900, lr = 0.01
I1013 13:36:55.944313 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:36:57.170929 12690 solver.cpp:330] Iteration 23000, Testing net (#0)
I1013 13:37:13.834713 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:37:14.179414 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.51655 (* 1 = 1.51655 loss)
I1013 13:37:14.179430 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6499
I1013 13:37:14.483947 12690 solver.cpp:218] Iteration 23000 (2.09553 iter/s, 47.7205s/100 iters), loss = 0.303373
I1013 13:37:14.483976 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303373 (* 1 = 0.303373 loss)
I1013 13:37:14.483983 12690 sgd_solver.cpp:105] Iteration 23000, lr = 0.01
I1013 13:37:45.130350 12690 solver.cpp:218] Iteration 23100 (3.26303 iter/s, 30.6464s/100 iters), loss = 0.257066
I1013 13:37:45.130461 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257067 (* 1 = 0.257067 loss)
I1013 13:37:45.130468 12690 sgd_solver.cpp:105] Iteration 23100, lr = 0.01
I1013 13:38:15.789304 12690 solver.cpp:218] Iteration 23200 (3.2617 iter/s, 30.6589s/100 iters), loss = 0.480147
I1013 13:38:15.789417 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.480148 (* 1 = 0.480148 loss)
I1013 13:38:15.789435 12690 sgd_solver.cpp:105] Iteration 23200, lr = 0.01
I1013 13:38:46.465533 12690 solver.cpp:218] Iteration 23300 (3.25986 iter/s, 30.6761s/100 iters), loss = 0.226639
I1013 13:38:46.467206 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22664 (* 1 = 0.22664 loss)
I1013 13:38:46.467213 12690 sgd_solver.cpp:105] Iteration 23300, lr = 0.01
I1013 13:39:17.164307 12690 solver.cpp:218] Iteration 23400 (3.25764 iter/s, 30.6971s/100 iters), loss = 0.285253
I1013 13:39:17.164400 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285253 (* 1 = 0.285253 loss)
I1013 13:39:17.164418 12690 sgd_solver.cpp:105] Iteration 23400, lr = 0.01
I1013 13:39:47.520162 12690 solver.cpp:330] Iteration 23500, Testing net (#0)
I1013 13:40:04.096284 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:40:04.434254 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.63352 (* 1 = 1.63352 loss)
I1013 13:40:04.434269 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6349
I1013 13:40:04.734484 12690 solver.cpp:218] Iteration 23500 (2.10216 iter/s, 47.5701s/100 iters), loss = 0.343144
I1013 13:40:04.734519 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.343144 (* 1 = 0.343144 loss)
I1013 13:40:04.734525 12690 sgd_solver.cpp:105] Iteration 23500, lr = 0.01
I1013 13:40:35.394240 12690 solver.cpp:218] Iteration 23600 (3.26161 iter/s, 30.6597s/100 iters), loss = 0.245042
I1013 13:40:35.394383 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245042 (* 1 = 0.245042 loss)
I1013 13:40:35.394392 12690 sgd_solver.cpp:105] Iteration 23600, lr = 0.01
I1013 13:41:05.968808 12690 solver.cpp:218] Iteration 23700 (3.27071 iter/s, 30.5744s/100 iters), loss = 0.34598
I1013 13:41:05.968930 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.345981 (* 1 = 0.345981 loss)
I1013 13:41:05.968940 12690 sgd_solver.cpp:105] Iteration 23700, lr = 0.01
I1013 13:41:36.617671 12690 solver.cpp:218] Iteration 23800 (3.26278 iter/s, 30.6487s/100 iters), loss = 0.309913
I1013 13:41:36.617780 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309913 (* 1 = 0.309913 loss)
I1013 13:41:36.617790 12690 sgd_solver.cpp:105] Iteration 23800, lr = 0.01
I1013 13:42:07.241255 12690 solver.cpp:218] Iteration 23900 (3.26547 iter/s, 30.6235s/100 iters), loss = 0.360759
I1013 13:42:07.241333 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.36076 (* 1 = 0.36076 loss)
I1013 13:42:07.241350 12690 sgd_solver.cpp:105] Iteration 23900, lr = 0.01
I1013 13:42:36.328518 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:42:37.555454 12690 solver.cpp:330] Iteration 24000, Testing net (#0)
I1013 13:42:54.217947 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:42:54.558571 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.54691 (* 1 = 1.54691 loss)
I1013 13:42:54.558586 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6431
I1013 13:42:54.862048 12690 solver.cpp:218] Iteration 24000 (2.09993 iter/s, 47.6207s/100 iters), loss = 0.175928
I1013 13:42:54.862082 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175929 (* 1 = 0.175929 loss)
I1013 13:42:54.862089 12690 sgd_solver.cpp:105] Iteration 24000, lr = 0.01
I1013 13:43:25.487571 12690 solver.cpp:218] Iteration 24100 (3.26525 iter/s, 30.6255s/100 iters), loss = 0.310302
I1013 13:43:25.487709 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310302 (* 1 = 0.310302 loss)
I1013 13:43:25.487716 12690 sgd_solver.cpp:105] Iteration 24100, lr = 0.01
I1013 13:43:56.096180 12690 solver.cpp:218] Iteration 24200 (3.26707 iter/s, 30.6085s/100 iters), loss = 0.6152
I1013 13:43:56.096299 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.6152 (* 1 = 0.6152 loss)
I1013 13:43:56.096309 12690 sgd_solver.cpp:105] Iteration 24200, lr = 0.01
I1013 13:44:26.750793 12690 solver.cpp:218] Iteration 24300 (3.26216 iter/s, 30.6545s/100 iters), loss = 0.224141
I1013 13:44:26.750919 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224142 (* 1 = 0.224142 loss)
I1013 13:44:26.750927 12690 sgd_solver.cpp:105] Iteration 24300, lr = 0.01
I1013 13:44:57.421527 12690 solver.cpp:218] Iteration 24400 (3.26045 iter/s, 30.6706s/100 iters), loss = 0.406023
I1013 13:44:57.421635 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.406023 (* 1 = 0.406023 loss)
I1013 13:44:57.421643 12690 sgd_solver.cpp:105] Iteration 24400, lr = 0.01
I1013 13:45:27.753121 12690 solver.cpp:330] Iteration 24500, Testing net (#0)
I1013 13:45:44.392285 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:45:44.732102 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56831 (* 1 = 1.56831 loss)
I1013 13:45:44.732117 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6415
I1013 13:45:45.033812 12690 solver.cpp:218] Iteration 24500 (2.1003 iter/s, 47.6122s/100 iters), loss = 0.502861
I1013 13:45:45.033848 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.502862 (* 1 = 0.502862 loss)
I1013 13:45:45.033854 12690 sgd_solver.cpp:105] Iteration 24500, lr = 0.01
I1013 13:46:15.704244 12690 solver.cpp:218] Iteration 24600 (3.26047 iter/s, 30.6704s/100 iters), loss = 0.60475
I1013 13:46:15.704362 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.60475 (* 1 = 0.60475 loss)
I1013 13:46:15.704370 12690 sgd_solver.cpp:105] Iteration 24600, lr = 0.01
I1013 13:46:46.329047 12690 solver.cpp:218] Iteration 24700 (3.26534 iter/s, 30.6247s/100 iters), loss = 0.408067
I1013 13:46:46.329174 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.408068 (* 1 = 0.408068 loss)
I1013 13:46:46.329182 12690 sgd_solver.cpp:105] Iteration 24700, lr = 0.01
I1013 13:47:16.970160 12690 solver.cpp:218] Iteration 24800 (3.2636 iter/s, 30.641s/100 iters), loss = 0.250976
I1013 13:47:16.970283 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250976 (* 1 = 0.250976 loss)
I1013 13:47:16.970290 12690 sgd_solver.cpp:105] Iteration 24800, lr = 0.01
I1013 13:47:47.575590 12690 solver.cpp:218] Iteration 24900 (3.26741 iter/s, 30.6053s/100 iters), loss = 0.453664
I1013 13:47:47.575696 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.453665 (* 1 = 0.453665 loss)
I1013 13:47:47.575712 12690 sgd_solver.cpp:105] Iteration 24900, lr = 0.01
I1013 13:48:16.684160 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:48:17.899814 12690 solver.cpp:330] Iteration 25000, Testing net (#0)
I1013 13:48:34.548694 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:48:34.889387 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.54928 (* 1 = 1.54928 loss)
I1013 13:48:34.889401 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6449
I1013 13:48:35.191198 12690 solver.cpp:218] Iteration 25000 (2.10016 iter/s, 47.6155s/100 iters), loss = 0.21006
I1013 13:48:35.191226 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210061 (* 1 = 0.210061 loss)
I1013 13:48:35.191233 12690 sgd_solver.cpp:105] Iteration 25000, lr = 0.01
I1013 13:49:05.806478 12690 solver.cpp:218] Iteration 25100 (3.26635 iter/s, 30.6153s/100 iters), loss = 0.309762
I1013 13:49:05.806620 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309762 (* 1 = 0.309762 loss)
I1013 13:49:05.806639 12690 sgd_solver.cpp:105] Iteration 25100, lr = 0.01
I1013 13:49:36.390300 12690 solver.cpp:218] Iteration 25200 (3.26972 iter/s, 30.5837s/100 iters), loss = 0.298741
I1013 13:49:36.390441 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.298742 (* 1 = 0.298742 loss)
I1013 13:49:36.390450 12690 sgd_solver.cpp:105] Iteration 25200, lr = 0.01
I1013 13:50:06.981973 12690 solver.cpp:218] Iteration 25300 (3.26888 iter/s, 30.5915s/100 iters), loss = 0.392572
I1013 13:50:06.982118 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.392573 (* 1 = 0.392573 loss)
I1013 13:50:06.982128 12690 sgd_solver.cpp:105] Iteration 25300, lr = 0.01
I1013 13:50:37.556917 12690 solver.cpp:218] Iteration 25400 (3.27067 iter/s, 30.5748s/100 iters), loss = 0.270502
I1013 13:50:37.557024 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270503 (* 1 = 0.270503 loss)
I1013 13:50:37.557041 12690 sgd_solver.cpp:105] Iteration 25400, lr = 0.01
I1013 13:51:07.911495 12690 solver.cpp:330] Iteration 25500, Testing net (#0)
I1013 13:51:24.517928 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:51:24.858088 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.60721 (* 1 = 1.60721 loss)
I1013 13:51:24.858103 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6298
I1013 13:51:25.160653 12690 solver.cpp:218] Iteration 25500 (2.10068 iter/s, 47.6037s/100 iters), loss = 0.442087
I1013 13:51:25.160687 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.442088 (* 1 = 0.442088 loss)
I1013 13:51:25.160696 12690 sgd_solver.cpp:105] Iteration 25500, lr = 0.01
I1013 13:51:55.753955 12690 solver.cpp:218] Iteration 25600 (3.26869 iter/s, 30.5933s/100 iters), loss = 0.284663
I1013 13:51:55.754098 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284664 (* 1 = 0.284664 loss)
I1013 13:51:55.754107 12690 sgd_solver.cpp:105] Iteration 25600, lr = 0.01
I1013 13:52:26.424749 12690 solver.cpp:218] Iteration 25700 (3.26044 iter/s, 30.6707s/100 iters), loss = 0.210267
I1013 13:52:26.424888 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210268 (* 1 = 0.210268 loss)
I1013 13:52:26.424897 12690 sgd_solver.cpp:105] Iteration 25700, lr = 0.01
I1013 13:52:57.011867 12690 solver.cpp:218] Iteration 25800 (3.26936 iter/s, 30.587s/100 iters), loss = 0.210911
I1013 13:52:57.012050 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210911 (* 1 = 0.210911 loss)
I1013 13:52:57.012063 12690 sgd_solver.cpp:105] Iteration 25800, lr = 0.01
I1013 13:53:27.608369 12690 solver.cpp:218] Iteration 25900 (3.26836 iter/s, 30.5963s/100 iters), loss = 0.390592
I1013 13:53:27.608515 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390593 (* 1 = 0.390593 loss)
I1013 13:53:27.608523 12690 sgd_solver.cpp:105] Iteration 25900, lr = 0.01
I1013 13:53:56.706943 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:53:57.927431 12690 solver.cpp:330] Iteration 26000, Testing net (#0)
I1013 13:54:14.599246 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:54:14.937736 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56922 (* 1 = 1.56922 loss)
I1013 13:54:14.937752 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6452
I1013 13:54:15.240205 12690 solver.cpp:218] Iteration 26000 (2.09944 iter/s, 47.6317s/100 iters), loss = 0.312828
I1013 13:54:15.240234 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312828 (* 1 = 0.312828 loss)
I1013 13:54:15.240241 12690 sgd_solver.cpp:105] Iteration 26000, lr = 0.01
I1013 13:54:45.850152 12690 solver.cpp:218] Iteration 26100 (3.26691 iter/s, 30.6099s/100 iters), loss = 0.336574
I1013 13:54:45.850270 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.336574 (* 1 = 0.336574 loss)
I1013 13:54:45.850288 12690 sgd_solver.cpp:105] Iteration 26100, lr = 0.01
I1013 13:55:16.468580 12690 solver.cpp:218] Iteration 26200 (3.26602 iter/s, 30.6183s/100 iters), loss = 0.374936
I1013 13:55:16.468677 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.374937 (* 1 = 0.374937 loss)
I1013 13:55:16.468694 12690 sgd_solver.cpp:105] Iteration 26200, lr = 0.01
I1013 13:55:47.041535 12690 solver.cpp:218] Iteration 26300 (3.27087 iter/s, 30.5729s/100 iters), loss = 0.279741
I1013 13:55:47.041678 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279741 (* 1 = 0.279741 loss)
I1013 13:55:47.041688 12690 sgd_solver.cpp:105] Iteration 26300, lr = 0.01
I1013 13:56:17.666558 12690 solver.cpp:218] Iteration 26400 (3.26532 iter/s, 30.6249s/100 iters), loss = 0.358273
I1013 13:56:17.666663 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.358273 (* 1 = 0.358273 loss)
I1013 13:56:17.666679 12690 sgd_solver.cpp:105] Iteration 26400, lr = 0.01
I1013 13:56:47.993422 12690 solver.cpp:330] Iteration 26500, Testing net (#0)
I1013 13:57:04.632849 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:57:04.978698 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.6239 (* 1 = 1.6239 loss)
I1013 13:57:04.978714 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6311
I1013 13:57:05.282784 12690 solver.cpp:218] Iteration 26500 (2.10013 iter/s, 47.6161s/100 iters), loss = 0.493073
I1013 13:57:05.282812 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.493073 (* 1 = 0.493073 loss)
I1013 13:57:05.282819 12690 sgd_solver.cpp:105] Iteration 26500, lr = 0.01
I1013 13:57:35.938653 12690 solver.cpp:218] Iteration 26600 (3.26202 iter/s, 30.6558s/100 iters), loss = 0.285306
I1013 13:57:35.938781 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285306 (* 1 = 0.285306 loss)
I1013 13:57:35.938791 12690 sgd_solver.cpp:105] Iteration 26600, lr = 0.01
I1013 13:58:06.545254 12690 solver.cpp:218] Iteration 26700 (3.26728 iter/s, 30.6065s/100 iters), loss = 0.309959
I1013 13:58:06.545392 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309959 (* 1 = 0.309959 loss)
I1013 13:58:06.545400 12690 sgd_solver.cpp:105] Iteration 26700, lr = 0.01
I1013 13:58:37.126448 12690 solver.cpp:218] Iteration 26800 (3.27 iter/s, 30.5811s/100 iters), loss = 0.339638
I1013 13:58:37.126597 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339638 (* 1 = 0.339638 loss)
I1013 13:58:37.126610 12690 sgd_solver.cpp:105] Iteration 26800, lr = 0.01
I1013 13:59:07.736253 12690 solver.cpp:218] Iteration 26900 (3.26694 iter/s, 30.6097s/100 iters), loss = 0.363174
I1013 13:59:07.736374 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.363174 (* 1 = 0.363174 loss)
I1013 13:59:07.736384 12690 sgd_solver.cpp:105] Iteration 26900, lr = 0.01
I1013 13:59:36.811447 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:59:38.043684 12690 solver.cpp:330] Iteration 27000, Testing net (#0)
I1013 13:59:54.677363 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 13:59:55.016999 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56971 (* 1 = 1.56971 loss)
I1013 13:59:55.017015 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6428
I1013 13:59:55.316663 12690 solver.cpp:218] Iteration 27000 (2.10171 iter/s, 47.5803s/100 iters), loss = 0.279425
I1013 13:59:55.316692 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279425 (* 1 = 0.279425 loss)
I1013 13:59:55.316699 12690 sgd_solver.cpp:105] Iteration 27000, lr = 0.01
I1013 14:00:25.882707 12690 solver.cpp:218] Iteration 27100 (3.27161 iter/s, 30.566s/100 iters), loss = 0.288541
I1013 14:00:25.882820 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.288541 (* 1 = 0.288541 loss)
I1013 14:00:25.882838 12690 sgd_solver.cpp:105] Iteration 27100, lr = 0.01
I1013 14:00:56.464651 12690 solver.cpp:218] Iteration 27200 (3.26991 iter/s, 30.5819s/100 iters), loss = 0.233224
I1013 14:00:56.464762 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233224 (* 1 = 0.233224 loss)
I1013 14:00:56.464771 12690 sgd_solver.cpp:105] Iteration 27200, lr = 0.01
I1013 14:01:27.040184 12690 solver.cpp:218] Iteration 27300 (3.2706 iter/s, 30.5754s/100 iters), loss = 0.352793
I1013 14:01:27.040413 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.352793 (* 1 = 0.352793 loss)
I1013 14:01:27.040449 12690 sgd_solver.cpp:105] Iteration 27300, lr = 0.01
I1013 14:01:57.628602 12690 solver.cpp:218] Iteration 27400 (3.26923 iter/s, 30.5882s/100 iters), loss = 0.288422
I1013 14:01:57.628734 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.288422 (* 1 = 0.288422 loss)
I1013 14:01:57.628742 12690 sgd_solver.cpp:105] Iteration 27400, lr = 0.01
I1013 14:02:27.906888 12690 solver.cpp:330] Iteration 27500, Testing net (#0)
I1013 14:02:44.537176 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:02:44.874733 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.58999 (* 1 = 1.58999 loss)
I1013 14:02:44.874749 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6403
I1013 14:02:45.175518 12690 solver.cpp:218] Iteration 27500 (2.10319 iter/s, 47.5468s/100 iters), loss = 0.230034
I1013 14:02:45.175547 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.230034 (* 1 = 0.230034 loss)
I1013 14:02:45.175554 12690 sgd_solver.cpp:105] Iteration 27500, lr = 0.01
I1013 14:03:15.781779 12690 solver.cpp:218] Iteration 27600 (3.26731 iter/s, 30.6062s/100 iters), loss = 0.40023
I1013 14:03:15.781903 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.40023 (* 1 = 0.40023 loss)
I1013 14:03:15.781927 12690 sgd_solver.cpp:105] Iteration 27600, lr = 0.01
I1013 14:03:46.384053 12690 solver.cpp:218] Iteration 27700 (3.26774 iter/s, 30.6022s/100 iters), loss = 0.38975
I1013 14:03:46.384197 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.38975 (* 1 = 0.38975 loss)
I1013 14:03:46.384207 12690 sgd_solver.cpp:105] Iteration 27700, lr = 0.01
I1013 14:04:17.002468 12690 solver.cpp:218] Iteration 27800 (3.26602 iter/s, 30.6183s/100 iters), loss = 0.232967
I1013 14:04:17.002581 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232967 (* 1 = 0.232967 loss)
I1013 14:04:17.002590 12690 sgd_solver.cpp:105] Iteration 27800, lr = 0.01
I1013 14:04:47.642462 12690 solver.cpp:218] Iteration 27900 (3.26372 iter/s, 30.6399s/100 iters), loss = 0.193863
I1013 14:04:47.642593 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193863 (* 1 = 0.193863 loss)
I1013 14:04:47.642601 12690 sgd_solver.cpp:105] Iteration 27900, lr = 0.01
I1013 14:05:16.693680 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:05:17.915993 12690 solver.cpp:330] Iteration 28000, Testing net (#0)
I1013 14:05:34.508297 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:05:34.846760 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.54354 (* 1 = 1.54354 loss)
I1013 14:05:34.846776 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6512
I1013 14:05:35.146190 12690 solver.cpp:218] Iteration 28000 (2.1051 iter/s, 47.5036s/100 iters), loss = 0.156764
I1013 14:05:35.146219 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156764 (* 1 = 0.156764 loss)
I1013 14:05:35.146225 12690 sgd_solver.cpp:105] Iteration 28000, lr = 0.01
I1013 14:06:05.735764 12690 solver.cpp:218] Iteration 28100 (3.26909 iter/s, 30.5895s/100 iters), loss = 0.294274
I1013 14:06:05.735869 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.294274 (* 1 = 0.294274 loss)
I1013 14:06:05.735878 12690 sgd_solver.cpp:105] Iteration 28100, lr = 0.01
I1013 14:06:36.283102 12690 solver.cpp:218] Iteration 28200 (3.27362 iter/s, 30.5473s/100 iters), loss = 0.456201
I1013 14:06:36.283238 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.456201 (* 1 = 0.456201 loss)
I1013 14:06:36.283248 12690 sgd_solver.cpp:105] Iteration 28200, lr = 0.01
I1013 14:07:06.894290 12690 solver.cpp:218] Iteration 28300 (3.26679 iter/s, 30.6111s/100 iters), loss = 0.392771
I1013 14:07:06.894397 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.392771 (* 1 = 0.392771 loss)
I1013 14:07:06.894405 12690 sgd_solver.cpp:105] Iteration 28300, lr = 0.01
I1013 14:07:37.494918 12690 solver.cpp:218] Iteration 28400 (3.26792 iter/s, 30.6005s/100 iters), loss = 0.393562
I1013 14:07:37.495028 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.393562 (* 1 = 0.393562 loss)
I1013 14:07:37.495048 12690 sgd_solver.cpp:105] Iteration 28400, lr = 0.01
I1013 14:08:07.821449 12690 solver.cpp:330] Iteration 28500, Testing net (#0)
I1013 14:08:24.507674 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:08:24.849045 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.59167 (* 1 = 1.59167 loss)
I1013 14:08:24.849061 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6406
I1013 14:08:25.149492 12690 solver.cpp:218] Iteration 28500 (2.09844 iter/s, 47.6545s/100 iters), loss = 0.381783
I1013 14:08:25.149523 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.381783 (* 1 = 0.381783 loss)
I1013 14:08:25.149528 12690 sgd_solver.cpp:105] Iteration 28500, lr = 0.01
I1013 14:08:55.744909 12690 solver.cpp:218] Iteration 28600 (3.26847 iter/s, 30.5954s/100 iters), loss = 0.202972
I1013 14:08:55.745007 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202972 (* 1 = 0.202972 loss)
I1013 14:08:55.745014 12690 sgd_solver.cpp:105] Iteration 28600, lr = 0.01
I1013 14:09:26.341481 12690 solver.cpp:218] Iteration 28700 (3.26835 iter/s, 30.5965s/100 iters), loss = 0.34469
I1013 14:09:26.341589 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.34469 (* 1 = 0.34469 loss)
I1013 14:09:26.341598 12690 sgd_solver.cpp:105] Iteration 28700, lr = 0.01
I1013 14:09:56.925483 12690 solver.cpp:218] Iteration 28800 (3.26969 iter/s, 30.5839s/100 iters), loss = 0.125185
I1013 14:09:56.925626 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125185 (* 1 = 0.125185 loss)
I1013 14:09:56.925635 12690 sgd_solver.cpp:105] Iteration 28800, lr = 0.01
I1013 14:10:27.548032 12690 solver.cpp:218] Iteration 28900 (3.26558 iter/s, 30.6224s/100 iters), loss = 0.375969
I1013 14:10:27.548182 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.375969 (* 1 = 0.375969 loss)
I1013 14:10:27.548220 12690 sgd_solver.cpp:105] Iteration 28900, lr = 0.01
I1013 14:10:56.657002 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:10:57.883127 12690 solver.cpp:330] Iteration 29000, Testing net (#0)
I1013 14:11:14.526159 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:11:14.867974 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.57909 (* 1 = 1.57909 loss)
I1013 14:11:14.867990 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6465
I1013 14:11:15.169886 12690 solver.cpp:218] Iteration 29000 (2.09988 iter/s, 47.6217s/100 iters), loss = 0.261346
I1013 14:11:15.169921 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261346 (* 1 = 0.261346 loss)
I1013 14:11:15.169929 12690 sgd_solver.cpp:105] Iteration 29000, lr = 0.01
I1013 14:11:45.805963 12690 solver.cpp:218] Iteration 29100 (3.26413 iter/s, 30.6361s/100 iters), loss = 0.241024
I1013 14:11:45.806076 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241024 (* 1 = 0.241024 loss)
I1013 14:11:45.806082 12690 sgd_solver.cpp:105] Iteration 29100, lr = 0.01
I1013 14:12:16.463083 12690 solver.cpp:218] Iteration 29200 (3.26189 iter/s, 30.657s/100 iters), loss = 0.239071
I1013 14:12:16.463218 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239071 (* 1 = 0.239071 loss)
I1013 14:12:16.463227 12690 sgd_solver.cpp:105] Iteration 29200, lr = 0.01
I1013 14:12:47.125836 12690 solver.cpp:218] Iteration 29300 (3.2613 iter/s, 30.6626s/100 iters), loss = 0.333159
I1013 14:12:47.125941 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.33316 (* 1 = 0.33316 loss)
I1013 14:12:47.125950 12690 sgd_solver.cpp:105] Iteration 29300, lr = 0.01
I1013 14:13:17.704146 12690 solver.cpp:218] Iteration 29400 (3.2703 iter/s, 30.5782s/100 iters), loss = 0.244249
I1013 14:13:17.704243 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24425 (* 1 = 0.24425 loss)
I1013 14:13:17.704262 12690 sgd_solver.cpp:105] Iteration 29400, lr = 0.01
I1013 14:13:48.039038 12690 solver.cpp:330] Iteration 29500, Testing net (#0)
I1013 14:14:04.645982 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:14:04.984741 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.61684 (* 1 = 1.61684 loss)
I1013 14:14:04.984756 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6359
I1013 14:14:05.285979 12690 solver.cpp:218] Iteration 29500 (2.10165 iter/s, 47.5817s/100 iters), loss = 0.192492
I1013 14:14:05.286008 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192492 (* 1 = 0.192492 loss)
I1013 14:14:05.286015 12690 sgd_solver.cpp:105] Iteration 29500, lr = 0.01
I1013 14:14:35.991583 12690 solver.cpp:218] Iteration 29600 (3.25674 iter/s, 30.7056s/100 iters), loss = 0.170802
I1013 14:14:35.991700 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170802 (* 1 = 0.170802 loss)
I1013 14:14:35.991719 12690 sgd_solver.cpp:105] Iteration 29600, lr = 0.01
I1013 14:15:06.606443 12690 solver.cpp:218] Iteration 29700 (3.2664 iter/s, 30.6148s/100 iters), loss = 0.404956
I1013 14:15:06.606544 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.404957 (* 1 = 0.404957 loss)
I1013 14:15:06.606551 12690 sgd_solver.cpp:105] Iteration 29700, lr = 0.01
I1013 14:15:37.215740 12690 solver.cpp:218] Iteration 29800 (3.26699 iter/s, 30.6092s/100 iters), loss = 0.220296
I1013 14:15:37.215842 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220296 (* 1 = 0.220296 loss)
I1013 14:15:37.215857 12690 sgd_solver.cpp:105] Iteration 29800, lr = 0.01
I1013 14:16:07.816377 12690 solver.cpp:218] Iteration 29900 (3.26791 iter/s, 30.6006s/100 iters), loss = 0.429626
I1013 14:16:07.816517 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.429626 (* 1 = 0.429626 loss)
I1013 14:16:07.816527 12690 sgd_solver.cpp:105] Iteration 29900, lr = 0.01
I1013 14:16:36.917155 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:16:38.131554 12690 solver.cpp:330] Iteration 30000, Testing net (#0)
I1013 14:16:54.720129 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:16:55.068192 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.57497 (* 1 = 1.57497 loss)
I1013 14:16:55.068208 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6459
I1013 14:16:55.373616 12690 solver.cpp:218] Iteration 30000 (2.10273 iter/s, 47.5571s/100 iters), loss = 0.26429
I1013 14:16:55.373646 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26429 (* 1 = 0.26429 loss)
I1013 14:16:55.373652 12690 sgd_solver.cpp:105] Iteration 30000, lr = 0.01
I1013 14:17:26.041481 12690 solver.cpp:218] Iteration 30100 (3.26074 iter/s, 30.6678s/100 iters), loss = 0.290019
I1013 14:17:26.041597 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.290019 (* 1 = 0.290019 loss)
I1013 14:17:26.041617 12690 sgd_solver.cpp:105] Iteration 30100, lr = 0.01
I1013 14:17:56.637846 12690 solver.cpp:218] Iteration 30200 (3.26837 iter/s, 30.5963s/100 iters), loss = 0.302386
I1013 14:17:56.637965 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.302386 (* 1 = 0.302386 loss)
I1013 14:17:56.637984 12690 sgd_solver.cpp:105] Iteration 30200, lr = 0.01
I1013 14:18:27.254400 12690 solver.cpp:218] Iteration 30300 (3.26622 iter/s, 30.6164s/100 iters), loss = 0.218575
I1013 14:18:27.254505 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218576 (* 1 = 0.218576 loss)
I1013 14:18:27.254523 12690 sgd_solver.cpp:105] Iteration 30300, lr = 0.01
I1013 14:18:57.893810 12690 solver.cpp:218] Iteration 30400 (3.26378 iter/s, 30.6393s/100 iters), loss = 0.276568
I1013 14:18:57.893957 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276568 (* 1 = 0.276568 loss)
I1013 14:18:57.893976 12690 sgd_solver.cpp:105] Iteration 30400, lr = 0.01
I1013 14:19:28.236055 12690 solver.cpp:330] Iteration 30500, Testing net (#0)
I1013 14:19:44.850028 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:19:45.189232 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.57605 (* 1 = 1.57605 loss)
I1013 14:19:45.189245 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6448
I1013 14:19:45.488847 12690 solver.cpp:218] Iteration 30500 (2.10106 iter/s, 47.5949s/100 iters), loss = 0.444286
I1013 14:19:45.488876 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.444287 (* 1 = 0.444287 loss)
I1013 14:19:45.488883 12690 sgd_solver.cpp:105] Iteration 30500, lr = 0.01
I1013 14:20:16.122159 12690 solver.cpp:218] Iteration 30600 (3.26442 iter/s, 30.6333s/100 iters), loss = 0.457845
I1013 14:20:16.122304 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.457846 (* 1 = 0.457846 loss)
I1013 14:20:16.122313 12690 sgd_solver.cpp:105] Iteration 30600, lr = 0.01
I1013 14:20:46.732576 12690 solver.cpp:218] Iteration 30700 (3.26688 iter/s, 30.6103s/100 iters), loss = 0.203916
I1013 14:20:46.732692 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203916 (* 1 = 0.203916 loss)
I1013 14:20:46.732700 12690 sgd_solver.cpp:105] Iteration 30700, lr = 0.01
I1013 14:21:17.327405 12690 solver.cpp:218] Iteration 30800 (3.26854 iter/s, 30.5947s/100 iters), loss = 0.23304
I1013 14:21:17.327546 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23304 (* 1 = 0.23304 loss)
I1013 14:21:17.327555 12690 sgd_solver.cpp:105] Iteration 30800, lr = 0.01
I1013 14:21:47.939927 12690 solver.cpp:218] Iteration 30900 (3.26665 iter/s, 30.6124s/100 iters), loss = 0.436511
I1013 14:21:47.940058 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.436512 (* 1 = 0.436512 loss)
I1013 14:21:47.940068 12690 sgd_solver.cpp:105] Iteration 30900, lr = 0.01
I1013 14:22:17.071252 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:22:18.295701 12690 solver.cpp:330] Iteration 31000, Testing net (#0)
I1013 14:22:34.896244 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:22:35.238468 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56469 (* 1 = 1.56469 loss)
I1013 14:22:35.238483 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6493
I1013 14:22:35.540925 12690 solver.cpp:218] Iteration 31000 (2.1008 iter/s, 47.6009s/100 iters), loss = 0.261375
I1013 14:22:35.540954 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261375 (* 1 = 0.261375 loss)
I1013 14:22:35.540961 12690 sgd_solver.cpp:105] Iteration 31000, lr = 0.01
I1013 14:23:06.236083 12690 solver.cpp:218] Iteration 31100 (3.25784 iter/s, 30.6951s/100 iters), loss = 0.339099
I1013 14:23:06.236373 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.3391 (* 1 = 0.3391 loss)
I1013 14:23:06.236392 12690 sgd_solver.cpp:105] Iteration 31100, lr = 0.01
I1013 14:23:36.857838 12690 solver.cpp:218] Iteration 31200 (3.26568 iter/s, 30.6215s/100 iters), loss = 0.361274
I1013 14:23:36.857944 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.361275 (* 1 = 0.361275 loss)
I1013 14:23:36.857951 12690 sgd_solver.cpp:105] Iteration 31200, lr = 0.01
I1013 14:24:07.486800 12690 solver.cpp:218] Iteration 31300 (3.26489 iter/s, 30.6289s/100 iters), loss = 0.151341
I1013 14:24:07.486937 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151342 (* 1 = 0.151342 loss)
I1013 14:24:07.486955 12690 sgd_solver.cpp:105] Iteration 31300, lr = 0.01
I1013 14:24:38.137701 12690 solver.cpp:218] Iteration 31400 (3.26256 iter/s, 30.6508s/100 iters), loss = 0.376111
I1013 14:24:38.137828 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376112 (* 1 = 0.376112 loss)
I1013 14:24:38.137837 12690 sgd_solver.cpp:105] Iteration 31400, lr = 0.01
I1013 14:25:08.509363 12690 solver.cpp:330] Iteration 31500, Testing net (#0)
I1013 14:25:25.135710 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:25:25.473279 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.63217 (* 1 = 1.63217 loss)
I1013 14:25:25.473295 12690 solver.cpp:397]     Test net output #1: accuracy = 0.639
I1013 14:25:25.774183 12690 solver.cpp:218] Iteration 31500 (2.09924 iter/s, 47.6364s/100 iters), loss = 0.376252
I1013 14:25:25.774211 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376253 (* 1 = 0.376253 loss)
I1013 14:25:25.774219 12690 sgd_solver.cpp:105] Iteration 31500, lr = 0.01
I1013 14:25:56.395473 12690 solver.cpp:218] Iteration 31600 (3.2657 iter/s, 30.6213s/100 iters), loss = 0.331842
I1013 14:25:56.395567 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.331843 (* 1 = 0.331843 loss)
I1013 14:25:56.395575 12690 sgd_solver.cpp:105] Iteration 31600, lr = 0.01
I1013 14:26:27.012519 12690 solver.cpp:218] Iteration 31700 (3.26616 iter/s, 30.617s/100 iters), loss = 0.358332
I1013 14:26:27.012617 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.358333 (* 1 = 0.358333 loss)
I1013 14:26:27.012626 12690 sgd_solver.cpp:105] Iteration 31700, lr = 0.01
I1013 14:26:57.624459 12690 solver.cpp:218] Iteration 31800 (3.26671 iter/s, 30.6118s/100 iters), loss = 0.0995926
I1013 14:26:57.624563 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0995933 (* 1 = 0.0995933 loss)
I1013 14:26:57.624583 12690 sgd_solver.cpp:105] Iteration 31800, lr = 0.01
I1013 14:27:28.258147 12690 solver.cpp:218] Iteration 31900 (3.26439 iter/s, 30.6336s/100 iters), loss = 0.326143
I1013 14:27:28.258285 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.326144 (* 1 = 0.326144 loss)
I1013 14:27:28.258293 12690 sgd_solver.cpp:105] Iteration 31900, lr = 0.01
I1013 14:27:57.294404 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:27:58.518993 12690 solver.cpp:330] Iteration 32000, Testing net (#0)
I1013 14:28:15.164088 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:28:15.505306 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.6132 (* 1 = 1.6132 loss)
I1013 14:28:15.505321 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6472
I1013 14:28:15.807085 12690 solver.cpp:218] Iteration 32000 (2.1031 iter/s, 47.5488s/100 iters), loss = 0.0550064
I1013 14:28:15.807114 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0550071 (* 1 = 0.0550071 loss)
I1013 14:28:15.807121 12690 sgd_solver.cpp:105] Iteration 32000, lr = 0.01
I1013 14:28:46.435155 12690 solver.cpp:218] Iteration 32100 (3.26498 iter/s, 30.628s/100 iters), loss = 0.15248
I1013 14:28:46.435330 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15248 (* 1 = 0.15248 loss)
I1013 14:28:46.435340 12690 sgd_solver.cpp:105] Iteration 32100, lr = 0.01
I1013 14:29:17.117853 12690 solver.cpp:218] Iteration 32200 (3.25918 iter/s, 30.6825s/100 iters), loss = 0.272318
I1013 14:29:17.117944 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272318 (* 1 = 0.272318 loss)
I1013 14:29:17.117951 12690 sgd_solver.cpp:105] Iteration 32200, lr = 0.01
I1013 14:29:47.732317 12690 solver.cpp:218] Iteration 32300 (3.26644 iter/s, 30.6144s/100 iters), loss = 0.247574
I1013 14:29:47.732415 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247575 (* 1 = 0.247575 loss)
I1013 14:29:47.732422 12690 sgd_solver.cpp:105] Iteration 32300, lr = 0.01
I1013 14:30:18.363030 12690 solver.cpp:218] Iteration 32400 (3.26471 iter/s, 30.6306s/100 iters), loss = 0.18711
I1013 14:30:18.363119 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.187111 (* 1 = 0.187111 loss)
I1013 14:30:18.363127 12690 sgd_solver.cpp:105] Iteration 32400, lr = 0.01
I1013 14:30:48.699944 12690 solver.cpp:330] Iteration 32500, Testing net (#0)
I1013 14:31:05.309371 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:31:05.650450 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.59014 (* 1 = 1.59014 loss)
I1013 14:31:05.650466 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6453
I1013 14:31:05.953979 12690 solver.cpp:218] Iteration 32500 (2.10124 iter/s, 47.5909s/100 iters), loss = 0.380993
I1013 14:31:05.954012 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.380994 (* 1 = 0.380994 loss)
I1013 14:31:05.954020 12690 sgd_solver.cpp:105] Iteration 32500, lr = 0.01
I1013 14:31:36.541015 12690 solver.cpp:218] Iteration 32600 (3.26936 iter/s, 30.587s/100 iters), loss = 0.222721
I1013 14:31:36.541146 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222722 (* 1 = 0.222722 loss)
I1013 14:31:36.541154 12690 sgd_solver.cpp:105] Iteration 32600, lr = 0.01
I1013 14:32:07.151904 12690 solver.cpp:218] Iteration 32700 (3.26682 iter/s, 30.6108s/100 iters), loss = 0.312753
I1013 14:32:07.152009 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312754 (* 1 = 0.312754 loss)
I1013 14:32:07.152025 12690 sgd_solver.cpp:105] Iteration 32700, lr = 0.01
I1013 14:32:37.779929 12690 solver.cpp:218] Iteration 32800 (3.26499 iter/s, 30.6279s/100 iters), loss = 0.195196
I1013 14:32:37.780036 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195196 (* 1 = 0.195196 loss)
I1013 14:32:37.780045 12690 sgd_solver.cpp:105] Iteration 32800, lr = 0.01
I1013 14:33:08.385169 12690 solver.cpp:218] Iteration 32900 (3.26742 iter/s, 30.6051s/100 iters), loss = 0.219685
I1013 14:33:08.385308 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219686 (* 1 = 0.219686 loss)
I1013 14:33:08.385318 12690 sgd_solver.cpp:105] Iteration 32900, lr = 0.01
I1013 14:33:37.482336 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:33:38.718549 12690 solver.cpp:330] Iteration 33000, Testing net (#0)
I1013 14:33:55.331388 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:33:55.670940 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.59584 (* 1 = 1.59584 loss)
I1013 14:33:55.670956 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6529
I1013 14:33:55.973719 12690 solver.cpp:218] Iteration 33000 (2.10135 iter/s, 47.5884s/100 iters), loss = 0.096577
I1013 14:33:55.973753 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0965777 (* 1 = 0.0965777 loss)
I1013 14:33:55.973760 12690 sgd_solver.cpp:105] Iteration 33000, lr = 0.01
I1013 14:34:26.632484 12690 solver.cpp:218] Iteration 33100 (3.26171 iter/s, 30.6587s/100 iters), loss = 0.288013
I1013 14:34:26.632643 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.288014 (* 1 = 0.288014 loss)
I1013 14:34:26.632663 12690 sgd_solver.cpp:105] Iteration 33100, lr = 0.01
I1013 14:34:57.235993 12690 solver.cpp:218] Iteration 33200 (3.26761 iter/s, 30.6034s/100 iters), loss = 0.32427
I1013 14:34:57.236129 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.324271 (* 1 = 0.324271 loss)
I1013 14:34:57.236136 12690 sgd_solver.cpp:105] Iteration 33200, lr = 0.01
I1013 14:35:27.832147 12690 solver.cpp:218] Iteration 33300 (3.2684 iter/s, 30.596s/100 iters), loss = 0.268567
I1013 14:35:27.832242 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268568 (* 1 = 0.268568 loss)
I1013 14:35:27.832259 12690 sgd_solver.cpp:105] Iteration 33300, lr = 0.01
I1013 14:35:58.413442 12690 solver.cpp:218] Iteration 33400 (3.26998 iter/s, 30.5812s/100 iters), loss = 0.443522
I1013 14:35:58.413564 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.443523 (* 1 = 0.443523 loss)
I1013 14:35:58.413573 12690 sgd_solver.cpp:105] Iteration 33400, lr = 0.01
I1013 14:36:28.698642 12690 solver.cpp:330] Iteration 33500, Testing net (#0)
I1013 14:36:45.360630 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:36:45.700351 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.57267 (* 1 = 1.57267 loss)
I1013 14:36:45.700366 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6515
I1013 14:36:45.999430 12690 solver.cpp:218] Iteration 33500 (2.10146 iter/s, 47.5859s/100 iters), loss = 0.268107
I1013 14:36:45.999464 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268107 (* 1 = 0.268107 loss)
I1013 14:36:45.999469 12690 sgd_solver.cpp:105] Iteration 33500, lr = 0.01
I1013 14:37:16.587860 12690 solver.cpp:218] Iteration 33600 (3.26921 iter/s, 30.5884s/100 iters), loss = 0.253347
I1013 14:37:16.587994 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253348 (* 1 = 0.253348 loss)
I1013 14:37:16.588002 12690 sgd_solver.cpp:105] Iteration 33600, lr = 0.01
I1013 14:37:47.184885 12690 solver.cpp:218] Iteration 33700 (3.26831 iter/s, 30.5969s/100 iters), loss = 0.245094
I1013 14:37:47.185022 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245095 (* 1 = 0.245095 loss)
I1013 14:37:47.185031 12690 sgd_solver.cpp:105] Iteration 33700, lr = 0.01
I1013 14:38:17.812985 12690 solver.cpp:218] Iteration 33800 (3.26499 iter/s, 30.628s/100 iters), loss = 0.264058
I1013 14:38:17.813112 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264059 (* 1 = 0.264059 loss)
I1013 14:38:17.813120 12690 sgd_solver.cpp:105] Iteration 33800, lr = 0.01
I1013 14:38:48.702025 12690 solver.cpp:218] Iteration 33900 (3.23741 iter/s, 30.8889s/100 iters), loss = 0.317761
I1013 14:38:48.702142 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.317762 (* 1 = 0.317762 loss)
I1013 14:38:48.702160 12690 sgd_solver.cpp:105] Iteration 33900, lr = 0.01
I1013 14:39:17.761198 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:39:18.989365 12690 solver.cpp:330] Iteration 34000, Testing net (#0)
I1013 14:39:35.652386 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:39:35.994076 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.60259 (* 1 = 1.60259 loss)
I1013 14:39:35.994091 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6468
I1013 14:39:36.295516 12690 solver.cpp:218] Iteration 34000 (2.10113 iter/s, 47.5934s/100 iters), loss = 0.15393
I1013 14:39:36.295552 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.153931 (* 1 = 0.153931 loss)
I1013 14:39:36.295558 12690 sgd_solver.cpp:105] Iteration 34000, lr = 0.01
I1013 14:40:06.938961 12690 solver.cpp:218] Iteration 34100 (3.26334 iter/s, 30.6434s/100 iters), loss = 0.566541
I1013 14:40:06.939105 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.566541 (* 1 = 0.566541 loss)
I1013 14:40:06.939113 12690 sgd_solver.cpp:105] Iteration 34100, lr = 0.01
I1013 14:40:37.609886 12690 solver.cpp:218] Iteration 34200 (3.26043 iter/s, 30.6708s/100 iters), loss = 0.121966
I1013 14:40:37.610020 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121967 (* 1 = 0.121967 loss)
I1013 14:40:37.610028 12690 sgd_solver.cpp:105] Iteration 34200, lr = 0.01
I1013 14:41:08.248752 12690 solver.cpp:218] Iteration 34300 (3.26384 iter/s, 30.6388s/100 iters), loss = 0.181539
I1013 14:41:08.248898 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18154 (* 1 = 0.18154 loss)
I1013 14:41:08.248906 12690 sgd_solver.cpp:105] Iteration 34300, lr = 0.01
I1013 14:41:38.850703 12690 solver.cpp:218] Iteration 34400 (3.26778 iter/s, 30.6018s/100 iters), loss = 0.34013
I1013 14:41:38.850818 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340131 (* 1 = 0.340131 loss)
I1013 14:41:38.850827 12690 sgd_solver.cpp:105] Iteration 34400, lr = 0.01
I1013 14:42:09.181262 12690 solver.cpp:330] Iteration 34500, Testing net (#0)
I1013 14:42:25.807991 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:42:26.150218 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.62114 (* 1 = 1.62114 loss)
I1013 14:42:26.150234 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6429
I1013 14:42:26.453609 12690 solver.cpp:218] Iteration 34500 (2.10072 iter/s, 47.6028s/100 iters), loss = 0.31248
I1013 14:42:26.453639 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312481 (* 1 = 0.312481 loss)
I1013 14:42:26.453646 12690 sgd_solver.cpp:105] Iteration 34500, lr = 0.01
I1013 14:42:57.074489 12690 solver.cpp:218] Iteration 34600 (3.26575 iter/s, 30.6209s/100 iters), loss = 0.4124
I1013 14:42:57.074597 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.4124 (* 1 = 0.4124 loss)
I1013 14:42:57.074615 12690 sgd_solver.cpp:105] Iteration 34600, lr = 0.01
I1013 14:43:27.730849 12690 solver.cpp:218] Iteration 34700 (3.26198 iter/s, 30.6563s/100 iters), loss = 0.121675
I1013 14:43:27.730960 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121676 (* 1 = 0.121676 loss)
I1013 14:43:27.730968 12690 sgd_solver.cpp:105] Iteration 34700, lr = 0.01
I1013 14:43:58.353008 12690 solver.cpp:218] Iteration 34800 (3.26562 iter/s, 30.6221s/100 iters), loss = 0.127954
I1013 14:43:58.353116 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127954 (* 1 = 0.127954 loss)
I1013 14:43:58.353124 12690 sgd_solver.cpp:105] Iteration 34800, lr = 0.01
I1013 14:44:29.007431 12690 solver.cpp:218] Iteration 34900 (3.26218 iter/s, 30.6543s/100 iters), loss = 0.25674
I1013 14:44:29.007553 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25674 (* 1 = 0.25674 loss)
I1013 14:44:29.007562 12690 sgd_solver.cpp:105] Iteration 34900, lr = 0.01
I1013 14:44:58.123771 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:44:59.354216 12690 solver.cpp:330] Iteration 35000, Testing net (#0)
I1013 14:45:15.947604 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:45:16.287720 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.60153 (* 1 = 1.60153 loss)
I1013 14:45:16.287735 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6519
I1013 14:45:16.589574 12690 solver.cpp:218] Iteration 35000 (2.10163 iter/s, 47.582s/100 iters), loss = 0.0921905
I1013 14:45:16.589606 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0921913 (* 1 = 0.0921913 loss)
I1013 14:45:16.589612 12690 sgd_solver.cpp:105] Iteration 35000, lr = 0.01
I1013 14:45:47.255235 12690 solver.cpp:218] Iteration 35100 (3.26098 iter/s, 30.6656s/100 iters), loss = 0.369967
I1013 14:45:47.255379 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.369968 (* 1 = 0.369968 loss)
I1013 14:45:47.255388 12690 sgd_solver.cpp:105] Iteration 35100, lr = 0.01
I1013 14:46:17.908715 12690 solver.cpp:218] Iteration 35200 (3.26229 iter/s, 30.6533s/100 iters), loss = 0.271853
I1013 14:46:17.908849 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.271854 (* 1 = 0.271854 loss)
I1013 14:46:17.908857 12690 sgd_solver.cpp:105] Iteration 35200, lr = 0.01
I1013 14:46:48.541565 12690 solver.cpp:218] Iteration 35300 (3.26448 iter/s, 30.6327s/100 iters), loss = 0.207012
I1013 14:46:48.541689 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207013 (* 1 = 0.207013 loss)
I1013 14:46:48.541708 12690 sgd_solver.cpp:105] Iteration 35300, lr = 0.01
I1013 14:47:19.177582 12690 solver.cpp:218] Iteration 35400 (3.26414 iter/s, 30.6359s/100 iters), loss = 0.270894
I1013 14:47:19.177717 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270895 (* 1 = 0.270895 loss)
I1013 14:47:19.177726 12690 sgd_solver.cpp:105] Iteration 35400, lr = 0.01
I1013 14:47:49.521620 12690 solver.cpp:330] Iteration 35500, Testing net (#0)
I1013 14:48:06.138078 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:48:06.475610 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.61759 (* 1 = 1.61759 loss)
I1013 14:48:06.475625 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6436
I1013 14:48:06.775305 12690 solver.cpp:218] Iteration 35500 (2.10095 iter/s, 47.5976s/100 iters), loss = 0.26677
I1013 14:48:06.775347 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266771 (* 1 = 0.266771 loss)
I1013 14:48:06.775357 12690 sgd_solver.cpp:105] Iteration 35500, lr = 0.01
I1013 14:48:37.523825 12690 solver.cpp:218] Iteration 35600 (3.25219 iter/s, 30.7485s/100 iters), loss = 0.153886
I1013 14:48:37.523938 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.153887 (* 1 = 0.153887 loss)
I1013 14:48:37.523947 12690 sgd_solver.cpp:105] Iteration 35600, lr = 0.01
I1013 14:49:08.157281 12690 solver.cpp:218] Iteration 35700 (3.26442 iter/s, 30.6334s/100 iters), loss = 0.414547
I1013 14:49:08.157426 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.414548 (* 1 = 0.414548 loss)
I1013 14:49:08.157435 12690 sgd_solver.cpp:105] Iteration 35700, lr = 0.01
I1013 14:49:38.804903 12690 solver.cpp:218] Iteration 35800 (3.26291 iter/s, 30.6475s/100 iters), loss = 0.247161
I1013 14:49:38.805016 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247162 (* 1 = 0.247162 loss)
I1013 14:49:38.805035 12690 sgd_solver.cpp:105] Iteration 35800, lr = 0.01
I1013 14:50:09.441931 12690 solver.cpp:218] Iteration 35900 (3.26403 iter/s, 30.6369s/100 iters), loss = 0.337111
I1013 14:50:09.442067 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.337112 (* 1 = 0.337112 loss)
I1013 14:50:09.442075 12690 sgd_solver.cpp:105] Iteration 35900, lr = 0.01
I1013 14:50:38.542626 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:50:39.774943 12690 solver.cpp:330] Iteration 36000, Testing net (#0)
I1013 14:50:56.375946 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:50:56.722075 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.61633 (* 1 = 1.61633 loss)
I1013 14:50:56.722090 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6477
I1013 14:50:57.026707 12690 solver.cpp:218] Iteration 36000 (2.10152 iter/s, 47.5847s/100 iters), loss = 0.195448
I1013 14:50:57.026741 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195449 (* 1 = 0.195449 loss)
I1013 14:50:57.026748 12690 sgd_solver.cpp:105] Iteration 36000, lr = 0.01
I1013 14:51:27.707417 12690 solver.cpp:218] Iteration 36100 (3.25938 iter/s, 30.6807s/100 iters), loss = 0.38365
I1013 14:51:27.707512 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.383651 (* 1 = 0.383651 loss)
I1013 14:51:27.707521 12690 sgd_solver.cpp:105] Iteration 36100, lr = 0.01
I1013 14:51:58.327941 12690 solver.cpp:218] Iteration 36200 (3.26579 iter/s, 30.6204s/100 iters), loss = 0.323938
I1013 14:51:58.328050 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.323939 (* 1 = 0.323939 loss)
I1013 14:51:58.328060 12690 sgd_solver.cpp:105] Iteration 36200, lr = 0.01
I1013 14:52:29.019924 12690 solver.cpp:218] Iteration 36300 (3.25819 iter/s, 30.6919s/100 iters), loss = 0.273695
I1013 14:52:29.020090 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273696 (* 1 = 0.273696 loss)
I1013 14:52:29.020102 12690 sgd_solver.cpp:105] Iteration 36300, lr = 0.01
I1013 14:52:59.704113 12690 solver.cpp:218] Iteration 36400 (3.25902 iter/s, 30.684s/100 iters), loss = 0.0955262
I1013 14:52:59.704257 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.095527 (* 1 = 0.095527 loss)
I1013 14:52:59.704265 12690 sgd_solver.cpp:105] Iteration 36400, lr = 0.01
I1013 14:53:30.063338 12690 solver.cpp:330] Iteration 36500, Testing net (#0)
I1013 14:53:46.728667 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:53:47.072355 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.55576 (* 1 = 1.55576 loss)
I1013 14:53:47.072371 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6522
I1013 14:53:47.376952 12690 solver.cpp:218] Iteration 36500 (2.09764 iter/s, 47.6727s/100 iters), loss = 0.175176
I1013 14:53:47.376982 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175177 (* 1 = 0.175177 loss)
I1013 14:53:47.376989 12690 sgd_solver.cpp:105] Iteration 36500, lr = 0.01
I1013 14:54:18.063082 12690 solver.cpp:218] Iteration 36600 (3.2588 iter/s, 30.6861s/100 iters), loss = 0.262237
I1013 14:54:18.063210 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262238 (* 1 = 0.262238 loss)
I1013 14:54:18.063230 12690 sgd_solver.cpp:105] Iteration 36600, lr = 0.01
I1013 14:54:48.705546 12690 solver.cpp:218] Iteration 36700 (3.26346 iter/s, 30.6423s/100 iters), loss = 0.285324
I1013 14:54:48.705657 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285325 (* 1 = 0.285325 loss)
I1013 14:54:48.705674 12690 sgd_solver.cpp:105] Iteration 36700, lr = 0.01
I1013 14:55:19.361259 12690 solver.cpp:218] Iteration 36800 (3.26204 iter/s, 30.6556s/100 iters), loss = 0.279002
I1013 14:55:19.361398 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279002 (* 1 = 0.279002 loss)
I1013 14:55:19.361407 12690 sgd_solver.cpp:105] Iteration 36800, lr = 0.01
I1013 14:55:50.067976 12690 solver.cpp:218] Iteration 36900 (3.25663 iter/s, 30.7066s/100 iters), loss = 0.323434
I1013 14:55:50.071983 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.323435 (* 1 = 0.323435 loss)
I1013 14:55:50.072003 12690 sgd_solver.cpp:105] Iteration 36900, lr = 0.01
I1013 14:56:19.245290 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:56:20.470517 12690 solver.cpp:330] Iteration 37000, Testing net (#0)
I1013 14:56:37.133344 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:56:37.474015 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.55714 (* 1 = 1.55714 loss)
I1013 14:56:37.474031 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6555
I1013 14:56:37.777796 12690 solver.cpp:218] Iteration 37000 (2.09618 iter/s, 47.7058s/100 iters), loss = 0.103737
I1013 14:56:37.777825 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103738 (* 1 = 0.103738 loss)
I1013 14:56:37.777832 12690 sgd_solver.cpp:105] Iteration 37000, lr = 0.01
I1013 14:57:08.415369 12690 solver.cpp:218] Iteration 37100 (3.26397 iter/s, 30.6376s/100 iters), loss = 0.262245
I1013 14:57:08.415511 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262246 (* 1 = 0.262246 loss)
I1013 14:57:08.415520 12690 sgd_solver.cpp:105] Iteration 37100, lr = 0.01
I1013 14:57:39.139946 12690 solver.cpp:218] Iteration 37200 (3.25474 iter/s, 30.7244s/100 iters), loss = 0.222492
I1013 14:57:39.140064 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222493 (* 1 = 0.222493 loss)
I1013 14:57:39.140082 12690 sgd_solver.cpp:105] Iteration 37200, lr = 0.01
I1013 14:58:09.794005 12690 solver.cpp:218] Iteration 37300 (3.26222 iter/s, 30.654s/100 iters), loss = 0.214782
I1013 14:58:09.794117 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214783 (* 1 = 0.214783 loss)
I1013 14:58:09.794138 12690 sgd_solver.cpp:105] Iteration 37300, lr = 0.01
I1013 14:58:40.430452 12690 solver.cpp:218] Iteration 37400 (3.2641 iter/s, 30.6363s/100 iters), loss = 0.193934
I1013 14:58:40.430591 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193935 (* 1 = 0.193935 loss)
I1013 14:58:40.430611 12690 sgd_solver.cpp:105] Iteration 37400, lr = 0.01
I1013 14:59:10.796015 12690 solver.cpp:330] Iteration 37500, Testing net (#0)
I1013 14:59:27.457551 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 14:59:27.799218 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.57391 (* 1 = 1.57391 loss)
I1013 14:59:27.799234 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6523
I1013 14:59:28.101438 12690 solver.cpp:218] Iteration 37500 (2.09772 iter/s, 47.6709s/100 iters), loss = 0.158039
I1013 14:59:28.101469 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15804 (* 1 = 0.15804 loss)
I1013 14:59:28.101475 12690 sgd_solver.cpp:105] Iteration 37500, lr = 0.01
I1013 14:59:58.785779 12690 solver.cpp:218] Iteration 37600 (3.25899 iter/s, 30.6843s/100 iters), loss = 0.280712
I1013 14:59:58.785925 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.280712 (* 1 = 0.280712 loss)
I1013 14:59:58.785934 12690 sgd_solver.cpp:105] Iteration 37600, lr = 0.01
I1013 15:00:29.453135 12690 solver.cpp:218] Iteration 37700 (3.26081 iter/s, 30.6672s/100 iters), loss = 0.205267
I1013 15:00:29.453266 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205268 (* 1 = 0.205268 loss)
I1013 15:00:29.453274 12690 sgd_solver.cpp:105] Iteration 37700, lr = 0.01
I1013 15:01:00.322877 12690 solver.cpp:218] Iteration 37800 (3.23943 iter/s, 30.8696s/100 iters), loss = 0.265612
I1013 15:01:00.323019 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265613 (* 1 = 0.265613 loss)
I1013 15:01:00.323029 12690 sgd_solver.cpp:105] Iteration 37800, lr = 0.01
I1013 15:01:31.392544 12690 solver.cpp:218] Iteration 37900 (3.21859 iter/s, 31.0695s/100 iters), loss = 0.203516
I1013 15:01:31.392698 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203517 (* 1 = 0.203517 loss)
I1013 15:01:31.392719 12690 sgd_solver.cpp:105] Iteration 37900, lr = 0.01
I1013 15:02:00.721318 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:02:01.941190 12690 solver.cpp:330] Iteration 38000, Testing net (#0)
I1013 15:02:18.656399 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:02:19.012652 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.60332 (* 1 = 1.60332 loss)
I1013 15:02:19.012672 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6522
I1013 15:02:19.330060 12690 solver.cpp:218] Iteration 38000 (2.08605 iter/s, 47.9374s/100 iters), loss = 0.219236
I1013 15:02:19.330096 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219237 (* 1 = 0.219237 loss)
I1013 15:02:19.330103 12690 sgd_solver.cpp:105] Iteration 38000, lr = 0.01
I1013 15:02:50.111258 12690 solver.cpp:218] Iteration 38100 (3.24874 iter/s, 30.7812s/100 iters), loss = 0.101631
I1013 15:02:50.111357 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.101632 (* 1 = 0.101632 loss)
I1013 15:02:50.111376 12690 sgd_solver.cpp:105] Iteration 38100, lr = 0.01
I1013 15:03:21.122143 12690 solver.cpp:218] Iteration 38200 (3.22468 iter/s, 31.0108s/100 iters), loss = 0.208093
I1013 15:03:21.122280 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208094 (* 1 = 0.208094 loss)
I1013 15:03:21.122289 12690 sgd_solver.cpp:105] Iteration 38200, lr = 0.01
I1013 15:03:52.056648 12690 solver.cpp:218] Iteration 38300 (3.23265 iter/s, 30.9344s/100 iters), loss = 0.256893
I1013 15:03:52.056808 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256894 (* 1 = 0.256894 loss)
I1013 15:03:52.056828 12690 sgd_solver.cpp:105] Iteration 38300, lr = 0.01
I1013 15:04:23.034040 12690 solver.cpp:218] Iteration 38400 (3.22818 iter/s, 30.9772s/100 iters), loss = 0.241457
I1013 15:04:23.034209 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241458 (* 1 = 0.241458 loss)
I1013 15:04:23.034219 12690 sgd_solver.cpp:105] Iteration 38400, lr = 0.01
I1013 15:04:53.683151 12690 solver.cpp:330] Iteration 38500, Testing net (#0)
I1013 15:05:10.509703 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:05:10.855098 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.59058 (* 1 = 1.59058 loss)
I1013 15:05:10.855113 12690 solver.cpp:397]     Test net output #1: accuracy = 0.648
I1013 15:05:11.169186 12690 solver.cpp:218] Iteration 38500 (2.07749 iter/s, 48.135s/100 iters), loss = 0.226115
I1013 15:05:11.169320 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226116 (* 1 = 0.226116 loss)
I1013 15:05:11.169353 12690 sgd_solver.cpp:105] Iteration 38500, lr = 0.01
I1013 15:05:42.019654 12690 solver.cpp:218] Iteration 38600 (3.24145 iter/s, 30.8504s/100 iters), loss = 0.250475
I1013 15:05:42.019769 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250476 (* 1 = 0.250476 loss)
I1013 15:05:42.019788 12690 sgd_solver.cpp:105] Iteration 38600, lr = 0.01
I1013 15:06:12.985029 12690 solver.cpp:218] Iteration 38700 (3.22942 iter/s, 30.9653s/100 iters), loss = 0.168598
I1013 15:06:12.985174 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168599 (* 1 = 0.168599 loss)
I1013 15:06:12.985183 12690 sgd_solver.cpp:105] Iteration 38700, lr = 0.01
I1013 15:06:43.866502 12690 solver.cpp:218] Iteration 38800 (3.2382 iter/s, 30.8813s/100 iters), loss = 0.272702
I1013 15:06:43.866652 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272703 (* 1 = 0.272703 loss)
I1013 15:06:43.866662 12690 sgd_solver.cpp:105] Iteration 38800, lr = 0.01
I1013 15:07:15.027771 12690 solver.cpp:218] Iteration 38900 (3.20913 iter/s, 31.1611s/100 iters), loss = 0.167412
I1013 15:07:15.027855 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167412 (* 1 = 0.167412 loss)
I1013 15:07:15.027871 12690 sgd_solver.cpp:105] Iteration 38900, lr = 0.01
I1013 15:07:44.377473 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:07:45.607980 12690 solver.cpp:330] Iteration 39000, Testing net (#0)
I1013 15:08:02.350720 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:08:02.692659 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.55268 (* 1 = 1.55268 loss)
I1013 15:08:02.692675 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6591
I1013 15:08:02.994938 12690 solver.cpp:218] Iteration 39000 (2.08476 iter/s, 47.9671s/100 iters), loss = 0.493206
I1013 15:08:02.994971 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.493206 (* 1 = 0.493206 loss)
I1013 15:08:02.994977 12690 sgd_solver.cpp:105] Iteration 39000, lr = 0.01
I1013 15:08:34.133718 12690 solver.cpp:218] Iteration 39100 (3.21143 iter/s, 31.1388s/100 iters), loss = 0.256171
I1013 15:08:34.133880 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256172 (* 1 = 0.256172 loss)
I1013 15:08:34.133903 12690 sgd_solver.cpp:105] Iteration 39100, lr = 0.01
I1013 15:09:05.098974 12690 solver.cpp:218] Iteration 39200 (3.22944 iter/s, 30.9651s/100 iters), loss = 0.487213
I1013 15:09:05.099073 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.487214 (* 1 = 0.487214 loss)
I1013 15:09:05.099084 12690 sgd_solver.cpp:105] Iteration 39200, lr = 0.01
I1013 15:09:36.041095 12690 solver.cpp:218] Iteration 39300 (3.23185 iter/s, 30.942s/100 iters), loss = 0.263829
I1013 15:09:36.041239 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263829 (* 1 = 0.263829 loss)
I1013 15:09:36.041246 12690 sgd_solver.cpp:105] Iteration 39300, lr = 0.01
I1013 15:10:07.039980 12690 solver.cpp:218] Iteration 39400 (3.22594 iter/s, 30.9988s/100 iters), loss = 0.262084
I1013 15:10:07.040079 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262085 (* 1 = 0.262085 loss)
I1013 15:10:07.040098 12690 sgd_solver.cpp:105] Iteration 39400, lr = 0.01
I1013 15:10:37.716596 12690 solver.cpp:330] Iteration 39500, Testing net (#0)
I1013 15:10:54.524159 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:10:54.868626 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56865 (* 1 = 1.56865 loss)
I1013 15:10:54.868643 12690 solver.cpp:397]     Test net output #1: accuracy = 0.655
I1013 15:10:55.170392 12690 solver.cpp:218] Iteration 39500 (2.07769 iter/s, 48.1303s/100 iters), loss = 0.240056
I1013 15:10:55.170420 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240057 (* 1 = 0.240057 loss)
I1013 15:10:55.170426 12690 sgd_solver.cpp:105] Iteration 39500, lr = 0.01
I1013 15:11:26.101603 12690 solver.cpp:218] Iteration 39600 (3.23298 iter/s, 30.9312s/100 iters), loss = 0.252307
I1013 15:11:26.101712 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.252308 (* 1 = 0.252308 loss)
I1013 15:11:26.101730 12690 sgd_solver.cpp:105] Iteration 39600, lr = 0.01
I1013 15:11:56.977020 12690 solver.cpp:218] Iteration 39700 (3.23883 iter/s, 30.8753s/100 iters), loss = 0.401631
I1013 15:11:56.977129 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.401632 (* 1 = 0.401632 loss)
I1013 15:11:56.977147 12690 sgd_solver.cpp:105] Iteration 39700, lr = 0.01
I1013 15:12:27.883569 12690 solver.cpp:218] Iteration 39800 (3.23557 iter/s, 30.9065s/100 iters), loss = 0.219646
I1013 15:12:27.883672 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219647 (* 1 = 0.219647 loss)
I1013 15:12:27.883682 12690 sgd_solver.cpp:105] Iteration 39800, lr = 0.01
I1013 15:12:58.798133 12690 solver.cpp:218] Iteration 39900 (3.23473 iter/s, 30.9145s/100 iters), loss = 0.289641
I1013 15:12:58.800096 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289642 (* 1 = 0.289642 loss)
I1013 15:12:58.800113 12690 sgd_solver.cpp:105] Iteration 39900, lr = 0.01
I1013 15:13:28.197662 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:13:29.418409 12690 solver.cpp:330] Iteration 40000, Testing net (#0)
I1013 15:13:46.163151 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:13:46.504928 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.64122 (* 1 = 1.64122 loss)
I1013 15:13:46.504945 12690 solver.cpp:397]     Test net output #1: accuracy = 0.647
I1013 15:13:46.810439 12690 solver.cpp:218] Iteration 40000 (2.08288 iter/s, 48.0104s/100 iters), loss = 0.117494
I1013 15:13:46.810478 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117495 (* 1 = 0.117495 loss)
I1013 15:13:46.810485 12690 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I1013 15:14:17.730504 12690 solver.cpp:218] Iteration 40100 (3.23415 iter/s, 30.92s/100 iters), loss = 0.241398
I1013 15:14:17.730619 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241399 (* 1 = 0.241399 loss)
I1013 15:14:17.730628 12690 sgd_solver.cpp:105] Iteration 40100, lr = 0.01
I1013 15:14:48.662603 12690 solver.cpp:218] Iteration 40200 (3.2329 iter/s, 30.932s/100 iters), loss = 0.25783
I1013 15:14:48.663030 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257831 (* 1 = 0.257831 loss)
I1013 15:14:48.663048 12690 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I1013 15:15:19.610862 12690 solver.cpp:218] Iteration 40300 (3.23124 iter/s, 30.9478s/100 iters), loss = 0.271971
I1013 15:15:19.611004 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.271972 (* 1 = 0.271972 loss)
I1013 15:15:19.611014 12690 sgd_solver.cpp:105] Iteration 40300, lr = 0.01
I1013 15:15:50.556761 12690 solver.cpp:218] Iteration 40400 (3.23146 iter/s, 30.9458s/100 iters), loss = 0.268044
I1013 15:15:50.556859 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268045 (* 1 = 0.268045 loss)
I1013 15:15:50.556866 12690 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I1013 15:16:21.162099 12690 solver.cpp:330] Iteration 40500, Testing net (#0)
I1013 15:16:37.997984 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:16:38.341469 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.63958 (* 1 = 1.63958 loss)
I1013 15:16:38.341485 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6462
I1013 15:16:38.648645 12690 solver.cpp:218] Iteration 40500 (2.07936 iter/s, 48.0918s/100 iters), loss = 0.19935
I1013 15:16:38.648679 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199351 (* 1 = 0.199351 loss)
I1013 15:16:38.648685 12690 sgd_solver.cpp:105] Iteration 40500, lr = 0.01
I1013 15:17:09.587252 12690 solver.cpp:218] Iteration 40600 (3.23221 iter/s, 30.9386s/100 iters), loss = 0.281911
I1013 15:17:09.587422 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281912 (* 1 = 0.281912 loss)
I1013 15:17:09.587431 12690 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I1013 15:17:40.550523 12690 solver.cpp:218] Iteration 40700 (3.22965 iter/s, 30.9631s/100 iters), loss = 0.107452
I1013 15:17:40.550665 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107453 (* 1 = 0.107453 loss)
I1013 15:17:40.550684 12690 sgd_solver.cpp:105] Iteration 40700, lr = 0.01
I1013 15:18:11.568588 12690 solver.cpp:218] Iteration 40800 (3.22394 iter/s, 31.0179s/100 iters), loss = 0.0864616
I1013 15:18:11.568691 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0864626 (* 1 = 0.0864626 loss)
I1013 15:18:11.568701 12690 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I1013 15:18:42.611619 12690 solver.cpp:218] Iteration 40900 (3.22134 iter/s, 31.0429s/100 iters), loss = 0.269401
I1013 15:18:42.611771 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269402 (* 1 = 0.269402 loss)
I1013 15:18:42.611791 12690 sgd_solver.cpp:105] Iteration 40900, lr = 0.01
I1013 15:19:11.942723 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:19:13.181221 12690 solver.cpp:330] Iteration 41000, Testing net (#0)
I1013 15:19:29.959439 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:19:30.299541 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.60064 (* 1 = 1.60064 loss)
I1013 15:19:30.299556 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6556
I1013 15:19:30.603803 12690 solver.cpp:218] Iteration 41000 (2.08368 iter/s, 47.9921s/100 iters), loss = 0.255876
I1013 15:19:30.603834 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255877 (* 1 = 0.255877 loss)
I1013 15:19:30.603842 12690 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I1013 15:20:01.492452 12690 solver.cpp:218] Iteration 41100 (3.23744 iter/s, 30.8886s/100 iters), loss = 0.274329
I1013 15:20:01.492563 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27433 (* 1 = 0.27433 loss)
I1013 15:20:01.492571 12690 sgd_solver.cpp:105] Iteration 41100, lr = 0.01
I1013 15:20:32.279602 12690 solver.cpp:218] Iteration 41200 (3.24812 iter/s, 30.787s/100 iters), loss = 0.258633
I1013 15:20:32.279779 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258634 (* 1 = 0.258634 loss)
I1013 15:20:32.279824 12690 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I1013 15:21:03.044800 12690 solver.cpp:218] Iteration 41300 (3.25044 iter/s, 30.765s/100 iters), loss = 0.215756
I1013 15:21:03.044878 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215757 (* 1 = 0.215757 loss)
I1013 15:21:03.044894 12690 sgd_solver.cpp:105] Iteration 41300, lr = 0.01
I1013 15:21:33.770107 12690 solver.cpp:218] Iteration 41400 (3.25465 iter/s, 30.7252s/100 iters), loss = 0.105155
I1013 15:21:33.770241 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105156 (* 1 = 0.105156 loss)
I1013 15:21:33.770249 12690 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I1013 15:22:04.278023 12690 solver.cpp:330] Iteration 41500, Testing net (#0)
I1013 15:22:20.986258 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:22:21.329661 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56041 (* 1 = 1.56041 loss)
I1013 15:22:21.329679 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6569
I1013 15:22:21.635354 12690 solver.cpp:218] Iteration 41500 (2.0892 iter/s, 47.8651s/100 iters), loss = 0.238363
I1013 15:22:21.635390 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238364 (* 1 = 0.238364 loss)
I1013 15:22:21.635396 12690 sgd_solver.cpp:105] Iteration 41500, lr = 0.01
I1013 15:22:52.447830 12690 solver.cpp:218] Iteration 41600 (3.24544 iter/s, 30.8125s/100 iters), loss = 0.191998
I1013 15:22:52.447996 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191999 (* 1 = 0.191999 loss)
I1013 15:22:52.448005 12690 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I1013 15:23:23.219108 12690 solver.cpp:218] Iteration 41700 (3.2498 iter/s, 30.7711s/100 iters), loss = 0.160388
I1013 15:23:23.219225 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160389 (* 1 = 0.160389 loss)
I1013 15:23:23.219243 12690 sgd_solver.cpp:105] Iteration 41700, lr = 0.01
I1013 15:23:54.014317 12690 solver.cpp:218] Iteration 41800 (3.24727 iter/s, 30.7951s/100 iters), loss = 0.163449
I1013 15:23:54.014457 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.16345 (* 1 = 0.16345 loss)
I1013 15:23:54.014467 12690 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I1013 15:24:24.788553 12690 solver.cpp:218] Iteration 41900 (3.24949 iter/s, 30.7741s/100 iters), loss = 0.244385
I1013 15:24:24.788658 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244386 (* 1 = 0.244386 loss)
I1013 15:24:24.788667 12690 sgd_solver.cpp:105] Iteration 41900, lr = 0.01
I1013 15:24:54.063192 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:24:55.288769 12690 solver.cpp:330] Iteration 42000, Testing net (#0)
I1013 15:25:12.034072 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:25:12.375689 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.58235 (* 1 = 1.58235 loss)
I1013 15:25:12.375705 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6549
I1013 15:25:12.678891 12690 solver.cpp:218] Iteration 42000 (2.08811 iter/s, 47.8903s/100 iters), loss = 0.146394
I1013 15:25:12.678921 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.146395 (* 1 = 0.146395 loss)
I1013 15:25:12.678928 12690 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I1013 15:25:43.734259 12690 solver.cpp:218] Iteration 42100 (3.22006 iter/s, 31.0553s/100 iters), loss = 0.282545
I1013 15:25:43.734381 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.282545 (* 1 = 0.282545 loss)
I1013 15:25:43.734400 12690 sgd_solver.cpp:105] Iteration 42100, lr = 0.01
I1013 15:26:15.344566 12690 solver.cpp:218] Iteration 42200 (3.16354 iter/s, 31.6102s/100 iters), loss = 0.129069
I1013 15:26:15.344727 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.12907 (* 1 = 0.12907 loss)
I1013 15:26:15.344746 12690 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I1013 15:26:47.032918 12690 solver.cpp:218] Iteration 42300 (3.15575 iter/s, 31.6882s/100 iters), loss = 0.281749
I1013 15:26:47.033025 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28175 (* 1 = 0.28175 loss)
I1013 15:26:47.033035 12690 sgd_solver.cpp:105] Iteration 42300, lr = 0.01
I1013 15:27:18.739707 12690 solver.cpp:218] Iteration 42400 (3.15391 iter/s, 31.7067s/100 iters), loss = 0.230489
I1013 15:27:18.739817 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23049 (* 1 = 0.23049 loss)
I1013 15:27:18.739838 12690 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I1013 15:27:50.157307 12690 solver.cpp:330] Iteration 42500, Testing net (#0)
I1013 15:28:07.337942 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:28:07.686108 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.59813 (* 1 = 1.59813 loss)
I1013 15:28:07.686125 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6514
I1013 15:28:07.997105 12690 solver.cpp:218] Iteration 42500 (2.03016 iter/s, 49.2573s/100 iters), loss = 0.55258
I1013 15:28:07.997139 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.552581 (* 1 = 0.552581 loss)
I1013 15:28:07.997146 12690 sgd_solver.cpp:105] Iteration 42500, lr = 0.01
I1013 15:28:39.700152 12690 solver.cpp:218] Iteration 42600 (3.15427 iter/s, 31.703s/100 iters), loss = 0.254184
I1013 15:28:39.700332 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254185 (* 1 = 0.254185 loss)
I1013 15:28:39.700342 12690 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I1013 15:29:11.358150 12690 solver.cpp:218] Iteration 42700 (3.15878 iter/s, 31.6578s/100 iters), loss = 0.334582
I1013 15:29:11.358270 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334583 (* 1 = 0.334583 loss)
I1013 15:29:11.358289 12690 sgd_solver.cpp:105] Iteration 42700, lr = 0.01
I1013 15:29:43.051853 12690 solver.cpp:218] Iteration 42800 (3.15521 iter/s, 31.6936s/100 iters), loss = 0.171565
I1013 15:29:43.051951 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171566 (* 1 = 0.171566 loss)
I1013 15:29:43.051970 12690 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I1013 15:30:14.704869 12690 solver.cpp:218] Iteration 42900 (3.15927 iter/s, 31.6529s/100 iters), loss = 0.390361
I1013 15:30:14.704977 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390361 (* 1 = 0.390361 loss)
I1013 15:30:14.704985 12690 sgd_solver.cpp:105] Iteration 42900, lr = 0.01
I1013 15:30:44.835229 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:30:46.097499 12690 solver.cpp:330] Iteration 43000, Testing net (#0)
I1013 15:31:03.248884 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:31:03.602306 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.55334 (* 1 = 1.55334 loss)
I1013 15:31:03.602322 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6596
I1013 15:31:03.909354 12690 solver.cpp:218] Iteration 43000 (2.03234 iter/s, 49.2044s/100 iters), loss = 0.23233
I1013 15:31:03.909411 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23233 (* 1 = 0.23233 loss)
I1013 15:31:03.909420 12690 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I1013 15:31:35.626014 12690 solver.cpp:218] Iteration 43100 (3.15293 iter/s, 31.7165s/100 iters), loss = 0.0782226
I1013 15:31:35.626178 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0782233 (* 1 = 0.0782233 loss)
I1013 15:31:35.626197 12690 sgd_solver.cpp:105] Iteration 43100, lr = 0.01
I1013 15:32:07.303612 12690 solver.cpp:218] Iteration 43200 (3.15682 iter/s, 31.6774s/100 iters), loss = 0.231477
I1013 15:32:07.303725 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231477 (* 1 = 0.231477 loss)
I1013 15:32:07.303742 12690 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I1013 15:32:38.994784 12690 solver.cpp:218] Iteration 43300 (3.15546 iter/s, 31.6911s/100 iters), loss = 0.480209
I1013 15:32:38.994884 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.48021 (* 1 = 0.48021 loss)
I1013 15:32:38.994904 12690 sgd_solver.cpp:105] Iteration 43300, lr = 0.01
I1013 15:33:10.673063 12690 solver.cpp:218] Iteration 43400 (3.15675 iter/s, 31.6782s/100 iters), loss = 0.0867044
I1013 15:33:10.673164 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0867051 (* 1 = 0.0867051 loss)
I1013 15:33:10.673172 12690 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I1013 15:33:42.056424 12690 solver.cpp:330] Iteration 43500, Testing net (#0)
I1013 15:33:59.241580 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:33:59.592293 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.55335 (* 1 = 1.55335 loss)
I1013 15:33:59.592311 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6557
I1013 15:33:59.906426 12690 solver.cpp:218] Iteration 43500 (2.03115 iter/s, 49.2333s/100 iters), loss = 0.213035
I1013 15:33:59.906466 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213035 (* 1 = 0.213035 loss)
I1013 15:33:59.906473 12690 sgd_solver.cpp:105] Iteration 43500, lr = 0.01
I1013 15:34:31.597743 12690 solver.cpp:218] Iteration 43600 (3.15544 iter/s, 31.6913s/100 iters), loss = 0.249583
I1013 15:34:31.597884 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249584 (* 1 = 0.249584 loss)
I1013 15:34:31.597903 12690 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I1013 15:35:03.310467 12690 solver.cpp:218] Iteration 43700 (3.15332 iter/s, 31.7126s/100 iters), loss = 0.227338
I1013 15:35:03.310616 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227339 (* 1 = 0.227339 loss)
I1013 15:35:03.310626 12690 sgd_solver.cpp:105] Iteration 43700, lr = 0.01
I1013 15:35:35.015183 12690 solver.cpp:218] Iteration 43800 (3.15412 iter/s, 31.7046s/100 iters), loss = 0.138954
I1013 15:35:35.015324 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138955 (* 1 = 0.138955 loss)
I1013 15:35:35.015344 12690 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I1013 15:36:06.687644 12690 solver.cpp:218] Iteration 43900 (3.15733 iter/s, 31.6723s/100 iters), loss = 0.296609
I1013 15:36:06.687790 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296609 (* 1 = 0.296609 loss)
I1013 15:36:06.687800 12690 sgd_solver.cpp:105] Iteration 43900, lr = 0.01
I1013 15:36:36.784067 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:36:38.059797 12690 solver.cpp:330] Iteration 44000, Testing net (#0)
I1013 15:36:55.281997 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:36:55.638020 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.56547 (* 1 = 1.56547 loss)
I1013 15:36:55.638056 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6578
I1013 15:36:55.951735 12690 solver.cpp:218] Iteration 44000 (2.02988 iter/s, 49.264s/100 iters), loss = 0.12347
I1013 15:36:55.951771 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.123471 (* 1 = 0.123471 loss)
I1013 15:36:55.951777 12690 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I1013 15:37:27.632692 12690 solver.cpp:218] Iteration 44100 (3.15647 iter/s, 31.6809s/100 iters), loss = 0.140965
I1013 15:37:27.632807 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140966 (* 1 = 0.140966 loss)
I1013 15:37:27.632824 12690 sgd_solver.cpp:105] Iteration 44100, lr = 0.01
I1013 15:37:59.309093 12690 solver.cpp:218] Iteration 44200 (3.15693 iter/s, 31.6763s/100 iters), loss = 0.145538
I1013 15:37:59.309238 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145539 (* 1 = 0.145539 loss)
I1013 15:37:59.309248 12690 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I1013 15:38:31.005592 12690 solver.cpp:218] Iteration 44300 (3.15493 iter/s, 31.6964s/100 iters), loss = 0.238006
I1013 15:38:31.005709 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238007 (* 1 = 0.238007 loss)
I1013 15:38:31.005728 12690 sgd_solver.cpp:105] Iteration 44300, lr = 0.01
I1013 15:39:02.694890 12690 solver.cpp:218] Iteration 44400 (3.15567 iter/s, 31.689s/100 iters), loss = 0.11067
I1013 15:39:02.694999 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110671 (* 1 = 0.110671 loss)
I1013 15:39:02.695019 12690 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I1013 15:39:34.079883 12690 solver.cpp:330] Iteration 44500, Testing net (#0)
I1013 15:39:51.262562 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:39:51.615365 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.59653 (* 1 = 1.59653 loss)
I1013 15:39:51.615381 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6513
I1013 15:39:51.922370 12690 solver.cpp:218] Iteration 44500 (2.03139 iter/s, 49.2274s/100 iters), loss = 0.302991
I1013 15:39:51.922438 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.302992 (* 1 = 0.302992 loss)
I1013 15:39:51.922456 12690 sgd_solver.cpp:105] Iteration 44500, lr = 0.01
I1013 15:40:23.641345 12690 solver.cpp:218] Iteration 44600 (3.15269 iter/s, 31.7189s/100 iters), loss = 0.220093
I1013 15:40:23.641427 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220093 (* 1 = 0.220093 loss)
I1013 15:40:23.641435 12690 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I1013 15:40:55.337577 12690 solver.cpp:218] Iteration 44700 (3.15496 iter/s, 31.6962s/100 iters), loss = 0.258862
I1013 15:40:55.337699 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258863 (* 1 = 0.258863 loss)
I1013 15:40:55.337708 12690 sgd_solver.cpp:105] Iteration 44700, lr = 0.01
I1013 15:41:27.088178 12690 solver.cpp:218] Iteration 44800 (3.14956 iter/s, 31.7505s/100 iters), loss = 0.10673
I1013 15:41:27.088284 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106731 (* 1 = 0.106731 loss)
I1013 15:41:27.088302 12690 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I1013 15:41:58.817034 12690 solver.cpp:218] Iteration 44900 (3.15171 iter/s, 31.7288s/100 iters), loss = 0.157137
I1013 15:41:58.817167 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.157137 (* 1 = 0.157137 loss)
I1013 15:41:58.817175 12690 sgd_solver.cpp:105] Iteration 44900, lr = 0.01
I1013 15:42:29.003085 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:42:30.272608 12690 solver.cpp:330] Iteration 45000, Testing net (#0)
I1013 15:42:47.439962 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:42:47.786845 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.60351 (* 1 = 1.60351 loss)
I1013 15:42:47.786864 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6573
I1013 15:42:48.099427 12690 solver.cpp:218] Iteration 45000 (2.02913 iter/s, 49.2823s/100 iters), loss = 0.215247
I1013 15:42:48.099462 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215248 (* 1 = 0.215248 loss)
I1013 15:42:48.099478 12690 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I1013 15:43:19.882067 12690 solver.cpp:218] Iteration 45100 (3.14637 iter/s, 31.7826s/100 iters), loss = 0.42189
I1013 15:43:19.882179 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.421891 (* 1 = 0.421891 loss)
I1013 15:43:19.882194 12690 sgd_solver.cpp:105] Iteration 45100, lr = 0.01
I1013 15:43:51.597757 12690 solver.cpp:218] Iteration 45200 (3.15302 iter/s, 31.7156s/100 iters), loss = 0.338516
I1013 15:43:51.597856 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.338516 (* 1 = 0.338516 loss)
I1013 15:43:51.597865 12690 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I1013 15:44:23.339290 12690 solver.cpp:218] Iteration 45300 (3.15045 iter/s, 31.7414s/100 iters), loss = 0.159973
I1013 15:44:23.339432 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159973 (* 1 = 0.159973 loss)
I1013 15:44:23.339442 12690 sgd_solver.cpp:105] Iteration 45300, lr = 0.01
I1013 15:44:55.068925 12690 solver.cpp:218] Iteration 45400 (3.15164 iter/s, 31.7295s/100 iters), loss = 0.258826
I1013 15:44:55.069046 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258827 (* 1 = 0.258827 loss)
I1013 15:44:55.069056 12690 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I1013 15:45:26.475445 12690 solver.cpp:330] Iteration 45500, Testing net (#0)
I1013 15:45:43.729894 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:45:44.080476 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.63556 (* 1 = 1.63556 loss)
I1013 15:45:44.080492 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6457
I1013 15:45:44.387491 12690 solver.cpp:218] Iteration 45500 (2.02764 iter/s, 49.3185s/100 iters), loss = 0.207764
I1013 15:45:44.387524 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207764 (* 1 = 0.207764 loss)
I1013 15:45:44.387542 12690 sgd_solver.cpp:105] Iteration 45500, lr = 0.01
I1013 15:46:16.092087 12690 solver.cpp:218] Iteration 45600 (3.15412 iter/s, 31.7046s/100 iters), loss = 0.326866
I1013 15:46:16.092200 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.326867 (* 1 = 0.326867 loss)
I1013 15:46:16.092219 12690 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I1013 15:46:47.854135 12690 solver.cpp:218] Iteration 45700 (3.14842 iter/s, 31.7619s/100 iters), loss = 0.148941
I1013 15:46:47.854290 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148941 (* 1 = 0.148941 loss)
I1013 15:46:47.854310 12690 sgd_solver.cpp:105] Iteration 45700, lr = 0.01
I1013 15:47:19.584616 12690 solver.cpp:218] Iteration 45800 (3.15156 iter/s, 31.7303s/100 iters), loss = 0.21145
I1013 15:47:19.584750 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.211451 (* 1 = 0.211451 loss)
I1013 15:47:19.584769 12690 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I1013 15:47:51.332648 12690 solver.cpp:218] Iteration 45900 (3.14981 iter/s, 31.7479s/100 iters), loss = 0.193313
I1013 15:47:51.332751 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193313 (* 1 = 0.193313 loss)
I1013 15:47:51.332769 12690 sgd_solver.cpp:105] Iteration 45900, lr = 0.01
I1013 15:48:21.430443 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:48:22.709095 12690 solver.cpp:330] Iteration 46000, Testing net (#0)
I1013 15:48:39.911872 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:48:40.265364 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.55961 (* 1 = 1.55961 loss)
I1013 15:48:40.265380 12690 solver.cpp:397]     Test net output #1: accuracy = 0.662
I1013 15:48:40.578303 12690 solver.cpp:218] Iteration 46000 (2.03064 iter/s, 49.2456s/100 iters), loss = 0.0965645
I1013 15:48:40.578341 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0965653 (* 1 = 0.0965653 loss)
I1013 15:48:40.578348 12690 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I1013 15:49:12.328289 12690 solver.cpp:218] Iteration 46100 (3.14963 iter/s, 31.7498s/100 iters), loss = 0.324202
I1013 15:49:12.328399 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.324203 (* 1 = 0.324203 loss)
I1013 15:49:12.328408 12690 sgd_solver.cpp:105] Iteration 46100, lr = 0.01
I1013 15:49:44.049350 12690 solver.cpp:218] Iteration 46200 (3.15249 iter/s, 31.721s/100 iters), loss = 0.207384
I1013 15:49:44.049449 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207384 (* 1 = 0.207384 loss)
I1013 15:49:44.049468 12690 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I1013 15:50:15.759138 12690 solver.cpp:218] Iteration 46300 (3.15361 iter/s, 31.7097s/100 iters), loss = 0.138799
I1013 15:50:15.759275 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1388 (* 1 = 0.1388 loss)
I1013 15:50:15.759284 12690 sgd_solver.cpp:105] Iteration 46300, lr = 0.01
I1013 15:50:47.463529 12690 solver.cpp:218] Iteration 46400 (3.15415 iter/s, 31.7043s/100 iters), loss = 0.263516
I1013 15:50:47.463668 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263517 (* 1 = 0.263517 loss)
I1013 15:50:47.463688 12690 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I1013 15:51:18.894296 12690 solver.cpp:330] Iteration 46500, Testing net (#0)
I1013 15:51:36.087251 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:51:36.438419 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.53516 (* 1 = 1.53516 loss)
I1013 15:51:36.438438 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6642
I1013 15:51:36.751509 12690 solver.cpp:218] Iteration 46500 (2.0289 iter/s, 49.2879s/100 iters), loss = 0.280251
I1013 15:51:36.751543 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.280252 (* 1 = 0.280252 loss)
I1013 15:51:36.751549 12690 sgd_solver.cpp:105] Iteration 46500, lr = 0.01
I1013 15:52:08.507172 12690 solver.cpp:218] Iteration 46600 (3.14905 iter/s, 31.7556s/100 iters), loss = 0.231323
I1013 15:52:08.507272 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231324 (* 1 = 0.231324 loss)
I1013 15:52:08.507292 12690 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I1013 15:52:40.205569 12690 solver.cpp:218] Iteration 46700 (3.15474 iter/s, 31.6983s/100 iters), loss = 0.174759
I1013 15:52:40.205679 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17476 (* 1 = 0.17476 loss)
I1013 15:52:40.205687 12690 sgd_solver.cpp:105] Iteration 46700, lr = 0.01
I1013 15:53:11.871871 12690 solver.cpp:218] Iteration 46800 (3.15794 iter/s, 31.6662s/100 iters), loss = 0.163479
I1013 15:53:11.871968 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.16348 (* 1 = 0.16348 loss)
I1013 15:53:11.871987 12690 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I1013 15:53:43.541728 12690 solver.cpp:218] Iteration 46900 (3.15758 iter/s, 31.6698s/100 iters), loss = 0.490879
I1013 15:53:43.541852 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.49088 (* 1 = 0.49088 loss)
I1013 15:53:43.541860 12690 sgd_solver.cpp:105] Iteration 46900, lr = 0.01
I1013 15:54:13.700546 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:54:14.957481 12690 solver.cpp:330] Iteration 47000, Testing net (#0)
I1013 15:54:32.191864 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:54:32.543745 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.63782 (* 1 = 1.63782 loss)
I1013 15:54:32.543761 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6515
I1013 15:54:32.855283 12690 solver.cpp:218] Iteration 47000 (2.02784 iter/s, 49.3134s/100 iters), loss = 0.156418
I1013 15:54:32.855320 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156419 (* 1 = 0.156419 loss)
I1013 15:54:32.855327 12690 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I1013 15:55:04.537721 12690 solver.cpp:218] Iteration 47100 (3.15633 iter/s, 31.6824s/100 iters), loss = 0.168106
I1013 15:55:04.537839 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168107 (* 1 = 0.168107 loss)
I1013 15:55:04.537847 12690 sgd_solver.cpp:105] Iteration 47100, lr = 0.01
I1013 15:55:36.202050 12690 solver.cpp:218] Iteration 47200 (3.15814 iter/s, 31.6642s/100 iters), loss = 0.284531
I1013 15:55:36.202165 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284532 (* 1 = 0.284532 loss)
I1013 15:55:36.202173 12690 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I1013 15:56:07.851727 12690 solver.cpp:218] Iteration 47300 (3.1596 iter/s, 31.6496s/100 iters), loss = 0.164804
I1013 15:56:07.851848 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164805 (* 1 = 0.164805 loss)
I1013 15:56:07.851857 12690 sgd_solver.cpp:105] Iteration 47300, lr = 0.01
I1013 15:56:39.424681 12690 solver.cpp:218] Iteration 47400 (3.16728 iter/s, 31.5728s/100 iters), loss = 0.139159
I1013 15:56:39.424790 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.13916 (* 1 = 0.13916 loss)
I1013 15:56:39.424798 12690 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I1013 15:57:10.836555 12690 solver.cpp:330] Iteration 47500, Testing net (#0)
I1013 15:57:28.045619 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 15:57:28.395042 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.54294 (* 1 = 1.54294 loss)
I1013 15:57:28.395061 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6649
I1013 15:57:28.705206 12690 solver.cpp:218] Iteration 47500 (2.0292 iter/s, 49.2804s/100 iters), loss = 0.249642
I1013 15:57:28.705246 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249643 (* 1 = 0.249643 loss)
I1013 15:57:28.705255 12690 sgd_solver.cpp:105] Iteration 47500, lr = 0.01
I1013 15:58:00.266324 12690 solver.cpp:218] Iteration 47600 (3.16846 iter/s, 31.5611s/100 iters), loss = 0.171749
I1013 15:58:00.266419 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17175 (* 1 = 0.17175 loss)
I1013 15:58:00.266427 12690 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I1013 15:58:32.012069 12690 solver.cpp:218] Iteration 47700 (3.15004 iter/s, 31.7457s/100 iters), loss = 0.183596
I1013 15:58:32.012182 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183597 (* 1 = 0.183597 loss)
I1013 15:58:32.012192 12690 sgd_solver.cpp:105] Iteration 47700, lr = 0.01
I1013 15:59:03.628495 12690 solver.cpp:218] Iteration 47800 (3.16292 iter/s, 31.6163s/100 iters), loss = 0.114748
I1013 15:59:03.628576 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.114749 (* 1 = 0.114749 loss)
I1013 15:59:03.628593 12690 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I1013 15:59:35.251242 12690 solver.cpp:218] Iteration 47900 (3.16229 iter/s, 31.6227s/100 iters), loss = 0.164005
I1013 15:59:35.251387 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164006 (* 1 = 0.164006 loss)
I1013 15:59:35.251396 12690 sgd_solver.cpp:105] Iteration 47900, lr = 0.01
I1013 16:00:05.318675 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:00:06.578856 12690 solver.cpp:330] Iteration 48000, Testing net (#0)
I1013 16:00:23.758803 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:00:24.114082 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.54535 (* 1 = 1.54535 loss)
I1013 16:00:24.114100 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6649
I1013 16:00:24.426473 12690 solver.cpp:218] Iteration 48000 (2.03355 iter/s, 49.1751s/100 iters), loss = 0.234901
I1013 16:00:24.426517 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234902 (* 1 = 0.234902 loss)
I1013 16:00:24.426524 12690 sgd_solver.cpp:105] Iteration 48000, lr = 0.01
I1013 16:00:56.114922 12690 solver.cpp:218] Iteration 48100 (3.15573 iter/s, 31.6884s/100 iters), loss = 0.110295
I1013 16:00:56.115032 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110296 (* 1 = 0.110296 loss)
I1013 16:00:56.115041 12690 sgd_solver.cpp:105] Iteration 48100, lr = 0.01
I1013 16:01:27.774024 12690 solver.cpp:218] Iteration 48200 (3.15866 iter/s, 31.659s/100 iters), loss = 0.29514
I1013 16:01:27.774132 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.295141 (* 1 = 0.295141 loss)
I1013 16:01:27.774139 12690 sgd_solver.cpp:105] Iteration 48200, lr = 0.01
I1013 16:01:59.507622 12690 solver.cpp:218] Iteration 48300 (3.15124 iter/s, 31.7335s/100 iters), loss = 0.227496
I1013 16:01:59.507769 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227497 (* 1 = 0.227497 loss)
I1013 16:01:59.507779 12690 sgd_solver.cpp:105] Iteration 48300, lr = 0.01
I1013 16:02:31.154690 12690 solver.cpp:218] Iteration 48400 (3.15986 iter/s, 31.6469s/100 iters), loss = 0.170461
I1013 16:02:31.154786 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170462 (* 1 = 0.170462 loss)
I1013 16:02:31.154795 12690 sgd_solver.cpp:105] Iteration 48400, lr = 0.01
I1013 16:03:02.524478 12690 solver.cpp:330] Iteration 48500, Testing net (#0)
I1013 16:03:19.770493 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:03:20.124475 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.61736 (* 1 = 1.61736 loss)
I1013 16:03:20.124491 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6539
I1013 16:03:20.435194 12690 solver.cpp:218] Iteration 48500 (2.0292 iter/s, 49.2804s/100 iters), loss = 0.224826
I1013 16:03:20.435227 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224827 (* 1 = 0.224827 loss)
I1013 16:03:20.435235 12690 sgd_solver.cpp:105] Iteration 48500, lr = 0.01
I1013 16:03:52.078411 12690 solver.cpp:218] Iteration 48600 (3.16024 iter/s, 31.6432s/100 iters), loss = 0.376457
I1013 16:03:52.078534 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376458 (* 1 = 0.376458 loss)
I1013 16:03:52.078543 12690 sgd_solver.cpp:105] Iteration 48600, lr = 0.01
I1013 16:04:23.747287 12690 solver.cpp:218] Iteration 48700 (3.15769 iter/s, 31.6688s/100 iters), loss = 0.220071
I1013 16:04:23.747370 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220072 (* 1 = 0.220072 loss)
I1013 16:04:23.747390 12690 sgd_solver.cpp:105] Iteration 48700, lr = 0.01
I1013 16:04:55.435719 12690 solver.cpp:218] Iteration 48800 (3.15574 iter/s, 31.6883s/100 iters), loss = 0.113516
I1013 16:04:55.435827 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113517 (* 1 = 0.113517 loss)
I1013 16:04:55.435837 12690 sgd_solver.cpp:105] Iteration 48800, lr = 0.01
I1013 16:05:27.093775 12690 solver.cpp:218] Iteration 48900 (3.15876 iter/s, 31.658s/100 iters), loss = 0.195727
I1013 16:05:27.093888 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195728 (* 1 = 0.195728 loss)
I1013 16:05:27.093896 12690 sgd_solver.cpp:105] Iteration 48900, lr = 0.01
I1013 16:05:57.193084 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:05:58.456476 12690 solver.cpp:330] Iteration 49000, Testing net (#0)
I1013 16:06:15.677500 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:06:16.027807 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.53027 (* 1 = 1.53027 loss)
I1013 16:06:16.027823 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6644
I1013 16:06:16.338609 12690 solver.cpp:218] Iteration 49000 (2.03067 iter/s, 49.2448s/100 iters), loss = 0.138095
I1013 16:06:16.338641 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138096 (* 1 = 0.138096 loss)
I1013 16:06:16.338649 12690 sgd_solver.cpp:105] Iteration 49000, lr = 0.01
I1013 16:06:48.039991 12690 solver.cpp:218] Iteration 49100 (3.15444 iter/s, 31.7014s/100 iters), loss = 0.371425
I1013 16:06:48.040144 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.371426 (* 1 = 0.371426 loss)
I1013 16:06:48.040154 12690 sgd_solver.cpp:105] Iteration 49100, lr = 0.01
I1013 16:07:19.703964 12690 solver.cpp:218] Iteration 49200 (3.15818 iter/s, 31.6638s/100 iters), loss = 0.241066
I1013 16:07:19.704058 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241067 (* 1 = 0.241067 loss)
I1013 16:07:19.704068 12690 sgd_solver.cpp:105] Iteration 49200, lr = 0.01
I1013 16:07:51.380986 12690 solver.cpp:218] Iteration 49300 (3.15687 iter/s, 31.6769s/100 iters), loss = 0.205945
I1013 16:07:51.381078 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205946 (* 1 = 0.205946 loss)
I1013 16:07:51.381094 12690 sgd_solver.cpp:105] Iteration 49300, lr = 0.01
I1013 16:08:23.015761 12690 solver.cpp:218] Iteration 49400 (3.16109 iter/s, 31.6347s/100 iters), loss = 0.14421
I1013 16:08:23.015864 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.144211 (* 1 = 0.144211 loss)
I1013 16:08:23.015872 12690 sgd_solver.cpp:105] Iteration 49400, lr = 0.01
I1013 16:08:54.399463 12690 solver.cpp:330] Iteration 49500, Testing net (#0)
I1013 16:09:11.537365 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:09:11.890486 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.62659 (* 1 = 1.62659 loss)
I1013 16:09:11.890504 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6507
I1013 16:09:12.200346 12690 solver.cpp:218] Iteration 49500 (2.03316 iter/s, 49.1845s/100 iters), loss = 0.149864
I1013 16:09:12.200378 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149865 (* 1 = 0.149865 loss)
I1013 16:09:12.200386 12690 sgd_solver.cpp:105] Iteration 49500, lr = 0.01
I1013 16:09:43.881014 12690 solver.cpp:218] Iteration 49600 (3.1565 iter/s, 31.6806s/100 iters), loss = 0.196744
I1013 16:09:43.881155 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196745 (* 1 = 0.196745 loss)
I1013 16:09:43.881175 12690 sgd_solver.cpp:105] Iteration 49600, lr = 0.01
I1013 16:10:15.555971 12690 solver.cpp:218] Iteration 49700 (3.15708 iter/s, 31.6748s/100 iters), loss = 0.183168
I1013 16:10:15.556058 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183169 (* 1 = 0.183169 loss)
I1013 16:10:15.556067 12690 sgd_solver.cpp:105] Iteration 49700, lr = 0.01
I1013 16:10:47.288890 12690 solver.cpp:218] Iteration 49800 (3.15131 iter/s, 31.7328s/100 iters), loss = 0.231592
I1013 16:10:47.289039 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231593 (* 1 = 0.231593 loss)
I1013 16:10:47.289059 12690 sgd_solver.cpp:105] Iteration 49800, lr = 0.01
I1013 16:11:18.979961 12690 solver.cpp:218] Iteration 49900 (3.15548 iter/s, 31.6909s/100 iters), loss = 0.43409
I1013 16:11:18.980079 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.434091 (* 1 = 0.434091 loss)
I1013 16:11:18.980098 12690 sgd_solver.cpp:105] Iteration 49900, lr = 0.01
I1013 16:11:49.110222 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:11:50.375916 12690 solver.cpp:330] Iteration 50000, Testing net (#0)
I1013 16:12:07.603477 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:12:07.954958 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.57936 (* 1 = 1.57936 loss)
I1013 16:12:07.954974 12690 solver.cpp:397]     Test net output #1: accuracy = 0.6601
I1013 16:12:08.265298 12690 solver.cpp:218] Iteration 50000 (2.029 iter/s, 49.2853s/100 iters), loss = 0.334542
I1013 16:12:08.265333 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334543 (* 1 = 0.334543 loss)
I1013 16:12:08.265349 12690 sgd_solver.cpp:46] MultiStep Status: Iteration 50000, step = 1
I1013 16:12:08.265353 12690 sgd_solver.cpp:105] Iteration 50000, lr = 0.001
I1013 16:12:39.942363 12690 solver.cpp:218] Iteration 50100 (3.15686 iter/s, 31.677s/100 iters), loss = 0.123352
I1013 16:12:39.942477 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.123353 (* 1 = 0.123353 loss)
I1013 16:12:39.942487 12690 sgd_solver.cpp:105] Iteration 50100, lr = 0.001
I1013 16:13:11.661602 12690 solver.cpp:218] Iteration 50200 (3.15267 iter/s, 31.7191s/100 iters), loss = 0.216125
I1013 16:13:11.661685 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216126 (* 1 = 0.216126 loss)
I1013 16:13:11.661695 12690 sgd_solver.cpp:105] Iteration 50200, lr = 0.001
I1013 16:13:43.380403 12690 solver.cpp:218] Iteration 50300 (3.15271 iter/s, 31.7187s/100 iters), loss = 0.118086
I1013 16:13:43.380547 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118087 (* 1 = 0.118087 loss)
I1013 16:13:43.380555 12690 sgd_solver.cpp:105] Iteration 50300, lr = 0.001
I1013 16:14:15.081349 12690 solver.cpp:218] Iteration 50400 (3.15449 iter/s, 31.7008s/100 iters), loss = 0.134099
I1013 16:14:15.081465 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1341 (* 1 = 0.1341 loss)
I1013 16:14:15.081472 12690 sgd_solver.cpp:105] Iteration 50400, lr = 0.001
I1013 16:14:46.477039 12690 solver.cpp:330] Iteration 50500, Testing net (#0)
I1013 16:15:03.626377 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:15:03.977900 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.23658 (* 1 = 1.23658 loss)
I1013 16:15:03.977929 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7159
I1013 16:15:04.290753 12690 solver.cpp:218] Iteration 50500 (2.03214 iter/s, 49.2093s/100 iters), loss = 0.0892981
I1013 16:15:04.290789 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.089299 (* 1 = 0.089299 loss)
I1013 16:15:04.290796 12690 sgd_solver.cpp:105] Iteration 50500, lr = 0.001
I1013 16:15:36.037669 12690 solver.cpp:218] Iteration 50600 (3.14991 iter/s, 31.7469s/100 iters), loss = 0.0321445
I1013 16:15:36.037773 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0321454 (* 1 = 0.0321454 loss)
I1013 16:15:36.037781 12690 sgd_solver.cpp:105] Iteration 50600, lr = 0.001
I1013 16:16:07.735541 12690 solver.cpp:218] Iteration 50700 (3.1548 iter/s, 31.6978s/100 iters), loss = 0.105227
I1013 16:16:07.735692 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105228 (* 1 = 0.105228 loss)
I1013 16:16:07.735702 12690 sgd_solver.cpp:105] Iteration 50700, lr = 0.001
I1013 16:16:39.438951 12690 solver.cpp:218] Iteration 50800 (3.15425 iter/s, 31.7033s/100 iters), loss = 0.0185555
I1013 16:16:39.439069 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0185564 (* 1 = 0.0185564 loss)
I1013 16:16:39.439077 12690 sgd_solver.cpp:105] Iteration 50800, lr = 0.001
I1013 16:17:11.174150 12690 solver.cpp:218] Iteration 50900 (3.15109 iter/s, 31.7351s/100 iters), loss = 0.0644773
I1013 16:17:11.174309 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0644782 (* 1 = 0.0644782 loss)
I1013 16:17:11.174336 12690 sgd_solver.cpp:105] Iteration 50900, lr = 0.001
I1013 16:17:41.297017 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:17:42.557621 12690 solver.cpp:330] Iteration 51000, Testing net (#0)
I1013 16:17:59.807869 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:18:00.161123 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.20341 (* 1 = 1.20341 loss)
I1013 16:18:00.161139 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7208
I1013 16:18:00.473253 12690 solver.cpp:218] Iteration 51000 (2.02844 iter/s, 49.299s/100 iters), loss = 0.0503983
I1013 16:18:00.473301 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0503992 (* 1 = 0.0503992 loss)
I1013 16:18:00.473309 12690 sgd_solver.cpp:105] Iteration 51000, lr = 0.001
I1013 16:18:32.140280 12690 solver.cpp:218] Iteration 51100 (3.15787 iter/s, 31.6669s/100 iters), loss = 0.0291102
I1013 16:18:32.140432 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0291111 (* 1 = 0.0291111 loss)
I1013 16:18:32.140442 12690 sgd_solver.cpp:105] Iteration 51100, lr = 0.001
I1013 16:19:03.837834 12690 solver.cpp:218] Iteration 51200 (3.15483 iter/s, 31.6974s/100 iters), loss = 0.0795733
I1013 16:19:03.837950 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0795742 (* 1 = 0.0795742 loss)
I1013 16:19:03.837963 12690 sgd_solver.cpp:105] Iteration 51200, lr = 0.001
I1013 16:19:35.582020 12690 solver.cpp:218] Iteration 51300 (3.15019 iter/s, 31.7441s/100 iters), loss = 0.115286
I1013 16:19:35.582120 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.115286 (* 1 = 0.115286 loss)
I1013 16:19:35.582139 12690 sgd_solver.cpp:105] Iteration 51300, lr = 0.001
I1013 16:20:07.269789 12690 solver.cpp:218] Iteration 51400 (3.1558 iter/s, 31.6877s/100 iters), loss = 0.0402176
I1013 16:20:07.269927 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0402185 (* 1 = 0.0402185 loss)
I1013 16:20:07.269939 12690 sgd_solver.cpp:105] Iteration 51400, lr = 0.001
I1013 16:20:38.694005 12690 solver.cpp:330] Iteration 51500, Testing net (#0)
I1013 16:20:55.876978 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:20:56.226815 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.18164 (* 1 = 1.18164 loss)
I1013 16:20:56.226831 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7306
I1013 16:20:56.536276 12690 solver.cpp:218] Iteration 51500 (2.02978 iter/s, 49.2664s/100 iters), loss = 0.106235
I1013 16:20:56.536316 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106236 (* 1 = 0.106236 loss)
I1013 16:20:56.536324 12690 sgd_solver.cpp:105] Iteration 51500, lr = 0.001
I1013 16:21:28.288084 12690 solver.cpp:218] Iteration 51600 (3.14943 iter/s, 31.7518s/100 iters), loss = 0.00848647
I1013 16:21:28.288228 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00848744 (* 1 = 0.00848744 loss)
I1013 16:21:28.288236 12690 sgd_solver.cpp:105] Iteration 51600, lr = 0.001
I1013 16:21:59.987392 12690 solver.cpp:218] Iteration 51700 (3.15466 iter/s, 31.6992s/100 iters), loss = 0.0276615
I1013 16:21:59.987509 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0276625 (* 1 = 0.0276625 loss)
I1013 16:21:59.987526 12690 sgd_solver.cpp:105] Iteration 51700, lr = 0.001
I1013 16:22:31.742913 12690 solver.cpp:218] Iteration 51800 (3.14907 iter/s, 31.7554s/100 iters), loss = 0.030848
I1013 16:22:31.743044 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0308489 (* 1 = 0.0308489 loss)
I1013 16:22:31.743065 12690 sgd_solver.cpp:105] Iteration 51800, lr = 0.001
I1013 16:23:03.474303 12690 solver.cpp:218] Iteration 51900 (3.15146 iter/s, 31.7313s/100 iters), loss = 0.0162851
I1013 16:23:03.474449 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0162861 (* 1 = 0.0162861 loss)
I1013 16:23:03.474458 12690 sgd_solver.cpp:105] Iteration 51900, lr = 0.001
I1013 16:23:33.602448 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:23:34.863574 12690 solver.cpp:330] Iteration 52000, Testing net (#0)
I1013 16:23:52.107005 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:23:52.463831 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.17909 (* 1 = 1.17909 loss)
I1013 16:23:52.463848 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7306
I1013 16:23:52.776618 12690 solver.cpp:218] Iteration 52000 (2.02831 iter/s, 49.3022s/100 iters), loss = 0.0643191
I1013 16:23:52.776657 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.06432 (* 1 = 0.06432 loss)
I1013 16:23:52.776664 12690 sgd_solver.cpp:105] Iteration 52000, lr = 0.001
I1013 16:24:24.513914 12690 solver.cpp:218] Iteration 52100 (3.15087 iter/s, 31.7373s/100 iters), loss = 0.0264955
I1013 16:24:24.514060 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0264965 (* 1 = 0.0264965 loss)
I1013 16:24:24.514078 12690 sgd_solver.cpp:105] Iteration 52100, lr = 0.001
I1013 16:24:56.252605 12690 solver.cpp:218] Iteration 52200 (3.15075 iter/s, 31.7385s/100 iters), loss = 0.0250436
I1013 16:24:56.252701 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250445 (* 1 = 0.0250445 loss)
I1013 16:24:56.252719 12690 sgd_solver.cpp:105] Iteration 52200, lr = 0.001
I1013 16:25:27.958464 12690 solver.cpp:218] Iteration 52300 (3.154 iter/s, 31.7058s/100 iters), loss = 0.0694577
I1013 16:25:27.958616 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0694586 (* 1 = 0.0694586 loss)
I1013 16:25:27.958626 12690 sgd_solver.cpp:105] Iteration 52300, lr = 0.001
I1013 16:25:59.711999 12690 solver.cpp:218] Iteration 52400 (3.14927 iter/s, 31.7534s/100 iters), loss = 0.0260973
I1013 16:25:59.712144 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0260983 (* 1 = 0.0260983 loss)
I1013 16:25:59.712154 12690 sgd_solver.cpp:105] Iteration 52400, lr = 0.001
I1013 16:26:30.632076 12690 solver.cpp:330] Iteration 52500, Testing net (#0)
I1013 16:26:47.543265 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:26:47.886281 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.17399 (* 1 = 1.17399 loss)
I1013 16:26:47.886297 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7326
I1013 16:26:48.190049 12690 solver.cpp:218] Iteration 52500 (2.06279 iter/s, 48.4779s/100 iters), loss = 0.0154025
I1013 16:26:48.190081 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0154035 (* 1 = 0.0154035 loss)
I1013 16:26:48.190088 12690 sgd_solver.cpp:105] Iteration 52500, lr = 0.001
I1013 16:27:19.184413 12690 solver.cpp:218] Iteration 52600 (3.22639 iter/s, 30.9943s/100 iters), loss = 0.00590743
I1013 16:27:19.184512 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0059084 (* 1 = 0.0059084 loss)
I1013 16:27:19.184520 12690 sgd_solver.cpp:105] Iteration 52600, lr = 0.001
I1013 16:27:49.995772 12690 solver.cpp:218] Iteration 52700 (3.24557 iter/s, 30.8113s/100 iters), loss = 0.0303621
I1013 16:27:49.995921 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0303631 (* 1 = 0.0303631 loss)
I1013 16:27:49.995930 12690 sgd_solver.cpp:105] Iteration 52700, lr = 0.001
I1013 16:28:20.823189 12690 solver.cpp:218] Iteration 52800 (3.24388 iter/s, 30.8273s/100 iters), loss = 0.0114885
I1013 16:28:20.823303 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114895 (* 1 = 0.0114895 loss)
I1013 16:28:20.823312 12690 sgd_solver.cpp:105] Iteration 52800, lr = 0.001
I1013 16:28:51.623970 12690 solver.cpp:218] Iteration 52900 (3.24668 iter/s, 30.8007s/100 iters), loss = 0.0126444
I1013 16:28:51.624112 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126453 (* 1 = 0.0126453 loss)
I1013 16:28:51.624131 12690 sgd_solver.cpp:105] Iteration 52900, lr = 0.001
I1013 16:29:20.909009 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:29:22.137634 12690 solver.cpp:330] Iteration 53000, Testing net (#0)
I1013 16:29:38.858664 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:29:39.200388 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.17424 (* 1 = 1.17424 loss)
I1013 16:29:39.200403 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7336
I1013 16:29:39.504194 12690 solver.cpp:218] Iteration 53000 (2.08855 iter/s, 47.8801s/100 iters), loss = 0.0176522
I1013 16:29:39.504225 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0176532 (* 1 = 0.0176532 loss)
I1013 16:29:39.504231 12690 sgd_solver.cpp:105] Iteration 53000, lr = 0.001
I1013 16:30:10.320087 12690 solver.cpp:218] Iteration 53100 (3.24508 iter/s, 30.8159s/100 iters), loss = 0.0209101
I1013 16:30:10.320216 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0209111 (* 1 = 0.0209111 loss)
I1013 16:30:10.320224 12690 sgd_solver.cpp:105] Iteration 53100, lr = 0.001
I1013 16:30:41.125311 12690 solver.cpp:218] Iteration 53200 (3.24621 iter/s, 30.8051s/100 iters), loss = 0.0200555
I1013 16:30:41.125422 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0200564 (* 1 = 0.0200564 loss)
I1013 16:30:41.125430 12690 sgd_solver.cpp:105] Iteration 53200, lr = 0.001
I1013 16:31:11.900488 12690 solver.cpp:218] Iteration 53300 (3.24938 iter/s, 30.7751s/100 iters), loss = 0.0446299
I1013 16:31:11.900600 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446308 (* 1 = 0.0446308 loss)
I1013 16:31:11.900609 12690 sgd_solver.cpp:105] Iteration 53300, lr = 0.001
I1013 16:31:42.688622 12690 solver.cpp:218] Iteration 53400 (3.24801 iter/s, 30.788s/100 iters), loss = 0.0314451
I1013 16:31:42.688736 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.031446 (* 1 = 0.031446 loss)
I1013 16:31:42.688745 12690 sgd_solver.cpp:105] Iteration 53400, lr = 0.001
I1013 16:32:13.188205 12690 solver.cpp:330] Iteration 53500, Testing net (#0)
I1013 16:32:29.893034 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:32:30.234894 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.17193 (* 1 = 1.17193 loss)
I1013 16:32:30.234910 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7371
I1013 16:32:30.537927 12690 solver.cpp:218] Iteration 53500 (2.0899 iter/s, 47.8492s/100 iters), loss = 0.0177193
I1013 16:32:30.537957 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0177202 (* 1 = 0.0177202 loss)
I1013 16:32:30.537963 12690 sgd_solver.cpp:105] Iteration 53500, lr = 0.001
I1013 16:33:01.282074 12690 solver.cpp:218] Iteration 53600 (3.25265 iter/s, 30.7441s/100 iters), loss = 0.00471184
I1013 16:33:01.282186 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00471279 (* 1 = 0.00471279 loss)
I1013 16:33:01.282192 12690 sgd_solver.cpp:105] Iteration 53600, lr = 0.001
I1013 16:33:32.070789 12690 solver.cpp:218] Iteration 53700 (3.24795 iter/s, 30.7886s/100 iters), loss = 0.0168618
I1013 16:33:32.070931 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168627 (* 1 = 0.0168627 loss)
I1013 16:33:32.070940 12690 sgd_solver.cpp:105] Iteration 53700, lr = 0.001
I1013 16:34:02.833428 12690 solver.cpp:218] Iteration 53800 (3.25071 iter/s, 30.7625s/100 iters), loss = 0.00473629
I1013 16:34:02.833555 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00473724 (* 1 = 0.00473724 loss)
I1013 16:34:02.833564 12690 sgd_solver.cpp:105] Iteration 53800, lr = 0.001
I1013 16:34:33.655166 12690 solver.cpp:218] Iteration 53900 (3.24448 iter/s, 30.8216s/100 iters), loss = 0.0101381
I1013 16:34:33.655310 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.010139 (* 1 = 0.010139 loss)
I1013 16:34:33.655319 12690 sgd_solver.cpp:105] Iteration 53900, lr = 0.001
I1013 16:35:02.909394 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:35:04.134568 12690 solver.cpp:330] Iteration 54000, Testing net (#0)
I1013 16:35:20.784407 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:35:21.131731 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.16869 (* 1 = 1.16869 loss)
I1013 16:35:21.131747 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7355
I1013 16:35:21.438362 12690 solver.cpp:218] Iteration 54000 (2.09279 iter/s, 47.7831s/100 iters), loss = 0.0145919
I1013 16:35:21.438393 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0145928 (* 1 = 0.0145928 loss)
I1013 16:35:21.438400 12690 sgd_solver.cpp:105] Iteration 54000, lr = 0.001
I1013 16:35:52.193413 12690 solver.cpp:218] Iteration 54100 (3.2515 iter/s, 30.755s/100 iters), loss = 0.0125121
I1013 16:35:52.193543 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125131 (* 1 = 0.0125131 loss)
I1013 16:35:52.193552 12690 sgd_solver.cpp:105] Iteration 54100, lr = 0.001
I1013 16:36:22.932371 12690 solver.cpp:218] Iteration 54200 (3.25321 iter/s, 30.7388s/100 iters), loss = 0.0231371
I1013 16:36:22.932516 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.023138 (* 1 = 0.023138 loss)
I1013 16:36:22.932525 12690 sgd_solver.cpp:105] Iteration 54200, lr = 0.001
I1013 16:36:53.734215 12690 solver.cpp:218] Iteration 54300 (3.24657 iter/s, 30.8017s/100 iters), loss = 0.0176476
I1013 16:36:53.734357 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0176485 (* 1 = 0.0176485 loss)
I1013 16:36:53.734366 12690 sgd_solver.cpp:105] Iteration 54300, lr = 0.001
I1013 16:37:24.486106 12690 solver.cpp:218] Iteration 54400 (3.25185 iter/s, 30.7518s/100 iters), loss = 0.0281454
I1013 16:37:24.486567 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0281464 (* 1 = 0.0281464 loss)
I1013 16:37:24.486577 12690 sgd_solver.cpp:105] Iteration 54400, lr = 0.001
I1013 16:37:54.983769 12690 solver.cpp:330] Iteration 54500, Testing net (#0)
I1013 16:38:11.701090 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:38:12.043614 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.16591 (* 1 = 1.16591 loss)
I1013 16:38:12.043630 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7357
I1013 16:38:12.346160 12690 solver.cpp:218] Iteration 54500 (2.08944 iter/s, 47.8596s/100 iters), loss = 0.0125295
I1013 16:38:12.346189 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125305 (* 1 = 0.0125305 loss)
I1013 16:38:12.346195 12690 sgd_solver.cpp:105] Iteration 54500, lr = 0.001
I1013 16:38:43.160897 12690 solver.cpp:218] Iteration 54600 (3.2452 iter/s, 30.8147s/100 iters), loss = 0.0143396
I1013 16:38:43.161043 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0143405 (* 1 = 0.0143405 loss)
I1013 16:38:43.161051 12690 sgd_solver.cpp:105] Iteration 54600, lr = 0.001
I1013 16:39:13.945667 12690 solver.cpp:218] Iteration 54700 (3.24837 iter/s, 30.7846s/100 iters), loss = 0.0072504
I1013 16:39:13.945812 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00725138 (* 1 = 0.00725138 loss)
I1013 16:39:13.945822 12690 sgd_solver.cpp:105] Iteration 54700, lr = 0.001
I1013 16:39:44.773973 12690 solver.cpp:218] Iteration 54800 (3.24379 iter/s, 30.8282s/100 iters), loss = 0.0028616
I1013 16:39:44.774116 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00286258 (* 1 = 0.00286258 loss)
I1013 16:39:44.774135 12690 sgd_solver.cpp:105] Iteration 54800, lr = 0.001
I1013 16:40:15.566103 12690 solver.cpp:218] Iteration 54900 (3.2476 iter/s, 30.792s/100 iters), loss = 0.00558015
I1013 16:40:15.566246 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00558114 (* 1 = 0.00558114 loss)
I1013 16:40:15.566256 12690 sgd_solver.cpp:105] Iteration 54900, lr = 0.001
I1013 16:40:44.848745 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:40:46.077422 12690 solver.cpp:330] Iteration 55000, Testing net (#0)
I1013 16:41:02.836228 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:41:03.180054 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.16275 (* 1 = 1.16275 loss)
I1013 16:41:03.180069 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7376
I1013 16:41:03.481691 12690 solver.cpp:218] Iteration 55000 (2.08701 iter/s, 47.9155s/100 iters), loss = 0.00634876
I1013 16:41:03.481724 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00634975 (* 1 = 0.00634975 loss)
I1013 16:41:03.481731 12690 sgd_solver.cpp:105] Iteration 55000, lr = 0.001
I1013 16:41:34.493774 12690 solver.cpp:218] Iteration 55100 (3.22455 iter/s, 31.0121s/100 iters), loss = 0.0486295
I1013 16:41:34.493904 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0486305 (* 1 = 0.0486305 loss)
I1013 16:41:34.493913 12690 sgd_solver.cpp:105] Iteration 55100, lr = 0.001
I1013 16:42:05.567507 12690 solver.cpp:218] Iteration 55200 (3.21817 iter/s, 31.0736s/100 iters), loss = 0.0232353
I1013 16:42:05.567648 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0232363 (* 1 = 0.0232363 loss)
I1013 16:42:05.567657 12690 sgd_solver.cpp:105] Iteration 55200, lr = 0.001
I1013 16:42:36.506986 12690 solver.cpp:218] Iteration 55300 (3.23213 iter/s, 30.9394s/100 iters), loss = 0.0104223
I1013 16:42:36.507100 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0104233 (* 1 = 0.0104233 loss)
I1013 16:42:36.507108 12690 sgd_solver.cpp:105] Iteration 55300, lr = 0.001
I1013 16:43:07.329571 12690 solver.cpp:218] Iteration 55400 (3.24438 iter/s, 30.8225s/100 iters), loss = 0.0325604
I1013 16:43:07.329717 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0325614 (* 1 = 0.0325614 loss)
I1013 16:43:07.329726 12690 sgd_solver.cpp:105] Iteration 55400, lr = 0.001
I1013 16:43:37.832661 12690 solver.cpp:330] Iteration 55500, Testing net (#0)
I1013 16:43:54.520238 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:43:54.860932 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.16104 (* 1 = 1.16104 loss)
I1013 16:43:54.860947 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7386
I1013 16:43:55.164402 12690 solver.cpp:218] Iteration 55500 (2.09053 iter/s, 47.8347s/100 iters), loss = 0.0188395
I1013 16:43:55.164433 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0188405 (* 1 = 0.0188405 loss)
I1013 16:43:55.164438 12690 sgd_solver.cpp:105] Iteration 55500, lr = 0.001
I1013 16:44:25.989336 12690 solver.cpp:218] Iteration 55600 (3.24413 iter/s, 30.8249s/100 iters), loss = 0.0109641
I1013 16:44:25.989464 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109652 (* 1 = 0.0109652 loss)
I1013 16:44:25.989471 12690 sgd_solver.cpp:105] Iteration 55600, lr = 0.001
I1013 16:44:56.804347 12690 solver.cpp:218] Iteration 55700 (3.24518 iter/s, 30.8149s/100 iters), loss = 0.00801835
I1013 16:44:56.804463 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00801939 (* 1 = 0.00801939 loss)
I1013 16:44:56.804482 12690 sgd_solver.cpp:105] Iteration 55700, lr = 0.001
I1013 16:45:27.587714 12690 solver.cpp:218] Iteration 55800 (3.24852 iter/s, 30.7833s/100 iters), loss = 0.00957768
I1013 16:45:27.587822 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00957872 (* 1 = 0.00957872 loss)
I1013 16:45:27.587831 12690 sgd_solver.cpp:105] Iteration 55800, lr = 0.001
I1013 16:45:58.362526 12690 solver.cpp:218] Iteration 55900 (3.24942 iter/s, 30.7747s/100 iters), loss = 0.00631082
I1013 16:45:58.362633 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00631187 (* 1 = 0.00631187 loss)
I1013 16:45:58.362650 12690 sgd_solver.cpp:105] Iteration 55900, lr = 0.001
I1013 16:46:27.564363 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:46:28.790894 12690 solver.cpp:330] Iteration 56000, Testing net (#0)
I1013 16:46:45.492807 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:46:45.836998 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.1603 (* 1 = 1.1603 loss)
I1013 16:46:45.837014 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7381
I1013 16:46:46.141387 12690 solver.cpp:218] Iteration 56000 (2.09298 iter/s, 47.7788s/100 iters), loss = 0.00625169
I1013 16:46:46.141417 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00625274 (* 1 = 0.00625274 loss)
I1013 16:46:46.141423 12690 sgd_solver.cpp:105] Iteration 56000, lr = 0.001
I1013 16:47:16.948925 12690 solver.cpp:218] Iteration 56100 (3.24596 iter/s, 30.8075s/100 iters), loss = 0.0112842
I1013 16:47:16.949095 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112852 (* 1 = 0.0112852 loss)
I1013 16:47:16.949105 12690 sgd_solver.cpp:105] Iteration 56100, lr = 0.001
I1013 16:47:47.716167 12690 solver.cpp:218] Iteration 56200 (3.25023 iter/s, 30.7671s/100 iters), loss = 0.014377
I1013 16:47:47.716303 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0143781 (* 1 = 0.0143781 loss)
I1013 16:47:47.716311 12690 sgd_solver.cpp:105] Iteration 56200, lr = 0.001
I1013 16:48:18.517422 12690 solver.cpp:218] Iteration 56300 (3.24663 iter/s, 30.8011s/100 iters), loss = 0.00996176
I1013 16:48:18.517561 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00996281 (* 1 = 0.00996281 loss)
I1013 16:48:18.517570 12690 sgd_solver.cpp:105] Iteration 56300, lr = 0.001
I1013 16:48:49.336191 12690 solver.cpp:218] Iteration 56400 (3.24479 iter/s, 30.8186s/100 iters), loss = 0.015898
I1013 16:48:49.336323 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158991 (* 1 = 0.0158991 loss)
I1013 16:48:49.336330 12690 sgd_solver.cpp:105] Iteration 56400, lr = 0.001
I1013 16:49:19.809641 12690 solver.cpp:330] Iteration 56500, Testing net (#0)
I1013 16:49:36.550781 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:49:36.893565 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.16072 (* 1 = 1.16072 loss)
I1013 16:49:36.893581 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7403
I1013 16:49:37.199328 12690 solver.cpp:218] Iteration 56500 (2.0893 iter/s, 47.863s/100 iters), loss = 0.00819174
I1013 16:49:37.199358 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00819278 (* 1 = 0.00819278 loss)
I1013 16:49:37.199365 12690 sgd_solver.cpp:105] Iteration 56500, lr = 0.001
I1013 16:50:07.992799 12690 solver.cpp:218] Iteration 56600 (3.24744 iter/s, 30.7935s/100 iters), loss = 0.00547875
I1013 16:50:07.992911 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00547978 (* 1 = 0.00547978 loss)
I1013 16:50:07.992929 12690 sgd_solver.cpp:105] Iteration 56600, lr = 0.001
I1013 16:50:38.758714 12690 solver.cpp:218] Iteration 56700 (3.25036 iter/s, 30.7658s/100 iters), loss = 0.00711433
I1013 16:50:38.758792 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00711536 (* 1 = 0.00711536 loss)
I1013 16:50:38.758810 12690 sgd_solver.cpp:105] Iteration 56700, lr = 0.001
I1013 16:51:09.539046 12690 solver.cpp:218] Iteration 56800 (3.24883 iter/s, 30.7803s/100 iters), loss = 0.00532419
I1013 16:51:09.539151 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00532521 (* 1 = 0.00532521 loss)
I1013 16:51:09.539160 12690 sgd_solver.cpp:105] Iteration 56800, lr = 0.001
I1013 16:51:40.326848 12690 solver.cpp:218] Iteration 56900 (3.24805 iter/s, 30.7877s/100 iters), loss = 0.0123659
I1013 16:51:40.327000 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123669 (* 1 = 0.0123669 loss)
I1013 16:51:40.327009 12690 sgd_solver.cpp:105] Iteration 56900, lr = 0.001
I1013 16:52:09.583967 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:52:10.814314 12690 solver.cpp:330] Iteration 57000, Testing net (#0)
I1013 16:52:27.531312 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:52:27.874096 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15809 (* 1 = 1.15809 loss)
I1013 16:52:27.874112 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7409
I1013 16:52:28.177647 12690 solver.cpp:218] Iteration 57000 (2.08983 iter/s, 47.8507s/100 iters), loss = 0.00621362
I1013 16:52:28.177675 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00621464 (* 1 = 0.00621464 loss)
I1013 16:52:28.177680 12690 sgd_solver.cpp:105] Iteration 57000, lr = 0.001
I1013 16:52:58.945871 12690 solver.cpp:218] Iteration 57100 (3.25011 iter/s, 30.7682s/100 iters), loss = 0.0112864
I1013 16:52:58.945971 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112875 (* 1 = 0.0112875 loss)
I1013 16:52:58.945978 12690 sgd_solver.cpp:105] Iteration 57100, lr = 0.001
I1013 16:53:29.759347 12690 solver.cpp:218] Iteration 57200 (3.24534 iter/s, 30.8134s/100 iters), loss = 0.0226682
I1013 16:53:29.759469 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0226692 (* 1 = 0.0226692 loss)
I1013 16:53:29.759477 12690 sgd_solver.cpp:105] Iteration 57200, lr = 0.001
I1013 16:54:00.536392 12690 solver.cpp:218] Iteration 57300 (3.24919 iter/s, 30.7769s/100 iters), loss = 0.00980681
I1013 16:54:00.536525 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00980783 (* 1 = 0.00980783 loss)
I1013 16:54:00.536533 12690 sgd_solver.cpp:105] Iteration 57300, lr = 0.001
I1013 16:54:31.283402 12690 solver.cpp:218] Iteration 57400 (3.25236 iter/s, 30.7469s/100 iters), loss = 0.0136541
I1013 16:54:31.283529 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0136551 (* 1 = 0.0136551 loss)
I1013 16:54:31.283536 12690 sgd_solver.cpp:105] Iteration 57400, lr = 0.001
I1013 16:55:01.741108 12690 solver.cpp:330] Iteration 57500, Testing net (#0)
I1013 16:55:18.455714 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:55:18.795830 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15698 (* 1 = 1.15698 loss)
I1013 16:55:18.795845 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7392
I1013 16:55:19.096408 12690 solver.cpp:218] Iteration 57500 (2.09149 iter/s, 47.8129s/100 iters), loss = 0.0116518
I1013 16:55:19.096439 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116529 (* 1 = 0.0116529 loss)
I1013 16:55:19.096446 12690 sgd_solver.cpp:105] Iteration 57500, lr = 0.001
I1013 16:55:49.879029 12690 solver.cpp:218] Iteration 57600 (3.24859 iter/s, 30.7826s/100 iters), loss = 0.00430767
I1013 16:55:49.879130 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00430871 (* 1 = 0.00430871 loss)
I1013 16:55:49.879137 12690 sgd_solver.cpp:105] Iteration 57600, lr = 0.001
I1013 16:56:20.655048 12690 solver.cpp:218] Iteration 57700 (3.24929 iter/s, 30.7759s/100 iters), loss = 0.0126841
I1013 16:56:20.655192 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126851 (* 1 = 0.0126851 loss)
I1013 16:56:20.655201 12690 sgd_solver.cpp:105] Iteration 57700, lr = 0.001
I1013 16:56:51.386826 12690 solver.cpp:218] Iteration 57800 (3.25397 iter/s, 30.7316s/100 iters), loss = 0.00231785
I1013 16:56:51.386966 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00231888 (* 1 = 0.00231888 loss)
I1013 16:56:51.386976 12690 sgd_solver.cpp:105] Iteration 57800, lr = 0.001
I1013 16:57:22.197209 12690 solver.cpp:218] Iteration 57900 (3.24567 iter/s, 30.8103s/100 iters), loss = 0.0086022
I1013 16:57:22.197324 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00860322 (* 1 = 0.00860322 loss)
I1013 16:57:22.197331 12690 sgd_solver.cpp:105] Iteration 57900, lr = 0.001
I1013 16:57:51.427526 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:57:52.658468 12690 solver.cpp:330] Iteration 58000, Testing net (#0)
I1013 16:58:09.327178 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 16:58:09.668376 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15451 (* 1 = 1.15451 loss)
I1013 16:58:09.668392 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7396
I1013 16:58:09.969369 12690 solver.cpp:218] Iteration 58000 (2.09327 iter/s, 47.7721s/100 iters), loss = 0.00795656
I1013 16:58:09.969413 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00795758 (* 1 = 0.00795758 loss)
I1013 16:58:09.969420 12690 sgd_solver.cpp:105] Iteration 58000, lr = 0.001
I1013 16:58:40.809917 12690 solver.cpp:218] Iteration 58100 (3.24249 iter/s, 30.8405s/100 iters), loss = 0.00713323
I1013 16:58:40.810045 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00713426 (* 1 = 0.00713426 loss)
I1013 16:58:40.810053 12690 sgd_solver.cpp:105] Iteration 58100, lr = 0.001
I1013 16:59:11.678359 12690 solver.cpp:218] Iteration 58200 (3.23957 iter/s, 30.8683s/100 iters), loss = 0.0210389
I1013 16:59:11.678521 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0210399 (* 1 = 0.0210399 loss)
I1013 16:59:11.678540 12690 sgd_solver.cpp:105] Iteration 58200, lr = 0.001
I1013 16:59:42.809232 12690 solver.cpp:218] Iteration 58300 (3.21226 iter/s, 31.1307s/100 iters), loss = 0.0285889
I1013 16:59:42.809342 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0285899 (* 1 = 0.0285899 loss)
I1013 16:59:42.809350 12690 sgd_solver.cpp:105] Iteration 58300, lr = 0.001
I1013 17:00:13.806768 12690 solver.cpp:218] Iteration 58400 (3.22607 iter/s, 30.9974s/100 iters), loss = 0.0158563
I1013 17:00:13.807000 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158573 (* 1 = 0.0158573 loss)
I1013 17:00:13.807044 12690 sgd_solver.cpp:105] Iteration 58400, lr = 0.001
I1013 17:00:44.760633 12690 solver.cpp:330] Iteration 58500, Testing net (#0)
I1013 17:01:01.571112 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:01:01.912791 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15498 (* 1 = 1.15498 loss)
I1013 17:01:01.912806 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7403
I1013 17:01:02.215850 12690 solver.cpp:218] Iteration 58500 (2.06574 iter/s, 48.4089s/100 iters), loss = 0.0155186
I1013 17:01:02.215876 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0155196 (* 1 = 0.0155196 loss)
I1013 17:01:02.215883 12690 sgd_solver.cpp:105] Iteration 58500, lr = 0.001
I1013 17:01:33.273561 12690 solver.cpp:218] Iteration 58600 (3.21982 iter/s, 31.0577s/100 iters), loss = 0.00676512
I1013 17:01:33.273668 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00676615 (* 1 = 0.00676615 loss)
I1013 17:01:33.273686 12690 sgd_solver.cpp:105] Iteration 58600, lr = 0.001
I1013 17:02:04.235106 12690 solver.cpp:218] Iteration 58700 (3.22983 iter/s, 30.9614s/100 iters), loss = 0.00761699
I1013 17:02:04.235221 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00761801 (* 1 = 0.00761801 loss)
I1013 17:02:04.235229 12690 sgd_solver.cpp:105] Iteration 58700, lr = 0.001
I1013 17:02:35.093660 12690 solver.cpp:218] Iteration 58800 (3.2406 iter/s, 30.8585s/100 iters), loss = 0.00809221
I1013 17:02:35.093760 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00809323 (* 1 = 0.00809323 loss)
I1013 17:02:35.093777 12690 sgd_solver.cpp:105] Iteration 58800, lr = 0.001
I1013 17:03:05.897204 12690 solver.cpp:218] Iteration 58900 (3.24639 iter/s, 30.8035s/100 iters), loss = 0.00501705
I1013 17:03:05.897346 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00501807 (* 1 = 0.00501807 loss)
I1013 17:03:05.897356 12690 sgd_solver.cpp:105] Iteration 58900, lr = 0.001
I1013 17:03:35.167378 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:03:36.398393 12690 solver.cpp:330] Iteration 59000, Testing net (#0)
I1013 17:03:53.096453 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:03:53.437371 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15552 (* 1 = 1.15552 loss)
I1013 17:03:53.437386 12690 solver.cpp:397]     Test net output #1: accuracy = 0.741
I1013 17:03:53.741421 12690 solver.cpp:218] Iteration 59000 (2.09012 iter/s, 47.8441s/100 iters), loss = 0.00601267
I1013 17:03:53.741457 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0060137 (* 1 = 0.0060137 loss)
I1013 17:03:53.741464 12690 sgd_solver.cpp:105] Iteration 59000, lr = 0.001
I1013 17:04:24.583916 12690 solver.cpp:218] Iteration 59100 (3.24228 iter/s, 30.8425s/100 iters), loss = 0.0056655
I1013 17:04:24.584040 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00566653 (* 1 = 0.00566653 loss)
I1013 17:04:24.584048 12690 sgd_solver.cpp:105] Iteration 59100, lr = 0.001
I1013 17:04:55.322856 12690 solver.cpp:218] Iteration 59200 (3.25321 iter/s, 30.7388s/100 iters), loss = 0.0144131
I1013 17:04:55.323343 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144141 (* 1 = 0.0144141 loss)
I1013 17:04:55.323351 12690 sgd_solver.cpp:105] Iteration 59200, lr = 0.001
I1013 17:05:26.144712 12690 solver.cpp:218] Iteration 59300 (3.2445 iter/s, 30.8214s/100 iters), loss = 0.00726998
I1013 17:05:26.144800 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00727101 (* 1 = 0.00727101 loss)
I1013 17:05:26.144809 12690 sgd_solver.cpp:105] Iteration 59300, lr = 0.001
I1013 17:05:56.919419 12690 solver.cpp:218] Iteration 59400 (3.24943 iter/s, 30.7746s/100 iters), loss = 0.0183561
I1013 17:05:56.919548 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0183572 (* 1 = 0.0183572 loss)
I1013 17:05:56.919556 12690 sgd_solver.cpp:105] Iteration 59400, lr = 0.001
I1013 17:06:27.396049 12690 solver.cpp:330] Iteration 59500, Testing net (#0)
I1013 17:06:44.102679 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:06:44.447764 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15362 (* 1 = 1.15362 loss)
I1013 17:06:44.447780 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7412
I1013 17:06:44.752408 12690 solver.cpp:218] Iteration 59500 (2.09061 iter/s, 47.8329s/100 iters), loss = 0.0296043
I1013 17:06:44.752444 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0296053 (* 1 = 0.0296053 loss)
I1013 17:06:44.752450 12690 sgd_solver.cpp:105] Iteration 59500, lr = 0.001
I1013 17:07:15.553630 12690 solver.cpp:218] Iteration 59600 (3.24663 iter/s, 30.8012s/100 iters), loss = 0.00347308
I1013 17:07:15.553727 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00347411 (* 1 = 0.00347411 loss)
I1013 17:07:15.553735 12690 sgd_solver.cpp:105] Iteration 59600, lr = 0.001
I1013 17:07:46.378007 12690 solver.cpp:218] Iteration 59700 (3.24419 iter/s, 30.8243s/100 iters), loss = 0.00617973
I1013 17:07:46.378103 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00618076 (* 1 = 0.00618076 loss)
I1013 17:07:46.378111 12690 sgd_solver.cpp:105] Iteration 59700, lr = 0.001
I1013 17:08:17.139971 12690 solver.cpp:218] Iteration 59800 (3.25078 iter/s, 30.7619s/100 iters), loss = 0.0041584
I1013 17:08:17.140112 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00415944 (* 1 = 0.00415944 loss)
I1013 17:08:17.140121 12690 sgd_solver.cpp:105] Iteration 59800, lr = 0.001
I1013 17:08:47.929692 12690 solver.cpp:218] Iteration 59900 (3.24785 iter/s, 30.7896s/100 iters), loss = 0.00379986
I1013 17:08:47.929811 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0038009 (* 1 = 0.0038009 loss)
I1013 17:08:47.929821 12690 sgd_solver.cpp:105] Iteration 59900, lr = 0.001
I1013 17:09:17.194509 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:09:18.414829 12690 solver.cpp:330] Iteration 60000, Testing net (#0)
I1013 17:09:35.097074 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:09:35.445021 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15253 (* 1 = 1.15253 loss)
I1013 17:09:35.445037 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7409
I1013 17:09:35.751945 12690 solver.cpp:218] Iteration 60000 (2.09108 iter/s, 47.8222s/100 iters), loss = 0.0137427
I1013 17:09:35.751981 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0137437 (* 1 = 0.0137437 loss)
I1013 17:09:35.751988 12690 sgd_solver.cpp:105] Iteration 60000, lr = 0.001
I1013 17:10:06.545250 12690 solver.cpp:218] Iteration 60100 (3.24746 iter/s, 30.7933s/100 iters), loss = 0.00420044
I1013 17:10:06.545398 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00420148 (* 1 = 0.00420148 loss)
I1013 17:10:06.545406 12690 sgd_solver.cpp:105] Iteration 60100, lr = 0.001
I1013 17:10:37.305404 12690 solver.cpp:218] Iteration 60200 (3.25097 iter/s, 30.76s/100 iters), loss = 0.0047367
I1013 17:10:37.305512 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00473774 (* 1 = 0.00473774 loss)
I1013 17:10:37.305519 12690 sgd_solver.cpp:105] Iteration 60200, lr = 0.001
I1013 17:11:08.067699 12690 solver.cpp:218] Iteration 60300 (3.25074 iter/s, 30.7622s/100 iters), loss = 0.0202588
I1013 17:11:08.067869 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0202598 (* 1 = 0.0202598 loss)
I1013 17:11:08.067879 12690 sgd_solver.cpp:105] Iteration 60300, lr = 0.001
I1013 17:11:38.795856 12690 solver.cpp:218] Iteration 60400 (3.25436 iter/s, 30.728s/100 iters), loss = 0.0190888
I1013 17:11:38.796001 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0190899 (* 1 = 0.0190899 loss)
I1013 17:11:38.796010 12690 sgd_solver.cpp:105] Iteration 60400, lr = 0.001
I1013 17:12:09.282346 12690 solver.cpp:330] Iteration 60500, Testing net (#0)
I1013 17:12:25.995131 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:12:26.338887 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15069 (* 1 = 1.15069 loss)
I1013 17:12:26.338902 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7412
I1013 17:12:26.643571 12690 solver.cpp:218] Iteration 60500 (2.08997 iter/s, 47.8476s/100 iters), loss = 0.0128064
I1013 17:12:26.643600 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0128074 (* 1 = 0.0128074 loss)
I1013 17:12:26.643606 12690 sgd_solver.cpp:105] Iteration 60500, lr = 0.001
I1013 17:12:57.399929 12690 solver.cpp:218] Iteration 60600 (3.25136 iter/s, 30.7563s/100 iters), loss = 0.00409373
I1013 17:12:57.400027 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00409477 (* 1 = 0.00409477 loss)
I1013 17:12:57.400034 12690 sgd_solver.cpp:105] Iteration 60600, lr = 0.001
I1013 17:13:28.180433 12690 solver.cpp:218] Iteration 60700 (3.24882 iter/s, 30.7804s/100 iters), loss = 0.00822062
I1013 17:13:28.180555 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00822167 (* 1 = 0.00822167 loss)
I1013 17:13:28.180563 12690 sgd_solver.cpp:105] Iteration 60700, lr = 0.001
I1013 17:13:58.964244 12690 solver.cpp:218] Iteration 60800 (3.24847 iter/s, 30.7837s/100 iters), loss = 0.00301781
I1013 17:13:58.964349 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00301885 (* 1 = 0.00301885 loss)
I1013 17:13:58.964356 12690 sgd_solver.cpp:105] Iteration 60800, lr = 0.001
I1013 17:14:29.737351 12690 solver.cpp:218] Iteration 60900 (3.2496 iter/s, 30.773s/100 iters), loss = 0.00473976
I1013 17:14:29.737506 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0047408 (* 1 = 0.0047408 loss)
I1013 17:14:29.737515 12690 sgd_solver.cpp:105] Iteration 60900, lr = 0.001
I1013 17:14:58.968207 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:15:00.197754 12690 solver.cpp:330] Iteration 61000, Testing net (#0)
I1013 17:15:16.942387 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:15:17.289454 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15076 (* 1 = 1.15076 loss)
I1013 17:15:17.289469 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7401
I1013 17:15:17.594876 12690 solver.cpp:218] Iteration 61000 (2.08954 iter/s, 47.8574s/100 iters), loss = 0.00537223
I1013 17:15:17.594907 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00537327 (* 1 = 0.00537327 loss)
I1013 17:15:17.594913 12690 sgd_solver.cpp:105] Iteration 61000, lr = 0.001
I1013 17:15:48.369103 12690 solver.cpp:218] Iteration 61100 (3.24947 iter/s, 30.7742s/100 iters), loss = 0.00801802
I1013 17:15:48.369194 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00801906 (* 1 = 0.00801906 loss)
I1013 17:15:48.369212 12690 sgd_solver.cpp:105] Iteration 61100, lr = 0.001
I1013 17:16:19.157625 12690 solver.cpp:218] Iteration 61200 (3.24797 iter/s, 30.7884s/100 iters), loss = 0.00948744
I1013 17:16:19.157757 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00948848 (* 1 = 0.00948848 loss)
I1013 17:16:19.157764 12690 sgd_solver.cpp:105] Iteration 61200, lr = 0.001
I1013 17:16:49.909296 12690 solver.cpp:218] Iteration 61300 (3.25187 iter/s, 30.7516s/100 iters), loss = 0.0138865
I1013 17:16:49.909401 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0138876 (* 1 = 0.0138876 loss)
I1013 17:16:49.909409 12690 sgd_solver.cpp:105] Iteration 61300, lr = 0.001
I1013 17:17:20.680255 12690 solver.cpp:218] Iteration 61400 (3.24983 iter/s, 30.7709s/100 iters), loss = 0.00547394
I1013 17:17:20.680434 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00547498 (* 1 = 0.00547498 loss)
I1013 17:17:20.680454 12690 sgd_solver.cpp:105] Iteration 61400, lr = 0.001
I1013 17:17:51.139199 12690 solver.cpp:330] Iteration 61500, Testing net (#0)
I1013 17:18:07.840456 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:18:08.181948 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15021 (* 1 = 1.15021 loss)
I1013 17:18:08.181964 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7413
I1013 17:18:08.486563 12690 solver.cpp:218] Iteration 61500 (2.09178 iter/s, 47.8062s/100 iters), loss = 0.0148978
I1013 17:18:08.486595 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0148988 (* 1 = 0.0148988 loss)
I1013 17:18:08.486601 12690 sgd_solver.cpp:105] Iteration 61500, lr = 0.001
I1013 17:18:39.269986 12690 solver.cpp:218] Iteration 61600 (3.2485 iter/s, 30.7834s/100 iters), loss = 0.00443553
I1013 17:18:39.270086 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00443658 (* 1 = 0.00443658 loss)
I1013 17:18:39.270103 12690 sgd_solver.cpp:105] Iteration 61600, lr = 0.001
I1013 17:19:10.023473 12690 solver.cpp:218] Iteration 61700 (3.25167 iter/s, 30.7534s/100 iters), loss = 0.00399615
I1013 17:19:10.024036 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0039972 (* 1 = 0.0039972 loss)
I1013 17:19:10.024045 12690 sgd_solver.cpp:105] Iteration 61700, lr = 0.001
I1013 17:19:40.822459 12690 solver.cpp:218] Iteration 61800 (3.24692 iter/s, 30.7984s/100 iters), loss = 0.00301062
I1013 17:19:40.822530 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00301167 (* 1 = 0.00301167 loss)
I1013 17:19:40.822548 12690 sgd_solver.cpp:105] Iteration 61800, lr = 0.001
I1013 17:20:11.642416 12690 solver.cpp:218] Iteration 61900 (3.24466 iter/s, 30.8199s/100 iters), loss = 0.00410723
I1013 17:20:11.642513 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00410828 (* 1 = 0.00410828 loss)
I1013 17:20:11.642520 12690 sgd_solver.cpp:105] Iteration 61900, lr = 0.001
I1013 17:20:40.877440 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:20:42.103623 12690 solver.cpp:330] Iteration 62000, Testing net (#0)
I1013 17:20:58.855098 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:20:59.199128 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.15007 (* 1 = 1.15007 loss)
I1013 17:20:59.199143 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7421
I1013 17:20:59.505264 12690 solver.cpp:218] Iteration 62000 (2.08931 iter/s, 47.8628s/100 iters), loss = 0.00343123
I1013 17:20:59.505300 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00343227 (* 1 = 0.00343227 loss)
I1013 17:20:59.505307 12690 sgd_solver.cpp:105] Iteration 62000, lr = 0.001
I1013 17:21:30.291440 12690 solver.cpp:218] Iteration 62100 (3.24821 iter/s, 30.7862s/100 iters), loss = 0.005222
I1013 17:21:30.291558 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00522305 (* 1 = 0.00522305 loss)
I1013 17:21:30.291576 12690 sgd_solver.cpp:105] Iteration 62100, lr = 0.001
I1013 17:22:01.050804 12690 solver.cpp:218] Iteration 62200 (3.25105 iter/s, 30.7593s/100 iters), loss = 0.0213082
I1013 17:22:01.050904 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0213092 (* 1 = 0.0213092 loss)
I1013 17:22:01.050914 12690 sgd_solver.cpp:105] Iteration 62200, lr = 0.001
I1013 17:22:31.881203 12690 solver.cpp:218] Iteration 62300 (3.24356 iter/s, 30.8303s/100 iters), loss = 0.0775852
I1013 17:22:31.881330 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0775863 (* 1 = 0.0775863 loss)
I1013 17:22:31.881337 12690 sgd_solver.cpp:105] Iteration 62300, lr = 0.001
I1013 17:23:02.721729 12690 solver.cpp:218] Iteration 62400 (3.2425 iter/s, 30.8404s/100 iters), loss = 0.00892522
I1013 17:23:02.721861 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00892629 (* 1 = 0.00892629 loss)
I1013 17:23:02.721870 12690 sgd_solver.cpp:105] Iteration 62400, lr = 0.001
I1013 17:23:33.198503 12690 solver.cpp:330] Iteration 62500, Testing net (#0)
I1013 17:23:49.871974 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:23:50.212765 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.14718 (* 1 = 1.14718 loss)
I1013 17:23:50.212780 12690 solver.cpp:397]     Test net output #1: accuracy = 0.743
I1013 17:23:50.515085 12690 solver.cpp:218] Iteration 62500 (2.09235 iter/s, 47.7933s/100 iters), loss = 0.0189288
I1013 17:23:50.515116 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0189299 (* 1 = 0.0189299 loss)
I1013 17:23:50.515122 12690 sgd_solver.cpp:105] Iteration 62500, lr = 0.001
I1013 17:24:21.334475 12690 solver.cpp:218] Iteration 62600 (3.24471 iter/s, 30.8194s/100 iters), loss = 0.00688905
I1013 17:24:21.334602 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00689011 (* 1 = 0.00689011 loss)
I1013 17:24:21.334611 12690 sgd_solver.cpp:105] Iteration 62600, lr = 0.001
I1013 17:24:52.141924 12690 solver.cpp:218] Iteration 62700 (3.24598 iter/s, 30.8073s/100 iters), loss = 0.00926481
I1013 17:24:52.142040 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00926587 (* 1 = 0.00926587 loss)
I1013 17:24:52.142048 12690 sgd_solver.cpp:105] Iteration 62700, lr = 0.001
I1013 17:25:22.946059 12690 solver.cpp:218] Iteration 62800 (3.24633 iter/s, 30.804s/100 iters), loss = 0.00263849
I1013 17:25:22.946171 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00263954 (* 1 = 0.00263954 loss)
I1013 17:25:22.946189 12690 sgd_solver.cpp:105] Iteration 62800, lr = 0.001
I1013 17:25:53.791146 12690 solver.cpp:218] Iteration 62900 (3.24202 iter/s, 30.845s/100 iters), loss = 0.00363661
I1013 17:25:53.791294 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00363767 (* 1 = 0.00363767 loss)
I1013 17:25:53.791303 12690 sgd_solver.cpp:105] Iteration 62900, lr = 0.001
I1013 17:26:23.052711 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:26:24.289451 12690 solver.cpp:330] Iteration 63000, Testing net (#0)
I1013 17:26:40.951645 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:26:41.291849 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.14685 (* 1 = 1.14685 loss)
I1013 17:26:41.291865 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7429
I1013 17:26:41.593960 12690 solver.cpp:218] Iteration 63000 (2.09193 iter/s, 47.8027s/100 iters), loss = 0.00804581
I1013 17:26:41.593989 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00804688 (* 1 = 0.00804688 loss)
I1013 17:26:41.593997 12690 sgd_solver.cpp:105] Iteration 63000, lr = 0.001
I1013 17:27:12.392858 12690 solver.cpp:218] Iteration 63100 (3.24687 iter/s, 30.7989s/100 iters), loss = 0.00927676
I1013 17:27:12.396246 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00927783 (* 1 = 0.00927783 loss)
I1013 17:27:12.396255 12690 sgd_solver.cpp:105] Iteration 63100, lr = 0.001
I1013 17:27:43.206542 12690 solver.cpp:218] Iteration 63200 (3.24567 iter/s, 30.8103s/100 iters), loss = 0.00341065
I1013 17:27:43.206650 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00341172 (* 1 = 0.00341172 loss)
I1013 17:27:43.206657 12690 sgd_solver.cpp:105] Iteration 63200, lr = 0.001
I1013 17:28:13.976361 12690 solver.cpp:218] Iteration 63300 (3.24995 iter/s, 30.7697s/100 iters), loss = 0.0144607
I1013 17:28:13.976464 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144618 (* 1 = 0.0144618 loss)
I1013 17:28:13.976472 12690 sgd_solver.cpp:105] Iteration 63300, lr = 0.001
I1013 17:28:44.747921 12690 solver.cpp:218] Iteration 63400 (3.24976 iter/s, 30.7715s/100 iters), loss = 0.0106476
I1013 17:28:44.748056 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0106486 (* 1 = 0.0106486 loss)
I1013 17:28:44.748077 12690 sgd_solver.cpp:105] Iteration 63400, lr = 0.001
I1013 17:29:15.227267 12690 solver.cpp:330] Iteration 63500, Testing net (#0)
I1013 17:29:31.951275 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:29:32.294423 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.14538 (* 1 = 1.14538 loss)
I1013 17:29:32.294437 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7434
I1013 17:29:32.598175 12690 solver.cpp:218] Iteration 63500 (2.08986 iter/s, 47.8501s/100 iters), loss = 0.0391435
I1013 17:29:32.598206 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0391446 (* 1 = 0.0391446 loss)
I1013 17:29:32.598212 12690 sgd_solver.cpp:105] Iteration 63500, lr = 0.001
I1013 17:30:03.418292 12690 solver.cpp:218] Iteration 63600 (3.24464 iter/s, 30.8201s/100 iters), loss = 0.00400051
I1013 17:30:03.418408 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00400158 (* 1 = 0.00400158 loss)
I1013 17:30:03.418416 12690 sgd_solver.cpp:105] Iteration 63600, lr = 0.001
I1013 17:30:34.191280 12690 solver.cpp:218] Iteration 63700 (3.24961 iter/s, 30.7729s/100 iters), loss = 0.00478407
I1013 17:30:34.191390 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00478513 (* 1 = 0.00478513 loss)
I1013 17:30:34.191397 12690 sgd_solver.cpp:105] Iteration 63700, lr = 0.001
I1013 17:31:04.990149 12690 solver.cpp:218] Iteration 63800 (3.24688 iter/s, 30.7988s/100 iters), loss = 0.00491269
I1013 17:31:04.990250 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00491375 (* 1 = 0.00491375 loss)
I1013 17:31:04.990258 12690 sgd_solver.cpp:105] Iteration 63800, lr = 0.001
I1013 17:31:35.801385 12690 solver.cpp:218] Iteration 63900 (3.24558 iter/s, 30.8111s/100 iters), loss = 0.00427919
I1013 17:31:35.801528 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00428026 (* 1 = 0.00428026 loss)
I1013 17:31:35.801538 12690 sgd_solver.cpp:105] Iteration 63900, lr = 0.001
I1013 17:32:05.059614 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:32:06.293807 12690 solver.cpp:330] Iteration 64000, Testing net (#0)
I1013 17:32:22.989743 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:32:23.330761 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.14507 (* 1 = 1.14507 loss)
I1013 17:32:23.330776 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7428
I1013 17:32:23.633733 12690 solver.cpp:218] Iteration 64000 (2.09064 iter/s, 47.8322s/100 iters), loss = 0.00330678
I1013 17:32:23.633767 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00330785 (* 1 = 0.00330785 loss)
I1013 17:32:23.633774 12690 sgd_solver.cpp:105] Iteration 64000, lr = 0.001
I1013 17:32:54.424423 12690 solver.cpp:218] Iteration 64100 (3.24774 iter/s, 30.7907s/100 iters), loss = 0.0116313
I1013 17:32:54.424563 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116323 (* 1 = 0.0116323 loss)
I1013 17:32:54.424572 12690 sgd_solver.cpp:105] Iteration 64100, lr = 0.001
I1013 17:33:25.208353 12690 solver.cpp:218] Iteration 64200 (3.24846 iter/s, 30.7838s/100 iters), loss = 0.00686483
I1013 17:33:25.208456 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0068659 (* 1 = 0.0068659 loss)
I1013 17:33:25.208465 12690 sgd_solver.cpp:105] Iteration 64200, lr = 0.001
I1013 17:33:56.008464 12690 solver.cpp:218] Iteration 64300 (3.24675 iter/s, 30.8s/100 iters), loss = 0.0148294
I1013 17:33:56.008607 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0148305 (* 1 = 0.0148305 loss)
I1013 17:33:56.008616 12690 sgd_solver.cpp:105] Iteration 64300, lr = 0.001
I1013 17:34:26.734588 12690 solver.cpp:218] Iteration 64400 (3.25457 iter/s, 30.726s/100 iters), loss = 0.00891564
I1013 17:34:26.734724 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00891671 (* 1 = 0.00891671 loss)
I1013 17:34:26.734745 12690 sgd_solver.cpp:105] Iteration 64400, lr = 0.001
I1013 17:34:57.234233 12690 solver.cpp:330] Iteration 64500, Testing net (#0)
I1013 17:35:13.917429 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:35:14.258605 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.14297 (* 1 = 1.14297 loss)
I1013 17:35:14.258620 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7425
I1013 17:35:14.560037 12690 solver.cpp:218] Iteration 64500 (2.09094 iter/s, 47.8253s/100 iters), loss = 0.00584981
I1013 17:35:14.560070 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00585089 (* 1 = 0.00585089 loss)
I1013 17:35:14.560075 12690 sgd_solver.cpp:105] Iteration 64500, lr = 0.001
I1013 17:35:45.311599 12690 solver.cpp:218] Iteration 64600 (3.25187 iter/s, 30.7515s/100 iters), loss = 0.0033812
I1013 17:35:45.311693 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00338227 (* 1 = 0.00338227 loss)
I1013 17:35:45.311702 12690 sgd_solver.cpp:105] Iteration 64600, lr = 0.001
I1013 17:36:16.079442 12690 solver.cpp:218] Iteration 64700 (3.25016 iter/s, 30.7678s/100 iters), loss = 0.00526255
I1013 17:36:16.079519 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00526361 (* 1 = 0.00526361 loss)
I1013 17:36:16.079535 12690 sgd_solver.cpp:105] Iteration 64700, lr = 0.001
I1013 17:36:46.813060 12690 solver.cpp:218] Iteration 64800 (3.25377 iter/s, 30.7336s/100 iters), loss = 0.0041379
I1013 17:36:46.813109 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00413897 (* 1 = 0.00413897 loss)
I1013 17:36:46.813117 12690 sgd_solver.cpp:105] Iteration 64800, lr = 0.001
I1013 17:37:17.597928 12690 solver.cpp:218] Iteration 64900 (3.24835 iter/s, 30.7848s/100 iters), loss = 0.00315781
I1013 17:37:17.598042 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00315887 (* 1 = 0.00315887 loss)
I1013 17:37:17.598048 12690 sgd_solver.cpp:105] Iteration 64900, lr = 0.001
I1013 17:37:46.797436 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:37:48.026885 12690 solver.cpp:330] Iteration 65000, Testing net (#0)
I1013 17:38:04.727819 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:38:05.069751 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.14232 (* 1 = 1.14232 loss)
I1013 17:38:05.069767 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7418
I1013 17:38:05.372748 12690 solver.cpp:218] Iteration 65000 (2.09316 iter/s, 47.7747s/100 iters), loss = 0.00488016
I1013 17:38:05.372788 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00488123 (* 1 = 0.00488123 loss)
I1013 17:38:05.372797 12690 sgd_solver.cpp:105] Iteration 65000, lr = 0.001
I1013 17:38:36.117568 12690 solver.cpp:218] Iteration 65100 (3.25258 iter/s, 30.7448s/100 iters), loss = 0.00438218
I1013 17:38:36.118046 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00438324 (* 1 = 0.00438324 loss)
I1013 17:38:36.118054 12690 sgd_solver.cpp:105] Iteration 65100, lr = 0.001
I1013 17:39:06.842793 12690 solver.cpp:218] Iteration 65200 (3.2547 iter/s, 30.7248s/100 iters), loss = 0.00916371
I1013 17:39:06.842926 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00916477 (* 1 = 0.00916477 loss)
I1013 17:39:06.842934 12690 sgd_solver.cpp:105] Iteration 65200, lr = 0.001
I1013 17:39:37.610786 12690 solver.cpp:218] Iteration 65300 (3.25014 iter/s, 30.7679s/100 iters), loss = 0.0112419
I1013 17:39:37.610883 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112429 (* 1 = 0.0112429 loss)
I1013 17:39:37.610890 12690 sgd_solver.cpp:105] Iteration 65300, lr = 0.001
I1013 17:40:08.387846 12690 solver.cpp:218] Iteration 65400 (3.24918 iter/s, 30.777s/100 iters), loss = 0.00942489
I1013 17:40:08.387984 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00942595 (* 1 = 0.00942595 loss)
I1013 17:40:08.387993 12690 sgd_solver.cpp:105] Iteration 65400, lr = 0.001
I1013 17:40:38.786689 12690 solver.cpp:330] Iteration 65500, Testing net (#0)
I1013 17:40:55.470152 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:40:55.810386 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.1402 (* 1 = 1.1402 loss)
I1013 17:40:55.810400 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7426
I1013 17:40:56.112543 12690 solver.cpp:218] Iteration 65500 (2.09536 iter/s, 47.7246s/100 iters), loss = 0.0171945
I1013 17:40:56.112571 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171955 (* 1 = 0.0171955 loss)
I1013 17:40:56.112577 12690 sgd_solver.cpp:105] Iteration 65500, lr = 0.001
I1013 17:41:26.804831 12690 solver.cpp:218] Iteration 65600 (3.25815 iter/s, 30.6923s/100 iters), loss = 0.00234419
I1013 17:41:26.805001 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00234525 (* 1 = 0.00234525 loss)
I1013 17:41:26.805022 12690 sgd_solver.cpp:105] Iteration 65600, lr = 0.001
I1013 17:41:57.524255 12690 solver.cpp:218] Iteration 65700 (3.25529 iter/s, 30.7193s/100 iters), loss = 0.00694198
I1013 17:41:57.524397 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00694304 (* 1 = 0.00694304 loss)
I1013 17:41:57.524406 12690 sgd_solver.cpp:105] Iteration 65700, lr = 0.001
I1013 17:42:28.245425 12690 solver.cpp:218] Iteration 65800 (3.2551 iter/s, 30.721s/100 iters), loss = 0.0045035
I1013 17:42:28.245533 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00450456 (* 1 = 0.00450456 loss)
I1013 17:42:28.245540 12690 sgd_solver.cpp:105] Iteration 65800, lr = 0.001
I1013 17:42:59.020617 12690 solver.cpp:218] Iteration 65900 (3.24938 iter/s, 30.7751s/100 iters), loss = 0.00702582
I1013 17:42:59.020756 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00702688 (* 1 = 0.00702688 loss)
I1013 17:42:59.020763 12690 sgd_solver.cpp:105] Iteration 65900, lr = 0.001
I1013 17:43:28.212090 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:43:29.441153 12690 solver.cpp:330] Iteration 66000, Testing net (#0)
I1013 17:43:46.143167 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:43:46.485397 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.13765 (* 1 = 1.13765 loss)
I1013 17:43:46.485414 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7428
I1013 17:43:46.788344 12690 solver.cpp:218] Iteration 66000 (2.09347 iter/s, 47.7676s/100 iters), loss = 0.0043379
I1013 17:43:46.788378 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00433896 (* 1 = 0.00433896 loss)
I1013 17:43:46.788386 12690 sgd_solver.cpp:105] Iteration 66000, lr = 0.001
I1013 17:44:17.506865 12690 solver.cpp:218] Iteration 66100 (3.25537 iter/s, 30.7185s/100 iters), loss = 0.0092286
I1013 17:44:17.507638 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00922966 (* 1 = 0.00922966 loss)
I1013 17:44:17.507647 12690 sgd_solver.cpp:105] Iteration 66100, lr = 0.001
I1013 17:44:48.232208 12690 solver.cpp:218] Iteration 66200 (3.25472 iter/s, 30.7246s/100 iters), loss = 0.00810804
I1013 17:44:48.232342 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0081091 (* 1 = 0.0081091 loss)
I1013 17:44:48.232352 12690 sgd_solver.cpp:105] Iteration 66200, lr = 0.001
I1013 17:45:18.995362 12690 solver.cpp:218] Iteration 66300 (3.25065 iter/s, 30.763s/100 iters), loss = 0.00632997
I1013 17:45:18.995517 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00633104 (* 1 = 0.00633104 loss)
I1013 17:45:18.995527 12690 sgd_solver.cpp:105] Iteration 66300, lr = 0.001
I1013 17:45:49.741040 12690 solver.cpp:218] Iteration 66400 (3.2525 iter/s, 30.7455s/100 iters), loss = 0.0158882
I1013 17:45:49.741144 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158893 (* 1 = 0.0158893 loss)
I1013 17:45:49.741152 12690 sgd_solver.cpp:105] Iteration 66400, lr = 0.001
I1013 17:46:20.156061 12690 solver.cpp:330] Iteration 66500, Testing net (#0)
I1013 17:46:36.777098 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:46:37.117272 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.13643 (* 1 = 1.13643 loss)
I1013 17:46:37.117288 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7438
I1013 17:46:37.420094 12690 solver.cpp:218] Iteration 66500 (2.09736 iter/s, 47.679s/100 iters), loss = 0.00481382
I1013 17:46:37.420125 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00481489 (* 1 = 0.00481489 loss)
I1013 17:46:37.420132 12690 sgd_solver.cpp:105] Iteration 66500, lr = 0.001
I1013 17:47:08.197621 12690 solver.cpp:218] Iteration 66600 (3.24913 iter/s, 30.7775s/100 iters), loss = 0.00555371
I1013 17:47:08.197782 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00555477 (* 1 = 0.00555477 loss)
I1013 17:47:08.197791 12690 sgd_solver.cpp:105] Iteration 66600, lr = 0.001
I1013 17:47:38.920559 12690 solver.cpp:218] Iteration 66700 (3.25491 iter/s, 30.7228s/100 iters), loss = 0.00894204
I1013 17:47:38.920706 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0089431 (* 1 = 0.0089431 loss)
I1013 17:47:38.920717 12690 sgd_solver.cpp:105] Iteration 66700, lr = 0.001
I1013 17:48:09.670328 12690 solver.cpp:218] Iteration 66800 (3.25207 iter/s, 30.7496s/100 iters), loss = 0.00317331
I1013 17:48:09.670439 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00317437 (* 1 = 0.00317437 loss)
I1013 17:48:09.670456 12690 sgd_solver.cpp:105] Iteration 66800, lr = 0.001
I1013 17:48:40.443327 12690 solver.cpp:218] Iteration 66900 (3.24961 iter/s, 30.7729s/100 iters), loss = 0.00179471
I1013 17:48:40.443433 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00179577 (* 1 = 0.00179577 loss)
I1013 17:48:40.443449 12690 sgd_solver.cpp:105] Iteration 66900, lr = 0.001
I1013 17:49:09.660039 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:49:10.896817 12690 solver.cpp:330] Iteration 67000, Testing net (#0)
I1013 17:49:27.554095 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:49:27.893841 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.13587 (* 1 = 1.13587 loss)
I1013 17:49:27.893857 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7429
I1013 17:49:28.196702 12690 solver.cpp:218] Iteration 67000 (2.0941 iter/s, 47.7533s/100 iters), loss = 0.00438635
I1013 17:49:28.196733 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00438741 (* 1 = 0.00438741 loss)
I1013 17:49:28.196740 12690 sgd_solver.cpp:105] Iteration 67000, lr = 0.001
I1013 17:49:58.954577 12690 solver.cpp:218] Iteration 67100 (3.2512 iter/s, 30.7579s/100 iters), loss = 0.00441617
I1013 17:49:58.954704 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00441723 (* 1 = 0.00441723 loss)
I1013 17:49:58.954715 12690 sgd_solver.cpp:105] Iteration 67100, lr = 0.001
I1013 17:50:29.752833 12690 solver.cpp:218] Iteration 67200 (3.24695 iter/s, 30.7982s/100 iters), loss = 0.00363126
I1013 17:50:29.752970 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00363232 (* 1 = 0.00363232 loss)
I1013 17:50:29.752979 12690 sgd_solver.cpp:105] Iteration 67200, lr = 0.001
I1013 17:51:00.530633 12690 solver.cpp:218] Iteration 67300 (3.24911 iter/s, 30.7777s/100 iters), loss = 0.0116265
I1013 17:51:00.530742 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116276 (* 1 = 0.0116276 loss)
I1013 17:51:00.530761 12690 sgd_solver.cpp:105] Iteration 67300, lr = 0.001
I1013 17:51:31.306387 12690 solver.cpp:218] Iteration 67400 (3.24932 iter/s, 30.7757s/100 iters), loss = 0.0151831
I1013 17:51:31.306532 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0151841 (* 1 = 0.0151841 loss)
I1013 17:51:31.306542 12690 sgd_solver.cpp:105] Iteration 67400, lr = 0.001
I1013 17:52:01.807996 12690 solver.cpp:330] Iteration 67500, Testing net (#0)
I1013 17:52:18.447362 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:52:18.792245 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.13562 (* 1 = 1.13562 loss)
I1013 17:52:18.792260 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7437
I1013 17:52:19.098271 12690 solver.cpp:218] Iteration 67500 (2.09241 iter/s, 47.7918s/100 iters), loss = 0.00429389
I1013 17:52:19.098305 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00429494 (* 1 = 0.00429494 loss)
I1013 17:52:19.098314 12690 sgd_solver.cpp:105] Iteration 67500, lr = 0.001
I1013 17:52:49.813706 12690 solver.cpp:218] Iteration 67600 (3.25569 iter/s, 30.7154s/100 iters), loss = 0.00471294
I1013 17:52:49.813812 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.004714 (* 1 = 0.004714 loss)
I1013 17:52:49.813820 12690 sgd_solver.cpp:105] Iteration 67600, lr = 0.001
I1013 17:53:20.534754 12690 solver.cpp:218] Iteration 67700 (3.25511 iter/s, 30.721s/100 iters), loss = 0.00456921
I1013 17:53:20.534865 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00457027 (* 1 = 0.00457027 loss)
I1013 17:53:20.534883 12690 sgd_solver.cpp:105] Iteration 67700, lr = 0.001
I1013 17:53:51.251965 12690 solver.cpp:218] Iteration 67800 (3.25551 iter/s, 30.7171s/100 iters), loss = 0.00419715
I1013 17:53:51.252111 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00419821 (* 1 = 0.00419821 loss)
I1013 17:53:51.252120 12690 sgd_solver.cpp:105] Iteration 67800, lr = 0.001
I1013 17:54:22.008373 12690 solver.cpp:218] Iteration 67900 (3.25137 iter/s, 30.7563s/100 iters), loss = 0.00392633
I1013 17:54:22.008471 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00392739 (* 1 = 0.00392739 loss)
I1013 17:54:22.008489 12690 sgd_solver.cpp:105] Iteration 67900, lr = 0.001
I1013 17:54:51.253017 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:54:52.488710 12690 solver.cpp:330] Iteration 68000, Testing net (#0)
I1013 17:55:09.169456 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:55:09.509914 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.1338 (* 1 = 1.1338 loss)
I1013 17:55:09.509933 12690 solver.cpp:397]     Test net output #1: accuracy = 0.745
I1013 17:55:09.812019 12690 solver.cpp:218] Iteration 68000 (2.09189 iter/s, 47.8036s/100 iters), loss = 0.004656
I1013 17:55:09.812054 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00465706 (* 1 = 0.00465706 loss)
I1013 17:55:09.812062 12690 sgd_solver.cpp:105] Iteration 68000, lr = 0.001
I1013 17:55:40.551146 12690 solver.cpp:218] Iteration 68100 (3.25319 iter/s, 30.7391s/100 iters), loss = 0.00319565
I1013 17:55:40.551278 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00319671 (* 1 = 0.00319671 loss)
I1013 17:55:40.551286 12690 sgd_solver.cpp:105] Iteration 68100, lr = 0.001
I1013 17:56:11.297720 12690 solver.cpp:218] Iteration 68200 (3.25241 iter/s, 30.7465s/100 iters), loss = 0.0124843
I1013 17:56:11.297859 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0124854 (* 1 = 0.0124854 loss)
I1013 17:56:11.297868 12690 sgd_solver.cpp:105] Iteration 68200, lr = 0.001
I1013 17:56:42.062669 12690 solver.cpp:218] Iteration 68300 (3.25047 iter/s, 30.7648s/100 iters), loss = 0.00622591
I1013 17:56:42.062788 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00622697 (* 1 = 0.00622697 loss)
I1013 17:56:42.062798 12690 sgd_solver.cpp:105] Iteration 68300, lr = 0.001
I1013 17:57:12.806815 12690 solver.cpp:218] Iteration 68400 (3.25266 iter/s, 30.744s/100 iters), loss = 0.0111237
I1013 17:57:12.806903 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0111248 (* 1 = 0.0111248 loss)
I1013 17:57:12.806921 12690 sgd_solver.cpp:105] Iteration 68400, lr = 0.001
I1013 17:57:43.317983 12690 solver.cpp:330] Iteration 68500, Testing net (#0)
I1013 17:57:59.986831 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 17:58:00.336102 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.13091 (* 1 = 1.13091 loss)
I1013 17:58:00.336118 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7453
I1013 17:58:00.643167 12690 solver.cpp:218] Iteration 68500 (2.09046 iter/s, 47.8363s/100 iters), loss = 0.00827564
I1013 17:58:00.643198 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00827669 (* 1 = 0.00827669 loss)
I1013 17:58:00.643205 12690 sgd_solver.cpp:105] Iteration 68500, lr = 0.001
I1013 17:58:31.419217 12690 solver.cpp:218] Iteration 68600 (3.24928 iter/s, 30.776s/100 iters), loss = 0.00331389
I1013 17:58:31.419378 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00331495 (* 1 = 0.00331495 loss)
I1013 17:58:31.419396 12690 sgd_solver.cpp:105] Iteration 68600, lr = 0.001
I1013 17:59:02.182731 12690 solver.cpp:218] Iteration 68700 (3.25062 iter/s, 30.7634s/100 iters), loss = 0.00684008
I1013 17:59:02.182844 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00684113 (* 1 = 0.00684113 loss)
I1013 17:59:02.182853 12690 sgd_solver.cpp:105] Iteration 68700, lr = 0.001
I1013 17:59:32.888809 12690 solver.cpp:218] Iteration 68800 (3.25669 iter/s, 30.706s/100 iters), loss = 0.00338429
I1013 17:59:32.888947 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00338535 (* 1 = 0.00338535 loss)
I1013 17:59:32.888953 12690 sgd_solver.cpp:105] Iteration 68800, lr = 0.001
I1013 18:00:03.584025 12690 solver.cpp:218] Iteration 68900 (3.25785 iter/s, 30.6951s/100 iters), loss = 0.00307277
I1013 18:00:03.584167 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00307382 (* 1 = 0.00307382 loss)
I1013 18:00:03.584175 12690 sgd_solver.cpp:105] Iteration 68900, lr = 0.001
I1013 18:00:32.766119 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:00:33.993979 12690 solver.cpp:330] Iteration 69000, Testing net (#0)
I1013 18:00:50.660967 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:00:51.001103 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.12898 (* 1 = 1.12898 loss)
I1013 18:00:51.001119 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7451
I1013 18:00:51.303614 12690 solver.cpp:218] Iteration 69000 (2.09558 iter/s, 47.7195s/100 iters), loss = 0.0122363
I1013 18:00:51.303644 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0122373 (* 1 = 0.0122373 loss)
I1013 18:00:51.303652 12690 sgd_solver.cpp:105] Iteration 69000, lr = 0.001
I1013 18:01:22.097683 12690 solver.cpp:218] Iteration 69100 (3.24738 iter/s, 30.794s/100 iters), loss = 0.00868894
I1013 18:01:22.097793 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00868998 (* 1 = 0.00868998 loss)
I1013 18:01:22.097805 12690 sgd_solver.cpp:105] Iteration 69100, lr = 0.001
I1013 18:01:52.861032 12690 solver.cpp:218] Iteration 69200 (3.25063 iter/s, 30.7633s/100 iters), loss = 0.00606242
I1013 18:01:52.861176 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00606347 (* 1 = 0.00606347 loss)
I1013 18:01:52.861186 12690 sgd_solver.cpp:105] Iteration 69200, lr = 0.001
I1013 18:02:23.559518 12690 solver.cpp:218] Iteration 69300 (3.2575 iter/s, 30.6984s/100 iters), loss = 0.00911742
I1013 18:02:23.559628 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00911847 (* 1 = 0.00911847 loss)
I1013 18:02:23.559635 12690 sgd_solver.cpp:105] Iteration 69300, lr = 0.001
I1013 18:02:54.271993 12690 solver.cpp:218] Iteration 69400 (3.25602 iter/s, 30.7124s/100 iters), loss = 0.00467134
I1013 18:02:54.272138 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00467239 (* 1 = 0.00467239 loss)
I1013 18:02:54.272147 12690 sgd_solver.cpp:105] Iteration 69400, lr = 0.001
I1013 18:03:24.680850 12690 solver.cpp:330] Iteration 69500, Testing net (#0)
I1013 18:03:41.366101 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:03:41.707873 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.12675 (* 1 = 1.12675 loss)
I1013 18:03:41.707890 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7457
I1013 18:03:42.013247 12690 solver.cpp:218] Iteration 69500 (2.09463 iter/s, 47.7411s/100 iters), loss = 0.00765246
I1013 18:03:42.013279 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00765351 (* 1 = 0.00765351 loss)
I1013 18:03:42.013285 12690 sgd_solver.cpp:105] Iteration 69500, lr = 0.001
I1013 18:04:12.761447 12690 solver.cpp:218] Iteration 69600 (3.25223 iter/s, 30.7482s/100 iters), loss = 0.00568074
I1013 18:04:12.761569 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00568179 (* 1 = 0.00568179 loss)
I1013 18:04:12.761587 12690 sgd_solver.cpp:105] Iteration 69600, lr = 0.001
I1013 18:04:43.472457 12690 solver.cpp:218] Iteration 69700 (3.25617 iter/s, 30.7109s/100 iters), loss = 0.00612284
I1013 18:04:43.472548 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00612389 (* 1 = 0.00612389 loss)
I1013 18:04:43.472563 12690 sgd_solver.cpp:105] Iteration 69700, lr = 0.001
I1013 18:05:14.157210 12690 solver.cpp:218] Iteration 69800 (3.25896 iter/s, 30.6847s/100 iters), loss = 0.00624129
I1013 18:05:14.157363 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00624234 (* 1 = 0.00624234 loss)
I1013 18:05:14.157372 12690 sgd_solver.cpp:105] Iteration 69800, lr = 0.001
I1013 18:05:44.854394 12690 solver.cpp:218] Iteration 69900 (3.25764 iter/s, 30.697s/100 iters), loss = 0.00330235
I1013 18:05:44.854501 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0033034 (* 1 = 0.0033034 loss)
I1013 18:05:44.854508 12690 sgd_solver.cpp:105] Iteration 69900, lr = 0.001
I1013 18:06:14.020027 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:06:15.249817 12690 solver.cpp:330] Iteration 70000, Testing net (#0)
I1013 18:06:31.895092 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:06:32.236225 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.12599 (* 1 = 1.12599 loss)
I1013 18:06:32.236241 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7454
I1013 18:06:32.538030 12690 solver.cpp:218] Iteration 70000 (2.09716 iter/s, 47.6836s/100 iters), loss = 0.00581368
I1013 18:06:32.538060 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00581474 (* 1 = 0.00581474 loss)
I1013 18:06:32.538067 12690 sgd_solver.cpp:105] Iteration 70000, lr = 0.001
I1013 18:07:03.208185 12690 solver.cpp:218] Iteration 70100 (3.2605 iter/s, 30.6701s/100 iters), loss = 0.00648982
I1013 18:07:03.208315 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00649087 (* 1 = 0.00649087 loss)
I1013 18:07:03.208323 12690 sgd_solver.cpp:105] Iteration 70100, lr = 0.001
I1013 18:07:33.874858 12690 solver.cpp:218] Iteration 70200 (3.26088 iter/s, 30.6666s/100 iters), loss = 0.010333
I1013 18:07:33.874969 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0103341 (* 1 = 0.0103341 loss)
I1013 18:07:33.874976 12690 sgd_solver.cpp:105] Iteration 70200, lr = 0.001
I1013 18:08:04.592973 12690 solver.cpp:218] Iteration 70300 (3.25542 iter/s, 30.718s/100 iters), loss = 0.00693609
I1013 18:08:04.593116 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00693714 (* 1 = 0.00693714 loss)
I1013 18:08:04.593125 12690 sgd_solver.cpp:105] Iteration 70300, lr = 0.001
I1013 18:08:35.280437 12690 solver.cpp:218] Iteration 70400 (3.25867 iter/s, 30.6873s/100 iters), loss = 0.00755978
I1013 18:08:35.280535 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00756083 (* 1 = 0.00756083 loss)
I1013 18:08:35.280552 12690 sgd_solver.cpp:105] Iteration 70400, lr = 0.001
I1013 18:09:05.590029 12690 solver.cpp:330] Iteration 70500, Testing net (#0)
I1013 18:09:22.281779 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:09:22.622633 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.12435 (* 1 = 1.12435 loss)
I1013 18:09:22.622649 12690 solver.cpp:397]     Test net output #1: accuracy = 0.745
I1013 18:09:22.927628 12690 solver.cpp:218] Iteration 70500 (2.09876 iter/s, 47.6471s/100 iters), loss = 0.0141269
I1013 18:09:22.927660 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.014128 (* 1 = 0.014128 loss)
I1013 18:09:22.927667 12690 sgd_solver.cpp:105] Iteration 70500, lr = 0.001
I1013 18:09:53.571113 12690 solver.cpp:218] Iteration 70600 (3.26334 iter/s, 30.6435s/100 iters), loss = 0.00338927
I1013 18:09:53.571254 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00339032 (* 1 = 0.00339032 loss)
I1013 18:09:53.571260 12690 sgd_solver.cpp:105] Iteration 70600, lr = 0.001
I1013 18:10:24.200129 12690 solver.cpp:218] Iteration 70700 (3.26489 iter/s, 30.6289s/100 iters), loss = 0.00461952
I1013 18:10:24.200245 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00462057 (* 1 = 0.00462057 loss)
I1013 18:10:24.200263 12690 sgd_solver.cpp:105] Iteration 70700, lr = 0.001
I1013 18:10:54.880481 12690 solver.cpp:218] Iteration 70800 (3.25943 iter/s, 30.6802s/100 iters), loss = 0.00403152
I1013 18:10:54.880589 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00403257 (* 1 = 0.00403257 loss)
I1013 18:10:54.880607 12690 sgd_solver.cpp:105] Iteration 70800, lr = 0.001
I1013 18:11:25.582396 12690 solver.cpp:218] Iteration 70900 (3.25714 iter/s, 30.7018s/100 iters), loss = 0.00270593
I1013 18:11:25.582535 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00270697 (* 1 = 0.00270697 loss)
I1013 18:11:25.582542 12690 sgd_solver.cpp:105] Iteration 70900, lr = 0.001
I1013 18:11:54.720347 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:11:55.952061 12690 solver.cpp:330] Iteration 71000, Testing net (#0)
I1013 18:12:12.589068 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:12:12.928534 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.12346 (* 1 = 1.12346 loss)
I1013 18:12:12.928550 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7442
I1013 18:12:13.232574 12690 solver.cpp:218] Iteration 71000 (2.09863 iter/s, 47.6501s/100 iters), loss = 0.00816201
I1013 18:12:13.232607 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00816305 (* 1 = 0.00816305 loss)
I1013 18:12:13.232615 12690 sgd_solver.cpp:105] Iteration 71000, lr = 0.001
I1013 18:12:43.875406 12690 solver.cpp:218] Iteration 71100 (3.26341 iter/s, 30.6428s/100 iters), loss = 0.00673183
I1013 18:12:43.875521 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00673287 (* 1 = 0.00673287 loss)
I1013 18:12:43.875530 12690 sgd_solver.cpp:105] Iteration 71100, lr = 0.001
I1013 18:13:14.499384 12690 solver.cpp:218] Iteration 71200 (3.26543 iter/s, 30.6239s/100 iters), loss = 0.00677741
I1013 18:13:14.499501 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00677846 (* 1 = 0.00677846 loss)
I1013 18:13:14.499510 12690 sgd_solver.cpp:105] Iteration 71200, lr = 0.001
I1013 18:13:45.162364 12690 solver.cpp:218] Iteration 71300 (3.26127 iter/s, 30.6629s/100 iters), loss = 0.00828216
I1013 18:13:45.162472 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00828321 (* 1 = 0.00828321 loss)
I1013 18:13:45.162490 12690 sgd_solver.cpp:105] Iteration 71300, lr = 0.001
I1013 18:14:15.805426 12690 solver.cpp:218] Iteration 71400 (3.26339 iter/s, 30.643s/100 iters), loss = 0.0076927
I1013 18:14:15.805570 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00769375 (* 1 = 0.00769375 loss)
I1013 18:14:15.805579 12690 sgd_solver.cpp:105] Iteration 71400, lr = 0.001
I1013 18:14:46.183810 12690 solver.cpp:330] Iteration 71500, Testing net (#0)
I1013 18:15:02.829653 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:15:03.170054 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.12184 (* 1 = 1.12184 loss)
I1013 18:15:03.170070 12690 solver.cpp:397]     Test net output #1: accuracy = 0.743
I1013 18:15:03.474733 12690 solver.cpp:218] Iteration 71500 (2.09779 iter/s, 47.6692s/100 iters), loss = 0.0073973
I1013 18:15:03.474764 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00739835 (* 1 = 0.00739835 loss)
I1013 18:15:03.474772 12690 sgd_solver.cpp:105] Iteration 71500, lr = 0.001
I1013 18:15:34.108198 12690 solver.cpp:218] Iteration 71600 (3.26441 iter/s, 30.6334s/100 iters), loss = 0.00234975
I1013 18:15:34.108297 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00235079 (* 1 = 0.00235079 loss)
I1013 18:15:34.108305 12690 sgd_solver.cpp:105] Iteration 71600, lr = 0.001
I1013 18:16:04.755543 12690 solver.cpp:218] Iteration 71700 (3.26294 iter/s, 30.6473s/100 iters), loss = 0.010011
I1013 18:16:04.755728 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.010012 (* 1 = 0.010012 loss)
I1013 18:16:04.755738 12690 sgd_solver.cpp:105] Iteration 71700, lr = 0.001
I1013 18:16:35.452679 12690 solver.cpp:218] Iteration 71800 (3.25765 iter/s, 30.697s/100 iters), loss = 0.00256567
I1013 18:16:35.452822 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00256671 (* 1 = 0.00256671 loss)
I1013 18:16:35.452831 12690 sgd_solver.cpp:105] Iteration 71800, lr = 0.001
I1013 18:17:06.125159 12690 solver.cpp:218] Iteration 71900 (3.26027 iter/s, 30.6723s/100 iters), loss = 0.00310253
I1013 18:17:06.125255 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00310357 (* 1 = 0.00310357 loss)
I1013 18:17:06.125262 12690 sgd_solver.cpp:105] Iteration 71900, lr = 0.001
I1013 18:17:35.295989 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:17:36.514721 12690 solver.cpp:330] Iteration 72000, Testing net (#0)
I1013 18:17:53.178597 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:17:53.518916 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11963 (* 1 = 1.11963 loss)
I1013 18:17:53.518932 12690 solver.cpp:397]     Test net output #1: accuracy = 0.744
I1013 18:17:53.822386 12690 solver.cpp:218] Iteration 72000 (2.09656 iter/s, 47.6971s/100 iters), loss = 0.0046317
I1013 18:17:53.822419 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00463274 (* 1 = 0.00463274 loss)
I1013 18:17:53.822427 12690 sgd_solver.cpp:105] Iteration 72000, lr = 0.001
I1013 18:18:24.481807 12690 solver.cpp:218] Iteration 72100 (3.26164 iter/s, 30.6594s/100 iters), loss = 0.0050346
I1013 18:18:24.481954 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00503564 (* 1 = 0.00503564 loss)
I1013 18:18:24.481976 12690 sgd_solver.cpp:105] Iteration 72100, lr = 0.001
I1013 18:18:55.114576 12690 solver.cpp:218] Iteration 72200 (3.26449 iter/s, 30.6326s/100 iters), loss = 0.00405467
I1013 18:18:55.114624 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00405571 (* 1 = 0.00405571 loss)
I1013 18:18:55.114631 12690 sgd_solver.cpp:105] Iteration 72200, lr = 0.001
I1013 18:19:25.780851 12690 solver.cpp:218] Iteration 72300 (3.26092 iter/s, 30.6662s/100 iters), loss = 0.00494831
I1013 18:19:25.780995 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00494934 (* 1 = 0.00494934 loss)
I1013 18:19:25.781003 12690 sgd_solver.cpp:105] Iteration 72300, lr = 0.001
I1013 18:19:56.369839 12690 solver.cpp:218] Iteration 72400 (3.26916 iter/s, 30.5889s/100 iters), loss = 0.00947387
I1013 18:19:56.369979 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00947491 (* 1 = 0.00947491 loss)
I1013 18:19:56.369988 12690 sgd_solver.cpp:105] Iteration 72400, lr = 0.001
I1013 18:20:26.655529 12690 solver.cpp:330] Iteration 72500, Testing net (#0)
I1013 18:20:43.308087 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:20:43.648607 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11974 (* 1 = 1.11974 loss)
I1013 18:20:43.648624 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 18:20:43.952908 12690 solver.cpp:218] Iteration 72500 (2.10159 iter/s, 47.5829s/100 iters), loss = 0.00632736
I1013 18:20:43.952940 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00632839 (* 1 = 0.00632839 loss)
I1013 18:20:43.952947 12690 sgd_solver.cpp:105] Iteration 72500, lr = 0.001
I1013 18:21:14.636633 12690 solver.cpp:218] Iteration 72600 (3.25906 iter/s, 30.6837s/100 iters), loss = 0.00616044
I1013 18:21:14.636739 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00616147 (* 1 = 0.00616147 loss)
I1013 18:21:14.636757 12690 sgd_solver.cpp:105] Iteration 72600, lr = 0.001
I1013 18:21:45.291908 12690 solver.cpp:218] Iteration 72700 (3.26209 iter/s, 30.6552s/100 iters), loss = 0.00741643
I1013 18:21:45.292022 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00741746 (* 1 = 0.00741746 loss)
I1013 18:21:45.292032 12690 sgd_solver.cpp:105] Iteration 72700, lr = 0.001
I1013 18:22:15.960664 12690 solver.cpp:218] Iteration 72800 (3.26066 iter/s, 30.6686s/100 iters), loss = 0.00579599
I1013 18:22:15.960806 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00579703 (* 1 = 0.00579703 loss)
I1013 18:22:15.960815 12690 sgd_solver.cpp:105] Iteration 72800, lr = 0.001
I1013 18:22:46.626250 12690 solver.cpp:218] Iteration 72900 (3.261 iter/s, 30.6655s/100 iters), loss = 0.00392974
I1013 18:22:46.626292 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00393077 (* 1 = 0.00393077 loss)
I1013 18:22:46.626301 12690 sgd_solver.cpp:105] Iteration 72900, lr = 0.001
I1013 18:23:15.770218 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:23:17.004240 12690 solver.cpp:330] Iteration 73000, Testing net (#0)
I1013 18:23:33.646687 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:23:33.986975 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11734 (* 1 = 1.11734 loss)
I1013 18:23:33.986990 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7454
I1013 18:23:34.289863 12690 solver.cpp:218] Iteration 73000 (2.09804 iter/s, 47.6636s/100 iters), loss = 0.00719342
I1013 18:23:34.289906 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00719446 (* 1 = 0.00719446 loss)
I1013 18:23:34.289913 12690 sgd_solver.cpp:105] Iteration 73000, lr = 0.001
I1013 18:24:04.882557 12690 solver.cpp:218] Iteration 73100 (3.26876 iter/s, 30.5927s/100 iters), loss = 0.00532494
I1013 18:24:04.882655 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00532597 (* 1 = 0.00532597 loss)
I1013 18:24:04.882674 12690 sgd_solver.cpp:105] Iteration 73100, lr = 0.001
I1013 18:24:35.539474 12690 solver.cpp:218] Iteration 73200 (3.26192 iter/s, 30.6568s/100 iters), loss = 0.00466067
I1013 18:24:35.539580 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00466171 (* 1 = 0.00466171 loss)
I1013 18:24:35.539597 12690 sgd_solver.cpp:105] Iteration 73200, lr = 0.001
I1013 18:25:06.174260 12690 solver.cpp:218] Iteration 73300 (3.26427 iter/s, 30.6347s/100 iters), loss = 0.00748438
I1013 18:25:06.174401 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00748541 (* 1 = 0.00748541 loss)
I1013 18:25:06.174410 12690 sgd_solver.cpp:105] Iteration 73300, lr = 0.001
I1013 18:25:36.779825 12690 solver.cpp:218] Iteration 73400 (3.26739 iter/s, 30.6054s/100 iters), loss = 0.00794393
I1013 18:25:36.779925 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00794496 (* 1 = 0.00794496 loss)
I1013 18:25:36.779943 12690 sgd_solver.cpp:105] Iteration 73400, lr = 0.001
I1013 18:26:07.098886 12690 solver.cpp:330] Iteration 73500, Testing net (#0)
I1013 18:26:23.710791 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:26:24.050463 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11572 (* 1 = 1.11572 loss)
I1013 18:26:24.050479 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7457
I1013 18:26:24.351074 12690 solver.cpp:218] Iteration 73500 (2.10211 iter/s, 47.5712s/100 iters), loss = 0.00943581
I1013 18:26:24.351104 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00943684 (* 1 = 0.00943684 loss)
I1013 18:26:24.351110 12690 sgd_solver.cpp:105] Iteration 73500, lr = 0.001
I1013 18:26:55.047917 12690 solver.cpp:218] Iteration 73600 (3.25767 iter/s, 30.6968s/100 iters), loss = 0.00319527
I1013 18:26:55.048005 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0031963 (* 1 = 0.0031963 loss)
I1013 18:26:55.048017 12690 sgd_solver.cpp:105] Iteration 73600, lr = 0.001
I1013 18:27:25.756145 12690 solver.cpp:218] Iteration 73700 (3.25646 iter/s, 30.7082s/100 iters), loss = 0.0116463
I1013 18:27:25.756256 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116473 (* 1 = 0.0116473 loss)
I1013 18:27:25.756274 12690 sgd_solver.cpp:105] Iteration 73700, lr = 0.001
I1013 18:27:56.438524 12690 solver.cpp:218] Iteration 73800 (3.25921 iter/s, 30.6823s/100 iters), loss = 0.00225245
I1013 18:27:56.438686 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00225348 (* 1 = 0.00225348 loss)
I1013 18:27:56.438695 12690 sgd_solver.cpp:105] Iteration 73800, lr = 0.001
I1013 18:28:27.123741 12690 solver.cpp:218] Iteration 73900 (3.25891 iter/s, 30.6851s/100 iters), loss = 0.00517712
I1013 18:28:27.123852 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00517815 (* 1 = 0.00517815 loss)
I1013 18:28:27.123870 12690 sgd_solver.cpp:105] Iteration 73900, lr = 0.001
I1013 18:28:56.251705 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:28:57.482852 12690 solver.cpp:330] Iteration 74000, Testing net (#0)
I1013 18:29:14.142125 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:29:14.482903 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11275 (* 1 = 1.11275 loss)
I1013 18:29:14.482918 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7451
I1013 18:29:14.788319 12690 solver.cpp:218] Iteration 74000 (2.098 iter/s, 47.6645s/100 iters), loss = 0.0058293
I1013 18:29:14.788353 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00583033 (* 1 = 0.00583033 loss)
I1013 18:29:14.788362 12690 sgd_solver.cpp:105] Iteration 74000, lr = 0.001
I1013 18:29:45.426151 12690 solver.cpp:218] Iteration 74100 (3.26394 iter/s, 30.6378s/100 iters), loss = 0.00591653
I1013 18:29:45.426259 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00591756 (* 1 = 0.00591756 loss)
I1013 18:29:45.426267 12690 sgd_solver.cpp:105] Iteration 74100, lr = 0.001
I1013 18:30:16.072456 12690 solver.cpp:218] Iteration 74200 (3.26305 iter/s, 30.6462s/100 iters), loss = 0.00376311
I1013 18:30:16.072558 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00376414 (* 1 = 0.00376414 loss)
I1013 18:30:16.072566 12690 sgd_solver.cpp:105] Iteration 74200, lr = 0.001
I1013 18:30:46.729792 12690 solver.cpp:218] Iteration 74300 (3.26187 iter/s, 30.6572s/100 iters), loss = 0.00712334
I1013 18:30:46.729890 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00712437 (* 1 = 0.00712437 loss)
I1013 18:30:46.729897 12690 sgd_solver.cpp:105] Iteration 74300, lr = 0.001
I1013 18:31:17.419845 12690 solver.cpp:218] Iteration 74400 (3.25839 iter/s, 30.69s/100 iters), loss = 0.00614913
I1013 18:31:17.419997 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00615016 (* 1 = 0.00615016 loss)
I1013 18:31:17.420006 12690 sgd_solver.cpp:105] Iteration 74400, lr = 0.001
I1013 18:31:47.787715 12690 solver.cpp:330] Iteration 74500, Testing net (#0)
I1013 18:32:04.445976 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:32:04.785511 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11221 (* 1 = 1.11221 loss)
I1013 18:32:04.785527 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7454
I1013 18:32:05.089045 12690 solver.cpp:218] Iteration 74500 (2.0978 iter/s, 47.6691s/100 iters), loss = 0.00851997
I1013 18:32:05.089076 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.008521 (* 1 = 0.008521 loss)
I1013 18:32:05.089082 12690 sgd_solver.cpp:105] Iteration 74500, lr = 0.001
I1013 18:32:35.727527 12690 solver.cpp:218] Iteration 74600 (3.26387 iter/s, 30.6385s/100 iters), loss = 0.00432128
I1013 18:32:35.727668 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00432231 (* 1 = 0.00432231 loss)
I1013 18:32:35.727675 12690 sgd_solver.cpp:105] Iteration 74600, lr = 0.001
I1013 18:33:06.421602 12690 solver.cpp:218] Iteration 74700 (3.25797 iter/s, 30.6939s/100 iters), loss = 0.00378975
I1013 18:33:06.421741 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00379078 (* 1 = 0.00379078 loss)
I1013 18:33:06.421748 12690 sgd_solver.cpp:105] Iteration 74700, lr = 0.001
I1013 18:33:37.069339 12690 solver.cpp:218] Iteration 74800 (3.2629 iter/s, 30.6476s/100 iters), loss = 0.00343743
I1013 18:33:37.069458 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00343846 (* 1 = 0.00343846 loss)
I1013 18:33:37.069476 12690 sgd_solver.cpp:105] Iteration 74800, lr = 0.001
I1013 18:34:07.682163 12690 solver.cpp:218] Iteration 74900 (3.26662 iter/s, 30.6127s/100 iters), loss = 0.00214298
I1013 18:34:07.682286 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00214401 (* 1 = 0.00214401 loss)
I1013 18:34:07.682294 12690 sgd_solver.cpp:105] Iteration 74900, lr = 0.001
I1013 18:34:36.828850 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:34:38.058285 12690 solver.cpp:330] Iteration 75000, Testing net (#0)
I1013 18:34:54.666849 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:34:55.009163 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11144 (* 1 = 1.11144 loss)
I1013 18:34:55.009178 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7448
I1013 18:34:55.313441 12690 solver.cpp:218] Iteration 75000 (2.09947 iter/s, 47.6312s/100 iters), loss = 0.00381489
I1013 18:34:55.313473 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00381592 (* 1 = 0.00381592 loss)
I1013 18:34:55.313480 12690 sgd_solver.cpp:105] Iteration 75000, lr = 0.001
I1013 18:35:25.974676 12690 solver.cpp:218] Iteration 75100 (3.26145 iter/s, 30.6612s/100 iters), loss = 0.00375742
I1013 18:35:25.974773 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00375845 (* 1 = 0.00375845 loss)
I1013 18:35:25.974781 12690 sgd_solver.cpp:105] Iteration 75100, lr = 0.001
I1013 18:35:56.617537 12690 solver.cpp:218] Iteration 75200 (3.26341 iter/s, 30.6428s/100 iters), loss = 0.00850171
I1013 18:35:56.617664 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00850274 (* 1 = 0.00850274 loss)
I1013 18:35:56.617672 12690 sgd_solver.cpp:105] Iteration 75200, lr = 0.001
I1013 18:36:27.241969 12690 solver.cpp:218] Iteration 75300 (3.26538 iter/s, 30.6243s/100 iters), loss = 0.00627859
I1013 18:36:27.242118 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00627962 (* 1 = 0.00627962 loss)
I1013 18:36:27.242128 12690 sgd_solver.cpp:105] Iteration 75300, lr = 0.001
I1013 18:36:57.879549 12690 solver.cpp:218] Iteration 75400 (3.26398 iter/s, 30.6374s/100 iters), loss = 0.00730544
I1013 18:36:57.879662 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00730647 (* 1 = 0.00730647 loss)
I1013 18:36:57.879669 12690 sgd_solver.cpp:105] Iteration 75400, lr = 0.001
I1013 18:37:28.187819 12690 solver.cpp:330] Iteration 75500, Testing net (#0)
I1013 18:37:44.810925 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:37:45.147526 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.11077 (* 1 = 1.11077 loss)
I1013 18:37:45.147541 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7446
I1013 18:37:45.449051 12690 solver.cpp:218] Iteration 75500 (2.10219 iter/s, 47.5694s/100 iters), loss = 0.00413867
I1013 18:37:45.449090 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0041397 (* 1 = 0.0041397 loss)
I1013 18:37:45.449097 12690 sgd_solver.cpp:105] Iteration 75500, lr = 0.001
I1013 18:38:16.132936 12690 solver.cpp:218] Iteration 75600 (3.25904 iter/s, 30.6839s/100 iters), loss = 0.003987
I1013 18:38:16.133049 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00398803 (* 1 = 0.00398803 loss)
I1013 18:38:16.133057 12690 sgd_solver.cpp:105] Iteration 75600, lr = 0.001
I1013 18:38:46.775964 12690 solver.cpp:218] Iteration 75700 (3.2634 iter/s, 30.6429s/100 iters), loss = 0.00443537
I1013 18:38:46.776062 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0044364 (* 1 = 0.0044364 loss)
I1013 18:38:46.776070 12690 sgd_solver.cpp:105] Iteration 75700, lr = 0.001
I1013 18:39:17.470644 12690 solver.cpp:218] Iteration 75800 (3.2579 iter/s, 30.6946s/100 iters), loss = 0.00407095
I1013 18:39:17.470759 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00407198 (* 1 = 0.00407198 loss)
I1013 18:39:17.470778 12690 sgd_solver.cpp:105] Iteration 75800, lr = 0.001
I1013 18:39:48.089561 12690 solver.cpp:218] Iteration 75900 (3.26597 iter/s, 30.6188s/100 iters), loss = 0.00524791
I1013 18:39:48.089694 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00524894 (* 1 = 0.00524894 loss)
I1013 18:39:48.089701 12690 sgd_solver.cpp:105] Iteration 75900, lr = 0.001
I1013 18:40:17.235792 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:40:18.459918 12690 solver.cpp:330] Iteration 76000, Testing net (#0)
I1013 18:40:35.141877 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:40:35.485191 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.1091 (* 1 = 1.1091 loss)
I1013 18:40:35.485208 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7459
I1013 18:40:35.789990 12690 solver.cpp:218] Iteration 76000 (2.09642 iter/s, 47.7003s/100 iters), loss = 0.00437032
I1013 18:40:35.790026 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00437135 (* 1 = 0.00437135 loss)
I1013 18:40:35.790033 12690 sgd_solver.cpp:105] Iteration 76000, lr = 0.001
I1013 18:41:06.416498 12690 solver.cpp:218] Iteration 76100 (3.26515 iter/s, 30.6265s/100 iters), loss = 0.00388673
I1013 18:41:06.416637 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00388777 (* 1 = 0.00388777 loss)
I1013 18:41:06.416646 12690 sgd_solver.cpp:105] Iteration 76100, lr = 0.001
I1013 18:41:37.070899 12690 solver.cpp:218] Iteration 76200 (3.26219 iter/s, 30.6543s/100 iters), loss = 0.0140393
I1013 18:41:37.071301 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140403 (* 1 = 0.0140403 loss)
I1013 18:41:37.071310 12690 sgd_solver.cpp:105] Iteration 76200, lr = 0.001
I1013 18:42:07.939180 12690 solver.cpp:218] Iteration 76300 (3.23961 iter/s, 30.8679s/100 iters), loss = 0.0218236
I1013 18:42:07.939332 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0218247 (* 1 = 0.0218247 loss)
I1013 18:42:07.939355 12690 sgd_solver.cpp:105] Iteration 76300, lr = 0.001
I1013 18:42:39.053753 12690 solver.cpp:218] Iteration 76400 (3.21394 iter/s, 31.1144s/100 iters), loss = 0.00604517
I1013 18:42:39.053910 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0060462 (* 1 = 0.0060462 loss)
I1013 18:42:39.053947 12690 sgd_solver.cpp:105] Iteration 76400, lr = 0.001
I1013 18:43:09.435497 12690 solver.cpp:330] Iteration 76500, Testing net (#0)
I1013 18:43:26.090677 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:43:26.432921 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10686 (* 1 = 1.10686 loss)
I1013 18:43:26.432937 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 18:43:26.734258 12690 solver.cpp:218] Iteration 76500 (2.0973 iter/s, 47.6804s/100 iters), loss = 0.00616877
I1013 18:43:26.734294 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0061698 (* 1 = 0.0061698 loss)
I1013 18:43:26.734305 12690 sgd_solver.cpp:105] Iteration 76500, lr = 0.001
I1013 18:43:57.418929 12690 solver.cpp:218] Iteration 76600 (3.25896 iter/s, 30.6846s/100 iters), loss = 0.00521148
I1013 18:43:57.419045 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00521251 (* 1 = 0.00521251 loss)
I1013 18:43:57.419054 12690 sgd_solver.cpp:105] Iteration 76600, lr = 0.001
I1013 18:44:28.319499 12690 solver.cpp:218] Iteration 76700 (3.2362 iter/s, 30.9005s/100 iters), loss = 0.00560226
I1013 18:44:28.322290 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00560329 (* 1 = 0.00560329 loss)
I1013 18:44:28.322314 12690 sgd_solver.cpp:105] Iteration 76700, lr = 0.001
I1013 18:44:59.459941 12690 solver.cpp:218] Iteration 76800 (3.21155 iter/s, 31.1376s/100 iters), loss = 0.00444883
I1013 18:44:59.460170 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00444985 (* 1 = 0.00444985 loss)
I1013 18:44:59.460180 12690 sgd_solver.cpp:105] Iteration 76800, lr = 0.001
I1013 18:45:30.355345 12690 solver.cpp:218] Iteration 76900 (3.23675 iter/s, 30.8952s/100 iters), loss = 0.00342396
I1013 18:45:30.355670 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00342499 (* 1 = 0.00342499 loss)
I1013 18:45:30.355679 12690 sgd_solver.cpp:105] Iteration 76900, lr = 0.001
I1013 18:45:59.676928 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:46:00.919469 12690 solver.cpp:330] Iteration 77000, Testing net (#0)
I1013 18:46:17.697964 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:46:18.039556 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10574 (* 1 = 1.10574 loss)
I1013 18:46:18.039572 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 18:46:18.342684 12690 solver.cpp:218] Iteration 77000 (2.0839 iter/s, 47.987s/100 iters), loss = 0.00235772
I1013 18:46:18.342716 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00235875 (* 1 = 0.00235875 loss)
I1013 18:46:18.342725 12690 sgd_solver.cpp:105] Iteration 77000, lr = 0.001
I1013 18:46:49.164072 12690 solver.cpp:218] Iteration 77100 (3.2445 iter/s, 30.8214s/100 iters), loss = 0.00361116
I1013 18:46:49.164204 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00361219 (* 1 = 0.00361219 loss)
I1013 18:46:49.164212 12690 sgd_solver.cpp:105] Iteration 77100, lr = 0.001
I1013 18:47:19.951505 12690 solver.cpp:218] Iteration 77200 (3.24809 iter/s, 30.7873s/100 iters), loss = 0.00715824
I1013 18:47:19.951606 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00715927 (* 1 = 0.00715927 loss)
I1013 18:47:19.951632 12690 sgd_solver.cpp:105] Iteration 77200, lr = 0.001
I1013 18:47:50.634934 12690 solver.cpp:218] Iteration 77300 (3.2591 iter/s, 30.6833s/100 iters), loss = 0.00607238
I1013 18:47:50.635076 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0060734 (* 1 = 0.0060734 loss)
I1013 18:47:50.635085 12690 sgd_solver.cpp:105] Iteration 77300, lr = 0.001
I1013 18:48:21.354830 12690 solver.cpp:218] Iteration 77400 (3.25523 iter/s, 30.7198s/100 iters), loss = 0.0124945
I1013 18:48:21.354977 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0124955 (* 1 = 0.0124955 loss)
I1013 18:48:21.354995 12690 sgd_solver.cpp:105] Iteration 77400, lr = 0.001
I1013 18:48:51.997469 12690 solver.cpp:330] Iteration 77500, Testing net (#0)
I1013 18:49:08.844826 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:49:09.197481 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10469 (* 1 = 1.10469 loss)
I1013 18:49:09.197500 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7461
I1013 18:49:09.502152 12690 solver.cpp:218] Iteration 77500 (2.07696 iter/s, 48.1472s/100 iters), loss = 0.0072192
I1013 18:49:09.502187 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00722023 (* 1 = 0.00722023 loss)
I1013 18:49:09.502192 12690 sgd_solver.cpp:105] Iteration 77500, lr = 0.001
I1013 18:49:40.407605 12690 solver.cpp:218] Iteration 77600 (3.23568 iter/s, 30.9054s/100 iters), loss = 0.00445251
I1013 18:49:40.407712 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00445354 (* 1 = 0.00445354 loss)
I1013 18:49:40.407721 12690 sgd_solver.cpp:105] Iteration 77600, lr = 0.001
I1013 18:50:11.309269 12690 solver.cpp:218] Iteration 77700 (3.23608 iter/s, 30.9016s/100 iters), loss = 0.00543574
I1013 18:50:11.309424 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00543677 (* 1 = 0.00543677 loss)
I1013 18:50:11.309434 12690 sgd_solver.cpp:105] Iteration 77700, lr = 0.001
I1013 18:50:42.319067 12690 solver.cpp:218] Iteration 77800 (3.22482 iter/s, 31.0095s/100 iters), loss = 0.00588769
I1013 18:50:42.319223 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00588871 (* 1 = 0.00588871 loss)
I1013 18:50:42.319247 12690 sgd_solver.cpp:105] Iteration 77800, lr = 0.001
I1013 18:51:13.307157 12690 solver.cpp:218] Iteration 77900 (3.22706 iter/s, 30.9879s/100 iters), loss = 0.00199815
I1013 18:51:13.307289 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00199918 (* 1 = 0.00199918 loss)
I1013 18:51:13.307302 12690 sgd_solver.cpp:105] Iteration 77900, lr = 0.001
I1013 18:51:42.610806 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:51:43.841569 12690 solver.cpp:330] Iteration 78000, Testing net (#0)
I1013 18:52:00.653172 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:52:00.994175 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10288 (* 1 = 1.10288 loss)
I1013 18:52:00.994192 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 18:52:01.295235 12690 solver.cpp:218] Iteration 78000 (2.08386 iter/s, 47.988s/100 iters), loss = 0.00754703
I1013 18:52:01.295269 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00754805 (* 1 = 0.00754805 loss)
I1013 18:52:01.295279 12690 sgd_solver.cpp:105] Iteration 78000, lr = 0.001
I1013 18:52:32.360167 12690 solver.cpp:218] Iteration 78100 (3.21907 iter/s, 31.0649s/100 iters), loss = 0.00468477
I1013 18:52:32.360312 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0046858 (* 1 = 0.0046858 loss)
I1013 18:52:32.360321 12690 sgd_solver.cpp:105] Iteration 78100, lr = 0.001
I1013 18:53:03.250566 12690 solver.cpp:218] Iteration 78200 (3.23727 iter/s, 30.8903s/100 iters), loss = 0.00667433
I1013 18:53:03.250666 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00667535 (* 1 = 0.00667535 loss)
I1013 18:53:03.250674 12690 sgd_solver.cpp:105] Iteration 78200, lr = 0.001
I1013 18:53:34.261221 12690 solver.cpp:218] Iteration 78300 (3.22471 iter/s, 31.0106s/100 iters), loss = 0.00602311
I1013 18:53:34.261334 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00602413 (* 1 = 0.00602413 loss)
I1013 18:53:34.261353 12690 sgd_solver.cpp:105] Iteration 78300, lr = 0.001
I1013 18:54:05.129133 12690 solver.cpp:218] Iteration 78400 (3.23962 iter/s, 30.8678s/100 iters), loss = 0.0105832
I1013 18:54:05.129237 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0105842 (* 1 = 0.0105842 loss)
I1013 18:54:05.129256 12690 sgd_solver.cpp:105] Iteration 78400, lr = 0.001
I1013 18:54:35.694522 12690 solver.cpp:330] Iteration 78500, Testing net (#0)
I1013 18:54:52.572366 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:54:52.914324 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10192 (* 1 = 1.10192 loss)
I1013 18:54:52.914340 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7452
I1013 18:54:53.216534 12690 solver.cpp:218] Iteration 78500 (2.07955 iter/s, 48.0873s/100 iters), loss = 0.00521616
I1013 18:54:53.216562 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00521719 (* 1 = 0.00521719 loss)
I1013 18:54:53.216567 12690 sgd_solver.cpp:105] Iteration 78500, lr = 0.001
I1013 18:55:24.249788 12690 solver.cpp:218] Iteration 78600 (3.22235 iter/s, 31.0332s/100 iters), loss = 0.00272537
I1013 18:55:24.249886 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00272639 (* 1 = 0.00272639 loss)
I1013 18:55:24.249902 12690 sgd_solver.cpp:105] Iteration 78600, lr = 0.001
I1013 18:55:54.999689 12690 solver.cpp:218] Iteration 78700 (3.25205 iter/s, 30.7498s/100 iters), loss = 0.0116059
I1013 18:55:55.000402 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116069 (* 1 = 0.0116069 loss)
I1013 18:55:55.000412 12690 sgd_solver.cpp:105] Iteration 78700, lr = 0.001
I1013 18:56:25.931135 12690 solver.cpp:218] Iteration 78800 (3.23303 iter/s, 30.9307s/100 iters), loss = 0.0022837
I1013 18:56:25.931277 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00228472 (* 1 = 0.00228472 loss)
I1013 18:56:25.931288 12690 sgd_solver.cpp:105] Iteration 78800, lr = 0.001
I1013 18:56:56.842685 12690 solver.cpp:218] Iteration 78900 (3.23505 iter/s, 30.9114s/100 iters), loss = 0.00444754
I1013 18:56:56.842793 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00444856 (* 1 = 0.00444856 loss)
I1013 18:56:56.842809 12690 sgd_solver.cpp:105] Iteration 78900, lr = 0.001
I1013 18:57:26.242404 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:57:27.479363 12690 solver.cpp:330] Iteration 79000, Testing net (#0)
I1013 18:57:44.330632 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 18:57:44.671711 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10152 (* 1 = 1.10152 loss)
I1013 18:57:44.671727 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7463
I1013 18:57:44.973532 12690 solver.cpp:218] Iteration 79000 (2.07767 iter/s, 48.1308s/100 iters), loss = 0.00683046
I1013 18:57:44.973565 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00683148 (* 1 = 0.00683148 loss)
I1013 18:57:44.973572 12690 sgd_solver.cpp:105] Iteration 79000, lr = 0.001
I1013 18:58:15.945843 12690 solver.cpp:218] Iteration 79100 (3.22869 iter/s, 30.9723s/100 iters), loss = 0.00397723
I1013 18:58:15.945984 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00397825 (* 1 = 0.00397825 loss)
I1013 18:58:15.945992 12690 sgd_solver.cpp:105] Iteration 79100, lr = 0.001
I1013 18:58:46.918395 12690 solver.cpp:218] Iteration 79200 (3.22868 iter/s, 30.9724s/100 iters), loss = 0.00590537
I1013 18:58:46.918500 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00590639 (* 1 = 0.00590639 loss)
I1013 18:58:46.918509 12690 sgd_solver.cpp:105] Iteration 79200, lr = 0.001
I1013 18:59:17.877888 12690 solver.cpp:218] Iteration 79300 (3.23004 iter/s, 30.9594s/100 iters), loss = 0.0123786
I1013 18:59:17.878002 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123796 (* 1 = 0.0123796 loss)
I1013 18:59:17.878015 12690 sgd_solver.cpp:105] Iteration 79300, lr = 0.001
I1013 18:59:48.862772 12690 solver.cpp:218] Iteration 79400 (3.22739 iter/s, 30.9848s/100 iters), loss = 0.0038657
I1013 18:59:48.862917 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00386672 (* 1 = 0.00386672 loss)
I1013 18:59:48.862926 12690 sgd_solver.cpp:105] Iteration 79400, lr = 0.001
I1013 19:00:19.503654 12690 solver.cpp:330] Iteration 79500, Testing net (#0)
I1013 19:00:36.303323 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:00:36.650600 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10105 (* 1 = 1.10105 loss)
I1013 19:00:36.650619 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7457
I1013 19:00:36.958592 12690 solver.cpp:218] Iteration 79500 (2.07919 iter/s, 48.0957s/100 iters), loss = 0.00842245
I1013 19:00:36.958629 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00842347 (* 1 = 0.00842347 loss)
I1013 19:00:36.958636 12690 sgd_solver.cpp:105] Iteration 79500, lr = 0.001
I1013 19:01:07.893587 12690 solver.cpp:218] Iteration 79600 (3.23259 iter/s, 30.935s/100 iters), loss = 0.00467649
I1013 19:01:07.893725 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00467751 (* 1 = 0.00467751 loss)
I1013 19:01:07.893746 12690 sgd_solver.cpp:105] Iteration 79600, lr = 0.001
I1013 19:01:38.842923 12690 solver.cpp:218] Iteration 79700 (3.2311 iter/s, 30.9492s/100 iters), loss = 0.0059788
I1013 19:01:38.843039 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00597982 (* 1 = 0.00597982 loss)
I1013 19:01:38.843047 12690 sgd_solver.cpp:105] Iteration 79700, lr = 0.001
I1013 19:02:09.867471 12690 solver.cpp:218] Iteration 79800 (3.22326 iter/s, 31.0244s/100 iters), loss = 0.00625498
I1013 19:02:09.867588 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.006256 (* 1 = 0.006256 loss)
I1013 19:02:09.867596 12690 sgd_solver.cpp:105] Iteration 79800, lr = 0.001
I1013 19:02:40.816936 12690 solver.cpp:218] Iteration 79900 (3.23108 iter/s, 30.9494s/100 iters), loss = 0.00436147
I1013 19:02:40.817031 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00436249 (* 1 = 0.00436249 loss)
I1013 19:02:40.817049 12690 sgd_solver.cpp:105] Iteration 79900, lr = 0.001
I1013 19:03:10.206126 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:03:11.437399 12690 solver.cpp:330] Iteration 80000, Testing net (#0)
I1013 19:03:28.250308 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:03:28.592988 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.1008 (* 1 = 1.1008 loss)
I1013 19:03:28.593003 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 19:03:28.895999 12690 solver.cpp:218] Iteration 80000 (2.07991 iter/s, 48.079s/100 iters), loss = 0.00462867
I1013 19:03:28.896033 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00462969 (* 1 = 0.00462969 loss)
I1013 19:03:28.896039 12690 sgd_solver.cpp:46] MultiStep Status: Iteration 80000, step = 2
I1013 19:03:28.896041 12690 sgd_solver.cpp:105] Iteration 80000, lr = 0.0001
I1013 19:03:59.946118 12690 solver.cpp:218] Iteration 80100 (3.2206 iter/s, 31.0501s/100 iters), loss = 0.00910413
I1013 19:03:59.946262 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00910515 (* 1 = 0.00910515 loss)
I1013 19:03:59.946272 12690 sgd_solver.cpp:105] Iteration 80100, lr = 0.0001
I1013 19:04:30.820598 12690 solver.cpp:218] Iteration 80200 (3.23893 iter/s, 30.8743s/100 iters), loss = 0.00534421
I1013 19:04:30.820744 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00534522 (* 1 = 0.00534522 loss)
I1013 19:04:30.820752 12690 sgd_solver.cpp:105] Iteration 80200, lr = 0.0001
I1013 19:05:01.643730 12690 solver.cpp:218] Iteration 80300 (3.24433 iter/s, 30.823s/100 iters), loss = 0.00806916
I1013 19:05:01.643841 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00807018 (* 1 = 0.00807018 loss)
I1013 19:05:01.643849 12690 sgd_solver.cpp:105] Iteration 80300, lr = 0.0001
I1013 19:05:32.614044 12690 solver.cpp:218] Iteration 80400 (3.22891 iter/s, 30.9702s/100 iters), loss = 0.00688157
I1013 19:05:32.614198 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00688259 (* 1 = 0.00688259 loss)
I1013 19:05:32.614208 12690 sgd_solver.cpp:105] Iteration 80400, lr = 0.0001
I1013 19:06:03.216984 12690 solver.cpp:330] Iteration 80500, Testing net (#0)
I1013 19:06:19.939719 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:06:20.279089 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10064 (* 1 = 1.10064 loss)
I1013 19:06:20.279104 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7452
I1013 19:06:20.582428 12690 solver.cpp:218] Iteration 80500 (2.08471 iter/s, 47.9683s/100 iters), loss = 0.0139162
I1013 19:06:20.582458 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0139173 (* 1 = 0.0139173 loss)
I1013 19:06:20.582464 12690 sgd_solver.cpp:105] Iteration 80500, lr = 0.0001
I1013 19:06:51.344095 12690 solver.cpp:218] Iteration 80600 (3.2508 iter/s, 30.7616s/100 iters), loss = 0.00282002
I1013 19:06:51.344235 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00282104 (* 1 = 0.00282104 loss)
I1013 19:06:51.344244 12690 sgd_solver.cpp:105] Iteration 80600, lr = 0.0001
I1013 19:07:22.164980 12690 solver.cpp:218] Iteration 80700 (3.24457 iter/s, 30.8208s/100 iters), loss = 0.00472888
I1013 19:07:22.165119 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0047299 (* 1 = 0.0047299 loss)
I1013 19:07:22.165128 12690 sgd_solver.cpp:105] Iteration 80700, lr = 0.0001
I1013 19:07:52.921900 12690 solver.cpp:218] Iteration 80800 (3.25131 iter/s, 30.7568s/100 iters), loss = 0.00366441
I1013 19:07:52.922024 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00366543 (* 1 = 0.00366543 loss)
I1013 19:07:52.922042 12690 sgd_solver.cpp:105] Iteration 80800, lr = 0.0001
I1013 19:08:23.710691 12690 solver.cpp:218] Iteration 80900 (3.24795 iter/s, 30.7887s/100 iters), loss = 0.00303242
I1013 19:08:23.710801 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00303344 (* 1 = 0.00303344 loss)
I1013 19:08:23.710809 12690 sgd_solver.cpp:105] Iteration 80900, lr = 0.0001
I1013 19:08:52.899899 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:08:54.125008 12690 solver.cpp:330] Iteration 81000, Testing net (#0)
I1013 19:09:10.838626 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:09:11.179163 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10037 (* 1 = 1.10037 loss)
I1013 19:09:11.179179 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7453
I1013 19:09:11.482326 12690 solver.cpp:218] Iteration 81000 (2.0933 iter/s, 47.7716s/100 iters), loss = 0.00912404
I1013 19:09:11.482359 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00912505 (* 1 = 0.00912505 loss)
I1013 19:09:11.482367 12690 sgd_solver.cpp:105] Iteration 81000, lr = 0.0001
I1013 19:09:42.219538 12690 solver.cpp:218] Iteration 81100 (3.25339 iter/s, 30.7372s/100 iters), loss = 0.0039949
I1013 19:09:42.219638 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00399592 (* 1 = 0.00399592 loss)
I1013 19:09:42.219645 12690 sgd_solver.cpp:105] Iteration 81100, lr = 0.0001
I1013 19:10:12.983878 12690 solver.cpp:218] Iteration 81200 (3.25053 iter/s, 30.7643s/100 iters), loss = 0.00652846
I1013 19:10:12.984019 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00652947 (* 1 = 0.00652947 loss)
I1013 19:10:12.984027 12690 sgd_solver.cpp:105] Iteration 81200, lr = 0.0001
I1013 19:10:43.696110 12690 solver.cpp:218] Iteration 81300 (3.25605 iter/s, 30.7121s/100 iters), loss = 0.00436503
I1013 19:10:43.696214 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00436604 (* 1 = 0.00436604 loss)
I1013 19:10:43.696233 12690 sgd_solver.cpp:105] Iteration 81300, lr = 0.0001
I1013 19:11:14.456058 12690 solver.cpp:218] Iteration 81400 (3.25099 iter/s, 30.7599s/100 iters), loss = 0.0071525
I1013 19:11:14.456198 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00715351 (* 1 = 0.00715351 loss)
I1013 19:11:14.456207 12690 sgd_solver.cpp:105] Iteration 81400, lr = 0.0001
I1013 19:11:44.846555 12690 solver.cpp:330] Iteration 81500, Testing net (#0)
I1013 19:12:01.569851 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:12:01.912111 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.10009 (* 1 = 1.10009 loss)
I1013 19:12:01.912127 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7453
I1013 19:12:02.216284 12690 solver.cpp:218] Iteration 81500 (2.0938 iter/s, 47.7601s/100 iters), loss = 0.00575906
I1013 19:12:02.216311 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00576008 (* 1 = 0.00576008 loss)
I1013 19:12:02.216318 12690 sgd_solver.cpp:105] Iteration 81500, lr = 0.0001
I1013 19:12:32.979382 12690 solver.cpp:218] Iteration 81600 (3.25065 iter/s, 30.7631s/100 iters), loss = 0.0110627
I1013 19:12:32.979498 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110638 (* 1 = 0.0110638 loss)
I1013 19:12:32.979516 12690 sgd_solver.cpp:105] Iteration 81600, lr = 0.0001
I1013 19:13:03.725358 12690 solver.cpp:218] Iteration 81700 (3.25247 iter/s, 30.7459s/100 iters), loss = 0.0197725
I1013 19:13:03.725471 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0197735 (* 1 = 0.0197735 loss)
I1013 19:13:03.725491 12690 sgd_solver.cpp:105] Iteration 81700, lr = 0.0001
I1013 19:13:34.435813 12690 solver.cpp:218] Iteration 81800 (3.25623 iter/s, 30.7104s/100 iters), loss = 0.00631201
I1013 19:13:34.435892 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00631303 (* 1 = 0.00631303 loss)
I1013 19:13:34.435909 12690 sgd_solver.cpp:105] Iteration 81800, lr = 0.0001
I1013 19:14:05.130758 12690 solver.cpp:218] Iteration 81900 (3.25787 iter/s, 30.6949s/100 iters), loss = 0.0024385
I1013 19:14:05.130887 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00243952 (* 1 = 0.00243952 loss)
I1013 19:14:05.130897 12690 sgd_solver.cpp:105] Iteration 81900, lr = 0.0001
I1013 19:14:34.322350 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:14:35.550714 12690 solver.cpp:330] Iteration 82000, Testing net (#0)
I1013 19:14:52.254930 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:14:52.596321 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09982 (* 1 = 1.09982 loss)
I1013 19:14:52.596338 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7454
I1013 19:14:52.899451 12690 solver.cpp:218] Iteration 82000 (2.09343 iter/s, 47.7686s/100 iters), loss = 0.00444172
I1013 19:14:52.899480 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00444274 (* 1 = 0.00444274 loss)
I1013 19:14:52.899488 12690 sgd_solver.cpp:105] Iteration 82000, lr = 0.0001
I1013 19:15:23.619932 12690 solver.cpp:218] Iteration 82100 (3.25516 iter/s, 30.7205s/100 iters), loss = 0.0061764
I1013 19:15:23.620417 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00617741 (* 1 = 0.00617741 loss)
I1013 19:15:23.620426 12690 sgd_solver.cpp:105] Iteration 82100, lr = 0.0001
I1013 19:15:54.317181 12690 solver.cpp:218] Iteration 82200 (3.25767 iter/s, 30.6968s/100 iters), loss = 0.0079062
I1013 19:15:54.317292 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00790721 (* 1 = 0.00790721 loss)
I1013 19:15:54.317311 12690 sgd_solver.cpp:105] Iteration 82200, lr = 0.0001
I1013 19:16:25.019079 12690 solver.cpp:218] Iteration 82300 (3.25714 iter/s, 30.7018s/100 iters), loss = 0.00756812
I1013 19:16:25.019215 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00756913 (* 1 = 0.00756913 loss)
I1013 19:16:25.019224 12690 sgd_solver.cpp:105] Iteration 82300, lr = 0.0001
I1013 19:16:55.723696 12690 solver.cpp:218] Iteration 82400 (3.25685 iter/s, 30.7045s/100 iters), loss = 0.00531431
I1013 19:16:55.723796 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00531532 (* 1 = 0.00531532 loss)
I1013 19:16:55.723814 12690 sgd_solver.cpp:105] Iteration 82400, lr = 0.0001
I1013 19:17:26.120937 12690 solver.cpp:330] Iteration 82500, Testing net (#0)
I1013 19:17:42.793694 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:17:43.134328 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09945 (* 1 = 1.09945 loss)
I1013 19:17:43.134344 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7456
I1013 19:17:43.435336 12690 solver.cpp:218] Iteration 82500 (2.09593 iter/s, 47.7116s/100 iters), loss = 0.0098014
I1013 19:17:43.435370 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00980242 (* 1 = 0.00980242 loss)
I1013 19:17:43.435377 12690 sgd_solver.cpp:105] Iteration 82500, lr = 0.0001
I1013 19:18:14.164655 12690 solver.cpp:218] Iteration 82600 (3.25422 iter/s, 30.7293s/100 iters), loss = 0.00386155
I1013 19:18:14.164796 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00386256 (* 1 = 0.00386256 loss)
I1013 19:18:14.164805 12690 sgd_solver.cpp:105] Iteration 82600, lr = 0.0001
I1013 19:18:44.871820 12690 solver.cpp:218] Iteration 82700 (3.25658 iter/s, 30.707s/100 iters), loss = 0.00322736
I1013 19:18:44.871959 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00322837 (* 1 = 0.00322837 loss)
I1013 19:18:44.871968 12690 sgd_solver.cpp:105] Iteration 82700, lr = 0.0001
I1013 19:19:15.545024 12690 solver.cpp:218] Iteration 82800 (3.26019 iter/s, 30.6731s/100 iters), loss = 0.00364893
I1013 19:19:15.545123 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00364995 (* 1 = 0.00364995 loss)
I1013 19:19:15.545131 12690 sgd_solver.cpp:105] Iteration 82800, lr = 0.0001
I1013 19:19:46.271930 12690 solver.cpp:218] Iteration 82900 (3.25449 iter/s, 30.7268s/100 iters), loss = 0.00330935
I1013 19:19:46.272032 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00331037 (* 1 = 0.00331037 loss)
I1013 19:19:46.272050 12690 sgd_solver.cpp:105] Iteration 82900, lr = 0.0001
I1013 19:20:15.428877 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:20:16.657932 12690 solver.cpp:330] Iteration 83000, Testing net (#0)
I1013 19:20:33.353489 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:20:33.693809 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09916 (* 1 = 1.09916 loss)
I1013 19:20:33.693825 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 19:20:33.996860 12690 solver.cpp:218] Iteration 83000 (2.09534 iter/s, 47.7248s/100 iters), loss = 0.00441443
I1013 19:20:33.996889 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00441544 (* 1 = 0.00441544 loss)
I1013 19:20:33.996896 12690 sgd_solver.cpp:105] Iteration 83000, lr = 0.0001
I1013 19:21:04.754467 12690 solver.cpp:218] Iteration 83100 (3.25123 iter/s, 30.7576s/100 iters), loss = 0.0057121
I1013 19:21:04.754616 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00571312 (* 1 = 0.00571312 loss)
I1013 19:21:04.754623 12690 sgd_solver.cpp:105] Iteration 83100, lr = 0.0001
I1013 19:21:35.455962 12690 solver.cpp:218] Iteration 83200 (3.25718 iter/s, 30.7014s/100 iters), loss = 0.00329234
I1013 19:21:35.456096 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00329335 (* 1 = 0.00329335 loss)
I1013 19:21:35.456105 12690 sgd_solver.cpp:105] Iteration 83200, lr = 0.0001
I1013 19:22:06.170558 12690 solver.cpp:218] Iteration 83300 (3.25579 iter/s, 30.7145s/100 iters), loss = 0.0049191
I1013 19:22:06.170667 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00492011 (* 1 = 0.00492011 loss)
I1013 19:22:06.170684 12690 sgd_solver.cpp:105] Iteration 83300, lr = 0.0001
I1013 19:22:36.836434 12690 solver.cpp:218] Iteration 83400 (3.26096 iter/s, 30.6658s/100 iters), loss = 0.00729981
I1013 19:22:36.836581 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00730082 (* 1 = 0.00730082 loss)
I1013 19:22:36.836591 12690 sgd_solver.cpp:105] Iteration 83400, lr = 0.0001
I1013 19:23:07.268317 12690 solver.cpp:330] Iteration 83500, Testing net (#0)
I1013 19:23:23.937883 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:23:24.277922 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09888 (* 1 = 1.09888 loss)
I1013 19:23:24.277938 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 19:23:24.581387 12690 solver.cpp:218] Iteration 83500 (2.09447 iter/s, 47.7448s/100 iters), loss = 0.00741494
I1013 19:23:24.581418 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00741595 (* 1 = 0.00741595 loss)
I1013 19:23:24.581425 12690 sgd_solver.cpp:105] Iteration 83500, lr = 0.0001
I1013 19:23:55.269661 12690 solver.cpp:218] Iteration 83600 (3.25858 iter/s, 30.6883s/100 iters), loss = 0.00379917
I1013 19:23:55.269793 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00380018 (* 1 = 0.00380018 loss)
I1013 19:23:55.269804 12690 sgd_solver.cpp:105] Iteration 83600, lr = 0.0001
I1013 19:24:25.957990 12690 solver.cpp:218] Iteration 83700 (3.25858 iter/s, 30.6882s/100 iters), loss = 0.00460243
I1013 19:24:25.958087 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00460344 (* 1 = 0.00460344 loss)
I1013 19:24:25.958103 12690 sgd_solver.cpp:105] Iteration 83700, lr = 0.0001
I1013 19:24:56.637537 12690 solver.cpp:218] Iteration 83800 (3.25951 iter/s, 30.6795s/100 iters), loss = 0.00384861
I1013 19:24:56.637661 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00384962 (* 1 = 0.00384962 loss)
I1013 19:24:56.637668 12690 sgd_solver.cpp:105] Iteration 83800, lr = 0.0001
I1013 19:25:27.342429 12690 solver.cpp:218] Iteration 83900 (3.25682 iter/s, 30.7048s/100 iters), loss = 0.00230871
I1013 19:25:27.342521 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00230972 (* 1 = 0.00230972 loss)
I1013 19:25:27.342530 12690 sgd_solver.cpp:105] Iteration 83900, lr = 0.0001
I1013 19:25:56.480362 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:25:57.705369 12690 solver.cpp:330] Iteration 84000, Testing net (#0)
I1013 19:26:14.399657 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:26:14.741521 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09855 (* 1 = 1.09855 loss)
I1013 19:26:14.741538 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7454
I1013 19:26:15.048108 12690 solver.cpp:218] Iteration 84000 (2.09619 iter/s, 47.7056s/100 iters), loss = 0.00523554
I1013 19:26:15.048140 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00523655 (* 1 = 0.00523655 loss)
I1013 19:26:15.048146 12690 sgd_solver.cpp:105] Iteration 84000, lr = 0.0001
I1013 19:26:45.751597 12690 solver.cpp:218] Iteration 84100 (3.25696 iter/s, 30.7035s/100 iters), loss = 0.00629059
I1013 19:26:45.751724 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0062916 (* 1 = 0.0062916 loss)
I1013 19:26:45.751731 12690 sgd_solver.cpp:105] Iteration 84100, lr = 0.0001
I1013 19:27:16.450808 12690 solver.cpp:218] Iteration 84200 (3.25742 iter/s, 30.6991s/100 iters), loss = 0.00850462
I1013 19:27:16.450950 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00850563 (* 1 = 0.00850563 loss)
I1013 19:27:16.450958 12690 sgd_solver.cpp:105] Iteration 84200, lr = 0.0001
I1013 19:27:47.179543 12690 solver.cpp:218] Iteration 84300 (3.2543 iter/s, 30.7286s/100 iters), loss = 0.00720337
I1013 19:27:47.179669 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00720438 (* 1 = 0.00720438 loss)
I1013 19:27:47.179678 12690 sgd_solver.cpp:105] Iteration 84300, lr = 0.0001
I1013 19:28:17.838320 12690 solver.cpp:218] Iteration 84400 (3.26172 iter/s, 30.6587s/100 iters), loss = 0.0107854
I1013 19:28:17.838416 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107864 (* 1 = 0.0107864 loss)
I1013 19:28:17.838424 12690 sgd_solver.cpp:105] Iteration 84400, lr = 0.0001
I1013 19:28:48.240175 12690 solver.cpp:330] Iteration 84500, Testing net (#0)
I1013 19:29:04.929502 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:29:05.272387 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09841 (* 1 = 1.09841 loss)
I1013 19:29:05.272403 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 19:29:05.573014 12690 solver.cpp:218] Iteration 84500 (2.09492 iter/s, 47.7346s/100 iters), loss = 0.00671383
I1013 19:29:05.573045 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00671484 (* 1 = 0.00671484 loss)
I1013 19:29:05.573052 12690 sgd_solver.cpp:105] Iteration 84500, lr = 0.0001
I1013 19:29:36.270105 12690 solver.cpp:218] Iteration 84600 (3.25764 iter/s, 30.6971s/100 iters), loss = 0.00912997
I1013 19:29:36.270191 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00913099 (* 1 = 0.00913099 loss)
I1013 19:29:36.270200 12690 sgd_solver.cpp:105] Iteration 84600, lr = 0.0001
I1013 19:30:06.984721 12690 solver.cpp:218] Iteration 84700 (3.25579 iter/s, 30.7145s/100 iters), loss = 0.00430218
I1013 19:30:06.984843 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00430319 (* 1 = 0.00430319 loss)
I1013 19:30:06.984850 12690 sgd_solver.cpp:105] Iteration 84700, lr = 0.0001
I1013 19:30:37.700479 12690 solver.cpp:218] Iteration 84800 (3.25567 iter/s, 30.7156s/100 iters), loss = 0.00330688
I1013 19:30:37.700588 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0033079 (* 1 = 0.0033079 loss)
I1013 19:30:37.700605 12690 sgd_solver.cpp:105] Iteration 84800, lr = 0.0001
I1013 19:31:08.398856 12690 solver.cpp:218] Iteration 84900 (3.25751 iter/s, 30.6983s/100 iters), loss = 0.00581801
I1013 19:31:08.398996 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00581903 (* 1 = 0.00581903 loss)
I1013 19:31:08.399005 12690 sgd_solver.cpp:105] Iteration 84900, lr = 0.0001
I1013 19:31:37.519091 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:31:38.744143 12690 solver.cpp:330] Iteration 85000, Testing net (#0)
I1013 19:31:55.432814 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:31:55.774328 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09818 (* 1 = 1.09818 loss)
I1013 19:31:55.774343 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7463
I1013 19:31:56.078591 12690 solver.cpp:218] Iteration 85000 (2.09733 iter/s, 47.6796s/100 iters), loss = 0.00318704
I1013 19:31:56.078621 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00318805 (* 1 = 0.00318805 loss)
I1013 19:31:56.078629 12690 sgd_solver.cpp:105] Iteration 85000, lr = 0.0001
I1013 19:32:26.738087 12690 solver.cpp:218] Iteration 85100 (3.26163 iter/s, 30.6595s/100 iters), loss = 0.0031135
I1013 19:32:26.738260 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00311452 (* 1 = 0.00311452 loss)
I1013 19:32:26.738281 12690 sgd_solver.cpp:105] Iteration 85100, lr = 0.0001
I1013 19:32:57.467433 12690 solver.cpp:218] Iteration 85200 (3.25424 iter/s, 30.7292s/100 iters), loss = 0.00415934
I1013 19:32:57.467559 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00416035 (* 1 = 0.00416035 loss)
I1013 19:32:57.467568 12690 sgd_solver.cpp:105] Iteration 85200, lr = 0.0001
I1013 19:33:28.179050 12690 solver.cpp:218] Iteration 85300 (3.25611 iter/s, 30.7115s/100 iters), loss = 0.00919766
I1013 19:33:28.179159 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00919867 (* 1 = 0.00919867 loss)
I1013 19:33:28.179177 12690 sgd_solver.cpp:105] Iteration 85300, lr = 0.0001
I1013 19:33:58.906211 12690 solver.cpp:218] Iteration 85400 (3.25446 iter/s, 30.7271s/100 iters), loss = 0.00571702
I1013 19:33:58.906347 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00571803 (* 1 = 0.00571803 loss)
I1013 19:33:58.906357 12690 sgd_solver.cpp:105] Iteration 85400, lr = 0.0001
I1013 19:34:29.256708 12690 solver.cpp:330] Iteration 85500, Testing net (#0)
I1013 19:34:45.929311 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:34:46.272366 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.0981 (* 1 = 1.0981 loss)
I1013 19:34:46.272380 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7463
I1013 19:34:46.575278 12690 solver.cpp:218] Iteration 85500 (2.0978 iter/s, 47.6689s/100 iters), loss = 0.00452229
I1013 19:34:46.575309 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0045233 (* 1 = 0.0045233 loss)
I1013 19:34:46.575317 12690 sgd_solver.cpp:105] Iteration 85500, lr = 0.0001
I1013 19:35:17.287497 12690 solver.cpp:218] Iteration 85600 (3.25604 iter/s, 30.7122s/100 iters), loss = 0.00541399
I1013 19:35:17.287636 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.005415 (* 1 = 0.005415 loss)
I1013 19:35:17.287645 12690 sgd_solver.cpp:105] Iteration 85600, lr = 0.0001
I1013 19:35:47.978593 12690 solver.cpp:218] Iteration 85700 (3.25829 iter/s, 30.691s/100 iters), loss = 0.00411625
I1013 19:35:47.978682 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00411727 (* 1 = 0.00411727 loss)
I1013 19:35:47.978699 12690 sgd_solver.cpp:105] Iteration 85700, lr = 0.0001
I1013 19:36:18.635365 12690 solver.cpp:218] Iteration 85800 (3.26193 iter/s, 30.6567s/100 iters), loss = 0.00691849
I1013 19:36:18.635537 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0069195 (* 1 = 0.0069195 loss)
I1013 19:36:18.635548 12690 sgd_solver.cpp:105] Iteration 85800, lr = 0.0001
I1013 19:36:49.268097 12690 solver.cpp:218] Iteration 85900 (3.2645 iter/s, 30.6326s/100 iters), loss = 0.00453694
I1013 19:36:49.268199 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00453796 (* 1 = 0.00453796 loss)
I1013 19:36:49.268218 12690 sgd_solver.cpp:105] Iteration 85900, lr = 0.0001
I1013 19:37:18.467087 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:37:19.692606 12690 solver.cpp:330] Iteration 86000, Testing net (#0)
I1013 19:37:36.363375 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:37:36.706254 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.0979 (* 1 = 1.0979 loss)
I1013 19:37:36.706270 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7462
I1013 19:37:37.011564 12690 solver.cpp:218] Iteration 86000 (2.09453 iter/s, 47.7434s/100 iters), loss = 0.00478755
I1013 19:37:37.011592 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00478856 (* 1 = 0.00478856 loss)
I1013 19:37:37.011598 12690 sgd_solver.cpp:105] Iteration 86000, lr = 0.0001
I1013 19:38:07.718482 12690 solver.cpp:218] Iteration 86100 (3.2566 iter/s, 30.7069s/100 iters), loss = 0.00405233
I1013 19:38:07.718634 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00405334 (* 1 = 0.00405334 loss)
I1013 19:38:07.718659 12690 sgd_solver.cpp:105] Iteration 86100, lr = 0.0001
I1013 19:38:38.438269 12690 solver.cpp:218] Iteration 86200 (3.25525 iter/s, 30.7196s/100 iters), loss = 0.00804931
I1013 19:38:38.438408 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00805032 (* 1 = 0.00805032 loss)
I1013 19:38:38.438416 12690 sgd_solver.cpp:105] Iteration 86200, lr = 0.0001
I1013 19:39:09.176069 12690 solver.cpp:218] Iteration 86300 (3.25334 iter/s, 30.7377s/100 iters), loss = 0.0109006
I1013 19:39:09.176209 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109016 (* 1 = 0.0109016 loss)
I1013 19:39:09.176218 12690 sgd_solver.cpp:105] Iteration 86300, lr = 0.0001
I1013 19:39:39.873257 12690 solver.cpp:218] Iteration 86400 (3.25764 iter/s, 30.6971s/100 iters), loss = 0.00673044
I1013 19:39:39.873373 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00673145 (* 1 = 0.00673145 loss)
I1013 19:39:39.873381 12690 sgd_solver.cpp:105] Iteration 86400, lr = 0.0001
I1013 19:40:10.238570 12690 solver.cpp:330] Iteration 86500, Testing net (#0)
I1013 19:40:26.958495 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:40:27.299860 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09789 (* 1 = 1.09789 loss)
I1013 19:40:27.299875 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7463
I1013 19:40:27.601613 12690 solver.cpp:218] Iteration 86500 (2.09519 iter/s, 47.7283s/100 iters), loss = 0.011314
I1013 19:40:27.601649 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.011315 (* 1 = 0.011315 loss)
I1013 19:40:27.601655 12690 sgd_solver.cpp:105] Iteration 86500, lr = 0.0001
I1013 19:40:58.275913 12690 solver.cpp:218] Iteration 86600 (3.26006 iter/s, 30.6743s/100 iters), loss = 0.005996
I1013 19:40:58.276023 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00599701 (* 1 = 0.00599701 loss)
I1013 19:40:58.276031 12690 sgd_solver.cpp:105] Iteration 86600, lr = 0.0001
I1013 19:41:28.938812 12690 solver.cpp:218] Iteration 86700 (3.26128 iter/s, 30.6628s/100 iters), loss = 0.00312127
I1013 19:41:28.938956 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00312228 (* 1 = 0.00312228 loss)
I1013 19:41:28.938966 12690 sgd_solver.cpp:105] Iteration 86700, lr = 0.0001
I1013 19:41:59.658323 12690 solver.cpp:218] Iteration 86800 (3.25527 iter/s, 30.7194s/100 iters), loss = 0.00140544
I1013 19:41:59.658437 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00140645 (* 1 = 0.00140645 loss)
I1013 19:41:59.658453 12690 sgd_solver.cpp:105] Iteration 86800, lr = 0.0001
I1013 19:42:30.337971 12690 solver.cpp:218] Iteration 86900 (3.2595 iter/s, 30.6796s/100 iters), loss = 0.00323482
I1013 19:42:30.338112 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00323583 (* 1 = 0.00323583 loss)
I1013 19:42:30.338121 12690 sgd_solver.cpp:105] Iteration 86900, lr = 0.0001
I1013 19:42:59.515974 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:43:00.744385 12690 solver.cpp:330] Iteration 87000, Testing net (#0)
I1013 19:43:17.418267 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:43:17.759845 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.0977 (* 1 = 1.0977 loss)
I1013 19:43:17.759861 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7465
I1013 19:43:18.062142 12690 solver.cpp:218] Iteration 87000 (2.09538 iter/s, 47.7241s/100 iters), loss = 0.00760051
I1013 19:43:18.062175 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00760153 (* 1 = 0.00760153 loss)
I1013 19:43:18.062181 12690 sgd_solver.cpp:105] Iteration 87000, lr = 0.0001
I1013 19:43:48.801118 12690 solver.cpp:218] Iteration 87100 (3.2532 iter/s, 30.739s/100 iters), loss = 0.00461154
I1013 19:43:48.801245 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00461255 (* 1 = 0.00461255 loss)
I1013 19:43:48.801254 12690 sgd_solver.cpp:105] Iteration 87100, lr = 0.0001
I1013 19:44:19.474956 12690 solver.cpp:218] Iteration 87200 (3.26012 iter/s, 30.6737s/100 iters), loss = 0.00431153
I1013 19:44:19.475036 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00431255 (* 1 = 0.00431255 loss)
I1013 19:44:19.475054 12690 sgd_solver.cpp:105] Iteration 87200, lr = 0.0001
I1013 19:44:50.165215 12690 solver.cpp:218] Iteration 87300 (3.25837 iter/s, 30.6902s/100 iters), loss = 0.00725207
I1013 19:44:50.165323 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00725309 (* 1 = 0.00725309 loss)
I1013 19:44:50.165339 12690 sgd_solver.cpp:105] Iteration 87300, lr = 0.0001
I1013 19:45:20.791954 12690 solver.cpp:218] Iteration 87400 (3.26513 iter/s, 30.6266s/100 iters), loss = 0.00405956
I1013 19:45:20.792073 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00406057 (* 1 = 0.00406057 loss)
I1013 19:45:20.792083 12690 sgd_solver.cpp:105] Iteration 87400, lr = 0.0001
I1013 19:45:51.195065 12690 solver.cpp:330] Iteration 87500, Testing net (#0)
I1013 19:46:07.865130 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:46:08.205184 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.0977 (* 1 = 1.0977 loss)
I1013 19:46:08.205199 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7461
I1013 19:46:08.507632 12690 solver.cpp:218] Iteration 87500 (2.09575 iter/s, 47.7156s/100 iters), loss = 0.00604313
I1013 19:46:08.507670 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00604415 (* 1 = 0.00604415 loss)
I1013 19:46:08.507678 12690 sgd_solver.cpp:105] Iteration 87500, lr = 0.0001
I1013 19:46:39.153429 12690 solver.cpp:218] Iteration 87600 (3.26309 iter/s, 30.6458s/100 iters), loss = 0.0030693
I1013 19:46:39.153570 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00307031 (* 1 = 0.00307031 loss)
I1013 19:46:39.153579 12690 sgd_solver.cpp:105] Iteration 87600, lr = 0.0001
I1013 19:47:09.832446 12690 solver.cpp:218] Iteration 87700 (3.25957 iter/s, 30.6789s/100 iters), loss = 0.00560739
I1013 19:47:09.832592 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0056084 (* 1 = 0.0056084 loss)
I1013 19:47:09.832602 12690 sgd_solver.cpp:105] Iteration 87700, lr = 0.0001
I1013 19:47:40.558797 12690 solver.cpp:218] Iteration 87800 (3.25455 iter/s, 30.7262s/100 iters), loss = 0.00260868
I1013 19:47:40.558892 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00260969 (* 1 = 0.00260969 loss)
I1013 19:47:40.558898 12690 sgd_solver.cpp:105] Iteration 87800, lr = 0.0001
I1013 19:48:11.234812 12690 solver.cpp:218] Iteration 87900 (3.25988 iter/s, 30.6759s/100 iters), loss = 0.00555058
I1013 19:48:11.234921 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00555159 (* 1 = 0.00555159 loss)
I1013 19:48:11.234930 12690 sgd_solver.cpp:105] Iteration 87900, lr = 0.0001
I1013 19:48:40.369794 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:48:41.596832 12690 solver.cpp:330] Iteration 88000, Testing net (#0)
I1013 19:48:58.313699 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:48:58.655447 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09743 (* 1 = 1.09743 loss)
I1013 19:48:58.655463 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 19:48:58.959440 12690 solver.cpp:218] Iteration 88000 (2.09536 iter/s, 47.7245s/100 iters), loss = 0.0053115
I1013 19:48:58.959473 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00531251 (* 1 = 0.00531251 loss)
I1013 19:48:58.959481 12690 sgd_solver.cpp:105] Iteration 88000, lr = 0.0001
I1013 19:49:29.619719 12690 solver.cpp:218] Iteration 88100 (3.26155 iter/s, 30.6603s/100 iters), loss = 0.00847867
I1013 19:49:29.619818 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00847969 (* 1 = 0.00847969 loss)
I1013 19:49:29.619827 12690 sgd_solver.cpp:105] Iteration 88100, lr = 0.0001
I1013 19:50:00.266782 12690 solver.cpp:218] Iteration 88200 (3.26296 iter/s, 30.647s/100 iters), loss = 0.00686042
I1013 19:50:00.266906 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00686144 (* 1 = 0.00686144 loss)
I1013 19:50:00.266916 12690 sgd_solver.cpp:105] Iteration 88200, lr = 0.0001
I1013 19:50:30.998414 12690 solver.cpp:218] Iteration 88300 (3.25399 iter/s, 30.7315s/100 iters), loss = 0.00815337
I1013 19:50:30.998824 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00815439 (* 1 = 0.00815439 loss)
I1013 19:50:30.998842 12690 sgd_solver.cpp:105] Iteration 88300, lr = 0.0001
I1013 19:51:01.730751 12690 solver.cpp:218] Iteration 88400 (3.25394 iter/s, 30.7319s/100 iters), loss = 0.0112968
I1013 19:51:01.730895 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112978 (* 1 = 0.0112978 loss)
I1013 19:51:01.730904 12690 sgd_solver.cpp:105] Iteration 88400, lr = 0.0001
I1013 19:51:32.122311 12690 solver.cpp:330] Iteration 88500, Testing net (#0)
I1013 19:51:48.818678 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:51:49.160682 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09731 (* 1 = 1.09731 loss)
I1013 19:51:49.160699 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7461
I1013 19:51:49.461588 12690 solver.cpp:218] Iteration 88500 (2.09509 iter/s, 47.7307s/100 iters), loss = 0.00722329
I1013 19:51:49.461619 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00722431 (* 1 = 0.00722431 loss)
I1013 19:51:49.461625 12690 sgd_solver.cpp:105] Iteration 88500, lr = 0.0001
I1013 19:52:20.121660 12690 solver.cpp:218] Iteration 88600 (3.26157 iter/s, 30.6601s/100 iters), loss = 0.00537867
I1013 19:52:20.121803 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00537968 (* 1 = 0.00537968 loss)
I1013 19:52:20.121812 12690 sgd_solver.cpp:105] Iteration 88600, lr = 0.0001
I1013 19:52:50.767832 12690 solver.cpp:218] Iteration 88700 (3.26306 iter/s, 30.646s/100 iters), loss = 0.00518808
I1013 19:52:50.767944 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0051891 (* 1 = 0.0051891 loss)
I1013 19:52:50.767952 12690 sgd_solver.cpp:105] Iteration 88700, lr = 0.0001
I1013 19:53:21.409134 12690 solver.cpp:218] Iteration 88800 (3.26358 iter/s, 30.6412s/100 iters), loss = 0.0130125
I1013 19:53:21.409274 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130135 (* 1 = 0.0130135 loss)
I1013 19:53:21.409281 12690 sgd_solver.cpp:105] Iteration 88800, lr = 0.0001
I1013 19:53:52.009743 12690 solver.cpp:218] Iteration 88900 (3.26792 iter/s, 30.6005s/100 iters), loss = 0.00293708
I1013 19:53:52.009886 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0029381 (* 1 = 0.0029381 loss)
I1013 19:53:52.009894 12690 sgd_solver.cpp:105] Iteration 88900, lr = 0.0001
I1013 19:54:21.140560 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:54:22.370149 12690 solver.cpp:330] Iteration 89000, Testing net (#0)
I1013 19:54:38.999382 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:54:39.342609 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09699 (* 1 = 1.09699 loss)
I1013 19:54:39.342625 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7464
I1013 19:54:39.646209 12690 solver.cpp:218] Iteration 89000 (2.09924 iter/s, 47.6363s/100 iters), loss = 0.00429057
I1013 19:54:39.646244 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00429159 (* 1 = 0.00429159 loss)
I1013 19:54:39.646250 12690 sgd_solver.cpp:105] Iteration 89000, lr = 0.0001
I1013 19:55:10.313650 12690 solver.cpp:218] Iteration 89100 (3.26079 iter/s, 30.6674s/100 iters), loss = 0.00422933
I1013 19:55:10.313747 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00423035 (* 1 = 0.00423035 loss)
I1013 19:55:10.313763 12690 sgd_solver.cpp:105] Iteration 89100, lr = 0.0001
I1013 19:55:40.935031 12690 solver.cpp:218] Iteration 89200 (3.2657 iter/s, 30.6213s/100 iters), loss = 0.00447611
I1013 19:55:40.935206 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00447713 (* 1 = 0.00447713 loss)
I1013 19:55:40.935226 12690 sgd_solver.cpp:105] Iteration 89200, lr = 0.0001
I1013 19:56:11.594976 12690 solver.cpp:218] Iteration 89300 (3.2616 iter/s, 30.6598s/100 iters), loss = 0.00830085
I1013 19:56:11.595118 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00830187 (* 1 = 0.00830187 loss)
I1013 19:56:11.595125 12690 sgd_solver.cpp:105] Iteration 89300, lr = 0.0001
I1013 19:56:42.238649 12690 solver.cpp:218] Iteration 89400 (3.26333 iter/s, 30.6435s/100 iters), loss = 0.00626158
I1013 19:56:42.238742 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00626261 (* 1 = 0.00626261 loss)
I1013 19:56:42.238760 12690 sgd_solver.cpp:105] Iteration 89400, lr = 0.0001
I1013 19:57:12.569757 12690 solver.cpp:330] Iteration 89500, Testing net (#0)
I1013 19:57:29.257200 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 19:57:29.598698 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09695 (* 1 = 1.09695 loss)
I1013 19:57:29.598713 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7456
I1013 19:57:29.900496 12690 solver.cpp:218] Iteration 89500 (2.09812 iter/s, 47.6618s/100 iters), loss = 0.0075253
I1013 19:57:29.900526 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00752632 (* 1 = 0.00752632 loss)
I1013 19:57:29.900532 12690 sgd_solver.cpp:105] Iteration 89500, lr = 0.0001
I1013 19:58:00.553617 12690 solver.cpp:218] Iteration 89600 (3.26231 iter/s, 30.6531s/100 iters), loss = 0.00424446
I1013 19:58:00.553725 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00424548 (* 1 = 0.00424548 loss)
I1013 19:58:00.553742 12690 sgd_solver.cpp:105] Iteration 89600, lr = 0.0001
I1013 19:58:31.174026 12690 solver.cpp:218] Iteration 89700 (3.26581 iter/s, 30.6203s/100 iters), loss = 0.00437525
I1013 19:58:31.174119 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00437628 (* 1 = 0.00437628 loss)
I1013 19:58:31.174137 12690 sgd_solver.cpp:105] Iteration 89700, lr = 0.0001
I1013 19:59:01.821301 12690 solver.cpp:218] Iteration 89800 (3.26294 iter/s, 30.6472s/100 iters), loss = 0.00569814
I1013 19:59:01.821408 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00569917 (* 1 = 0.00569917 loss)
I1013 19:59:01.821425 12690 sgd_solver.cpp:105] Iteration 89800, lr = 0.0001
I1013 19:59:32.502745 12690 solver.cpp:218] Iteration 89900 (3.25931 iter/s, 30.6814s/100 iters), loss = 0.00602524
I1013 19:59:32.502840 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00602626 (* 1 = 0.00602626 loss)
I1013 19:59:32.502857 12690 sgd_solver.cpp:105] Iteration 89900, lr = 0.0001
I1013 20:00:01.604815 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:00:02.825459 12690 solver.cpp:330] Iteration 90000, Testing net (#0)
I1013 20:00:19.497103 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:00:19.836129 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09673 (* 1 = 1.09673 loss)
I1013 20:00:19.836146 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7457
I1013 20:00:20.138535 12690 solver.cpp:218] Iteration 90000 (2.09927 iter/s, 47.6357s/100 iters), loss = 0.00355975
I1013 20:00:20.138562 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00356078 (* 1 = 0.00356078 loss)
I1013 20:00:20.138568 12690 sgd_solver.cpp:105] Iteration 90000, lr = 0.0001
I1013 20:00:50.819744 12690 solver.cpp:218] Iteration 90100 (3.25933 iter/s, 30.6812s/100 iters), loss = 0.00710637
I1013 20:00:50.819882 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0071074 (* 1 = 0.0071074 loss)
I1013 20:00:50.819890 12690 sgd_solver.cpp:105] Iteration 90100, lr = 0.0001
I1013 20:01:21.478024 12690 solver.cpp:218] Iteration 90200 (3.26178 iter/s, 30.6581s/100 iters), loss = 0.00823932
I1013 20:01:21.478116 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00824034 (* 1 = 0.00824034 loss)
I1013 20:01:21.478132 12690 sgd_solver.cpp:105] Iteration 90200, lr = 0.0001
I1013 20:01:52.119511 12690 solver.cpp:218] Iteration 90300 (3.26356 iter/s, 30.6414s/100 iters), loss = 0.00507639
I1013 20:01:52.119632 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00507741 (* 1 = 0.00507741 loss)
I1013 20:01:52.119639 12690 sgd_solver.cpp:105] Iteration 90300, lr = 0.0001
I1013 20:02:22.801367 12690 solver.cpp:218] Iteration 90400 (3.25927 iter/s, 30.6817s/100 iters), loss = 0.00778257
I1013 20:02:22.801513 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0077836 (* 1 = 0.0077836 loss)
I1013 20:02:22.801522 12690 sgd_solver.cpp:105] Iteration 90400, lr = 0.0001
I1013 20:02:53.182008 12690 solver.cpp:330] Iteration 90500, Testing net (#0)
I1013 20:03:09.832221 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:03:10.172894 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09665 (* 1 = 1.09665 loss)
I1013 20:03:10.172909 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7453
I1013 20:03:10.479493 12690 solver.cpp:218] Iteration 90500 (2.0974 iter/s, 47.678s/100 iters), loss = 0.0107081
I1013 20:03:10.479523 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107091 (* 1 = 0.0107091 loss)
I1013 20:03:10.479529 12690 sgd_solver.cpp:105] Iteration 90500, lr = 0.0001
I1013 20:03:41.160907 12690 solver.cpp:218] Iteration 90600 (3.2593 iter/s, 30.6814s/100 iters), loss = 0.00339734
I1013 20:03:41.161018 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00339837 (* 1 = 0.00339837 loss)
I1013 20:03:41.161036 12690 sgd_solver.cpp:105] Iteration 90600, lr = 0.0001
I1013 20:04:11.782495 12690 solver.cpp:218] Iteration 90700 (3.26568 iter/s, 30.6215s/100 iters), loss = 0.00588987
I1013 20:04:11.782647 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00589089 (* 1 = 0.00589089 loss)
I1013 20:04:11.782655 12690 sgd_solver.cpp:105] Iteration 90700, lr = 0.0001
I1013 20:04:42.460979 12690 solver.cpp:218] Iteration 90800 (3.25963 iter/s, 30.6783s/100 iters), loss = 0.0101079
I1013 20:04:42.461670 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0101089 (* 1 = 0.0101089 loss)
I1013 20:04:42.461679 12690 sgd_solver.cpp:105] Iteration 90800, lr = 0.0001
I1013 20:05:13.193796 12690 solver.cpp:218] Iteration 90900 (3.25392 iter/s, 30.7321s/100 iters), loss = 0.00292721
I1013 20:05:13.193910 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00292823 (* 1 = 0.00292823 loss)
I1013 20:05:13.193923 12690 sgd_solver.cpp:105] Iteration 90900, lr = 0.0001
I1013 20:05:42.362179 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:05:43.589823 12690 solver.cpp:330] Iteration 91000, Testing net (#0)
I1013 20:06:00.255386 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:06:00.593310 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09648 (* 1 = 1.09648 loss)
I1013 20:06:00.593327 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 20:06:00.892765 12690 solver.cpp:218] Iteration 91000 (2.09649 iter/s, 47.6989s/100 iters), loss = 0.00240361
I1013 20:06:00.892793 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00240463 (* 1 = 0.00240463 loss)
I1013 20:06:00.892801 12690 sgd_solver.cpp:105] Iteration 91000, lr = 0.0001
I1013 20:06:31.611074 12690 solver.cpp:218] Iteration 91100 (3.25539 iter/s, 30.7183s/100 iters), loss = 0.0059881
I1013 20:06:31.611217 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00598913 (* 1 = 0.00598913 loss)
I1013 20:06:31.611225 12690 sgd_solver.cpp:105] Iteration 91100, lr = 0.0001
I1013 20:07:02.287364 12690 solver.cpp:218] Iteration 91200 (3.25986 iter/s, 30.6762s/100 iters), loss = 0.00414084
I1013 20:07:02.287511 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00414186 (* 1 = 0.00414186 loss)
I1013 20:07:02.287520 12690 sgd_solver.cpp:105] Iteration 91200, lr = 0.0001
I1013 20:07:32.962502 12690 solver.cpp:218] Iteration 91300 (3.25998 iter/s, 30.675s/100 iters), loss = 0.00644919
I1013 20:07:32.962646 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00645022 (* 1 = 0.00645022 loss)
I1013 20:07:32.962654 12690 sgd_solver.cpp:105] Iteration 91300, lr = 0.0001
I1013 20:08:03.631196 12690 solver.cpp:218] Iteration 91400 (3.26067 iter/s, 30.6686s/100 iters), loss = 0.00499832
I1013 20:08:03.631327 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00499934 (* 1 = 0.00499934 loss)
I1013 20:08:03.631345 12690 sgd_solver.cpp:105] Iteration 91400, lr = 0.0001
I1013 20:08:34.014547 12690 solver.cpp:330] Iteration 91500, Testing net (#0)
I1013 20:08:50.693169 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:08:51.034705 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09636 (* 1 = 1.09636 loss)
I1013 20:08:51.034721 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7452
I1013 20:08:51.336515 12690 solver.cpp:218] Iteration 91500 (2.09621 iter/s, 47.7052s/100 iters), loss = 0.00565476
I1013 20:08:51.336545 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00565579 (* 1 = 0.00565579 loss)
I1013 20:08:51.336552 12690 sgd_solver.cpp:105] Iteration 91500, lr = 0.0001
I1013 20:09:21.981905 12690 solver.cpp:218] Iteration 91600 (3.26314 iter/s, 30.6454s/100 iters), loss = 0.00421172
I1013 20:09:21.982046 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00421274 (* 1 = 0.00421274 loss)
I1013 20:09:21.982055 12690 sgd_solver.cpp:105] Iteration 91600, lr = 0.0001
I1013 20:09:52.606802 12690 solver.cpp:218] Iteration 91700 (3.26533 iter/s, 30.6248s/100 iters), loss = 0.00748432
I1013 20:09:52.607226 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00748534 (* 1 = 0.00748534 loss)
I1013 20:09:52.607247 12690 sgd_solver.cpp:105] Iteration 91700, lr = 0.0001
I1013 20:10:23.285517 12690 solver.cpp:218] Iteration 91800 (3.25963 iter/s, 30.6783s/100 iters), loss = 0.00301904
I1013 20:10:23.285658 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00302007 (* 1 = 0.00302007 loss)
I1013 20:10:23.285667 12690 sgd_solver.cpp:105] Iteration 91800, lr = 0.0001
I1013 20:10:53.944744 12690 solver.cpp:218] Iteration 91900 (3.26167 iter/s, 30.6591s/100 iters), loss = 0.00431935
I1013 20:10:53.944871 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00432037 (* 1 = 0.00432037 loss)
I1013 20:10:53.944880 12690 sgd_solver.cpp:105] Iteration 91900, lr = 0.0001
I1013 20:11:23.046293 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:11:24.277236 12690 solver.cpp:330] Iteration 92000, Testing net (#0)
I1013 20:11:40.947515 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:11:41.290086 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.0961 (* 1 = 1.0961 loss)
I1013 20:11:41.290102 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7458
I1013 20:11:41.594837 12690 solver.cpp:218] Iteration 92000 (2.09864 iter/s, 47.65s/100 iters), loss = 0.00478182
I1013 20:11:41.594872 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00478284 (* 1 = 0.00478284 loss)
I1013 20:11:41.594879 12690 sgd_solver.cpp:105] Iteration 92000, lr = 0.0001
I1013 20:12:12.251461 12690 solver.cpp:218] Iteration 92100 (3.26194 iter/s, 30.6566s/100 iters), loss = 0.00393323
I1013 20:12:12.251583 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00393425 (* 1 = 0.00393425 loss)
I1013 20:12:12.251600 12690 sgd_solver.cpp:105] Iteration 92100, lr = 0.0001
I1013 20:12:42.881914 12690 solver.cpp:218] Iteration 92200 (3.26474 iter/s, 30.6303s/100 iters), loss = 0.0087477
I1013 20:12:42.882019 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00874872 (* 1 = 0.00874872 loss)
I1013 20:12:42.882037 12690 sgd_solver.cpp:105] Iteration 92200, lr = 0.0001
I1013 20:13:13.594122 12690 solver.cpp:218] Iteration 92300 (3.25604 iter/s, 30.7121s/100 iters), loss = 0.00805128
I1013 20:13:13.594257 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0080523 (* 1 = 0.0080523 loss)
I1013 20:13:13.594277 12690 sgd_solver.cpp:105] Iteration 92300, lr = 0.0001
I1013 20:13:44.182590 12690 solver.cpp:218] Iteration 92400 (3.26922 iter/s, 30.5883s/100 iters), loss = 0.00951936
I1013 20:13:44.182703 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00952038 (* 1 = 0.00952038 loss)
I1013 20:13:44.182710 12690 sgd_solver.cpp:105] Iteration 92400, lr = 0.0001
I1013 20:14:14.529541 12690 solver.cpp:330] Iteration 92500, Testing net (#0)
I1013 20:14:31.203975 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:14:31.544441 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09595 (* 1 = 1.09595 loss)
I1013 20:14:31.544459 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7456
I1013 20:14:31.846196 12690 solver.cpp:218] Iteration 92500 (2.09804 iter/s, 47.6635s/100 iters), loss = 0.00930735
I1013 20:14:31.846235 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00930838 (* 1 = 0.00930838 loss)
I1013 20:14:31.846241 12690 sgd_solver.cpp:105] Iteration 92500, lr = 0.0001
I1013 20:15:02.519230 12690 solver.cpp:218] Iteration 92600 (3.2602 iter/s, 30.673s/100 iters), loss = 0.00262154
I1013 20:15:02.519371 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00262257 (* 1 = 0.00262257 loss)
I1013 20:15:02.519381 12690 sgd_solver.cpp:105] Iteration 92600, lr = 0.0001
I1013 20:15:33.139695 12690 solver.cpp:218] Iteration 92700 (3.2658 iter/s, 30.6203s/100 iters), loss = 0.0037414
I1013 20:15:33.139811 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00374243 (* 1 = 0.00374243 loss)
I1013 20:15:33.139818 12690 sgd_solver.cpp:105] Iteration 92700, lr = 0.0001
I1013 20:16:03.782137 12690 solver.cpp:218] Iteration 92800 (3.26346 iter/s, 30.6423s/100 iters), loss = 0.00321869
I1013 20:16:03.782246 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00321971 (* 1 = 0.00321971 loss)
I1013 20:16:03.782264 12690 sgd_solver.cpp:105] Iteration 92800, lr = 0.0001
I1013 20:16:34.425487 12690 solver.cpp:218] Iteration 92900 (3.26336 iter/s, 30.6433s/100 iters), loss = 0.00717294
I1013 20:16:34.425602 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00717397 (* 1 = 0.00717397 loss)
I1013 20:16:34.425611 12690 sgd_solver.cpp:105] Iteration 92900, lr = 0.0001
I1013 20:17:03.532622 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:17:04.759701 12690 solver.cpp:330] Iteration 93000, Testing net (#0)
I1013 20:17:21.405370 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:17:21.745147 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.0958 (* 1 = 1.0958 loss)
I1013 20:17:21.745163 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7462
I1013 20:17:22.050487 12690 solver.cpp:218] Iteration 93000 (2.09974 iter/s, 47.6249s/100 iters), loss = 0.00624852
I1013 20:17:22.050514 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00624955 (* 1 = 0.00624955 loss)
I1013 20:17:22.050521 12690 sgd_solver.cpp:105] Iteration 93000, lr = 0.0001
I1013 20:17:52.716883 12690 solver.cpp:218] Iteration 93100 (3.2609 iter/s, 30.6664s/100 iters), loss = 0.00658978
I1013 20:17:52.717005 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00659081 (* 1 = 0.00659081 loss)
I1013 20:17:52.717015 12690 sgd_solver.cpp:105] Iteration 93100, lr = 0.0001
I1013 20:18:23.326568 12690 solver.cpp:218] Iteration 93200 (3.26695 iter/s, 30.6096s/100 iters), loss = 0.0060377
I1013 20:18:23.326711 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00603873 (* 1 = 0.00603873 loss)
I1013 20:18:23.326719 12690 sgd_solver.cpp:105] Iteration 93200, lr = 0.0001
I1013 20:18:53.955875 12690 solver.cpp:218] Iteration 93300 (3.26486 iter/s, 30.6292s/100 iters), loss = 0.00509818
I1013 20:18:53.956019 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00509921 (* 1 = 0.00509921 loss)
I1013 20:18:53.956028 12690 sgd_solver.cpp:105] Iteration 93300, lr = 0.0001
I1013 20:19:24.582289 12690 solver.cpp:218] Iteration 93400 (3.26517 iter/s, 30.6263s/100 iters), loss = 0.00617261
I1013 20:19:24.582446 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00617364 (* 1 = 0.00617364 loss)
I1013 20:19:24.582458 12690 sgd_solver.cpp:105] Iteration 93400, lr = 0.0001
I1013 20:19:54.918678 12690 solver.cpp:330] Iteration 93500, Testing net (#0)
I1013 20:20:11.574805 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:20:11.916421 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09565 (* 1 = 1.09565 loss)
I1013 20:20:11.916437 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7457
I1013 20:20:12.219763 12690 solver.cpp:218] Iteration 93500 (2.09919 iter/s, 47.6373s/100 iters), loss = 0.00579697
I1013 20:20:12.219791 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.005798 (* 1 = 0.005798 loss)
I1013 20:20:12.219799 12690 sgd_solver.cpp:105] Iteration 93500, lr = 0.0001
I1013 20:20:42.827069 12690 solver.cpp:218] Iteration 93600 (3.2672 iter/s, 30.6073s/100 iters), loss = 0.0055217
I1013 20:20:42.827173 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00552273 (* 1 = 0.00552273 loss)
I1013 20:20:42.827181 12690 sgd_solver.cpp:105] Iteration 93600, lr = 0.0001
I1013 20:21:13.487052 12690 solver.cpp:218] Iteration 93700 (3.26159 iter/s, 30.6599s/100 iters), loss = 0.00543788
I1013 20:21:13.487193 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00543891 (* 1 = 0.00543891 loss)
I1013 20:21:13.487201 12690 sgd_solver.cpp:105] Iteration 93700, lr = 0.0001
I1013 20:21:44.124536 12690 solver.cpp:218] Iteration 93800 (3.26399 iter/s, 30.6374s/100 iters), loss = 0.00544165
I1013 20:21:44.124649 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00544267 (* 1 = 0.00544267 loss)
I1013 20:21:44.124658 12690 sgd_solver.cpp:105] Iteration 93800, lr = 0.0001
I1013 20:22:14.734050 12690 solver.cpp:218] Iteration 93900 (3.26697 iter/s, 30.6094s/100 iters), loss = 0.00246355
I1013 20:22:14.734191 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00246457 (* 1 = 0.00246457 loss)
I1013 20:22:14.734200 12690 sgd_solver.cpp:105] Iteration 93900, lr = 0.0001
I1013 20:22:43.871495 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:22:45.096920 12690 solver.cpp:330] Iteration 94000, Testing net (#0)
I1013 20:23:01.799883 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:23:02.141855 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09565 (* 1 = 1.09565 loss)
I1013 20:23:02.141870 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7459
I1013 20:23:02.446815 12690 solver.cpp:218] Iteration 94000 (2.09588 iter/s, 47.7126s/100 iters), loss = 0.00345409
I1013 20:23:02.446846 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00345511 (* 1 = 0.00345511 loss)
I1013 20:23:02.446854 12690 sgd_solver.cpp:105] Iteration 94000, lr = 0.0001
I1013 20:23:33.154594 12690 solver.cpp:218] Iteration 94100 (3.25651 iter/s, 30.7078s/100 iters), loss = 0.00389809
I1013 20:23:33.154732 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00389912 (* 1 = 0.00389912 loss)
I1013 20:23:33.154741 12690 sgd_solver.cpp:105] Iteration 94100, lr = 0.0001
I1013 20:24:03.781118 12690 solver.cpp:218] Iteration 94200 (3.26516 iter/s, 30.6264s/100 iters), loss = 0.00651459
I1013 20:24:03.781261 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00651562 (* 1 = 0.00651562 loss)
I1013 20:24:03.781272 12690 sgd_solver.cpp:105] Iteration 94200, lr = 0.0001
I1013 20:24:34.429236 12690 solver.cpp:218] Iteration 94300 (3.26286 iter/s, 30.648s/100 iters), loss = 0.00960514
I1013 20:24:34.429378 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00960617 (* 1 = 0.00960617 loss)
I1013 20:24:34.429385 12690 sgd_solver.cpp:105] Iteration 94300, lr = 0.0001
I1013 20:25:05.093678 12690 solver.cpp:218] Iteration 94400 (3.26112 iter/s, 30.6643s/100 iters), loss = 0.00792466
I1013 20:25:05.093971 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00792568 (* 1 = 0.00792568 loss)
I1013 20:25:05.093992 12690 sgd_solver.cpp:105] Iteration 94400, lr = 0.0001
I1013 20:25:35.443616 12690 solver.cpp:330] Iteration 94500, Testing net (#0)
I1013 20:25:52.099046 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:25:52.439370 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09545 (* 1 = 1.09545 loss)
I1013 20:25:52.439386 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 20:25:52.739003 12690 solver.cpp:218] Iteration 94500 (2.09885 iter/s, 47.6451s/100 iters), loss = 0.00850446
I1013 20:25:52.739037 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00850548 (* 1 = 0.00850548 loss)
I1013 20:25:52.739043 12690 sgd_solver.cpp:105] Iteration 94500, lr = 0.0001
I1013 20:26:23.448297 12690 solver.cpp:218] Iteration 94600 (3.25635 iter/s, 30.7093s/100 iters), loss = 0.00425123
I1013 20:26:23.448712 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00425225 (* 1 = 0.00425225 loss)
I1013 20:26:23.448721 12690 sgd_solver.cpp:105] Iteration 94600, lr = 0.0001
I1013 20:26:54.066650 12690 solver.cpp:218] Iteration 94700 (3.26606 iter/s, 30.6179s/100 iters), loss = 0.00498627
I1013 20:26:54.070781 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00498729 (* 1 = 0.00498729 loss)
I1013 20:26:54.070788 12690 sgd_solver.cpp:105] Iteration 94700, lr = 0.0001
I1013 20:27:24.815505 12690 solver.cpp:218] Iteration 94800 (3.25259 iter/s, 30.7447s/100 iters), loss = 0.00254312
I1013 20:27:24.815601 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00254414 (* 1 = 0.00254414 loss)
I1013 20:27:24.815609 12690 sgd_solver.cpp:105] Iteration 94800, lr = 0.0001
I1013 20:27:55.474679 12690 solver.cpp:218] Iteration 94900 (3.26168 iter/s, 30.6591s/100 iters), loss = 0.0053408
I1013 20:27:55.474822 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00534183 (* 1 = 0.00534183 loss)
I1013 20:27:55.474831 12690 sgd_solver.cpp:105] Iteration 94900, lr = 0.0001
I1013 20:28:24.596879 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:28:25.823856 12690 solver.cpp:330] Iteration 95000, Testing net (#0)
I1013 20:28:42.478808 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:28:42.819344 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09517 (* 1 = 1.09517 loss)
I1013 20:28:42.819361 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 20:28:43.123180 12690 solver.cpp:218] Iteration 95000 (2.09871 iter/s, 47.6484s/100 iters), loss = 0.00458667
I1013 20:28:43.123212 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00458769 (* 1 = 0.00458769 loss)
I1013 20:28:43.123219 12690 sgd_solver.cpp:105] Iteration 95000, lr = 0.0001
I1013 20:29:13.834139 12690 solver.cpp:218] Iteration 95100 (3.25617 iter/s, 30.7109s/100 iters), loss = 0.00826574
I1013 20:29:13.834244 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00826676 (* 1 = 0.00826676 loss)
I1013 20:29:13.834264 12690 sgd_solver.cpp:105] Iteration 95100, lr = 0.0001
I1013 20:29:44.529060 12690 solver.cpp:218] Iteration 95200 (3.25788 iter/s, 30.6948s/100 iters), loss = 0.00722621
I1013 20:29:44.529158 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00722724 (* 1 = 0.00722724 loss)
I1013 20:29:44.529166 12690 sgd_solver.cpp:105] Iteration 95200, lr = 0.0001
I1013 20:30:15.220336 12690 solver.cpp:218] Iteration 95300 (3.25826 iter/s, 30.6912s/100 iters), loss = 0.00565697
I1013 20:30:15.220479 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00565799 (* 1 = 0.00565799 loss)
I1013 20:30:15.220489 12690 sgd_solver.cpp:105] Iteration 95300, lr = 0.0001
I1013 20:30:45.906966 12690 solver.cpp:218] Iteration 95400 (3.25876 iter/s, 30.6865s/100 iters), loss = 0.00617469
I1013 20:30:45.907444 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00617571 (* 1 = 0.00617571 loss)
I1013 20:30:45.907454 12690 sgd_solver.cpp:105] Iteration 95400, lr = 0.0001
I1013 20:31:16.377548 12690 solver.cpp:330] Iteration 95500, Testing net (#0)
I1013 20:31:33.070089 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:31:33.410359 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09494 (* 1 = 1.09494 loss)
I1013 20:31:33.410374 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7455
I1013 20:31:33.714049 12690 solver.cpp:218] Iteration 95500 (2.09176 iter/s, 47.8066s/100 iters), loss = 0.00564708
I1013 20:31:33.714087 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0056481 (* 1 = 0.0056481 loss)
I1013 20:31:33.714093 12690 sgd_solver.cpp:105] Iteration 95500, lr = 0.0001
I1013 20:32:04.396698 12690 solver.cpp:218] Iteration 95600 (3.25917 iter/s, 30.6826s/100 iters), loss = 0.00690801
I1013 20:32:04.396841 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00690903 (* 1 = 0.00690903 loss)
I1013 20:32:04.396848 12690 sgd_solver.cpp:105] Iteration 95600, lr = 0.0001
I1013 20:32:35.065361 12690 solver.cpp:218] Iteration 95700 (3.26067 iter/s, 30.6685s/100 iters), loss = 0.00347946
I1013 20:32:35.065510 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00348048 (* 1 = 0.00348048 loss)
I1013 20:32:35.065517 12690 sgd_solver.cpp:105] Iteration 95700, lr = 0.0001
I1013 20:33:05.676249 12690 solver.cpp:218] Iteration 95800 (3.26683 iter/s, 30.6108s/100 iters), loss = 0.00244506
I1013 20:33:05.676375 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00244608 (* 1 = 0.00244608 loss)
I1013 20:33:05.676383 12690 sgd_solver.cpp:105] Iteration 95800, lr = 0.0001
I1013 20:33:36.363298 12690 solver.cpp:218] Iteration 95900 (3.25872 iter/s, 30.6869s/100 iters), loss = 0.0035742
I1013 20:33:36.363392 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00357522 (* 1 = 0.00357522 loss)
I1013 20:33:36.363399 12690 sgd_solver.cpp:105] Iteration 95900, lr = 0.0001
I1013 20:34:05.497409 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:34:06.741188 12690 solver.cpp:330] Iteration 96000, Testing net (#0)
I1013 20:34:23.391340 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:34:23.731988 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09494 (* 1 = 1.09494 loss)
I1013 20:34:23.732004 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 20:34:24.032718 12690 solver.cpp:218] Iteration 96000 (2.09778 iter/s, 47.6693s/100 iters), loss = 0.00270764
I1013 20:34:24.032752 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00270866 (* 1 = 0.00270866 loss)
I1013 20:34:24.032759 12690 sgd_solver.cpp:105] Iteration 96000, lr = 0.0001
I1013 20:34:54.673995 12690 solver.cpp:218] Iteration 96100 (3.26357 iter/s, 30.6412s/100 iters), loss = 0.00535597
I1013 20:34:54.674110 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00535698 (* 1 = 0.00535698 loss)
I1013 20:34:54.674129 12690 sgd_solver.cpp:105] Iteration 96100, lr = 0.0001
I1013 20:35:25.285571 12690 solver.cpp:218] Iteration 96200 (3.26675 iter/s, 30.6115s/100 iters), loss = 0.00569966
I1013 20:35:25.285704 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00570068 (* 1 = 0.00570068 loss)
I1013 20:35:25.285712 12690 sgd_solver.cpp:105] Iteration 96200, lr = 0.0001
I1013 20:35:55.863932 12690 solver.cpp:218] Iteration 96300 (3.2703 iter/s, 30.5782s/100 iters), loss = 0.00573913
I1013 20:35:55.864068 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00574015 (* 1 = 0.00574015 loss)
I1013 20:35:55.864075 12690 sgd_solver.cpp:105] Iteration 96300, lr = 0.0001
I1013 20:36:26.449244 12690 solver.cpp:218] Iteration 96400 (3.26956 iter/s, 30.5852s/100 iters), loss = 0.00449985
I1013 20:36:26.449379 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00450087 (* 1 = 0.00450087 loss)
I1013 20:36:26.449388 12690 sgd_solver.cpp:105] Iteration 96400, lr = 0.0001
I1013 20:36:56.862095 12690 solver.cpp:330] Iteration 96500, Testing net (#0)
I1013 20:37:13.467046 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:37:13.813935 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09482 (* 1 = 1.09482 loss)
I1013 20:37:13.813951 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7457
I1013 20:37:14.119262 12690 solver.cpp:218] Iteration 96500 (2.09776 iter/s, 47.6699s/100 iters), loss = 0.00512315
I1013 20:37:14.119297 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00512417 (* 1 = 0.00512417 loss)
I1013 20:37:14.119304 12690 sgd_solver.cpp:105] Iteration 96500, lr = 0.0001
I1013 20:37:44.783148 12690 solver.cpp:218] Iteration 96600 (3.26117 iter/s, 30.6639s/100 iters), loss = 0.0032487
I1013 20:37:44.783294 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00324972 (* 1 = 0.00324972 loss)
I1013 20:37:44.783304 12690 sgd_solver.cpp:105] Iteration 96600, lr = 0.0001
I1013 20:38:15.426650 12690 solver.cpp:218] Iteration 96700 (3.26335 iter/s, 30.6434s/100 iters), loss = 0.00546152
I1013 20:38:15.426765 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00546254 (* 1 = 0.00546254 loss)
I1013 20:38:15.426772 12690 sgd_solver.cpp:105] Iteration 96700, lr = 0.0001
I1013 20:38:45.997102 12690 solver.cpp:218] Iteration 96800 (3.27114 iter/s, 30.5704s/100 iters), loss = 0.00412336
I1013 20:38:45.997196 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00412438 (* 1 = 0.00412438 loss)
I1013 20:38:45.997213 12690 sgd_solver.cpp:105] Iteration 96800, lr = 0.0001
I1013 20:39:16.629257 12690 solver.cpp:218] Iteration 96900 (3.26455 iter/s, 30.6321s/100 iters), loss = 0.00304231
I1013 20:39:16.629396 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00304334 (* 1 = 0.00304334 loss)
I1013 20:39:16.629405 12690 sgd_solver.cpp:105] Iteration 96900, lr = 0.0001
I1013 20:39:45.704948 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:39:46.926281 12690 solver.cpp:330] Iteration 97000, Testing net (#0)
I1013 20:40:03.568080 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:40:03.908938 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09463 (* 1 = 1.09463 loss)
I1013 20:40:03.908954 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7459
I1013 20:40:04.212604 12690 solver.cpp:218] Iteration 97000 (2.10158 iter/s, 47.5832s/100 iters), loss = 0.00308782
I1013 20:40:04.212651 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00308885 (* 1 = 0.00308885 loss)
I1013 20:40:04.212658 12690 sgd_solver.cpp:105] Iteration 97000, lr = 0.0001
I1013 20:40:34.895148 12690 solver.cpp:218] Iteration 97100 (3.25919 iter/s, 30.6825s/100 iters), loss = 0.00723419
I1013 20:40:34.895284 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00723522 (* 1 = 0.00723522 loss)
I1013 20:40:34.895292 12690 sgd_solver.cpp:105] Iteration 97100, lr = 0.0001
I1013 20:41:05.557556 12690 solver.cpp:218] Iteration 97200 (3.26134 iter/s, 30.6623s/100 iters), loss = 0.00726443
I1013 20:41:05.557690 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00726545 (* 1 = 0.00726545 loss)
I1013 20:41:05.557698 12690 sgd_solver.cpp:105] Iteration 97200, lr = 0.0001
I1013 20:41:36.189759 12690 solver.cpp:218] Iteration 97300 (3.26455 iter/s, 30.6321s/100 iters), loss = 0.00562058
I1013 20:41:36.189865 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00562161 (* 1 = 0.00562161 loss)
I1013 20:41:36.189872 12690 sgd_solver.cpp:105] Iteration 97300, lr = 0.0001
I1013 20:42:06.847934 12690 solver.cpp:218] Iteration 97400 (3.26178 iter/s, 30.6581s/100 iters), loss = 0.00624066
I1013 20:42:06.848024 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00624168 (* 1 = 0.00624168 loss)
I1013 20:42:06.848042 12690 sgd_solver.cpp:105] Iteration 97400, lr = 0.0001
I1013 20:42:37.212195 12690 solver.cpp:330] Iteration 97500, Testing net (#0)
I1013 20:42:53.874274 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:42:54.215566 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09446 (* 1 = 1.09446 loss)
I1013 20:42:54.215582 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7458
I1013 20:42:54.518756 12690 solver.cpp:218] Iteration 97500 (2.09772 iter/s, 47.6708s/100 iters), loss = 0.00409781
I1013 20:42:54.518788 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00409883 (* 1 = 0.00409883 loss)
I1013 20:42:54.518795 12690 sgd_solver.cpp:105] Iteration 97500, lr = 0.0001
I1013 20:43:25.207691 12690 solver.cpp:218] Iteration 97600 (3.25851 iter/s, 30.6889s/100 iters), loss = 0.00700893
I1013 20:43:25.207803 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00700995 (* 1 = 0.00700995 loss)
I1013 20:43:25.207820 12690 sgd_solver.cpp:105] Iteration 97600, lr = 0.0001
I1013 20:43:55.792601 12690 solver.cpp:218] Iteration 97700 (3.2696 iter/s, 30.5848s/100 iters), loss = 0.00315315
I1013 20:43:55.792743 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00315417 (* 1 = 0.00315417 loss)
I1013 20:43:55.792752 12690 sgd_solver.cpp:105] Iteration 97700, lr = 0.0001
I1013 20:44:26.436357 12690 solver.cpp:218] Iteration 97800 (3.26332 iter/s, 30.6436s/100 iters), loss = 0.00512131
I1013 20:44:26.436462 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00512233 (* 1 = 0.00512233 loss)
I1013 20:44:26.436470 12690 sgd_solver.cpp:105] Iteration 97800, lr = 0.0001
I1013 20:44:57.101864 12690 solver.cpp:218] Iteration 97900 (3.261 iter/s, 30.6654s/100 iters), loss = 0.00308113
I1013 20:44:57.102016 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00308215 (* 1 = 0.00308215 loss)
I1013 20:44:57.102025 12690 sgd_solver.cpp:105] Iteration 97900, lr = 0.0001
I1013 20:45:26.247321 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:45:27.474839 12690 solver.cpp:330] Iteration 98000, Testing net (#0)
I1013 20:45:44.068080 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:45:44.409307 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09433 (* 1 = 1.09433 loss)
I1013 20:45:44.409323 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7459
I1013 20:45:44.709667 12690 solver.cpp:218] Iteration 98000 (2.1005 iter/s, 47.6077s/100 iters), loss = 0.00584925
I1013 20:45:44.709702 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00585027 (* 1 = 0.00585027 loss)
I1013 20:45:44.709709 12690 sgd_solver.cpp:105] Iteration 98000, lr = 0.0001
I1013 20:46:15.345466 12690 solver.cpp:218] Iteration 98100 (3.26416 iter/s, 30.6358s/100 iters), loss = 0.00727513
I1013 20:46:15.345609 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00727615 (* 1 = 0.00727615 loss)
I1013 20:46:15.345618 12690 sgd_solver.cpp:105] Iteration 98100, lr = 0.0001
I1013 20:46:46.024893 12690 solver.cpp:218] Iteration 98200 (3.25953 iter/s, 30.6793s/100 iters), loss = 0.00805069
I1013 20:46:46.025033 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00805171 (* 1 = 0.00805171 loss)
I1013 20:46:46.025043 12690 sgd_solver.cpp:105] Iteration 98200, lr = 0.0001
I1013 20:47:16.696534 12690 solver.cpp:218] Iteration 98300 (3.26035 iter/s, 30.6715s/100 iters), loss = 0.00606302
I1013 20:47:16.696681 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00606403 (* 1 = 0.00606403 loss)
I1013 20:47:16.696689 12690 sgd_solver.cpp:105] Iteration 98300, lr = 0.0001
I1013 20:47:47.357717 12690 solver.cpp:218] Iteration 98400 (3.26147 iter/s, 30.661s/100 iters), loss = 0.00565921
I1013 20:47:47.357854 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00566023 (* 1 = 0.00566023 loss)
I1013 20:47:47.357863 12690 sgd_solver.cpp:105] Iteration 98400, lr = 0.0001
I1013 20:48:17.698205 12690 solver.cpp:330] Iteration 98500, Testing net (#0)
I1013 20:48:34.309587 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:48:34.651329 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09431 (* 1 = 1.09431 loss)
I1013 20:48:34.651345 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7458
I1013 20:48:34.955428 12690 solver.cpp:218] Iteration 98500 (2.10095 iter/s, 47.5976s/100 iters), loss = 0.00343836
I1013 20:48:34.955457 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00343937 (* 1 = 0.00343937 loss)
I1013 20:48:34.955463 12690 sgd_solver.cpp:105] Iteration 98500, lr = 0.0001
I1013 20:49:05.592244 12690 solver.cpp:218] Iteration 98600 (3.26405 iter/s, 30.6368s/100 iters), loss = 0.00507592
I1013 20:49:05.592375 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00507694 (* 1 = 0.00507694 loss)
I1013 20:49:05.592383 12690 sgd_solver.cpp:105] Iteration 98600, lr = 0.0001
I1013 20:49:36.223656 12690 solver.cpp:218] Iteration 98700 (3.26464 iter/s, 30.6313s/100 iters), loss = 0.00448408
I1013 20:49:36.223803 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00448509 (* 1 = 0.00448509 loss)
I1013 20:49:36.223811 12690 sgd_solver.cpp:105] Iteration 98700, lr = 0.0001
I1013 20:50:06.852840 12690 solver.cpp:218] Iteration 98800 (3.26487 iter/s, 30.629s/100 iters), loss = 0.00319838
I1013 20:50:06.852994 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0031994 (* 1 = 0.0031994 loss)
I1013 20:50:06.853013 12690 sgd_solver.cpp:105] Iteration 98800, lr = 0.0001
I1013 20:50:37.481215 12690 solver.cpp:218] Iteration 98900 (3.26496 iter/s, 30.6282s/100 iters), loss = 0.00396424
I1013 20:50:37.481308 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00396525 (* 1 = 0.00396525 loss)
I1013 20:50:37.481326 12690 sgd_solver.cpp:105] Iteration 98900, lr = 0.0001
I1013 20:51:06.589908 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:51:07.816972 12690 solver.cpp:330] Iteration 99000, Testing net (#0)
I1013 20:51:24.453027 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:51:24.791646 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09413 (* 1 = 1.09413 loss)
I1013 20:51:24.791661 12690 solver.cpp:397]     Test net output #1: accuracy = 0.7456
I1013 20:51:25.092311 12690 solver.cpp:218] Iteration 99000 (2.10035 iter/s, 47.611s/100 iters), loss = 0.00873576
I1013 20:51:25.092341 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00873678 (* 1 = 0.00873678 loss)
I1013 20:51:25.092347 12690 sgd_solver.cpp:105] Iteration 99000, lr = 0.0001
I1013 20:51:55.744280 12690 solver.cpp:218] Iteration 99100 (3.26244 iter/s, 30.6519s/100 iters), loss = 0.00343473
I1013 20:51:55.744426 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00343575 (* 1 = 0.00343575 loss)
I1013 20:51:55.744434 12690 sgd_solver.cpp:105] Iteration 99100, lr = 0.0001
I1013 20:52:26.400817 12690 solver.cpp:218] Iteration 99200 (3.26196 iter/s, 30.6564s/100 iters), loss = 0.00380708
I1013 20:52:26.400959 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00380809 (* 1 = 0.00380809 loss)
I1013 20:52:26.400966 12690 sgd_solver.cpp:105] Iteration 99200, lr = 0.0001
I1013 20:52:57.018174 12690 solver.cpp:218] Iteration 99300 (3.26614 iter/s, 30.6172s/100 iters), loss = 0.00590789
I1013 20:52:57.018282 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00590891 (* 1 = 0.00590891 loss)
I1013 20:52:57.018301 12690 sgd_solver.cpp:105] Iteration 99300, lr = 0.0001
I1013 20:53:27.685911 12690 solver.cpp:218] Iteration 99400 (3.26077 iter/s, 30.6676s/100 iters), loss = 0.00721736
I1013 20:53:27.686034 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00721838 (* 1 = 0.00721838 loss)
I1013 20:53:27.686043 12690 sgd_solver.cpp:105] Iteration 99400, lr = 0.0001
I1013 20:53:58.020807 12690 solver.cpp:330] Iteration 99500, Testing net (#0)
I1013 20:54:14.731698 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:54:15.073711 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09414 (* 1 = 1.09414 loss)
I1013 20:54:15.073727 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 20:54:15.376495 12690 solver.cpp:218] Iteration 99500 (2.09685 iter/s, 47.6905s/100 iters), loss = 0.00638065
I1013 20:54:15.376526 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00638167 (* 1 = 0.00638167 loss)
I1013 20:54:15.376533 12690 sgd_solver.cpp:105] Iteration 99500, lr = 0.0001
I1013 20:54:46.000640 12690 solver.cpp:218] Iteration 99600 (3.2654 iter/s, 30.6241s/100 iters), loss = 0.00479433
I1013 20:54:46.000779 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00479535 (* 1 = 0.00479535 loss)
I1013 20:54:46.000788 12690 sgd_solver.cpp:105] Iteration 99600, lr = 0.0001
I1013 20:55:16.676376 12690 solver.cpp:218] Iteration 99700 (3.25992 iter/s, 30.6756s/100 iters), loss = 0.00563283
I1013 20:55:16.676491 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00563385 (* 1 = 0.00563385 loss)
I1013 20:55:16.676507 12690 sgd_solver.cpp:105] Iteration 99700, lr = 0.0001
I1013 20:55:47.376868 12690 solver.cpp:218] Iteration 99800 (3.25729 iter/s, 30.7004s/100 iters), loss = 0.00765458
I1013 20:55:47.377020 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0076556 (* 1 = 0.0076556 loss)
I1013 20:55:47.377028 12690 sgd_solver.cpp:105] Iteration 99800, lr = 0.0001
I1013 20:56:18.019358 12690 solver.cpp:218] Iteration 99900 (3.26346 iter/s, 30.6423s/100 iters), loss = 0.0033906
I1013 20:56:18.019510 12690 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00339161 (* 1 = 0.00339161 loss)
I1013 20:56:18.019529 12690 sgd_solver.cpp:105] Iteration 99900, lr = 0.0001
I1013 20:56:47.200001 12698 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:56:48.421957 12690 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/WRN/WRN_penlu_0.25_2study_2decay_iter_100000.caffemodel
I1013 20:56:51.297857 12690 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/WRN/WRN_penlu_0.25_2study_2decay_iter_100000.solverstate
I1013 20:56:51.511924 12690 solver.cpp:310] Iteration 100000, loss = 0.00339391
I1013 20:56:51.511946 12690 solver.cpp:330] Iteration 100000, Testing net (#0)
I1013 20:57:07.941273 12699 data_layer.cpp:73] Restarting data prefetching from start.
I1013 20:57:08.279410 12690 solver.cpp:397]     Test net output #0: SoftmaxWithLoss1 = 1.09399 (* 1 = 1.09399 loss)
I1013 20:57:08.279426 12690 solver.cpp:397]     Test net output #1: accuracy = 0.746
I1013 20:57:08.279431 12690 solver.cpp:315] Optimization Done.
I1013 20:57:08.279433 12690 caffe.cpp:259] Optimization Done.
