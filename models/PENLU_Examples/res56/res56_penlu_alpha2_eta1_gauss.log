I0925 11:03:16.176684  2600 caffe.cpp:218] Using GPUs 0
I0925 11:03:16.210873  2600 caffe.cpp:223] GPU 0: GeForce GTX 1080
I0925 11:03:16.437366  2600 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 100000
snapshot_prefix: "xn/PENLU/snapshot/resnet/res56_penlu_alpha2_eta1_gauss"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 900
stepvalue: 40000
stepvalue: 80000
I0925 11:03:16.437518  2600 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0925 11:03:16.441447  2600 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0925 11:03:16.441459  2600 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0925 11:03:16.441748  2600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I0925 11:03:16.441884  2600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I0925 11:03:16.443068  2600 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution17"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution19"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution20"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu22"
  type: "PENLU"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std
I0925 11:03:16.443943  2600 layer_factory.hpp:77] Creating layer Data1
I0925 11:03:16.444020  2600 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb
I0925 11:03:16.444041  2600 net.cpp:84] Creating Layer Data1
I0925 11:03:16.444046  2600 net.cpp:380] Data1 -> Data1
I0925 11:03:16.444063  2600 net.cpp:380] Data1 -> Data2
I0925 11:03:16.444072  2600 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I0925 11:03:16.445495  2600 data_layer.cpp:45] output data size: 100,3,28,28
I0925 11:03:16.447796  2600 net.cpp:122] Setting up Data1
I0925 11:03:16.447808  2600 net.cpp:129] Top shape: 100 3 28 28 (235200)
I0925 11:03:16.447813  2600 net.cpp:129] Top shape: 100 (100)
I0925 11:03:16.447815  2600 net.cpp:137] Memory required for data: 941200
I0925 11:03:16.447821  2600 layer_factory.hpp:77] Creating layer Convolution1
I0925 11:03:16.447839  2600 net.cpp:84] Creating Layer Convolution1
I0925 11:03:16.447844  2600 net.cpp:406] Convolution1 <- Data1
I0925 11:03:16.447852  2600 net.cpp:380] Convolution1 -> Convolution1
I0925 11:03:16.595898  2600 net.cpp:122] Setting up Convolution1
I0925 11:03:16.595926  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.595928  2600 net.cpp:137] Memory required for data: 5958800
I0925 11:03:16.595943  2600 layer_factory.hpp:77] Creating layer BatchNorm1
I0925 11:03:16.595965  2600 net.cpp:84] Creating Layer BatchNorm1
I0925 11:03:16.595968  2600 net.cpp:406] BatchNorm1 <- Convolution1
I0925 11:03:16.596012  2600 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0925 11:03:16.596177  2600 net.cpp:122] Setting up BatchNorm1
I0925 11:03:16.596185  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.596187  2600 net.cpp:137] Memory required for data: 10976400
I0925 11:03:16.596195  2600 layer_factory.hpp:77] Creating layer Scale1
I0925 11:03:16.596205  2600 net.cpp:84] Creating Layer Scale1
I0925 11:03:16.596218  2600 net.cpp:406] Scale1 <- Convolution1
I0925 11:03:16.596223  2600 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0925 11:03:16.596273  2600 layer_factory.hpp:77] Creating layer Scale1
I0925 11:03:16.596380  2600 net.cpp:122] Setting up Scale1
I0925 11:03:16.596387  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.596390  2600 net.cpp:137] Memory required for data: 15994000
I0925 11:03:16.596393  2600 layer_factory.hpp:77] Creating layer penlu1
I0925 11:03:16.596403  2600 net.cpp:84] Creating Layer penlu1
I0925 11:03:16.596415  2600 net.cpp:406] penlu1 <- Convolution1
I0925 11:03:16.596420  2600 net.cpp:367] penlu1 -> Convolution1 (in-place)
I0925 11:03:16.597040  2600 net.cpp:122] Setting up penlu1
I0925 11:03:16.597050  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.597054  2600 net.cpp:137] Memory required for data: 21011600
I0925 11:03:16.597060  2600 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I0925 11:03:16.597069  2600 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I0925 11:03:16.597081  2600 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I0925 11:03:16.597085  2600 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I0925 11:03:16.597095  2600 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I0925 11:03:16.597136  2600 net.cpp:122] Setting up Convolution1_penlu1_0_split
I0925 11:03:16.597143  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.597147  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.597149  2600 net.cpp:137] Memory required for data: 31046800
I0925 11:03:16.597151  2600 layer_factory.hpp:77] Creating layer Convolution2
I0925 11:03:16.597158  2600 net.cpp:84] Creating Layer Convolution2
I0925 11:03:16.597162  2600 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I0925 11:03:16.597167  2600 net.cpp:380] Convolution2 -> Convolution2
I0925 11:03:16.598002  2600 net.cpp:122] Setting up Convolution2
I0925 11:03:16.598013  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.598018  2600 net.cpp:137] Memory required for data: 36064400
I0925 11:03:16.598027  2600 layer_factory.hpp:77] Creating layer BatchNorm2
I0925 11:03:16.598036  2600 net.cpp:84] Creating Layer BatchNorm2
I0925 11:03:16.598042  2600 net.cpp:406] BatchNorm2 <- Convolution2
I0925 11:03:16.598048  2600 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0925 11:03:16.598170  2600 net.cpp:122] Setting up BatchNorm2
I0925 11:03:16.598177  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.598181  2600 net.cpp:137] Memory required for data: 41082000
I0925 11:03:16.598189  2600 layer_factory.hpp:77] Creating layer Scale2
I0925 11:03:16.598198  2600 net.cpp:84] Creating Layer Scale2
I0925 11:03:16.598203  2600 net.cpp:406] Scale2 <- Convolution2
I0925 11:03:16.598211  2600 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0925 11:03:16.598246  2600 layer_factory.hpp:77] Creating layer Scale2
I0925 11:03:16.598325  2600 net.cpp:122] Setting up Scale2
I0925 11:03:16.598331  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.598335  2600 net.cpp:137] Memory required for data: 46099600
I0925 11:03:16.598346  2600 layer_factory.hpp:77] Creating layer penlu2
I0925 11:03:16.598356  2600 net.cpp:84] Creating Layer penlu2
I0925 11:03:16.598361  2600 net.cpp:406] penlu2 <- Convolution2
I0925 11:03:16.598364  2600 net.cpp:367] penlu2 -> Convolution2 (in-place)
I0925 11:03:16.598459  2600 net.cpp:122] Setting up penlu2
I0925 11:03:16.598465  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.598475  2600 net.cpp:137] Memory required for data: 51117200
I0925 11:03:16.598481  2600 layer_factory.hpp:77] Creating layer Convolution3
I0925 11:03:16.598489  2600 net.cpp:84] Creating Layer Convolution3
I0925 11:03:16.598491  2600 net.cpp:406] Convolution3 <- Convolution2
I0925 11:03:16.598495  2600 net.cpp:380] Convolution3 -> Convolution3
I0925 11:03:16.599345  2600 net.cpp:122] Setting up Convolution3
I0925 11:03:16.599355  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.599359  2600 net.cpp:137] Memory required for data: 56134800
I0925 11:03:16.599364  2600 layer_factory.hpp:77] Creating layer BatchNorm3
I0925 11:03:16.599370  2600 net.cpp:84] Creating Layer BatchNorm3
I0925 11:03:16.599373  2600 net.cpp:406] BatchNorm3 <- Convolution3
I0925 11:03:16.599376  2600 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0925 11:03:16.599498  2600 net.cpp:122] Setting up BatchNorm3
I0925 11:03:16.599504  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.599508  2600 net.cpp:137] Memory required for data: 61152400
I0925 11:03:16.599512  2600 layer_factory.hpp:77] Creating layer Scale3
I0925 11:03:16.599517  2600 net.cpp:84] Creating Layer Scale3
I0925 11:03:16.599520  2600 net.cpp:406] Scale3 <- Convolution3
I0925 11:03:16.599524  2600 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0925 11:03:16.599548  2600 layer_factory.hpp:77] Creating layer Scale3
I0925 11:03:16.599623  2600 net.cpp:122] Setting up Scale3
I0925 11:03:16.599629  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.599632  2600 net.cpp:137] Memory required for data: 66170000
I0925 11:03:16.599637  2600 layer_factory.hpp:77] Creating layer Eltwise1
I0925 11:03:16.599642  2600 net.cpp:84] Creating Layer Eltwise1
I0925 11:03:16.599645  2600 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I0925 11:03:16.599648  2600 net.cpp:406] Eltwise1 <- Convolution3
I0925 11:03:16.599653  2600 net.cpp:380] Eltwise1 -> Eltwise1
I0925 11:03:16.599669  2600 net.cpp:122] Setting up Eltwise1
I0925 11:03:16.599674  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.599678  2600 net.cpp:137] Memory required for data: 71187600
I0925 11:03:16.599680  2600 layer_factory.hpp:77] Creating layer penlu3
I0925 11:03:16.599685  2600 net.cpp:84] Creating Layer penlu3
I0925 11:03:16.599689  2600 net.cpp:406] penlu3 <- Eltwise1
I0925 11:03:16.599692  2600 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I0925 11:03:16.599789  2600 net.cpp:122] Setting up penlu3
I0925 11:03:16.599795  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.599798  2600 net.cpp:137] Memory required for data: 76205200
I0925 11:03:16.599802  2600 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I0925 11:03:16.599807  2600 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I0925 11:03:16.599810  2600 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I0925 11:03:16.599813  2600 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I0925 11:03:16.599818  2600 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I0925 11:03:16.599839  2600 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I0925 11:03:16.599844  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.599849  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.599851  2600 net.cpp:137] Memory required for data: 86240400
I0925 11:03:16.599853  2600 layer_factory.hpp:77] Creating layer Convolution4
I0925 11:03:16.599862  2600 net.cpp:84] Creating Layer Convolution4
I0925 11:03:16.599865  2600 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I0925 11:03:16.599869  2600 net.cpp:380] Convolution4 -> Convolution4
I0925 11:03:16.600749  2600 net.cpp:122] Setting up Convolution4
I0925 11:03:16.600759  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.600764  2600 net.cpp:137] Memory required for data: 91258000
I0925 11:03:16.600767  2600 layer_factory.hpp:77] Creating layer BatchNorm4
I0925 11:03:16.600775  2600 net.cpp:84] Creating Layer BatchNorm4
I0925 11:03:16.600783  2600 net.cpp:406] BatchNorm4 <- Convolution4
I0925 11:03:16.600788  2600 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0925 11:03:16.600908  2600 net.cpp:122] Setting up BatchNorm4
I0925 11:03:16.600914  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.600916  2600 net.cpp:137] Memory required for data: 96275600
I0925 11:03:16.600925  2600 layer_factory.hpp:77] Creating layer Scale4
I0925 11:03:16.600930  2600 net.cpp:84] Creating Layer Scale4
I0925 11:03:16.600934  2600 net.cpp:406] Scale4 <- Convolution4
I0925 11:03:16.600936  2600 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0925 11:03:16.600961  2600 layer_factory.hpp:77] Creating layer Scale4
I0925 11:03:16.601034  2600 net.cpp:122] Setting up Scale4
I0925 11:03:16.601040  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.601043  2600 net.cpp:137] Memory required for data: 101293200
I0925 11:03:16.601047  2600 layer_factory.hpp:77] Creating layer penlu4
I0925 11:03:16.601053  2600 net.cpp:84] Creating Layer penlu4
I0925 11:03:16.601058  2600 net.cpp:406] penlu4 <- Convolution4
I0925 11:03:16.601061  2600 net.cpp:367] penlu4 -> Convolution4 (in-place)
I0925 11:03:16.601155  2600 net.cpp:122] Setting up penlu4
I0925 11:03:16.601161  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.601164  2600 net.cpp:137] Memory required for data: 106310800
I0925 11:03:16.601168  2600 layer_factory.hpp:77] Creating layer Convolution5
I0925 11:03:16.601176  2600 net.cpp:84] Creating Layer Convolution5
I0925 11:03:16.601179  2600 net.cpp:406] Convolution5 <- Convolution4
I0925 11:03:16.601183  2600 net.cpp:380] Convolution5 -> Convolution5
I0925 11:03:16.602030  2600 net.cpp:122] Setting up Convolution5
I0925 11:03:16.602039  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.602043  2600 net.cpp:137] Memory required for data: 111328400
I0925 11:03:16.602047  2600 layer_factory.hpp:77] Creating layer BatchNorm5
I0925 11:03:16.602053  2600 net.cpp:84] Creating Layer BatchNorm5
I0925 11:03:16.602057  2600 net.cpp:406] BatchNorm5 <- Convolution5
I0925 11:03:16.602061  2600 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0925 11:03:16.602183  2600 net.cpp:122] Setting up BatchNorm5
I0925 11:03:16.602190  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.602191  2600 net.cpp:137] Memory required for data: 116346000
I0925 11:03:16.602196  2600 layer_factory.hpp:77] Creating layer Scale5
I0925 11:03:16.602202  2600 net.cpp:84] Creating Layer Scale5
I0925 11:03:16.602205  2600 net.cpp:406] Scale5 <- Convolution5
I0925 11:03:16.602208  2600 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0925 11:03:16.602233  2600 layer_factory.hpp:77] Creating layer Scale5
I0925 11:03:16.602309  2600 net.cpp:122] Setting up Scale5
I0925 11:03:16.602315  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.602319  2600 net.cpp:137] Memory required for data: 121363600
I0925 11:03:16.602322  2600 layer_factory.hpp:77] Creating layer Eltwise2
I0925 11:03:16.602327  2600 net.cpp:84] Creating Layer Eltwise2
I0925 11:03:16.602330  2600 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I0925 11:03:16.602334  2600 net.cpp:406] Eltwise2 <- Convolution5
I0925 11:03:16.602337  2600 net.cpp:380] Eltwise2 -> Eltwise2
I0925 11:03:16.602352  2600 net.cpp:122] Setting up Eltwise2
I0925 11:03:16.602357  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.602360  2600 net.cpp:137] Memory required for data: 126381200
I0925 11:03:16.602362  2600 layer_factory.hpp:77] Creating layer penlu5
I0925 11:03:16.602368  2600 net.cpp:84] Creating Layer penlu5
I0925 11:03:16.602371  2600 net.cpp:406] penlu5 <- Eltwise2
I0925 11:03:16.602375  2600 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I0925 11:03:16.602473  2600 net.cpp:122] Setting up penlu5
I0925 11:03:16.602478  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.602481  2600 net.cpp:137] Memory required for data: 131398800
I0925 11:03:16.602485  2600 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I0925 11:03:16.602495  2600 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I0925 11:03:16.602499  2600 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I0925 11:03:16.602502  2600 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I0925 11:03:16.602509  2600 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I0925 11:03:16.602530  2600 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I0925 11:03:16.602535  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.602538  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.602540  2600 net.cpp:137] Memory required for data: 141434000
I0925 11:03:16.602542  2600 layer_factory.hpp:77] Creating layer Convolution6
I0925 11:03:16.602550  2600 net.cpp:84] Creating Layer Convolution6
I0925 11:03:16.602552  2600 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I0925 11:03:16.602556  2600 net.cpp:380] Convolution6 -> Convolution6
I0925 11:03:16.603407  2600 net.cpp:122] Setting up Convolution6
I0925 11:03:16.603417  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.603420  2600 net.cpp:137] Memory required for data: 146451600
I0925 11:03:16.603425  2600 layer_factory.hpp:77] Creating layer BatchNorm6
I0925 11:03:16.603431  2600 net.cpp:84] Creating Layer BatchNorm6
I0925 11:03:16.603435  2600 net.cpp:406] BatchNorm6 <- Convolution6
I0925 11:03:16.603440  2600 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0925 11:03:16.603562  2600 net.cpp:122] Setting up BatchNorm6
I0925 11:03:16.603569  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.603571  2600 net.cpp:137] Memory required for data: 151469200
I0925 11:03:16.603576  2600 layer_factory.hpp:77] Creating layer Scale6
I0925 11:03:16.603581  2600 net.cpp:84] Creating Layer Scale6
I0925 11:03:16.603585  2600 net.cpp:406] Scale6 <- Convolution6
I0925 11:03:16.603587  2600 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0925 11:03:16.603613  2600 layer_factory.hpp:77] Creating layer Scale6
I0925 11:03:16.603688  2600 net.cpp:122] Setting up Scale6
I0925 11:03:16.603694  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.603698  2600 net.cpp:137] Memory required for data: 156486800
I0925 11:03:16.603701  2600 layer_factory.hpp:77] Creating layer penlu6
I0925 11:03:16.603708  2600 net.cpp:84] Creating Layer penlu6
I0925 11:03:16.603710  2600 net.cpp:406] penlu6 <- Convolution6
I0925 11:03:16.603715  2600 net.cpp:367] penlu6 -> Convolution6 (in-place)
I0925 11:03:16.603814  2600 net.cpp:122] Setting up penlu6
I0925 11:03:16.603821  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.603822  2600 net.cpp:137] Memory required for data: 161504400
I0925 11:03:16.603826  2600 layer_factory.hpp:77] Creating layer Convolution7
I0925 11:03:16.603834  2600 net.cpp:84] Creating Layer Convolution7
I0925 11:03:16.603837  2600 net.cpp:406] Convolution7 <- Convolution6
I0925 11:03:16.603840  2600 net.cpp:380] Convolution7 -> Convolution7
I0925 11:03:16.604375  2600 net.cpp:122] Setting up Convolution7
I0925 11:03:16.604384  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.604389  2600 net.cpp:137] Memory required for data: 166522000
I0925 11:03:16.604393  2600 layer_factory.hpp:77] Creating layer BatchNorm7
I0925 11:03:16.604398  2600 net.cpp:84] Creating Layer BatchNorm7
I0925 11:03:16.604401  2600 net.cpp:406] BatchNorm7 <- Convolution7
I0925 11:03:16.604405  2600 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0925 11:03:16.604554  2600 net.cpp:122] Setting up BatchNorm7
I0925 11:03:16.604562  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.604563  2600 net.cpp:137] Memory required for data: 171539600
I0925 11:03:16.604573  2600 layer_factory.hpp:77] Creating layer Scale7
I0925 11:03:16.604579  2600 net.cpp:84] Creating Layer Scale7
I0925 11:03:16.604583  2600 net.cpp:406] Scale7 <- Convolution7
I0925 11:03:16.604588  2600 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0925 11:03:16.604614  2600 layer_factory.hpp:77] Creating layer Scale7
I0925 11:03:16.604688  2600 net.cpp:122] Setting up Scale7
I0925 11:03:16.604701  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.604703  2600 net.cpp:137] Memory required for data: 176557200
I0925 11:03:16.604707  2600 layer_factory.hpp:77] Creating layer Eltwise3
I0925 11:03:16.604712  2600 net.cpp:84] Creating Layer Eltwise3
I0925 11:03:16.604715  2600 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I0925 11:03:16.604718  2600 net.cpp:406] Eltwise3 <- Convolution7
I0925 11:03:16.604722  2600 net.cpp:380] Eltwise3 -> Eltwise3
I0925 11:03:16.604738  2600 net.cpp:122] Setting up Eltwise3
I0925 11:03:16.604743  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.604745  2600 net.cpp:137] Memory required for data: 181574800
I0925 11:03:16.604748  2600 layer_factory.hpp:77] Creating layer penlu7
I0925 11:03:16.604753  2600 net.cpp:84] Creating Layer penlu7
I0925 11:03:16.604755  2600 net.cpp:406] penlu7 <- Eltwise3
I0925 11:03:16.604760  2600 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I0925 11:03:16.604862  2600 net.cpp:122] Setting up penlu7
I0925 11:03:16.604867  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.604871  2600 net.cpp:137] Memory required for data: 186592400
I0925 11:03:16.604876  2600 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I0925 11:03:16.604879  2600 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I0925 11:03:16.604882  2600 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I0925 11:03:16.604887  2600 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I0925 11:03:16.604890  2600 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I0925 11:03:16.604912  2600 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I0925 11:03:16.604918  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.604919  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.604923  2600 net.cpp:137] Memory required for data: 196627600
I0925 11:03:16.604924  2600 layer_factory.hpp:77] Creating layer Convolution8
I0925 11:03:16.604929  2600 net.cpp:84] Creating Layer Convolution8
I0925 11:03:16.604933  2600 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I0925 11:03:16.604938  2600 net.cpp:380] Convolution8 -> Convolution8
I0925 11:03:16.605784  2600 net.cpp:122] Setting up Convolution8
I0925 11:03:16.605795  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.605799  2600 net.cpp:137] Memory required for data: 201645200
I0925 11:03:16.605803  2600 layer_factory.hpp:77] Creating layer BatchNorm8
I0925 11:03:16.605809  2600 net.cpp:84] Creating Layer BatchNorm8
I0925 11:03:16.605813  2600 net.cpp:406] BatchNorm8 <- Convolution8
I0925 11:03:16.605816  2600 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0925 11:03:16.605940  2600 net.cpp:122] Setting up BatchNorm8
I0925 11:03:16.605947  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.605949  2600 net.cpp:137] Memory required for data: 206662800
I0925 11:03:16.605954  2600 layer_factory.hpp:77] Creating layer Scale8
I0925 11:03:16.605959  2600 net.cpp:84] Creating Layer Scale8
I0925 11:03:16.605962  2600 net.cpp:406] Scale8 <- Convolution8
I0925 11:03:16.605967  2600 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0925 11:03:16.605993  2600 layer_factory.hpp:77] Creating layer Scale8
I0925 11:03:16.606067  2600 net.cpp:122] Setting up Scale8
I0925 11:03:16.606073  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.606076  2600 net.cpp:137] Memory required for data: 211680400
I0925 11:03:16.606081  2600 layer_factory.hpp:77] Creating layer penlu8
I0925 11:03:16.606096  2600 net.cpp:84] Creating Layer penlu8
I0925 11:03:16.606099  2600 net.cpp:406] penlu8 <- Convolution8
I0925 11:03:16.606103  2600 net.cpp:367] penlu8 -> Convolution8 (in-place)
I0925 11:03:16.606214  2600 net.cpp:122] Setting up penlu8
I0925 11:03:16.606220  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.606222  2600 net.cpp:137] Memory required for data: 216698000
I0925 11:03:16.606236  2600 layer_factory.hpp:77] Creating layer Convolution9
I0925 11:03:16.606245  2600 net.cpp:84] Creating Layer Convolution9
I0925 11:03:16.606253  2600 net.cpp:406] Convolution9 <- Convolution8
I0925 11:03:16.606259  2600 net.cpp:380] Convolution9 -> Convolution9
I0925 11:03:16.607408  2600 net.cpp:122] Setting up Convolution9
I0925 11:03:16.607421  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.607427  2600 net.cpp:137] Memory required for data: 221715600
I0925 11:03:16.607434  2600 layer_factory.hpp:77] Creating layer BatchNorm9
I0925 11:03:16.607444  2600 net.cpp:84] Creating Layer BatchNorm9
I0925 11:03:16.607448  2600 net.cpp:406] BatchNorm9 <- Convolution9
I0925 11:03:16.607455  2600 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0925 11:03:16.607635  2600 net.cpp:122] Setting up BatchNorm9
I0925 11:03:16.607645  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.607657  2600 net.cpp:137] Memory required for data: 226733200
I0925 11:03:16.607662  2600 layer_factory.hpp:77] Creating layer Scale9
I0925 11:03:16.607667  2600 net.cpp:84] Creating Layer Scale9
I0925 11:03:16.607671  2600 net.cpp:406] Scale9 <- Convolution9
I0925 11:03:16.607676  2600 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0925 11:03:16.607727  2600 layer_factory.hpp:77] Creating layer Scale9
I0925 11:03:16.607877  2600 net.cpp:122] Setting up Scale9
I0925 11:03:16.607897  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.607902  2600 net.cpp:137] Memory required for data: 231750800
I0925 11:03:16.607918  2600 layer_factory.hpp:77] Creating layer Eltwise4
I0925 11:03:16.607926  2600 net.cpp:84] Creating Layer Eltwise4
I0925 11:03:16.607931  2600 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I0925 11:03:16.607936  2600 net.cpp:406] Eltwise4 <- Convolution9
I0925 11:03:16.607944  2600 net.cpp:380] Eltwise4 -> Eltwise4
I0925 11:03:16.607980  2600 net.cpp:122] Setting up Eltwise4
I0925 11:03:16.607988  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.608001  2600 net.cpp:137] Memory required for data: 236768400
I0925 11:03:16.608006  2600 layer_factory.hpp:77] Creating layer penlu9
I0925 11:03:16.608014  2600 net.cpp:84] Creating Layer penlu9
I0925 11:03:16.608021  2600 net.cpp:406] penlu9 <- Eltwise4
I0925 11:03:16.608028  2600 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I0925 11:03:16.608217  2600 net.cpp:122] Setting up penlu9
I0925 11:03:16.608230  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.608235  2600 net.cpp:137] Memory required for data: 241786000
I0925 11:03:16.608243  2600 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I0925 11:03:16.608253  2600 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I0925 11:03:16.608258  2600 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I0925 11:03:16.608264  2600 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I0925 11:03:16.608271  2600 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I0925 11:03:16.608306  2600 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I0925 11:03:16.608314  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.608319  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.608324  2600 net.cpp:137] Memory required for data: 251821200
I0925 11:03:16.608328  2600 layer_factory.hpp:77] Creating layer Convolution10
I0925 11:03:16.608345  2600 net.cpp:84] Creating Layer Convolution10
I0925 11:03:16.608350  2600 net.cpp:406] Convolution10 <- Eltwise4_penlu9_0_split_0
I0925 11:03:16.608355  2600 net.cpp:380] Convolution10 -> Convolution10
I0925 11:03:16.609247  2600 net.cpp:122] Setting up Convolution10
I0925 11:03:16.609258  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.609263  2600 net.cpp:137] Memory required for data: 256838800
I0925 11:03:16.609272  2600 layer_factory.hpp:77] Creating layer BatchNorm10
I0925 11:03:16.609279  2600 net.cpp:84] Creating Layer BatchNorm10
I0925 11:03:16.609283  2600 net.cpp:406] BatchNorm10 <- Convolution10
I0925 11:03:16.609289  2600 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0925 11:03:16.609421  2600 net.cpp:122] Setting up BatchNorm10
I0925 11:03:16.609434  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.609449  2600 net.cpp:137] Memory required for data: 261856400
I0925 11:03:16.609457  2600 layer_factory.hpp:77] Creating layer Scale10
I0925 11:03:16.609463  2600 net.cpp:84] Creating Layer Scale10
I0925 11:03:16.609468  2600 net.cpp:406] Scale10 <- Convolution10
I0925 11:03:16.609473  2600 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0925 11:03:16.609513  2600 layer_factory.hpp:77] Creating layer Scale10
I0925 11:03:16.609614  2600 net.cpp:122] Setting up Scale10
I0925 11:03:16.609622  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.609624  2600 net.cpp:137] Memory required for data: 266874000
I0925 11:03:16.609630  2600 layer_factory.hpp:77] Creating layer penlu10
I0925 11:03:16.609638  2600 net.cpp:84] Creating Layer penlu10
I0925 11:03:16.609642  2600 net.cpp:406] penlu10 <- Convolution10
I0925 11:03:16.609648  2600 net.cpp:367] penlu10 -> Convolution10 (in-place)
I0925 11:03:16.609760  2600 net.cpp:122] Setting up penlu10
I0925 11:03:16.609766  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.609768  2600 net.cpp:137] Memory required for data: 271891600
I0925 11:03:16.609786  2600 layer_factory.hpp:77] Creating layer Convolution11
I0925 11:03:16.609794  2600 net.cpp:84] Creating Layer Convolution11
I0925 11:03:16.609798  2600 net.cpp:406] Convolution11 <- Convolution10
I0925 11:03:16.609804  2600 net.cpp:380] Convolution11 -> Convolution11
I0925 11:03:16.610743  2600 net.cpp:122] Setting up Convolution11
I0925 11:03:16.610754  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.610756  2600 net.cpp:137] Memory required for data: 276909200
I0925 11:03:16.610764  2600 layer_factory.hpp:77] Creating layer BatchNorm11
I0925 11:03:16.610771  2600 net.cpp:84] Creating Layer BatchNorm11
I0925 11:03:16.610775  2600 net.cpp:406] BatchNorm11 <- Convolution11
I0925 11:03:16.610780  2600 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0925 11:03:16.610908  2600 net.cpp:122] Setting up BatchNorm11
I0925 11:03:16.610915  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.610919  2600 net.cpp:137] Memory required for data: 281926800
I0925 11:03:16.610926  2600 layer_factory.hpp:77] Creating layer Scale11
I0925 11:03:16.610932  2600 net.cpp:84] Creating Layer Scale11
I0925 11:03:16.610936  2600 net.cpp:406] Scale11 <- Convolution11
I0925 11:03:16.610941  2600 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0925 11:03:16.610971  2600 layer_factory.hpp:77] Creating layer Scale11
I0925 11:03:16.611048  2600 net.cpp:122] Setting up Scale11
I0925 11:03:16.611055  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.611059  2600 net.cpp:137] Memory required for data: 286944400
I0925 11:03:16.611065  2600 layer_factory.hpp:77] Creating layer Eltwise5
I0925 11:03:16.611071  2600 net.cpp:84] Creating Layer Eltwise5
I0925 11:03:16.611075  2600 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I0925 11:03:16.611080  2600 net.cpp:406] Eltwise5 <- Convolution11
I0925 11:03:16.611086  2600 net.cpp:380] Eltwise5 -> Eltwise5
I0925 11:03:16.611105  2600 net.cpp:122] Setting up Eltwise5
I0925 11:03:16.611110  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.611114  2600 net.cpp:137] Memory required for data: 291962000
I0925 11:03:16.611117  2600 layer_factory.hpp:77] Creating layer penlu11
I0925 11:03:16.611125  2600 net.cpp:84] Creating Layer penlu11
I0925 11:03:16.611129  2600 net.cpp:406] penlu11 <- Eltwise5
I0925 11:03:16.611133  2600 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I0925 11:03:16.611243  2600 net.cpp:122] Setting up penlu11
I0925 11:03:16.611248  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.611253  2600 net.cpp:137] Memory required for data: 296979600
I0925 11:03:16.611259  2600 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I0925 11:03:16.611263  2600 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I0925 11:03:16.611268  2600 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I0925 11:03:16.611281  2600 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I0925 11:03:16.611289  2600 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I0925 11:03:16.611313  2600 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I0925 11:03:16.611320  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.611325  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.611328  2600 net.cpp:137] Memory required for data: 307014800
I0925 11:03:16.611331  2600 layer_factory.hpp:77] Creating layer Convolution12
I0925 11:03:16.611341  2600 net.cpp:84] Creating Layer Convolution12
I0925 11:03:16.611344  2600 net.cpp:406] Convolution12 <- Eltwise5_penlu11_0_split_0
I0925 11:03:16.611351  2600 net.cpp:380] Convolution12 -> Convolution12
I0925 11:03:16.612221  2600 net.cpp:122] Setting up Convolution12
I0925 11:03:16.612232  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.612237  2600 net.cpp:137] Memory required for data: 312032400
I0925 11:03:16.612244  2600 layer_factory.hpp:77] Creating layer BatchNorm12
I0925 11:03:16.612252  2600 net.cpp:84] Creating Layer BatchNorm12
I0925 11:03:16.612257  2600 net.cpp:406] BatchNorm12 <- Convolution12
I0925 11:03:16.612262  2600 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0925 11:03:16.612390  2600 net.cpp:122] Setting up BatchNorm12
I0925 11:03:16.612396  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.612401  2600 net.cpp:137] Memory required for data: 317050000
I0925 11:03:16.612408  2600 layer_factory.hpp:77] Creating layer Scale12
I0925 11:03:16.612416  2600 net.cpp:84] Creating Layer Scale12
I0925 11:03:16.612419  2600 net.cpp:406] Scale12 <- Convolution12
I0925 11:03:16.612424  2600 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0925 11:03:16.612452  2600 layer_factory.hpp:77] Creating layer Scale12
I0925 11:03:16.612538  2600 net.cpp:122] Setting up Scale12
I0925 11:03:16.612545  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.612550  2600 net.cpp:137] Memory required for data: 322067600
I0925 11:03:16.612556  2600 layer_factory.hpp:77] Creating layer penlu12
I0925 11:03:16.612565  2600 net.cpp:84] Creating Layer penlu12
I0925 11:03:16.612567  2600 net.cpp:406] penlu12 <- Convolution12
I0925 11:03:16.612572  2600 net.cpp:367] penlu12 -> Convolution12 (in-place)
I0925 11:03:16.612680  2600 net.cpp:122] Setting up penlu12
I0925 11:03:16.612687  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.612691  2600 net.cpp:137] Memory required for data: 327085200
I0925 11:03:16.612699  2600 layer_factory.hpp:77] Creating layer Convolution13
I0925 11:03:16.612707  2600 net.cpp:84] Creating Layer Convolution13
I0925 11:03:16.612711  2600 net.cpp:406] Convolution13 <- Convolution12
I0925 11:03:16.612717  2600 net.cpp:380] Convolution13 -> Convolution13
I0925 11:03:16.613581  2600 net.cpp:122] Setting up Convolution13
I0925 11:03:16.613592  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.613597  2600 net.cpp:137] Memory required for data: 332102800
I0925 11:03:16.613605  2600 layer_factory.hpp:77] Creating layer BatchNorm13
I0925 11:03:16.613612  2600 net.cpp:84] Creating Layer BatchNorm13
I0925 11:03:16.613616  2600 net.cpp:406] BatchNorm13 <- Convolution13
I0925 11:03:16.613623  2600 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0925 11:03:16.613762  2600 net.cpp:122] Setting up BatchNorm13
I0925 11:03:16.613770  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.613773  2600 net.cpp:137] Memory required for data: 337120400
I0925 11:03:16.613790  2600 layer_factory.hpp:77] Creating layer Scale13
I0925 11:03:16.613796  2600 net.cpp:84] Creating Layer Scale13
I0925 11:03:16.613801  2600 net.cpp:406] Scale13 <- Convolution13
I0925 11:03:16.613806  2600 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0925 11:03:16.613836  2600 layer_factory.hpp:77] Creating layer Scale13
I0925 11:03:16.613914  2600 net.cpp:122] Setting up Scale13
I0925 11:03:16.613920  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.613931  2600 net.cpp:137] Memory required for data: 342138000
I0925 11:03:16.613939  2600 layer_factory.hpp:77] Creating layer Eltwise6
I0925 11:03:16.613945  2600 net.cpp:84] Creating Layer Eltwise6
I0925 11:03:16.613950  2600 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I0925 11:03:16.613955  2600 net.cpp:406] Eltwise6 <- Convolution13
I0925 11:03:16.613961  2600 net.cpp:380] Eltwise6 -> Eltwise6
I0925 11:03:16.613981  2600 net.cpp:122] Setting up Eltwise6
I0925 11:03:16.613989  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.613992  2600 net.cpp:137] Memory required for data: 347155600
I0925 11:03:16.613996  2600 layer_factory.hpp:77] Creating layer penlu13
I0925 11:03:16.614006  2600 net.cpp:84] Creating Layer penlu13
I0925 11:03:16.614009  2600 net.cpp:406] penlu13 <- Eltwise6
I0925 11:03:16.614017  2600 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I0925 11:03:16.614123  2600 net.cpp:122] Setting up penlu13
I0925 11:03:16.614131  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.614135  2600 net.cpp:137] Memory required for data: 352173200
I0925 11:03:16.614151  2600 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I0925 11:03:16.614156  2600 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I0925 11:03:16.614161  2600 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I0925 11:03:16.614166  2600 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I0925 11:03:16.614173  2600 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I0925 11:03:16.614199  2600 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I0925 11:03:16.614204  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.614209  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.614213  2600 net.cpp:137] Memory required for data: 362208400
I0925 11:03:16.614217  2600 layer_factory.hpp:77] Creating layer Convolution14
I0925 11:03:16.614224  2600 net.cpp:84] Creating Layer Convolution14
I0925 11:03:16.614228  2600 net.cpp:406] Convolution14 <- Eltwise6_penlu13_0_split_0
I0925 11:03:16.614233  2600 net.cpp:380] Convolution14 -> Convolution14
I0925 11:03:16.615093  2600 net.cpp:122] Setting up Convolution14
I0925 11:03:16.615103  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.615105  2600 net.cpp:137] Memory required for data: 367226000
I0925 11:03:16.615110  2600 layer_factory.hpp:77] Creating layer BatchNorm14
I0925 11:03:16.615115  2600 net.cpp:84] Creating Layer BatchNorm14
I0925 11:03:16.615118  2600 net.cpp:406] BatchNorm14 <- Convolution14
I0925 11:03:16.615121  2600 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0925 11:03:16.615252  2600 net.cpp:122] Setting up BatchNorm14
I0925 11:03:16.615257  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.615258  2600 net.cpp:137] Memory required for data: 372243600
I0925 11:03:16.615262  2600 layer_factory.hpp:77] Creating layer Scale14
I0925 11:03:16.615267  2600 net.cpp:84] Creating Layer Scale14
I0925 11:03:16.615269  2600 net.cpp:406] Scale14 <- Convolution14
I0925 11:03:16.615272  2600 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0925 11:03:16.615298  2600 layer_factory.hpp:77] Creating layer Scale14
I0925 11:03:16.615371  2600 net.cpp:122] Setting up Scale14
I0925 11:03:16.615376  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.615378  2600 net.cpp:137] Memory required for data: 377261200
I0925 11:03:16.615382  2600 layer_factory.hpp:77] Creating layer penlu14
I0925 11:03:16.615387  2600 net.cpp:84] Creating Layer penlu14
I0925 11:03:16.615391  2600 net.cpp:406] penlu14 <- Convolution14
I0925 11:03:16.615393  2600 net.cpp:367] penlu14 -> Convolution14 (in-place)
I0925 11:03:16.615495  2600 net.cpp:122] Setting up penlu14
I0925 11:03:16.615499  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.615501  2600 net.cpp:137] Memory required for data: 382278800
I0925 11:03:16.615505  2600 layer_factory.hpp:77] Creating layer Convolution15
I0925 11:03:16.615512  2600 net.cpp:84] Creating Layer Convolution15
I0925 11:03:16.615521  2600 net.cpp:406] Convolution15 <- Convolution14
I0925 11:03:16.615525  2600 net.cpp:380] Convolution15 -> Convolution15
I0925 11:03:16.616405  2600 net.cpp:122] Setting up Convolution15
I0925 11:03:16.616415  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.616417  2600 net.cpp:137] Memory required for data: 387296400
I0925 11:03:16.616421  2600 layer_factory.hpp:77] Creating layer BatchNorm15
I0925 11:03:16.616426  2600 net.cpp:84] Creating Layer BatchNorm15
I0925 11:03:16.616430  2600 net.cpp:406] BatchNorm15 <- Convolution15
I0925 11:03:16.616433  2600 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0925 11:03:16.616564  2600 net.cpp:122] Setting up BatchNorm15
I0925 11:03:16.616569  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.616571  2600 net.cpp:137] Memory required for data: 392314000
I0925 11:03:16.616576  2600 layer_factory.hpp:77] Creating layer Scale15
I0925 11:03:16.616581  2600 net.cpp:84] Creating Layer Scale15
I0925 11:03:16.616585  2600 net.cpp:406] Scale15 <- Convolution15
I0925 11:03:16.616587  2600 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0925 11:03:16.616612  2600 layer_factory.hpp:77] Creating layer Scale15
I0925 11:03:16.616688  2600 net.cpp:122] Setting up Scale15
I0925 11:03:16.616691  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.616693  2600 net.cpp:137] Memory required for data: 397331600
I0925 11:03:16.616698  2600 layer_factory.hpp:77] Creating layer Eltwise7
I0925 11:03:16.616701  2600 net.cpp:84] Creating Layer Eltwise7
I0925 11:03:16.616703  2600 net.cpp:406] Eltwise7 <- Eltwise6_penlu13_0_split_1
I0925 11:03:16.616706  2600 net.cpp:406] Eltwise7 <- Convolution15
I0925 11:03:16.616710  2600 net.cpp:380] Eltwise7 -> Eltwise7
I0925 11:03:16.616725  2600 net.cpp:122] Setting up Eltwise7
I0925 11:03:16.616729  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.616732  2600 net.cpp:137] Memory required for data: 402349200
I0925 11:03:16.616734  2600 layer_factory.hpp:77] Creating layer penlu15
I0925 11:03:16.616739  2600 net.cpp:84] Creating Layer penlu15
I0925 11:03:16.616741  2600 net.cpp:406] penlu15 <- Eltwise7
I0925 11:03:16.616745  2600 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I0925 11:03:16.616848  2600 net.cpp:122] Setting up penlu15
I0925 11:03:16.616853  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.616855  2600 net.cpp:137] Memory required for data: 407366800
I0925 11:03:16.616859  2600 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I0925 11:03:16.616863  2600 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I0925 11:03:16.616865  2600 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I0925 11:03:16.616868  2600 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I0925 11:03:16.616873  2600 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I0925 11:03:16.616894  2600 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I0925 11:03:16.616899  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.616900  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.616902  2600 net.cpp:137] Memory required for data: 417402000
I0925 11:03:16.616904  2600 layer_factory.hpp:77] Creating layer Convolution16
I0925 11:03:16.616911  2600 net.cpp:84] Creating Layer Convolution16
I0925 11:03:16.616914  2600 net.cpp:406] Convolution16 <- Eltwise7_penlu15_0_split_0
I0925 11:03:16.616917  2600 net.cpp:380] Convolution16 -> Convolution16
I0925 11:03:16.617772  2600 net.cpp:122] Setting up Convolution16
I0925 11:03:16.617781  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.617784  2600 net.cpp:137] Memory required for data: 422419600
I0925 11:03:16.617789  2600 layer_factory.hpp:77] Creating layer BatchNorm16
I0925 11:03:16.617794  2600 net.cpp:84] Creating Layer BatchNorm16
I0925 11:03:16.617795  2600 net.cpp:406] BatchNorm16 <- Convolution16
I0925 11:03:16.617800  2600 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0925 11:03:16.617926  2600 net.cpp:122] Setting up BatchNorm16
I0925 11:03:16.617938  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.617940  2600 net.cpp:137] Memory required for data: 427437200
I0925 11:03:16.617945  2600 layer_factory.hpp:77] Creating layer Scale16
I0925 11:03:16.617950  2600 net.cpp:84] Creating Layer Scale16
I0925 11:03:16.617954  2600 net.cpp:406] Scale16 <- Convolution16
I0925 11:03:16.617956  2600 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0925 11:03:16.617982  2600 layer_factory.hpp:77] Creating layer Scale16
I0925 11:03:16.618058  2600 net.cpp:122] Setting up Scale16
I0925 11:03:16.618063  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.618065  2600 net.cpp:137] Memory required for data: 432454800
I0925 11:03:16.618069  2600 layer_factory.hpp:77] Creating layer penlu16
I0925 11:03:16.618074  2600 net.cpp:84] Creating Layer penlu16
I0925 11:03:16.618077  2600 net.cpp:406] penlu16 <- Convolution16
I0925 11:03:16.618079  2600 net.cpp:367] penlu16 -> Convolution16 (in-place)
I0925 11:03:16.618185  2600 net.cpp:122] Setting up penlu16
I0925 11:03:16.618190  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.618191  2600 net.cpp:137] Memory required for data: 437472400
I0925 11:03:16.618196  2600 layer_factory.hpp:77] Creating layer Convolution17
I0925 11:03:16.618201  2600 net.cpp:84] Creating Layer Convolution17
I0925 11:03:16.618204  2600 net.cpp:406] Convolution17 <- Convolution16
I0925 11:03:16.618207  2600 net.cpp:380] Convolution17 -> Convolution17
I0925 11:03:16.618754  2600 net.cpp:122] Setting up Convolution17
I0925 11:03:16.618762  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.618764  2600 net.cpp:137] Memory required for data: 442490000
I0925 11:03:16.618768  2600 layer_factory.hpp:77] Creating layer BatchNorm17
I0925 11:03:16.618773  2600 net.cpp:84] Creating Layer BatchNorm17
I0925 11:03:16.618777  2600 net.cpp:406] BatchNorm17 <- Convolution17
I0925 11:03:16.618779  2600 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0925 11:03:16.618906  2600 net.cpp:122] Setting up BatchNorm17
I0925 11:03:16.618911  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.618912  2600 net.cpp:137] Memory required for data: 447507600
I0925 11:03:16.618917  2600 layer_factory.hpp:77] Creating layer Scale17
I0925 11:03:16.618921  2600 net.cpp:84] Creating Layer Scale17
I0925 11:03:16.618923  2600 net.cpp:406] Scale17 <- Convolution17
I0925 11:03:16.618927  2600 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0925 11:03:16.618952  2600 layer_factory.hpp:77] Creating layer Scale17
I0925 11:03:16.619026  2600 net.cpp:122] Setting up Scale17
I0925 11:03:16.619031  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.619033  2600 net.cpp:137] Memory required for data: 452525200
I0925 11:03:16.619037  2600 layer_factory.hpp:77] Creating layer Eltwise8
I0925 11:03:16.619042  2600 net.cpp:84] Creating Layer Eltwise8
I0925 11:03:16.619045  2600 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I0925 11:03:16.619047  2600 net.cpp:406] Eltwise8 <- Convolution17
I0925 11:03:16.619050  2600 net.cpp:380] Eltwise8 -> Eltwise8
I0925 11:03:16.619065  2600 net.cpp:122] Setting up Eltwise8
I0925 11:03:16.619069  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.619071  2600 net.cpp:137] Memory required for data: 457542800
I0925 11:03:16.619073  2600 layer_factory.hpp:77] Creating layer penlu17
I0925 11:03:16.619078  2600 net.cpp:84] Creating Layer penlu17
I0925 11:03:16.619081  2600 net.cpp:406] penlu17 <- Eltwise8
I0925 11:03:16.619083  2600 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I0925 11:03:16.619189  2600 net.cpp:122] Setting up penlu17
I0925 11:03:16.619194  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.619195  2600 net.cpp:137] Memory required for data: 462560400
I0925 11:03:16.619200  2600 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I0925 11:03:16.619204  2600 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I0925 11:03:16.619206  2600 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I0925 11:03:16.619215  2600 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I0925 11:03:16.619220  2600 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I0925 11:03:16.619242  2600 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I0925 11:03:16.619246  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.619249  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.619251  2600 net.cpp:137] Memory required for data: 472595600
I0925 11:03:16.619253  2600 layer_factory.hpp:77] Creating layer Convolution18
I0925 11:03:16.619258  2600 net.cpp:84] Creating Layer Convolution18
I0925 11:03:16.619261  2600 net.cpp:406] Convolution18 <- Eltwise8_penlu17_0_split_0
I0925 11:03:16.619266  2600 net.cpp:380] Convolution18 -> Convolution18
I0925 11:03:16.620124  2600 net.cpp:122] Setting up Convolution18
I0925 11:03:16.620132  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.620136  2600 net.cpp:137] Memory required for data: 477613200
I0925 11:03:16.620139  2600 layer_factory.hpp:77] Creating layer BatchNorm18
I0925 11:03:16.620144  2600 net.cpp:84] Creating Layer BatchNorm18
I0925 11:03:16.620147  2600 net.cpp:406] BatchNorm18 <- Convolution18
I0925 11:03:16.620151  2600 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0925 11:03:16.620278  2600 net.cpp:122] Setting up BatchNorm18
I0925 11:03:16.620282  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.620285  2600 net.cpp:137] Memory required for data: 482630800
I0925 11:03:16.620290  2600 layer_factory.hpp:77] Creating layer Scale18
I0925 11:03:16.620293  2600 net.cpp:84] Creating Layer Scale18
I0925 11:03:16.620296  2600 net.cpp:406] Scale18 <- Convolution18
I0925 11:03:16.620298  2600 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0925 11:03:16.620326  2600 layer_factory.hpp:77] Creating layer Scale18
I0925 11:03:16.620400  2600 net.cpp:122] Setting up Scale18
I0925 11:03:16.620406  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.620409  2600 net.cpp:137] Memory required for data: 487648400
I0925 11:03:16.620411  2600 layer_factory.hpp:77] Creating layer penlu18
I0925 11:03:16.620416  2600 net.cpp:84] Creating Layer penlu18
I0925 11:03:16.620419  2600 net.cpp:406] penlu18 <- Convolution18
I0925 11:03:16.620422  2600 net.cpp:367] penlu18 -> Convolution18 (in-place)
I0925 11:03:16.620533  2600 net.cpp:122] Setting up penlu18
I0925 11:03:16.620539  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.620542  2600 net.cpp:137] Memory required for data: 492666000
I0925 11:03:16.620545  2600 layer_factory.hpp:77] Creating layer Convolution19
I0925 11:03:16.620553  2600 net.cpp:84] Creating Layer Convolution19
I0925 11:03:16.620554  2600 net.cpp:406] Convolution19 <- Convolution18
I0925 11:03:16.620558  2600 net.cpp:380] Convolution19 -> Convolution19
I0925 11:03:16.621434  2600 net.cpp:122] Setting up Convolution19
I0925 11:03:16.621443  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.621445  2600 net.cpp:137] Memory required for data: 497683600
I0925 11:03:16.621449  2600 layer_factory.hpp:77] Creating layer BatchNorm19
I0925 11:03:16.621454  2600 net.cpp:84] Creating Layer BatchNorm19
I0925 11:03:16.621457  2600 net.cpp:406] BatchNorm19 <- Convolution19
I0925 11:03:16.621461  2600 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0925 11:03:16.621588  2600 net.cpp:122] Setting up BatchNorm19
I0925 11:03:16.621593  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.621595  2600 net.cpp:137] Memory required for data: 502701200
I0925 11:03:16.621599  2600 layer_factory.hpp:77] Creating layer Scale19
I0925 11:03:16.621603  2600 net.cpp:84] Creating Layer Scale19
I0925 11:03:16.621606  2600 net.cpp:406] Scale19 <- Convolution19
I0925 11:03:16.621609  2600 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0925 11:03:16.621635  2600 layer_factory.hpp:77] Creating layer Scale19
I0925 11:03:16.621711  2600 net.cpp:122] Setting up Scale19
I0925 11:03:16.621716  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.621723  2600 net.cpp:137] Memory required for data: 507718800
I0925 11:03:16.621727  2600 layer_factory.hpp:77] Creating layer Eltwise9
I0925 11:03:16.621732  2600 net.cpp:84] Creating Layer Eltwise9
I0925 11:03:16.621736  2600 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I0925 11:03:16.621738  2600 net.cpp:406] Eltwise9 <- Convolution19
I0925 11:03:16.621742  2600 net.cpp:380] Eltwise9 -> Eltwise9
I0925 11:03:16.621757  2600 net.cpp:122] Setting up Eltwise9
I0925 11:03:16.621762  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.621763  2600 net.cpp:137] Memory required for data: 512736400
I0925 11:03:16.621765  2600 layer_factory.hpp:77] Creating layer penlu19
I0925 11:03:16.621772  2600 net.cpp:84] Creating Layer penlu19
I0925 11:03:16.621773  2600 net.cpp:406] penlu19 <- Eltwise9
I0925 11:03:16.621776  2600 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I0925 11:03:16.621883  2600 net.cpp:122] Setting up penlu19
I0925 11:03:16.621887  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.621889  2600 net.cpp:137] Memory required for data: 517754000
I0925 11:03:16.621893  2600 layer_factory.hpp:77] Creating layer Eltwise9_penlu19_0_split
I0925 11:03:16.621897  2600 net.cpp:84] Creating Layer Eltwise9_penlu19_0_split
I0925 11:03:16.621899  2600 net.cpp:406] Eltwise9_penlu19_0_split <- Eltwise9
I0925 11:03:16.621903  2600 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_0
I0925 11:03:16.621907  2600 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_1
I0925 11:03:16.621928  2600 net.cpp:122] Setting up Eltwise9_penlu19_0_split
I0925 11:03:16.621933  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.621937  2600 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0925 11:03:16.621938  2600 net.cpp:137] Memory required for data: 527789200
I0925 11:03:16.621940  2600 layer_factory.hpp:77] Creating layer Convolution20
I0925 11:03:16.621947  2600 net.cpp:84] Creating Layer Convolution20
I0925 11:03:16.621948  2600 net.cpp:406] Convolution20 <- Eltwise9_penlu19_0_split_0
I0925 11:03:16.621953  2600 net.cpp:380] Convolution20 -> Convolution20
I0925 11:03:16.623443  2600 net.cpp:122] Setting up Convolution20
I0925 11:03:16.623456  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.623459  2600 net.cpp:137] Memory required for data: 530298000
I0925 11:03:16.623467  2600 layer_factory.hpp:77] Creating layer BatchNorm20
I0925 11:03:16.623476  2600 net.cpp:84] Creating Layer BatchNorm20
I0925 11:03:16.623479  2600 net.cpp:406] BatchNorm20 <- Convolution20
I0925 11:03:16.623486  2600 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0925 11:03:16.623678  2600 net.cpp:122] Setting up BatchNorm20
I0925 11:03:16.623687  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.623690  2600 net.cpp:137] Memory required for data: 532806800
I0925 11:03:16.623699  2600 layer_factory.hpp:77] Creating layer Scale20
I0925 11:03:16.623708  2600 net.cpp:84] Creating Layer Scale20
I0925 11:03:16.623713  2600 net.cpp:406] Scale20 <- Convolution20
I0925 11:03:16.623718  2600 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0925 11:03:16.623755  2600 layer_factory.hpp:77] Creating layer Scale20
I0925 11:03:16.623862  2600 net.cpp:122] Setting up Scale20
I0925 11:03:16.623869  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.623872  2600 net.cpp:137] Memory required for data: 535315600
I0925 11:03:16.623879  2600 layer_factory.hpp:77] Creating layer Convolution21
I0925 11:03:16.623890  2600 net.cpp:84] Creating Layer Convolution21
I0925 11:03:16.623894  2600 net.cpp:406] Convolution21 <- Eltwise9_penlu19_0_split_1
I0925 11:03:16.623901  2600 net.cpp:380] Convolution21 -> Convolution21
I0925 11:03:16.626010  2600 net.cpp:122] Setting up Convolution21
I0925 11:03:16.626020  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.626024  2600 net.cpp:137] Memory required for data: 537824400
I0925 11:03:16.626029  2600 layer_factory.hpp:77] Creating layer BatchNorm21
I0925 11:03:16.626034  2600 net.cpp:84] Creating Layer BatchNorm21
I0925 11:03:16.626044  2600 net.cpp:406] BatchNorm21 <- Convolution21
I0925 11:03:16.626047  2600 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0925 11:03:16.626180  2600 net.cpp:122] Setting up BatchNorm21
I0925 11:03:16.626185  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.626188  2600 net.cpp:137] Memory required for data: 540333200
I0925 11:03:16.626194  2600 layer_factory.hpp:77] Creating layer Scale21
I0925 11:03:16.626199  2600 net.cpp:84] Creating Layer Scale21
I0925 11:03:16.626200  2600 net.cpp:406] Scale21 <- Convolution21
I0925 11:03:16.626204  2600 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0925 11:03:16.626230  2600 layer_factory.hpp:77] Creating layer Scale21
I0925 11:03:16.626305  2600 net.cpp:122] Setting up Scale21
I0925 11:03:16.626309  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.626312  2600 net.cpp:137] Memory required for data: 542842000
I0925 11:03:16.626315  2600 layer_factory.hpp:77] Creating layer penlu20
I0925 11:03:16.626322  2600 net.cpp:84] Creating Layer penlu20
I0925 11:03:16.626323  2600 net.cpp:406] penlu20 <- Convolution21
I0925 11:03:16.626327  2600 net.cpp:367] penlu20 -> Convolution21 (in-place)
I0925 11:03:16.626430  2600 net.cpp:122] Setting up penlu20
I0925 11:03:16.626435  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.626437  2600 net.cpp:137] Memory required for data: 545350800
I0925 11:03:16.626441  2600 layer_factory.hpp:77] Creating layer Convolution22
I0925 11:03:16.626448  2600 net.cpp:84] Creating Layer Convolution22
I0925 11:03:16.626451  2600 net.cpp:406] Convolution22 <- Convolution21
I0925 11:03:16.626454  2600 net.cpp:380] Convolution22 -> Convolution22
I0925 11:03:16.627667  2600 net.cpp:122] Setting up Convolution22
I0925 11:03:16.627677  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.627679  2600 net.cpp:137] Memory required for data: 547859600
I0925 11:03:16.627684  2600 layer_factory.hpp:77] Creating layer BatchNorm22
I0925 11:03:16.627691  2600 net.cpp:84] Creating Layer BatchNorm22
I0925 11:03:16.627694  2600 net.cpp:406] BatchNorm22 <- Convolution22
I0925 11:03:16.627698  2600 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I0925 11:03:16.627832  2600 net.cpp:122] Setting up BatchNorm22
I0925 11:03:16.627837  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.627840  2600 net.cpp:137] Memory required for data: 550368400
I0925 11:03:16.627846  2600 layer_factory.hpp:77] Creating layer Scale22
I0925 11:03:16.627851  2600 net.cpp:84] Creating Layer Scale22
I0925 11:03:16.627852  2600 net.cpp:406] Scale22 <- Convolution22
I0925 11:03:16.627856  2600 net.cpp:367] Scale22 -> Convolution22 (in-place)
I0925 11:03:16.627884  2600 layer_factory.hpp:77] Creating layer Scale22
I0925 11:03:16.627960  2600 net.cpp:122] Setting up Scale22
I0925 11:03:16.627965  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.627967  2600 net.cpp:137] Memory required for data: 552877200
I0925 11:03:16.627971  2600 layer_factory.hpp:77] Creating layer Eltwise10
I0925 11:03:16.627976  2600 net.cpp:84] Creating Layer Eltwise10
I0925 11:03:16.627979  2600 net.cpp:406] Eltwise10 <- Convolution20
I0925 11:03:16.627981  2600 net.cpp:406] Eltwise10 <- Convolution22
I0925 11:03:16.627985  2600 net.cpp:380] Eltwise10 -> Eltwise10
I0925 11:03:16.628001  2600 net.cpp:122] Setting up Eltwise10
I0925 11:03:16.628005  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.628007  2600 net.cpp:137] Memory required for data: 555386000
I0925 11:03:16.628010  2600 layer_factory.hpp:77] Creating layer penlu21
I0925 11:03:16.628015  2600 net.cpp:84] Creating Layer penlu21
I0925 11:03:16.628017  2600 net.cpp:406] penlu21 <- Eltwise10
I0925 11:03:16.628021  2600 net.cpp:367] penlu21 -> Eltwise10 (in-place)
I0925 11:03:16.628126  2600 net.cpp:122] Setting up penlu21
I0925 11:03:16.628131  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.628134  2600 net.cpp:137] Memory required for data: 557894800
I0925 11:03:16.628139  2600 layer_factory.hpp:77] Creating layer Eltwise10_penlu21_0_split
I0925 11:03:16.628149  2600 net.cpp:84] Creating Layer Eltwise10_penlu21_0_split
I0925 11:03:16.628151  2600 net.cpp:406] Eltwise10_penlu21_0_split <- Eltwise10
I0925 11:03:16.628155  2600 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_0
I0925 11:03:16.628159  2600 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_1
I0925 11:03:16.628183  2600 net.cpp:122] Setting up Eltwise10_penlu21_0_split
I0925 11:03:16.628188  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.628190  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.628192  2600 net.cpp:137] Memory required for data: 562912400
I0925 11:03:16.628196  2600 layer_factory.hpp:77] Creating layer Convolution23
I0925 11:03:16.628201  2600 net.cpp:84] Creating Layer Convolution23
I0925 11:03:16.628203  2600 net.cpp:406] Convolution23 <- Eltwise10_penlu21_0_split_0
I0925 11:03:16.628207  2600 net.cpp:380] Convolution23 -> Convolution23
I0925 11:03:16.629585  2600 net.cpp:122] Setting up Convolution23
I0925 11:03:16.629595  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.629597  2600 net.cpp:137] Memory required for data: 565421200
I0925 11:03:16.629603  2600 layer_factory.hpp:77] Creating layer BatchNorm23
I0925 11:03:16.629607  2600 net.cpp:84] Creating Layer BatchNorm23
I0925 11:03:16.629611  2600 net.cpp:406] BatchNorm23 <- Convolution23
I0925 11:03:16.629614  2600 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I0925 11:03:16.629753  2600 net.cpp:122] Setting up BatchNorm23
I0925 11:03:16.629758  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.629760  2600 net.cpp:137] Memory required for data: 567930000
I0925 11:03:16.629766  2600 layer_factory.hpp:77] Creating layer Scale23
I0925 11:03:16.629770  2600 net.cpp:84] Creating Layer Scale23
I0925 11:03:16.629772  2600 net.cpp:406] Scale23 <- Convolution23
I0925 11:03:16.629776  2600 net.cpp:367] Scale23 -> Convolution23 (in-place)
I0925 11:03:16.629802  2600 layer_factory.hpp:77] Creating layer Scale23
I0925 11:03:16.629884  2600 net.cpp:122] Setting up Scale23
I0925 11:03:16.629889  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.629890  2600 net.cpp:137] Memory required for data: 570438800
I0925 11:03:16.629894  2600 layer_factory.hpp:77] Creating layer penlu22
I0925 11:03:16.629899  2600 net.cpp:84] Creating Layer penlu22
I0925 11:03:16.629901  2600 net.cpp:406] penlu22 <- Convolution23
I0925 11:03:16.629905  2600 net.cpp:367] penlu22 -> Convolution23 (in-place)
I0925 11:03:16.630015  2600 net.cpp:122] Setting up penlu22
I0925 11:03:16.630020  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.630023  2600 net.cpp:137] Memory required for data: 572947600
I0925 11:03:16.630026  2600 layer_factory.hpp:77] Creating layer Convolution24
I0925 11:03:16.630033  2600 net.cpp:84] Creating Layer Convolution24
I0925 11:03:16.630035  2600 net.cpp:406] Convolution24 <- Convolution23
I0925 11:03:16.630039  2600 net.cpp:380] Convolution24 -> Convolution24
I0925 11:03:16.631098  2600 net.cpp:122] Setting up Convolution24
I0925 11:03:16.631108  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.631110  2600 net.cpp:137] Memory required for data: 575456400
I0925 11:03:16.631114  2600 layer_factory.hpp:77] Creating layer BatchNorm24
I0925 11:03:16.631119  2600 net.cpp:84] Creating Layer BatchNorm24
I0925 11:03:16.631122  2600 net.cpp:406] BatchNorm24 <- Convolution24
I0925 11:03:16.631126  2600 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I0925 11:03:16.631264  2600 net.cpp:122] Setting up BatchNorm24
I0925 11:03:16.631269  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.631271  2600 net.cpp:137] Memory required for data: 577965200
I0925 11:03:16.631276  2600 layer_factory.hpp:77] Creating layer Scale24
I0925 11:03:16.631280  2600 net.cpp:84] Creating Layer Scale24
I0925 11:03:16.631283  2600 net.cpp:406] Scale24 <- Convolution24
I0925 11:03:16.631286  2600 net.cpp:367] Scale24 -> Convolution24 (in-place)
I0925 11:03:16.631322  2600 layer_factory.hpp:77] Creating layer Scale24
I0925 11:03:16.631402  2600 net.cpp:122] Setting up Scale24
I0925 11:03:16.631407  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.631410  2600 net.cpp:137] Memory required for data: 580474000
I0925 11:03:16.631414  2600 layer_factory.hpp:77] Creating layer Eltwise11
I0925 11:03:16.631417  2600 net.cpp:84] Creating Layer Eltwise11
I0925 11:03:16.631420  2600 net.cpp:406] Eltwise11 <- Eltwise10_penlu21_0_split_1
I0925 11:03:16.631423  2600 net.cpp:406] Eltwise11 <- Convolution24
I0925 11:03:16.631427  2600 net.cpp:380] Eltwise11 -> Eltwise11
I0925 11:03:16.631443  2600 net.cpp:122] Setting up Eltwise11
I0925 11:03:16.631448  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.631449  2600 net.cpp:137] Memory required for data: 582982800
I0925 11:03:16.631451  2600 layer_factory.hpp:77] Creating layer penlu23
I0925 11:03:16.631458  2600 net.cpp:84] Creating Layer penlu23
I0925 11:03:16.631459  2600 net.cpp:406] penlu23 <- Eltwise11
I0925 11:03:16.631464  2600 net.cpp:367] penlu23 -> Eltwise11 (in-place)
I0925 11:03:16.631574  2600 net.cpp:122] Setting up penlu23
I0925 11:03:16.631578  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.631580  2600 net.cpp:137] Memory required for data: 585491600
I0925 11:03:16.631585  2600 layer_factory.hpp:77] Creating layer Eltwise11_penlu23_0_split
I0925 11:03:16.631589  2600 net.cpp:84] Creating Layer Eltwise11_penlu23_0_split
I0925 11:03:16.631592  2600 net.cpp:406] Eltwise11_penlu23_0_split <- Eltwise11
I0925 11:03:16.631595  2600 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_0
I0925 11:03:16.631600  2600 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_1
I0925 11:03:16.631623  2600 net.cpp:122] Setting up Eltwise11_penlu23_0_split
I0925 11:03:16.631628  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.631630  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.631633  2600 net.cpp:137] Memory required for data: 590509200
I0925 11:03:16.631634  2600 layer_factory.hpp:77] Creating layer Convolution25
I0925 11:03:16.631640  2600 net.cpp:84] Creating Layer Convolution25
I0925 11:03:16.631642  2600 net.cpp:406] Convolution25 <- Eltwise11_penlu23_0_split_0
I0925 11:03:16.631647  2600 net.cpp:380] Convolution25 -> Convolution25
I0925 11:03:16.632740  2600 net.cpp:122] Setting up Convolution25
I0925 11:03:16.632748  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.632751  2600 net.cpp:137] Memory required for data: 593018000
I0925 11:03:16.632755  2600 layer_factory.hpp:77] Creating layer BatchNorm25
I0925 11:03:16.632761  2600 net.cpp:84] Creating Layer BatchNorm25
I0925 11:03:16.632763  2600 net.cpp:406] BatchNorm25 <- Convolution25
I0925 11:03:16.632767  2600 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I0925 11:03:16.632901  2600 net.cpp:122] Setting up BatchNorm25
I0925 11:03:16.632905  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.632908  2600 net.cpp:137] Memory required for data: 595526800
I0925 11:03:16.632912  2600 layer_factory.hpp:77] Creating layer Scale25
I0925 11:03:16.632916  2600 net.cpp:84] Creating Layer Scale25
I0925 11:03:16.632920  2600 net.cpp:406] Scale25 <- Convolution25
I0925 11:03:16.632922  2600 net.cpp:367] Scale25 -> Convolution25 (in-place)
I0925 11:03:16.632949  2600 layer_factory.hpp:77] Creating layer Scale25
I0925 11:03:16.633024  2600 net.cpp:122] Setting up Scale25
I0925 11:03:16.633029  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.633031  2600 net.cpp:137] Memory required for data: 598035600
I0925 11:03:16.633035  2600 layer_factory.hpp:77] Creating layer penlu24
I0925 11:03:16.633040  2600 net.cpp:84] Creating Layer penlu24
I0925 11:03:16.633044  2600 net.cpp:406] penlu24 <- Convolution25
I0925 11:03:16.633047  2600 net.cpp:367] penlu24 -> Convolution25 (in-place)
I0925 11:03:16.633152  2600 net.cpp:122] Setting up penlu24
I0925 11:03:16.633157  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.633164  2600 net.cpp:137] Memory required for data: 600544400
I0925 11:03:16.633169  2600 layer_factory.hpp:77] Creating layer Convolution26
I0925 11:03:16.633177  2600 net.cpp:84] Creating Layer Convolution26
I0925 11:03:16.633179  2600 net.cpp:406] Convolution26 <- Convolution25
I0925 11:03:16.633183  2600 net.cpp:380] Convolution26 -> Convolution26
I0925 11:03:16.634213  2600 net.cpp:122] Setting up Convolution26
I0925 11:03:16.634222  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.634225  2600 net.cpp:137] Memory required for data: 603053200
I0925 11:03:16.634229  2600 layer_factory.hpp:77] Creating layer BatchNorm26
I0925 11:03:16.634234  2600 net.cpp:84] Creating Layer BatchNorm26
I0925 11:03:16.634238  2600 net.cpp:406] BatchNorm26 <- Convolution26
I0925 11:03:16.634241  2600 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I0925 11:03:16.634376  2600 net.cpp:122] Setting up BatchNorm26
I0925 11:03:16.634380  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.634382  2600 net.cpp:137] Memory required for data: 605562000
I0925 11:03:16.634387  2600 layer_factory.hpp:77] Creating layer Scale26
I0925 11:03:16.634392  2600 net.cpp:84] Creating Layer Scale26
I0925 11:03:16.634394  2600 net.cpp:406] Scale26 <- Convolution26
I0925 11:03:16.634397  2600 net.cpp:367] Scale26 -> Convolution26 (in-place)
I0925 11:03:16.634423  2600 layer_factory.hpp:77] Creating layer Scale26
I0925 11:03:16.634502  2600 net.cpp:122] Setting up Scale26
I0925 11:03:16.634507  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.634519  2600 net.cpp:137] Memory required for data: 608070800
I0925 11:03:16.634524  2600 layer_factory.hpp:77] Creating layer Eltwise12
I0925 11:03:16.634527  2600 net.cpp:84] Creating Layer Eltwise12
I0925 11:03:16.634531  2600 net.cpp:406] Eltwise12 <- Eltwise11_penlu23_0_split_1
I0925 11:03:16.634533  2600 net.cpp:406] Eltwise12 <- Convolution26
I0925 11:03:16.634537  2600 net.cpp:380] Eltwise12 -> Eltwise12
I0925 11:03:16.634562  2600 net.cpp:122] Setting up Eltwise12
I0925 11:03:16.634567  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.634567  2600 net.cpp:137] Memory required for data: 610579600
I0925 11:03:16.634570  2600 layer_factory.hpp:77] Creating layer penlu25
I0925 11:03:16.634575  2600 net.cpp:84] Creating Layer penlu25
I0925 11:03:16.634577  2600 net.cpp:406] penlu25 <- Eltwise12
I0925 11:03:16.634580  2600 net.cpp:367] penlu25 -> Eltwise12 (in-place)
I0925 11:03:16.634690  2600 net.cpp:122] Setting up penlu25
I0925 11:03:16.634694  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.634696  2600 net.cpp:137] Memory required for data: 613088400
I0925 11:03:16.634718  2600 layer_factory.hpp:77] Creating layer Eltwise12_penlu25_0_split
I0925 11:03:16.634730  2600 net.cpp:84] Creating Layer Eltwise12_penlu25_0_split
I0925 11:03:16.634732  2600 net.cpp:406] Eltwise12_penlu25_0_split <- Eltwise12
I0925 11:03:16.634737  2600 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_0
I0925 11:03:16.634744  2600 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_1
I0925 11:03:16.634770  2600 net.cpp:122] Setting up Eltwise12_penlu25_0_split
I0925 11:03:16.634775  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.634778  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.634779  2600 net.cpp:137] Memory required for data: 618106000
I0925 11:03:16.634783  2600 layer_factory.hpp:77] Creating layer Convolution27
I0925 11:03:16.634788  2600 net.cpp:84] Creating Layer Convolution27
I0925 11:03:16.634791  2600 net.cpp:406] Convolution27 <- Eltwise12_penlu25_0_split_0
I0925 11:03:16.634794  2600 net.cpp:380] Convolution27 -> Convolution27
I0925 11:03:16.635519  2600 net.cpp:122] Setting up Convolution27
I0925 11:03:16.635526  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.635529  2600 net.cpp:137] Memory required for data: 620614800
I0925 11:03:16.635534  2600 layer_factory.hpp:77] Creating layer BatchNorm27
I0925 11:03:16.635540  2600 net.cpp:84] Creating Layer BatchNorm27
I0925 11:03:16.635550  2600 net.cpp:406] BatchNorm27 <- Convolution27
I0925 11:03:16.635553  2600 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I0925 11:03:16.635705  2600 net.cpp:122] Setting up BatchNorm27
I0925 11:03:16.635711  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.635713  2600 net.cpp:137] Memory required for data: 623123600
I0925 11:03:16.635718  2600 layer_factory.hpp:77] Creating layer Scale27
I0925 11:03:16.635723  2600 net.cpp:84] Creating Layer Scale27
I0925 11:03:16.635725  2600 net.cpp:406] Scale27 <- Convolution27
I0925 11:03:16.635730  2600 net.cpp:367] Scale27 -> Convolution27 (in-place)
I0925 11:03:16.635759  2600 layer_factory.hpp:77] Creating layer Scale27
I0925 11:03:16.635844  2600 net.cpp:122] Setting up Scale27
I0925 11:03:16.635850  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.635852  2600 net.cpp:137] Memory required for data: 625632400
I0925 11:03:16.635856  2600 layer_factory.hpp:77] Creating layer penlu26
I0925 11:03:16.635861  2600 net.cpp:84] Creating Layer penlu26
I0925 11:03:16.635864  2600 net.cpp:406] penlu26 <- Convolution27
I0925 11:03:16.635867  2600 net.cpp:367] penlu26 -> Convolution27 (in-place)
I0925 11:03:16.635987  2600 net.cpp:122] Setting up penlu26
I0925 11:03:16.635993  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.635995  2600 net.cpp:137] Memory required for data: 628141200
I0925 11:03:16.635999  2600 layer_factory.hpp:77] Creating layer Convolution28
I0925 11:03:16.636006  2600 net.cpp:84] Creating Layer Convolution28
I0925 11:03:16.636009  2600 net.cpp:406] Convolution28 <- Convolution27
I0925 11:03:16.636014  2600 net.cpp:380] Convolution28 -> Convolution28
I0925 11:03:16.637112  2600 net.cpp:122] Setting up Convolution28
I0925 11:03:16.637121  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.637125  2600 net.cpp:137] Memory required for data: 630650000
I0925 11:03:16.637128  2600 layer_factory.hpp:77] Creating layer BatchNorm28
I0925 11:03:16.637135  2600 net.cpp:84] Creating Layer BatchNorm28
I0925 11:03:16.637137  2600 net.cpp:406] BatchNorm28 <- Convolution28
I0925 11:03:16.637141  2600 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I0925 11:03:16.637279  2600 net.cpp:122] Setting up BatchNorm28
I0925 11:03:16.637282  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.637284  2600 net.cpp:137] Memory required for data: 633158800
I0925 11:03:16.637290  2600 layer_factory.hpp:77] Creating layer Scale28
I0925 11:03:16.637295  2600 net.cpp:84] Creating Layer Scale28
I0925 11:03:16.637296  2600 net.cpp:406] Scale28 <- Convolution28
I0925 11:03:16.637300  2600 net.cpp:367] Scale28 -> Convolution28 (in-place)
I0925 11:03:16.637326  2600 layer_factory.hpp:77] Creating layer Scale28
I0925 11:03:16.637404  2600 net.cpp:122] Setting up Scale28
I0925 11:03:16.637409  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.637411  2600 net.cpp:137] Memory required for data: 635667600
I0925 11:03:16.637415  2600 layer_factory.hpp:77] Creating layer Eltwise13
I0925 11:03:16.637419  2600 net.cpp:84] Creating Layer Eltwise13
I0925 11:03:16.637421  2600 net.cpp:406] Eltwise13 <- Eltwise12_penlu25_0_split_1
I0925 11:03:16.637424  2600 net.cpp:406] Eltwise13 <- Convolution28
I0925 11:03:16.637428  2600 net.cpp:380] Eltwise13 -> Eltwise13
I0925 11:03:16.637444  2600 net.cpp:122] Setting up Eltwise13
I0925 11:03:16.637447  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.637450  2600 net.cpp:137] Memory required for data: 638176400
I0925 11:03:16.637452  2600 layer_factory.hpp:77] Creating layer penlu27
I0925 11:03:16.637457  2600 net.cpp:84] Creating Layer penlu27
I0925 11:03:16.637459  2600 net.cpp:406] penlu27 <- Eltwise13
I0925 11:03:16.637462  2600 net.cpp:367] penlu27 -> Eltwise13 (in-place)
I0925 11:03:16.637570  2600 net.cpp:122] Setting up penlu27
I0925 11:03:16.637575  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.637578  2600 net.cpp:137] Memory required for data: 640685200
I0925 11:03:16.637581  2600 layer_factory.hpp:77] Creating layer Eltwise13_penlu27_0_split
I0925 11:03:16.637591  2600 net.cpp:84] Creating Layer Eltwise13_penlu27_0_split
I0925 11:03:16.637594  2600 net.cpp:406] Eltwise13_penlu27_0_split <- Eltwise13
I0925 11:03:16.637598  2600 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_0
I0925 11:03:16.637603  2600 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_1
I0925 11:03:16.637626  2600 net.cpp:122] Setting up Eltwise13_penlu27_0_split
I0925 11:03:16.637630  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.637632  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.637634  2600 net.cpp:137] Memory required for data: 645702800
I0925 11:03:16.637637  2600 layer_factory.hpp:77] Creating layer Convolution29
I0925 11:03:16.637643  2600 net.cpp:84] Creating Layer Convolution29
I0925 11:03:16.637645  2600 net.cpp:406] Convolution29 <- Eltwise13_penlu27_0_split_0
I0925 11:03:16.637650  2600 net.cpp:380] Convolution29 -> Convolution29
I0925 11:03:16.638682  2600 net.cpp:122] Setting up Convolution29
I0925 11:03:16.638691  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.638694  2600 net.cpp:137] Memory required for data: 648211600
I0925 11:03:16.638697  2600 layer_factory.hpp:77] Creating layer BatchNorm29
I0925 11:03:16.638703  2600 net.cpp:84] Creating Layer BatchNorm29
I0925 11:03:16.638706  2600 net.cpp:406] BatchNorm29 <- Convolution29
I0925 11:03:16.638710  2600 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I0925 11:03:16.638846  2600 net.cpp:122] Setting up BatchNorm29
I0925 11:03:16.638850  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.638854  2600 net.cpp:137] Memory required for data: 650720400
I0925 11:03:16.638857  2600 layer_factory.hpp:77] Creating layer Scale29
I0925 11:03:16.638861  2600 net.cpp:84] Creating Layer Scale29
I0925 11:03:16.638864  2600 net.cpp:406] Scale29 <- Convolution29
I0925 11:03:16.638867  2600 net.cpp:367] Scale29 -> Convolution29 (in-place)
I0925 11:03:16.638895  2600 layer_factory.hpp:77] Creating layer Scale29
I0925 11:03:16.638973  2600 net.cpp:122] Setting up Scale29
I0925 11:03:16.638978  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.638980  2600 net.cpp:137] Memory required for data: 653229200
I0925 11:03:16.638984  2600 layer_factory.hpp:77] Creating layer penlu28
I0925 11:03:16.638990  2600 net.cpp:84] Creating Layer penlu28
I0925 11:03:16.638993  2600 net.cpp:406] penlu28 <- Convolution29
I0925 11:03:16.638995  2600 net.cpp:367] penlu28 -> Convolution29 (in-place)
I0925 11:03:16.639102  2600 net.cpp:122] Setting up penlu28
I0925 11:03:16.639107  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.639109  2600 net.cpp:137] Memory required for data: 655738000
I0925 11:03:16.639113  2600 layer_factory.hpp:77] Creating layer Convolution30
I0925 11:03:16.639122  2600 net.cpp:84] Creating Layer Convolution30
I0925 11:03:16.639123  2600 net.cpp:406] Convolution30 <- Convolution29
I0925 11:03:16.639127  2600 net.cpp:380] Convolution30 -> Convolution30
I0925 11:03:16.640159  2600 net.cpp:122] Setting up Convolution30
I0925 11:03:16.640167  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.640171  2600 net.cpp:137] Memory required for data: 658246800
I0925 11:03:16.640174  2600 layer_factory.hpp:77] Creating layer BatchNorm30
I0925 11:03:16.640180  2600 net.cpp:84] Creating Layer BatchNorm30
I0925 11:03:16.640182  2600 net.cpp:406] BatchNorm30 <- Convolution30
I0925 11:03:16.640187  2600 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I0925 11:03:16.640321  2600 net.cpp:122] Setting up BatchNorm30
I0925 11:03:16.640326  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.640327  2600 net.cpp:137] Memory required for data: 660755600
I0925 11:03:16.640331  2600 layer_factory.hpp:77] Creating layer Scale30
I0925 11:03:16.640336  2600 net.cpp:84] Creating Layer Scale30
I0925 11:03:16.640338  2600 net.cpp:406] Scale30 <- Convolution30
I0925 11:03:16.640341  2600 net.cpp:367] Scale30 -> Convolution30 (in-place)
I0925 11:03:16.640377  2600 layer_factory.hpp:77] Creating layer Scale30
I0925 11:03:16.640456  2600 net.cpp:122] Setting up Scale30
I0925 11:03:16.640461  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.640463  2600 net.cpp:137] Memory required for data: 663264400
I0925 11:03:16.640467  2600 layer_factory.hpp:77] Creating layer Eltwise14
I0925 11:03:16.640471  2600 net.cpp:84] Creating Layer Eltwise14
I0925 11:03:16.640475  2600 net.cpp:406] Eltwise14 <- Eltwise13_penlu27_0_split_1
I0925 11:03:16.640476  2600 net.cpp:406] Eltwise14 <- Convolution30
I0925 11:03:16.640480  2600 net.cpp:380] Eltwise14 -> Eltwise14
I0925 11:03:16.640513  2600 net.cpp:122] Setting up Eltwise14
I0925 11:03:16.640521  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.640522  2600 net.cpp:137] Memory required for data: 665773200
I0925 11:03:16.640524  2600 layer_factory.hpp:77] Creating layer penlu29
I0925 11:03:16.640539  2600 net.cpp:84] Creating Layer penlu29
I0925 11:03:16.640542  2600 net.cpp:406] penlu29 <- Eltwise14
I0925 11:03:16.640544  2600 net.cpp:367] penlu29 -> Eltwise14 (in-place)
I0925 11:03:16.640657  2600 net.cpp:122] Setting up penlu29
I0925 11:03:16.640661  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.640663  2600 net.cpp:137] Memory required for data: 668282000
I0925 11:03:16.640668  2600 layer_factory.hpp:77] Creating layer Eltwise14_penlu29_0_split
I0925 11:03:16.640671  2600 net.cpp:84] Creating Layer Eltwise14_penlu29_0_split
I0925 11:03:16.640674  2600 net.cpp:406] Eltwise14_penlu29_0_split <- Eltwise14
I0925 11:03:16.640677  2600 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_0
I0925 11:03:16.640681  2600 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_1
I0925 11:03:16.640705  2600 net.cpp:122] Setting up Eltwise14_penlu29_0_split
I0925 11:03:16.640709  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.640712  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.640714  2600 net.cpp:137] Memory required for data: 673299600
I0925 11:03:16.640717  2600 layer_factory.hpp:77] Creating layer Convolution31
I0925 11:03:16.640722  2600 net.cpp:84] Creating Layer Convolution31
I0925 11:03:16.640724  2600 net.cpp:406] Convolution31 <- Eltwise14_penlu29_0_split_0
I0925 11:03:16.640728  2600 net.cpp:380] Convolution31 -> Convolution31
I0925 11:03:16.641783  2600 net.cpp:122] Setting up Convolution31
I0925 11:03:16.641793  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.641795  2600 net.cpp:137] Memory required for data: 675808400
I0925 11:03:16.641799  2600 layer_factory.hpp:77] Creating layer BatchNorm31
I0925 11:03:16.641805  2600 net.cpp:84] Creating Layer BatchNorm31
I0925 11:03:16.641808  2600 net.cpp:406] BatchNorm31 <- Convolution31
I0925 11:03:16.641813  2600 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I0925 11:03:16.641952  2600 net.cpp:122] Setting up BatchNorm31
I0925 11:03:16.641957  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.641960  2600 net.cpp:137] Memory required for data: 678317200
I0925 11:03:16.641964  2600 layer_factory.hpp:77] Creating layer Scale31
I0925 11:03:16.641969  2600 net.cpp:84] Creating Layer Scale31
I0925 11:03:16.641973  2600 net.cpp:406] Scale31 <- Convolution31
I0925 11:03:16.641975  2600 net.cpp:367] Scale31 -> Convolution31 (in-place)
I0925 11:03:16.642004  2600 layer_factory.hpp:77] Creating layer Scale31
I0925 11:03:16.642083  2600 net.cpp:122] Setting up Scale31
I0925 11:03:16.642088  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.642091  2600 net.cpp:137] Memory required for data: 680826000
I0925 11:03:16.642094  2600 layer_factory.hpp:77] Creating layer penlu30
I0925 11:03:16.642101  2600 net.cpp:84] Creating Layer penlu30
I0925 11:03:16.642102  2600 net.cpp:406] penlu30 <- Convolution31
I0925 11:03:16.642107  2600 net.cpp:367] penlu30 -> Convolution31 (in-place)
I0925 11:03:16.642218  2600 net.cpp:122] Setting up penlu30
I0925 11:03:16.642223  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.642231  2600 net.cpp:137] Memory required for data: 683334800
I0925 11:03:16.642236  2600 layer_factory.hpp:77] Creating layer Convolution32
I0925 11:03:16.642243  2600 net.cpp:84] Creating Layer Convolution32
I0925 11:03:16.642246  2600 net.cpp:406] Convolution32 <- Convolution31
I0925 11:03:16.642249  2600 net.cpp:380] Convolution32 -> Convolution32
I0925 11:03:16.643314  2600 net.cpp:122] Setting up Convolution32
I0925 11:03:16.643323  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.643326  2600 net.cpp:137] Memory required for data: 685843600
I0925 11:03:16.643331  2600 layer_factory.hpp:77] Creating layer BatchNorm32
I0925 11:03:16.643335  2600 net.cpp:84] Creating Layer BatchNorm32
I0925 11:03:16.643338  2600 net.cpp:406] BatchNorm32 <- Convolution32
I0925 11:03:16.643342  2600 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I0925 11:03:16.643489  2600 net.cpp:122] Setting up BatchNorm32
I0925 11:03:16.643494  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.643496  2600 net.cpp:137] Memory required for data: 688352400
I0925 11:03:16.643501  2600 layer_factory.hpp:77] Creating layer Scale32
I0925 11:03:16.643507  2600 net.cpp:84] Creating Layer Scale32
I0925 11:03:16.643509  2600 net.cpp:406] Scale32 <- Convolution32
I0925 11:03:16.643512  2600 net.cpp:367] Scale32 -> Convolution32 (in-place)
I0925 11:03:16.643539  2600 layer_factory.hpp:77] Creating layer Scale32
I0925 11:03:16.643618  2600 net.cpp:122] Setting up Scale32
I0925 11:03:16.643623  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.643625  2600 net.cpp:137] Memory required for data: 690861200
I0925 11:03:16.643630  2600 layer_factory.hpp:77] Creating layer Eltwise15
I0925 11:03:16.643635  2600 net.cpp:84] Creating Layer Eltwise15
I0925 11:03:16.643636  2600 net.cpp:406] Eltwise15 <- Eltwise14_penlu29_0_split_1
I0925 11:03:16.643640  2600 net.cpp:406] Eltwise15 <- Convolution32
I0925 11:03:16.643642  2600 net.cpp:380] Eltwise15 -> Eltwise15
I0925 11:03:16.643659  2600 net.cpp:122] Setting up Eltwise15
I0925 11:03:16.643663  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.643666  2600 net.cpp:137] Memory required for data: 693370000
I0925 11:03:16.643667  2600 layer_factory.hpp:77] Creating layer penlu31
I0925 11:03:16.643673  2600 net.cpp:84] Creating Layer penlu31
I0925 11:03:16.643676  2600 net.cpp:406] penlu31 <- Eltwise15
I0925 11:03:16.643678  2600 net.cpp:367] penlu31 -> Eltwise15 (in-place)
I0925 11:03:16.643787  2600 net.cpp:122] Setting up penlu31
I0925 11:03:16.643792  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.643795  2600 net.cpp:137] Memory required for data: 695878800
I0925 11:03:16.643798  2600 layer_factory.hpp:77] Creating layer Eltwise15_penlu31_0_split
I0925 11:03:16.643802  2600 net.cpp:84] Creating Layer Eltwise15_penlu31_0_split
I0925 11:03:16.643805  2600 net.cpp:406] Eltwise15_penlu31_0_split <- Eltwise15
I0925 11:03:16.643807  2600 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_0
I0925 11:03:16.643811  2600 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_1
I0925 11:03:16.643834  2600 net.cpp:122] Setting up Eltwise15_penlu31_0_split
I0925 11:03:16.643838  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.643841  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.643843  2600 net.cpp:137] Memory required for data: 700896400
I0925 11:03:16.643846  2600 layer_factory.hpp:77] Creating layer Convolution33
I0925 11:03:16.643852  2600 net.cpp:84] Creating Layer Convolution33
I0925 11:03:16.643854  2600 net.cpp:406] Convolution33 <- Eltwise15_penlu31_0_split_0
I0925 11:03:16.643858  2600 net.cpp:380] Convolution33 -> Convolution33
I0925 11:03:16.645303  2600 net.cpp:122] Setting up Convolution33
I0925 11:03:16.645313  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.645316  2600 net.cpp:137] Memory required for data: 703405200
I0925 11:03:16.645321  2600 layer_factory.hpp:77] Creating layer BatchNorm33
I0925 11:03:16.645326  2600 net.cpp:84] Creating Layer BatchNorm33
I0925 11:03:16.645336  2600 net.cpp:406] BatchNorm33 <- Convolution33
I0925 11:03:16.645341  2600 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I0925 11:03:16.645503  2600 net.cpp:122] Setting up BatchNorm33
I0925 11:03:16.645508  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.645509  2600 net.cpp:137] Memory required for data: 705914000
I0925 11:03:16.645514  2600 layer_factory.hpp:77] Creating layer Scale33
I0925 11:03:16.645519  2600 net.cpp:84] Creating Layer Scale33
I0925 11:03:16.645520  2600 net.cpp:406] Scale33 <- Convolution33
I0925 11:03:16.645524  2600 net.cpp:367] Scale33 -> Convolution33 (in-place)
I0925 11:03:16.645551  2600 layer_factory.hpp:77] Creating layer Scale33
I0925 11:03:16.645630  2600 net.cpp:122] Setting up Scale33
I0925 11:03:16.645635  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.645637  2600 net.cpp:137] Memory required for data: 708422800
I0925 11:03:16.645642  2600 layer_factory.hpp:77] Creating layer penlu32
I0925 11:03:16.645647  2600 net.cpp:84] Creating Layer penlu32
I0925 11:03:16.645648  2600 net.cpp:406] penlu32 <- Convolution33
I0925 11:03:16.645653  2600 net.cpp:367] penlu32 -> Convolution33 (in-place)
I0925 11:03:16.645761  2600 net.cpp:122] Setting up penlu32
I0925 11:03:16.645766  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.645768  2600 net.cpp:137] Memory required for data: 710931600
I0925 11:03:16.645773  2600 layer_factory.hpp:77] Creating layer Convolution34
I0925 11:03:16.645781  2600 net.cpp:84] Creating Layer Convolution34
I0925 11:03:16.645782  2600 net.cpp:406] Convolution34 <- Convolution33
I0925 11:03:16.645787  2600 net.cpp:380] Convolution34 -> Convolution34
I0925 11:03:16.646850  2600 net.cpp:122] Setting up Convolution34
I0925 11:03:16.646859  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.646862  2600 net.cpp:137] Memory required for data: 713440400
I0925 11:03:16.646867  2600 layer_factory.hpp:77] Creating layer BatchNorm34
I0925 11:03:16.646872  2600 net.cpp:84] Creating Layer BatchNorm34
I0925 11:03:16.646875  2600 net.cpp:406] BatchNorm34 <- Convolution34
I0925 11:03:16.646879  2600 net.cpp:367] BatchNorm34 -> Convolution34 (in-place)
I0925 11:03:16.647022  2600 net.cpp:122] Setting up BatchNorm34
I0925 11:03:16.647027  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.647029  2600 net.cpp:137] Memory required for data: 715949200
I0925 11:03:16.647034  2600 layer_factory.hpp:77] Creating layer Scale34
I0925 11:03:16.647038  2600 net.cpp:84] Creating Layer Scale34
I0925 11:03:16.647042  2600 net.cpp:406] Scale34 <- Convolution34
I0925 11:03:16.647044  2600 net.cpp:367] Scale34 -> Convolution34 (in-place)
I0925 11:03:16.647073  2600 layer_factory.hpp:77] Creating layer Scale34
I0925 11:03:16.647152  2600 net.cpp:122] Setting up Scale34
I0925 11:03:16.647157  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.647159  2600 net.cpp:137] Memory required for data: 718458000
I0925 11:03:16.647163  2600 layer_factory.hpp:77] Creating layer Eltwise16
I0925 11:03:16.647167  2600 net.cpp:84] Creating Layer Eltwise16
I0925 11:03:16.647169  2600 net.cpp:406] Eltwise16 <- Eltwise15_penlu31_0_split_1
I0925 11:03:16.647172  2600 net.cpp:406] Eltwise16 <- Convolution34
I0925 11:03:16.647177  2600 net.cpp:380] Eltwise16 -> Eltwise16
I0925 11:03:16.647193  2600 net.cpp:122] Setting up Eltwise16
I0925 11:03:16.647197  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.647199  2600 net.cpp:137] Memory required for data: 720966800
I0925 11:03:16.647202  2600 layer_factory.hpp:77] Creating layer penlu33
I0925 11:03:16.647207  2600 net.cpp:84] Creating Layer penlu33
I0925 11:03:16.647209  2600 net.cpp:406] penlu33 <- Eltwise16
I0925 11:03:16.647212  2600 net.cpp:367] penlu33 -> Eltwise16 (in-place)
I0925 11:03:16.647325  2600 net.cpp:122] Setting up penlu33
I0925 11:03:16.647330  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.647331  2600 net.cpp:137] Memory required for data: 723475600
I0925 11:03:16.647342  2600 layer_factory.hpp:77] Creating layer Eltwise16_penlu33_0_split
I0925 11:03:16.647346  2600 net.cpp:84] Creating Layer Eltwise16_penlu33_0_split
I0925 11:03:16.647348  2600 net.cpp:406] Eltwise16_penlu33_0_split <- Eltwise16
I0925 11:03:16.647353  2600 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_0
I0925 11:03:16.647357  2600 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_1
I0925 11:03:16.647382  2600 net.cpp:122] Setting up Eltwise16_penlu33_0_split
I0925 11:03:16.647385  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.647388  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.647390  2600 net.cpp:137] Memory required for data: 728493200
I0925 11:03:16.647392  2600 layer_factory.hpp:77] Creating layer Convolution35
I0925 11:03:16.647399  2600 net.cpp:84] Creating Layer Convolution35
I0925 11:03:16.647402  2600 net.cpp:406] Convolution35 <- Eltwise16_penlu33_0_split_0
I0925 11:03:16.647405  2600 net.cpp:380] Convolution35 -> Convolution35
I0925 11:03:16.648460  2600 net.cpp:122] Setting up Convolution35
I0925 11:03:16.648469  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.648473  2600 net.cpp:137] Memory required for data: 731002000
I0925 11:03:16.648478  2600 layer_factory.hpp:77] Creating layer BatchNorm35
I0925 11:03:16.648483  2600 net.cpp:84] Creating Layer BatchNorm35
I0925 11:03:16.648485  2600 net.cpp:406] BatchNorm35 <- Convolution35
I0925 11:03:16.648489  2600 net.cpp:367] BatchNorm35 -> Convolution35 (in-place)
I0925 11:03:16.648658  2600 net.cpp:122] Setting up BatchNorm35
I0925 11:03:16.648663  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.648666  2600 net.cpp:137] Memory required for data: 733510800
I0925 11:03:16.648671  2600 layer_factory.hpp:77] Creating layer Scale35
I0925 11:03:16.648675  2600 net.cpp:84] Creating Layer Scale35
I0925 11:03:16.648679  2600 net.cpp:406] Scale35 <- Convolution35
I0925 11:03:16.648681  2600 net.cpp:367] Scale35 -> Convolution35 (in-place)
I0925 11:03:16.648715  2600 layer_factory.hpp:77] Creating layer Scale35
I0925 11:03:16.648802  2600 net.cpp:122] Setting up Scale35
I0925 11:03:16.648808  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.648809  2600 net.cpp:137] Memory required for data: 736019600
I0925 11:03:16.648813  2600 layer_factory.hpp:77] Creating layer penlu34
I0925 11:03:16.648819  2600 net.cpp:84] Creating Layer penlu34
I0925 11:03:16.648823  2600 net.cpp:406] penlu34 <- Convolution35
I0925 11:03:16.648825  2600 net.cpp:367] penlu34 -> Convolution35 (in-place)
I0925 11:03:16.648946  2600 net.cpp:122] Setting up penlu34
I0925 11:03:16.648950  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.648953  2600 net.cpp:137] Memory required for data: 738528400
I0925 11:03:16.648957  2600 layer_factory.hpp:77] Creating layer Convolution36
I0925 11:03:16.648965  2600 net.cpp:84] Creating Layer Convolution36
I0925 11:03:16.648968  2600 net.cpp:406] Convolution36 <- Convolution35
I0925 11:03:16.648972  2600 net.cpp:380] Convolution36 -> Convolution36
I0925 11:03:16.650069  2600 net.cpp:122] Setting up Convolution36
I0925 11:03:16.650079  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.650081  2600 net.cpp:137] Memory required for data: 741037200
I0925 11:03:16.650085  2600 layer_factory.hpp:77] Creating layer BatchNorm36
I0925 11:03:16.650090  2600 net.cpp:84] Creating Layer BatchNorm36
I0925 11:03:16.650094  2600 net.cpp:406] BatchNorm36 <- Convolution36
I0925 11:03:16.650097  2600 net.cpp:367] BatchNorm36 -> Convolution36 (in-place)
I0925 11:03:16.650233  2600 net.cpp:122] Setting up BatchNorm36
I0925 11:03:16.650238  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.650240  2600 net.cpp:137] Memory required for data: 743546000
I0925 11:03:16.650245  2600 layer_factory.hpp:77] Creating layer Scale36
I0925 11:03:16.650249  2600 net.cpp:84] Creating Layer Scale36
I0925 11:03:16.650252  2600 net.cpp:406] Scale36 <- Convolution36
I0925 11:03:16.650255  2600 net.cpp:367] Scale36 -> Convolution36 (in-place)
I0925 11:03:16.650290  2600 layer_factory.hpp:77] Creating layer Scale36
I0925 11:03:16.650372  2600 net.cpp:122] Setting up Scale36
I0925 11:03:16.650375  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.650378  2600 net.cpp:137] Memory required for data: 746054800
I0925 11:03:16.650382  2600 layer_factory.hpp:77] Creating layer Eltwise17
I0925 11:03:16.650387  2600 net.cpp:84] Creating Layer Eltwise17
I0925 11:03:16.650388  2600 net.cpp:406] Eltwise17 <- Eltwise16_penlu33_0_split_1
I0925 11:03:16.650391  2600 net.cpp:406] Eltwise17 <- Convolution36
I0925 11:03:16.650394  2600 net.cpp:380] Eltwise17 -> Eltwise17
I0925 11:03:16.650411  2600 net.cpp:122] Setting up Eltwise17
I0925 11:03:16.650415  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.650418  2600 net.cpp:137] Memory required for data: 748563600
I0925 11:03:16.650419  2600 layer_factory.hpp:77] Creating layer penlu35
I0925 11:03:16.650425  2600 net.cpp:84] Creating Layer penlu35
I0925 11:03:16.650427  2600 net.cpp:406] penlu35 <- Eltwise17
I0925 11:03:16.650430  2600 net.cpp:367] penlu35 -> Eltwise17 (in-place)
I0925 11:03:16.650552  2600 net.cpp:122] Setting up penlu35
I0925 11:03:16.650557  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.650558  2600 net.cpp:137] Memory required for data: 751072400
I0925 11:03:16.650563  2600 layer_factory.hpp:77] Creating layer Eltwise17_penlu35_0_split
I0925 11:03:16.650568  2600 net.cpp:84] Creating Layer Eltwise17_penlu35_0_split
I0925 11:03:16.650569  2600 net.cpp:406] Eltwise17_penlu35_0_split <- Eltwise17
I0925 11:03:16.650573  2600 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_0
I0925 11:03:16.650578  2600 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_1
I0925 11:03:16.650601  2600 net.cpp:122] Setting up Eltwise17_penlu35_0_split
I0925 11:03:16.650605  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.650609  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.650610  2600 net.cpp:137] Memory required for data: 756090000
I0925 11:03:16.650614  2600 layer_factory.hpp:77] Creating layer Convolution37
I0925 11:03:16.650619  2600 net.cpp:84] Creating Layer Convolution37
I0925 11:03:16.650621  2600 net.cpp:406] Convolution37 <- Eltwise17_penlu35_0_split_0
I0925 11:03:16.650625  2600 net.cpp:380] Convolution37 -> Convolution37
I0925 11:03:16.651356  2600 net.cpp:122] Setting up Convolution37
I0925 11:03:16.651365  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.651366  2600 net.cpp:137] Memory required for data: 758598800
I0925 11:03:16.651371  2600 layer_factory.hpp:77] Creating layer BatchNorm37
I0925 11:03:16.651376  2600 net.cpp:84] Creating Layer BatchNorm37
I0925 11:03:16.651378  2600 net.cpp:406] BatchNorm37 <- Convolution37
I0925 11:03:16.651382  2600 net.cpp:367] BatchNorm37 -> Convolution37 (in-place)
I0925 11:03:16.651525  2600 net.cpp:122] Setting up BatchNorm37
I0925 11:03:16.651530  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.651532  2600 net.cpp:137] Memory required for data: 761107600
I0925 11:03:16.651536  2600 layer_factory.hpp:77] Creating layer Scale37
I0925 11:03:16.651541  2600 net.cpp:84] Creating Layer Scale37
I0925 11:03:16.651543  2600 net.cpp:406] Scale37 <- Convolution37
I0925 11:03:16.651546  2600 net.cpp:367] Scale37 -> Convolution37 (in-place)
I0925 11:03:16.651576  2600 layer_factory.hpp:77] Creating layer Scale37
I0925 11:03:16.651656  2600 net.cpp:122] Setting up Scale37
I0925 11:03:16.651661  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.651664  2600 net.cpp:137] Memory required for data: 763616400
I0925 11:03:16.651667  2600 layer_factory.hpp:77] Creating layer penlu36
I0925 11:03:16.651672  2600 net.cpp:84] Creating Layer penlu36
I0925 11:03:16.651674  2600 net.cpp:406] penlu36 <- Convolution37
I0925 11:03:16.651679  2600 net.cpp:367] penlu36 -> Convolution37 (in-place)
I0925 11:03:16.651792  2600 net.cpp:122] Setting up penlu36
I0925 11:03:16.651796  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.651803  2600 net.cpp:137] Memory required for data: 766125200
I0925 11:03:16.651808  2600 layer_factory.hpp:77] Creating layer Convolution38
I0925 11:03:16.651815  2600 net.cpp:84] Creating Layer Convolution38
I0925 11:03:16.651818  2600 net.cpp:406] Convolution38 <- Convolution37
I0925 11:03:16.651823  2600 net.cpp:380] Convolution38 -> Convolution38
I0925 11:03:16.652909  2600 net.cpp:122] Setting up Convolution38
I0925 11:03:16.652918  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.652921  2600 net.cpp:137] Memory required for data: 768634000
I0925 11:03:16.652925  2600 layer_factory.hpp:77] Creating layer BatchNorm38
I0925 11:03:16.652930  2600 net.cpp:84] Creating Layer BatchNorm38
I0925 11:03:16.652933  2600 net.cpp:406] BatchNorm38 <- Convolution38
I0925 11:03:16.652937  2600 net.cpp:367] BatchNorm38 -> Convolution38 (in-place)
I0925 11:03:16.653077  2600 net.cpp:122] Setting up BatchNorm38
I0925 11:03:16.653082  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.653084  2600 net.cpp:137] Memory required for data: 771142800
I0925 11:03:16.653090  2600 layer_factory.hpp:77] Creating layer Scale38
I0925 11:03:16.653093  2600 net.cpp:84] Creating Layer Scale38
I0925 11:03:16.653095  2600 net.cpp:406] Scale38 <- Convolution38
I0925 11:03:16.653098  2600 net.cpp:367] Scale38 -> Convolution38 (in-place)
I0925 11:03:16.653128  2600 layer_factory.hpp:77] Creating layer Scale38
I0925 11:03:16.653208  2600 net.cpp:122] Setting up Scale38
I0925 11:03:16.653213  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.653214  2600 net.cpp:137] Memory required for data: 773651600
I0925 11:03:16.653218  2600 layer_factory.hpp:77] Creating layer Eltwise18
I0925 11:03:16.653223  2600 net.cpp:84] Creating Layer Eltwise18
I0925 11:03:16.653225  2600 net.cpp:406] Eltwise18 <- Eltwise17_penlu35_0_split_1
I0925 11:03:16.653228  2600 net.cpp:406] Eltwise18 <- Convolution38
I0925 11:03:16.653231  2600 net.cpp:380] Eltwise18 -> Eltwise18
I0925 11:03:16.653247  2600 net.cpp:122] Setting up Eltwise18
I0925 11:03:16.653251  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.653254  2600 net.cpp:137] Memory required for data: 776160400
I0925 11:03:16.653255  2600 layer_factory.hpp:77] Creating layer penlu37
I0925 11:03:16.653260  2600 net.cpp:84] Creating Layer penlu37
I0925 11:03:16.653262  2600 net.cpp:406] penlu37 <- Eltwise18
I0925 11:03:16.653266  2600 net.cpp:367] penlu37 -> Eltwise18 (in-place)
I0925 11:03:16.653376  2600 net.cpp:122] Setting up penlu37
I0925 11:03:16.653381  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.653383  2600 net.cpp:137] Memory required for data: 778669200
I0925 11:03:16.653388  2600 layer_factory.hpp:77] Creating layer Eltwise18_penlu37_0_split
I0925 11:03:16.653390  2600 net.cpp:84] Creating Layer Eltwise18_penlu37_0_split
I0925 11:03:16.653393  2600 net.cpp:406] Eltwise18_penlu37_0_split <- Eltwise18
I0925 11:03:16.653398  2600 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_0
I0925 11:03:16.653401  2600 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_1
I0925 11:03:16.653424  2600 net.cpp:122] Setting up Eltwise18_penlu37_0_split
I0925 11:03:16.653429  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.653430  2600 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0925 11:03:16.653432  2600 net.cpp:137] Memory required for data: 783686800
I0925 11:03:16.653434  2600 layer_factory.hpp:77] Creating layer Convolution39
I0925 11:03:16.653441  2600 net.cpp:84] Creating Layer Convolution39
I0925 11:03:16.653443  2600 net.cpp:406] Convolution39 <- Eltwise18_penlu37_0_split_0
I0925 11:03:16.653447  2600 net.cpp:380] Convolution39 -> Convolution39
I0925 11:03:16.654336  2600 net.cpp:122] Setting up Convolution39
I0925 11:03:16.654345  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.654348  2600 net.cpp:137] Memory required for data: 784941200
I0925 11:03:16.654352  2600 layer_factory.hpp:77] Creating layer BatchNorm39
I0925 11:03:16.654364  2600 net.cpp:84] Creating Layer BatchNorm39
I0925 11:03:16.654367  2600 net.cpp:406] BatchNorm39 <- Convolution39
I0925 11:03:16.654371  2600 net.cpp:367] BatchNorm39 -> Convolution39 (in-place)
I0925 11:03:16.654506  2600 net.cpp:122] Setting up BatchNorm39
I0925 11:03:16.654510  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.654513  2600 net.cpp:137] Memory required for data: 786195600
I0925 11:03:16.654518  2600 layer_factory.hpp:77] Creating layer Scale39
I0925 11:03:16.654521  2600 net.cpp:84] Creating Layer Scale39
I0925 11:03:16.654525  2600 net.cpp:406] Scale39 <- Convolution39
I0925 11:03:16.654527  2600 net.cpp:367] Scale39 -> Convolution39 (in-place)
I0925 11:03:16.654556  2600 layer_factory.hpp:77] Creating layer Scale39
I0925 11:03:16.654633  2600 net.cpp:122] Setting up Scale39
I0925 11:03:16.654639  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.654640  2600 net.cpp:137] Memory required for data: 787450000
I0925 11:03:16.654644  2600 layer_factory.hpp:77] Creating layer Convolution40
I0925 11:03:16.654650  2600 net.cpp:84] Creating Layer Convolution40
I0925 11:03:16.654654  2600 net.cpp:406] Convolution40 <- Eltwise18_penlu37_0_split_1
I0925 11:03:16.654657  2600 net.cpp:380] Convolution40 -> Convolution40
I0925 11:03:16.656689  2600 net.cpp:122] Setting up Convolution40
I0925 11:03:16.656702  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.656707  2600 net.cpp:137] Memory required for data: 788704400
I0925 11:03:16.656713  2600 layer_factory.hpp:77] Creating layer BatchNorm40
I0925 11:03:16.656720  2600 net.cpp:84] Creating Layer BatchNorm40
I0925 11:03:16.656725  2600 net.cpp:406] BatchNorm40 <- Convolution40
I0925 11:03:16.656731  2600 net.cpp:367] BatchNorm40 -> Convolution40 (in-place)
I0925 11:03:16.656924  2600 net.cpp:122] Setting up BatchNorm40
I0925 11:03:16.656932  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.656936  2600 net.cpp:137] Memory required for data: 789958800
I0925 11:03:16.656944  2600 layer_factory.hpp:77] Creating layer Scale40
I0925 11:03:16.656951  2600 net.cpp:84] Creating Layer Scale40
I0925 11:03:16.656955  2600 net.cpp:406] Scale40 <- Convolution40
I0925 11:03:16.656960  2600 net.cpp:367] Scale40 -> Convolution40 (in-place)
I0925 11:03:16.657001  2600 layer_factory.hpp:77] Creating layer Scale40
I0925 11:03:16.657111  2600 net.cpp:122] Setting up Scale40
I0925 11:03:16.657119  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.657124  2600 net.cpp:137] Memory required for data: 791213200
I0925 11:03:16.657130  2600 layer_factory.hpp:77] Creating layer penlu38
I0925 11:03:16.657138  2600 net.cpp:84] Creating Layer penlu38
I0925 11:03:16.657142  2600 net.cpp:406] penlu38 <- Convolution40
I0925 11:03:16.657148  2600 net.cpp:367] penlu38 -> Convolution40 (in-place)
I0925 11:03:16.657307  2600 net.cpp:122] Setting up penlu38
I0925 11:03:16.657315  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.657318  2600 net.cpp:137] Memory required for data: 792467600
I0925 11:03:16.657326  2600 layer_factory.hpp:77] Creating layer Convolution41
I0925 11:03:16.657337  2600 net.cpp:84] Creating Layer Convolution41
I0925 11:03:16.657341  2600 net.cpp:406] Convolution41 <- Convolution40
I0925 11:03:16.657347  2600 net.cpp:380] Convolution41 -> Convolution41
I0925 11:03:16.659909  2600 net.cpp:122] Setting up Convolution41
I0925 11:03:16.659919  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.659921  2600 net.cpp:137] Memory required for data: 793722000
I0925 11:03:16.659926  2600 layer_factory.hpp:77] Creating layer BatchNorm41
I0925 11:03:16.659930  2600 net.cpp:84] Creating Layer BatchNorm41
I0925 11:03:16.659934  2600 net.cpp:406] BatchNorm41 <- Convolution41
I0925 11:03:16.659937  2600 net.cpp:367] BatchNorm41 -> Convolution41 (in-place)
I0925 11:03:16.660084  2600 net.cpp:122] Setting up BatchNorm41
I0925 11:03:16.660087  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.660090  2600 net.cpp:137] Memory required for data: 794976400
I0925 11:03:16.660101  2600 layer_factory.hpp:77] Creating layer Scale41
I0925 11:03:16.660106  2600 net.cpp:84] Creating Layer Scale41
I0925 11:03:16.660109  2600 net.cpp:406] Scale41 <- Convolution41
I0925 11:03:16.660112  2600 net.cpp:367] Scale41 -> Convolution41 (in-place)
I0925 11:03:16.660141  2600 layer_factory.hpp:77] Creating layer Scale41
I0925 11:03:16.660224  2600 net.cpp:122] Setting up Scale41
I0925 11:03:16.660229  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.660231  2600 net.cpp:137] Memory required for data: 796230800
I0925 11:03:16.660235  2600 layer_factory.hpp:77] Creating layer Eltwise19
I0925 11:03:16.660239  2600 net.cpp:84] Creating Layer Eltwise19
I0925 11:03:16.660241  2600 net.cpp:406] Eltwise19 <- Convolution39
I0925 11:03:16.660244  2600 net.cpp:406] Eltwise19 <- Convolution41
I0925 11:03:16.660249  2600 net.cpp:380] Eltwise19 -> Eltwise19
I0925 11:03:16.660266  2600 net.cpp:122] Setting up Eltwise19
I0925 11:03:16.660271  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.660274  2600 net.cpp:137] Memory required for data: 797485200
I0925 11:03:16.660275  2600 layer_factory.hpp:77] Creating layer penlu39
I0925 11:03:16.660280  2600 net.cpp:84] Creating Layer penlu39
I0925 11:03:16.660282  2600 net.cpp:406] penlu39 <- Eltwise19
I0925 11:03:16.660286  2600 net.cpp:367] penlu39 -> Eltwise19 (in-place)
I0925 11:03:16.660423  2600 net.cpp:122] Setting up penlu39
I0925 11:03:16.660436  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.660439  2600 net.cpp:137] Memory required for data: 798739600
I0925 11:03:16.660444  2600 layer_factory.hpp:77] Creating layer Eltwise19_penlu39_0_split
I0925 11:03:16.660446  2600 net.cpp:84] Creating Layer Eltwise19_penlu39_0_split
I0925 11:03:16.660449  2600 net.cpp:406] Eltwise19_penlu39_0_split <- Eltwise19
I0925 11:03:16.660452  2600 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_0
I0925 11:03:16.660456  2600 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_1
I0925 11:03:16.660491  2600 net.cpp:122] Setting up Eltwise19_penlu39_0_split
I0925 11:03:16.660501  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.660506  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.660507  2600 net.cpp:137] Memory required for data: 801248400
I0925 11:03:16.660509  2600 layer_factory.hpp:77] Creating layer Convolution42
I0925 11:03:16.660516  2600 net.cpp:84] Creating Layer Convolution42
I0925 11:03:16.660519  2600 net.cpp:406] Convolution42 <- Eltwise19_penlu39_0_split_0
I0925 11:03:16.660523  2600 net.cpp:380] Convolution42 -> Convolution42
I0925 11:03:16.662349  2600 net.cpp:122] Setting up Convolution42
I0925 11:03:16.662358  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.662361  2600 net.cpp:137] Memory required for data: 802502800
I0925 11:03:16.662366  2600 layer_factory.hpp:77] Creating layer BatchNorm42
I0925 11:03:16.662370  2600 net.cpp:84] Creating Layer BatchNorm42
I0925 11:03:16.662374  2600 net.cpp:406] BatchNorm42 <- Convolution42
I0925 11:03:16.662377  2600 net.cpp:367] BatchNorm42 -> Convolution42 (in-place)
I0925 11:03:16.662523  2600 net.cpp:122] Setting up BatchNorm42
I0925 11:03:16.662528  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.662530  2600 net.cpp:137] Memory required for data: 803757200
I0925 11:03:16.662534  2600 layer_factory.hpp:77] Creating layer Scale42
I0925 11:03:16.662539  2600 net.cpp:84] Creating Layer Scale42
I0925 11:03:16.662541  2600 net.cpp:406] Scale42 <- Convolution42
I0925 11:03:16.662545  2600 net.cpp:367] Scale42 -> Convolution42 (in-place)
I0925 11:03:16.662573  2600 layer_factory.hpp:77] Creating layer Scale42
I0925 11:03:16.662657  2600 net.cpp:122] Setting up Scale42
I0925 11:03:16.662662  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.662663  2600 net.cpp:137] Memory required for data: 805011600
I0925 11:03:16.662667  2600 layer_factory.hpp:77] Creating layer penlu40
I0925 11:03:16.662673  2600 net.cpp:84] Creating Layer penlu40
I0925 11:03:16.662675  2600 net.cpp:406] penlu40 <- Convolution42
I0925 11:03:16.662686  2600 net.cpp:367] penlu40 -> Convolution42 (in-place)
I0925 11:03:16.662806  2600 net.cpp:122] Setting up penlu40
I0925 11:03:16.662811  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.662813  2600 net.cpp:137] Memory required for data: 806266000
I0925 11:03:16.662817  2600 layer_factory.hpp:77] Creating layer Convolution43
I0925 11:03:16.662824  2600 net.cpp:84] Creating Layer Convolution43
I0925 11:03:16.662827  2600 net.cpp:406] Convolution43 <- Convolution42
I0925 11:03:16.662832  2600 net.cpp:380] Convolution43 -> Convolution43
I0925 11:03:16.665171  2600 net.cpp:122] Setting up Convolution43
I0925 11:03:16.665180  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.665184  2600 net.cpp:137] Memory required for data: 807520400
I0925 11:03:16.665187  2600 layer_factory.hpp:77] Creating layer BatchNorm43
I0925 11:03:16.665194  2600 net.cpp:84] Creating Layer BatchNorm43
I0925 11:03:16.665195  2600 net.cpp:406] BatchNorm43 <- Convolution43
I0925 11:03:16.665199  2600 net.cpp:367] BatchNorm43 -> Convolution43 (in-place)
I0925 11:03:16.665350  2600 net.cpp:122] Setting up BatchNorm43
I0925 11:03:16.665356  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.665359  2600 net.cpp:137] Memory required for data: 808774800
I0925 11:03:16.665364  2600 layer_factory.hpp:77] Creating layer Scale43
I0925 11:03:16.665369  2600 net.cpp:84] Creating Layer Scale43
I0925 11:03:16.665370  2600 net.cpp:406] Scale43 <- Convolution43
I0925 11:03:16.665374  2600 net.cpp:367] Scale43 -> Convolution43 (in-place)
I0925 11:03:16.665403  2600 layer_factory.hpp:77] Creating layer Scale43
I0925 11:03:16.665488  2600 net.cpp:122] Setting up Scale43
I0925 11:03:16.665493  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.665495  2600 net.cpp:137] Memory required for data: 810029200
I0925 11:03:16.665499  2600 layer_factory.hpp:77] Creating layer Eltwise20
I0925 11:03:16.665503  2600 net.cpp:84] Creating Layer Eltwise20
I0925 11:03:16.665506  2600 net.cpp:406] Eltwise20 <- Eltwise19_penlu39_0_split_1
I0925 11:03:16.665509  2600 net.cpp:406] Eltwise20 <- Convolution43
I0925 11:03:16.665513  2600 net.cpp:380] Eltwise20 -> Eltwise20
I0925 11:03:16.665531  2600 net.cpp:122] Setting up Eltwise20
I0925 11:03:16.665544  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.665545  2600 net.cpp:137] Memory required for data: 811283600
I0925 11:03:16.665547  2600 layer_factory.hpp:77] Creating layer penlu41
I0925 11:03:16.665552  2600 net.cpp:84] Creating Layer penlu41
I0925 11:03:16.665555  2600 net.cpp:406] penlu41 <- Eltwise20
I0925 11:03:16.665558  2600 net.cpp:367] penlu41 -> Eltwise20 (in-place)
I0925 11:03:16.665673  2600 net.cpp:122] Setting up penlu41
I0925 11:03:16.665678  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.665680  2600 net.cpp:137] Memory required for data: 812538000
I0925 11:03:16.665684  2600 layer_factory.hpp:77] Creating layer Eltwise20_penlu41_0_split
I0925 11:03:16.665688  2600 net.cpp:84] Creating Layer Eltwise20_penlu41_0_split
I0925 11:03:16.665690  2600 net.cpp:406] Eltwise20_penlu41_0_split <- Eltwise20
I0925 11:03:16.665693  2600 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_0
I0925 11:03:16.665697  2600 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_1
I0925 11:03:16.665722  2600 net.cpp:122] Setting up Eltwise20_penlu41_0_split
I0925 11:03:16.665726  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.665729  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.665731  2600 net.cpp:137] Memory required for data: 815046800
I0925 11:03:16.665733  2600 layer_factory.hpp:77] Creating layer Convolution44
I0925 11:03:16.665740  2600 net.cpp:84] Creating Layer Convolution44
I0925 11:03:16.665742  2600 net.cpp:406] Convolution44 <- Eltwise20_penlu41_0_split_0
I0925 11:03:16.665746  2600 net.cpp:380] Convolution44 -> Convolution44
I0925 11:03:16.667423  2600 net.cpp:122] Setting up Convolution44
I0925 11:03:16.667430  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.667441  2600 net.cpp:137] Memory required for data: 816301200
I0925 11:03:16.667446  2600 layer_factory.hpp:77] Creating layer BatchNorm44
I0925 11:03:16.667451  2600 net.cpp:84] Creating Layer BatchNorm44
I0925 11:03:16.667454  2600 net.cpp:406] BatchNorm44 <- Convolution44
I0925 11:03:16.667457  2600 net.cpp:367] BatchNorm44 -> Convolution44 (in-place)
I0925 11:03:16.667603  2600 net.cpp:122] Setting up BatchNorm44
I0925 11:03:16.667606  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.667609  2600 net.cpp:137] Memory required for data: 817555600
I0925 11:03:16.667613  2600 layer_factory.hpp:77] Creating layer Scale44
I0925 11:03:16.667620  2600 net.cpp:84] Creating Layer Scale44
I0925 11:03:16.667623  2600 net.cpp:406] Scale44 <- Convolution44
I0925 11:03:16.667625  2600 net.cpp:367] Scale44 -> Convolution44 (in-place)
I0925 11:03:16.667655  2600 layer_factory.hpp:77] Creating layer Scale44
I0925 11:03:16.667738  2600 net.cpp:122] Setting up Scale44
I0925 11:03:16.667743  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.667745  2600 net.cpp:137] Memory required for data: 818810000
I0925 11:03:16.667749  2600 layer_factory.hpp:77] Creating layer penlu42
I0925 11:03:16.667755  2600 net.cpp:84] Creating Layer penlu42
I0925 11:03:16.667757  2600 net.cpp:406] penlu42 <- Convolution44
I0925 11:03:16.667762  2600 net.cpp:367] penlu42 -> Convolution44 (in-place)
I0925 11:03:16.667876  2600 net.cpp:122] Setting up penlu42
I0925 11:03:16.667881  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.667882  2600 net.cpp:137] Memory required for data: 820064400
I0925 11:03:16.667886  2600 layer_factory.hpp:77] Creating layer Convolution45
I0925 11:03:16.667893  2600 net.cpp:84] Creating Layer Convolution45
I0925 11:03:16.667896  2600 net.cpp:406] Convolution45 <- Convolution44
I0925 11:03:16.667899  2600 net.cpp:380] Convolution45 -> Convolution45
I0925 11:03:16.669926  2600 net.cpp:122] Setting up Convolution45
I0925 11:03:16.669935  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.669937  2600 net.cpp:137] Memory required for data: 821318800
I0925 11:03:16.669944  2600 layer_factory.hpp:77] Creating layer BatchNorm45
I0925 11:03:16.669948  2600 net.cpp:84] Creating Layer BatchNorm45
I0925 11:03:16.669950  2600 net.cpp:406] BatchNorm45 <- Convolution45
I0925 11:03:16.669955  2600 net.cpp:367] BatchNorm45 -> Convolution45 (in-place)
I0925 11:03:16.670102  2600 net.cpp:122] Setting up BatchNorm45
I0925 11:03:16.670107  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.670109  2600 net.cpp:137] Memory required for data: 822573200
I0925 11:03:16.670114  2600 layer_factory.hpp:77] Creating layer Scale45
I0925 11:03:16.670119  2600 net.cpp:84] Creating Layer Scale45
I0925 11:03:16.670120  2600 net.cpp:406] Scale45 <- Convolution45
I0925 11:03:16.670123  2600 net.cpp:367] Scale45 -> Convolution45 (in-place)
I0925 11:03:16.670152  2600 layer_factory.hpp:77] Creating layer Scale45
I0925 11:03:16.670236  2600 net.cpp:122] Setting up Scale45
I0925 11:03:16.670240  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.670243  2600 net.cpp:137] Memory required for data: 823827600
I0925 11:03:16.670246  2600 layer_factory.hpp:77] Creating layer Eltwise21
I0925 11:03:16.670250  2600 net.cpp:84] Creating Layer Eltwise21
I0925 11:03:16.670253  2600 net.cpp:406] Eltwise21 <- Eltwise20_penlu41_0_split_1
I0925 11:03:16.670255  2600 net.cpp:406] Eltwise21 <- Convolution45
I0925 11:03:16.670259  2600 net.cpp:380] Eltwise21 -> Eltwise21
I0925 11:03:16.670276  2600 net.cpp:122] Setting up Eltwise21
I0925 11:03:16.670281  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.670284  2600 net.cpp:137] Memory required for data: 825082000
I0925 11:03:16.670285  2600 layer_factory.hpp:77] Creating layer penlu43
I0925 11:03:16.670290  2600 net.cpp:84] Creating Layer penlu43
I0925 11:03:16.670292  2600 net.cpp:406] penlu43 <- Eltwise21
I0925 11:03:16.670296  2600 net.cpp:367] penlu43 -> Eltwise21 (in-place)
I0925 11:03:16.670413  2600 net.cpp:122] Setting up penlu43
I0925 11:03:16.670424  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.670426  2600 net.cpp:137] Memory required for data: 826336400
I0925 11:03:16.670431  2600 layer_factory.hpp:77] Creating layer Eltwise21_penlu43_0_split
I0925 11:03:16.670435  2600 net.cpp:84] Creating Layer Eltwise21_penlu43_0_split
I0925 11:03:16.670437  2600 net.cpp:406] Eltwise21_penlu43_0_split <- Eltwise21
I0925 11:03:16.670440  2600 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_0
I0925 11:03:16.670444  2600 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_1
I0925 11:03:16.670471  2600 net.cpp:122] Setting up Eltwise21_penlu43_0_split
I0925 11:03:16.670475  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.670478  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.670480  2600 net.cpp:137] Memory required for data: 828845200
I0925 11:03:16.670482  2600 layer_factory.hpp:77] Creating layer Convolution46
I0925 11:03:16.670488  2600 net.cpp:84] Creating Layer Convolution46
I0925 11:03:16.670490  2600 net.cpp:406] Convolution46 <- Eltwise21_penlu43_0_split_0
I0925 11:03:16.670495  2600 net.cpp:380] Convolution46 -> Convolution46
I0925 11:03:16.672161  2600 net.cpp:122] Setting up Convolution46
I0925 11:03:16.672170  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.672173  2600 net.cpp:137] Memory required for data: 830099600
I0925 11:03:16.672178  2600 layer_factory.hpp:77] Creating layer BatchNorm46
I0925 11:03:16.672183  2600 net.cpp:84] Creating Layer BatchNorm46
I0925 11:03:16.672184  2600 net.cpp:406] BatchNorm46 <- Convolution46
I0925 11:03:16.672189  2600 net.cpp:367] BatchNorm46 -> Convolution46 (in-place)
I0925 11:03:16.672335  2600 net.cpp:122] Setting up BatchNorm46
I0925 11:03:16.672340  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.672343  2600 net.cpp:137] Memory required for data: 831354000
I0925 11:03:16.672348  2600 layer_factory.hpp:77] Creating layer Scale46
I0925 11:03:16.672351  2600 net.cpp:84] Creating Layer Scale46
I0925 11:03:16.672353  2600 net.cpp:406] Scale46 <- Convolution46
I0925 11:03:16.672356  2600 net.cpp:367] Scale46 -> Convolution46 (in-place)
I0925 11:03:16.672386  2600 layer_factory.hpp:77] Creating layer Scale46
I0925 11:03:16.672471  2600 net.cpp:122] Setting up Scale46
I0925 11:03:16.672475  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.672477  2600 net.cpp:137] Memory required for data: 832608400
I0925 11:03:16.672482  2600 layer_factory.hpp:77] Creating layer penlu44
I0925 11:03:16.672485  2600 net.cpp:84] Creating Layer penlu44
I0925 11:03:16.672488  2600 net.cpp:406] penlu44 <- Convolution46
I0925 11:03:16.672492  2600 net.cpp:367] penlu44 -> Convolution46 (in-place)
I0925 11:03:16.672646  2600 net.cpp:122] Setting up penlu44
I0925 11:03:16.672652  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.672654  2600 net.cpp:137] Memory required for data: 833862800
I0925 11:03:16.672658  2600 layer_factory.hpp:77] Creating layer Convolution47
I0925 11:03:16.672665  2600 net.cpp:84] Creating Layer Convolution47
I0925 11:03:16.672668  2600 net.cpp:406] Convolution47 <- Convolution46
I0925 11:03:16.672672  2600 net.cpp:380] Convolution47 -> Convolution47
I0925 11:03:16.674352  2600 net.cpp:122] Setting up Convolution47
I0925 11:03:16.674361  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.674365  2600 net.cpp:137] Memory required for data: 835117200
I0925 11:03:16.674370  2600 layer_factory.hpp:77] Creating layer BatchNorm47
I0925 11:03:16.674373  2600 net.cpp:84] Creating Layer BatchNorm47
I0925 11:03:16.674376  2600 net.cpp:406] BatchNorm47 <- Convolution47
I0925 11:03:16.674381  2600 net.cpp:367] BatchNorm47 -> Convolution47 (in-place)
I0925 11:03:16.674533  2600 net.cpp:122] Setting up BatchNorm47
I0925 11:03:16.674538  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.674540  2600 net.cpp:137] Memory required for data: 836371600
I0925 11:03:16.674553  2600 layer_factory.hpp:77] Creating layer Scale47
I0925 11:03:16.674566  2600 net.cpp:84] Creating Layer Scale47
I0925 11:03:16.674569  2600 net.cpp:406] Scale47 <- Convolution47
I0925 11:03:16.674573  2600 net.cpp:367] Scale47 -> Convolution47 (in-place)
I0925 11:03:16.674602  2600 layer_factory.hpp:77] Creating layer Scale47
I0925 11:03:16.674688  2600 net.cpp:122] Setting up Scale47
I0925 11:03:16.674693  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.674695  2600 net.cpp:137] Memory required for data: 837626000
I0925 11:03:16.674700  2600 layer_factory.hpp:77] Creating layer Eltwise22
I0925 11:03:16.674703  2600 net.cpp:84] Creating Layer Eltwise22
I0925 11:03:16.674706  2600 net.cpp:406] Eltwise22 <- Eltwise21_penlu43_0_split_1
I0925 11:03:16.674708  2600 net.cpp:406] Eltwise22 <- Convolution47
I0925 11:03:16.674712  2600 net.cpp:380] Eltwise22 -> Eltwise22
I0925 11:03:16.674731  2600 net.cpp:122] Setting up Eltwise22
I0925 11:03:16.674736  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.674737  2600 net.cpp:137] Memory required for data: 838880400
I0925 11:03:16.674739  2600 layer_factory.hpp:77] Creating layer penlu45
I0925 11:03:16.674744  2600 net.cpp:84] Creating Layer penlu45
I0925 11:03:16.674746  2600 net.cpp:406] penlu45 <- Eltwise22
I0925 11:03:16.674751  2600 net.cpp:367] penlu45 -> Eltwise22 (in-place)
I0925 11:03:16.674866  2600 net.cpp:122] Setting up penlu45
I0925 11:03:16.674871  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.674873  2600 net.cpp:137] Memory required for data: 840134800
I0925 11:03:16.674877  2600 layer_factory.hpp:77] Creating layer Eltwise22_penlu45_0_split
I0925 11:03:16.674881  2600 net.cpp:84] Creating Layer Eltwise22_penlu45_0_split
I0925 11:03:16.674883  2600 net.cpp:406] Eltwise22_penlu45_0_split <- Eltwise22
I0925 11:03:16.674887  2600 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_0
I0925 11:03:16.674891  2600 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_1
I0925 11:03:16.674916  2600 net.cpp:122] Setting up Eltwise22_penlu45_0_split
I0925 11:03:16.674921  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.674922  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.674924  2600 net.cpp:137] Memory required for data: 842643600
I0925 11:03:16.674927  2600 layer_factory.hpp:77] Creating layer Convolution48
I0925 11:03:16.674933  2600 net.cpp:84] Creating Layer Convolution48
I0925 11:03:16.674937  2600 net.cpp:406] Convolution48 <- Eltwise22_penlu45_0_split_0
I0925 11:03:16.674939  2600 net.cpp:380] Convolution48 -> Convolution48
I0925 11:03:16.676618  2600 net.cpp:122] Setting up Convolution48
I0925 11:03:16.676627  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.676630  2600 net.cpp:137] Memory required for data: 843898000
I0925 11:03:16.676635  2600 layer_factory.hpp:77] Creating layer BatchNorm48
I0925 11:03:16.676640  2600 net.cpp:84] Creating Layer BatchNorm48
I0925 11:03:16.676642  2600 net.cpp:406] BatchNorm48 <- Convolution48
I0925 11:03:16.676646  2600 net.cpp:367] BatchNorm48 -> Convolution48 (in-place)
I0925 11:03:16.676797  2600 net.cpp:122] Setting up BatchNorm48
I0925 11:03:16.676802  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.676805  2600 net.cpp:137] Memory required for data: 845152400
I0925 11:03:16.676810  2600 layer_factory.hpp:77] Creating layer Scale48
I0925 11:03:16.676813  2600 net.cpp:84] Creating Layer Scale48
I0925 11:03:16.676815  2600 net.cpp:406] Scale48 <- Convolution48
I0925 11:03:16.676820  2600 net.cpp:367] Scale48 -> Convolution48 (in-place)
I0925 11:03:16.676849  2600 layer_factory.hpp:77] Creating layer Scale48
I0925 11:03:16.676936  2600 net.cpp:122] Setting up Scale48
I0925 11:03:16.676941  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.676944  2600 net.cpp:137] Memory required for data: 846406800
I0925 11:03:16.676947  2600 layer_factory.hpp:77] Creating layer penlu46
I0925 11:03:16.676952  2600 net.cpp:84] Creating Layer penlu46
I0925 11:03:16.676955  2600 net.cpp:406] penlu46 <- Convolution48
I0925 11:03:16.676959  2600 net.cpp:367] penlu46 -> Convolution48 (in-place)
I0925 11:03:16.677086  2600 net.cpp:122] Setting up penlu46
I0925 11:03:16.677091  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.677093  2600 net.cpp:137] Memory required for data: 847661200
I0925 11:03:16.677098  2600 layer_factory.hpp:77] Creating layer Convolution49
I0925 11:03:16.677104  2600 net.cpp:84] Creating Layer Convolution49
I0925 11:03:16.677108  2600 net.cpp:406] Convolution49 <- Convolution48
I0925 11:03:16.677111  2600 net.cpp:380] Convolution49 -> Convolution49
I0925 11:03:16.679095  2600 net.cpp:122] Setting up Convolution49
I0925 11:03:16.679102  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.679105  2600 net.cpp:137] Memory required for data: 848915600
I0925 11:03:16.679111  2600 layer_factory.hpp:77] Creating layer BatchNorm49
I0925 11:03:16.679116  2600 net.cpp:84] Creating Layer BatchNorm49
I0925 11:03:16.679118  2600 net.cpp:406] BatchNorm49 <- Convolution49
I0925 11:03:16.679121  2600 net.cpp:367] BatchNorm49 -> Convolution49 (in-place)
I0925 11:03:16.679272  2600 net.cpp:122] Setting up BatchNorm49
I0925 11:03:16.679277  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.679280  2600 net.cpp:137] Memory required for data: 850170000
I0925 11:03:16.679285  2600 layer_factory.hpp:77] Creating layer Scale49
I0925 11:03:16.679288  2600 net.cpp:84] Creating Layer Scale49
I0925 11:03:16.679291  2600 net.cpp:406] Scale49 <- Convolution49
I0925 11:03:16.679294  2600 net.cpp:367] Scale49 -> Convolution49 (in-place)
I0925 11:03:16.679324  2600 layer_factory.hpp:77] Creating layer Scale49
I0925 11:03:16.679409  2600 net.cpp:122] Setting up Scale49
I0925 11:03:16.679414  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.679416  2600 net.cpp:137] Memory required for data: 851424400
I0925 11:03:16.679420  2600 layer_factory.hpp:77] Creating layer Eltwise23
I0925 11:03:16.679425  2600 net.cpp:84] Creating Layer Eltwise23
I0925 11:03:16.679427  2600 net.cpp:406] Eltwise23 <- Eltwise22_penlu45_0_split_1
I0925 11:03:16.679430  2600 net.cpp:406] Eltwise23 <- Convolution49
I0925 11:03:16.679433  2600 net.cpp:380] Eltwise23 -> Eltwise23
I0925 11:03:16.679451  2600 net.cpp:122] Setting up Eltwise23
I0925 11:03:16.679455  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.679457  2600 net.cpp:137] Memory required for data: 852678800
I0925 11:03:16.679460  2600 layer_factory.hpp:77] Creating layer penlu47
I0925 11:03:16.679466  2600 net.cpp:84] Creating Layer penlu47
I0925 11:03:16.679467  2600 net.cpp:406] penlu47 <- Eltwise23
I0925 11:03:16.679471  2600 net.cpp:367] penlu47 -> Eltwise23 (in-place)
I0925 11:03:16.679587  2600 net.cpp:122] Setting up penlu47
I0925 11:03:16.679592  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.679594  2600 net.cpp:137] Memory required for data: 853933200
I0925 11:03:16.679599  2600 layer_factory.hpp:77] Creating layer Eltwise23_penlu47_0_split
I0925 11:03:16.679603  2600 net.cpp:84] Creating Layer Eltwise23_penlu47_0_split
I0925 11:03:16.679605  2600 net.cpp:406] Eltwise23_penlu47_0_split <- Eltwise23
I0925 11:03:16.679608  2600 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_0
I0925 11:03:16.679612  2600 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_1
I0925 11:03:16.679637  2600 net.cpp:122] Setting up Eltwise23_penlu47_0_split
I0925 11:03:16.679641  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.679644  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.679646  2600 net.cpp:137] Memory required for data: 856442000
I0925 11:03:16.679648  2600 layer_factory.hpp:77] Creating layer Convolution50
I0925 11:03:16.679654  2600 net.cpp:84] Creating Layer Convolution50
I0925 11:03:16.679656  2600 net.cpp:406] Convolution50 <- Eltwise23_penlu47_0_split_0
I0925 11:03:16.679661  2600 net.cpp:380] Convolution50 -> Convolution50
I0925 11:03:16.681370  2600 net.cpp:122] Setting up Convolution50
I0925 11:03:16.681380  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.681382  2600 net.cpp:137] Memory required for data: 857696400
I0925 11:03:16.681393  2600 layer_factory.hpp:77] Creating layer BatchNorm50
I0925 11:03:16.681401  2600 net.cpp:84] Creating Layer BatchNorm50
I0925 11:03:16.681402  2600 net.cpp:406] BatchNorm50 <- Convolution50
I0925 11:03:16.681406  2600 net.cpp:367] BatchNorm50 -> Convolution50 (in-place)
I0925 11:03:16.681557  2600 net.cpp:122] Setting up BatchNorm50
I0925 11:03:16.681562  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.681573  2600 net.cpp:137] Memory required for data: 858950800
I0925 11:03:16.681578  2600 layer_factory.hpp:77] Creating layer Scale50
I0925 11:03:16.681581  2600 net.cpp:84] Creating Layer Scale50
I0925 11:03:16.681584  2600 net.cpp:406] Scale50 <- Convolution50
I0925 11:03:16.681587  2600 net.cpp:367] Scale50 -> Convolution50 (in-place)
I0925 11:03:16.681619  2600 layer_factory.hpp:77] Creating layer Scale50
I0925 11:03:16.681704  2600 net.cpp:122] Setting up Scale50
I0925 11:03:16.681710  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.686491  2600 net.cpp:137] Memory required for data: 860205200
I0925 11:03:16.686502  2600 layer_factory.hpp:77] Creating layer penlu48
I0925 11:03:16.686509  2600 net.cpp:84] Creating Layer penlu48
I0925 11:03:16.686514  2600 net.cpp:406] penlu48 <- Convolution50
I0925 11:03:16.686522  2600 net.cpp:367] penlu48 -> Convolution50 (in-place)
I0925 11:03:16.686708  2600 net.cpp:122] Setting up penlu48
I0925 11:03:16.686717  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.686722  2600 net.cpp:137] Memory required for data: 861459600
I0925 11:03:16.686729  2600 layer_factory.hpp:77] Creating layer Convolution51
I0925 11:03:16.686740  2600 net.cpp:84] Creating Layer Convolution51
I0925 11:03:16.686745  2600 net.cpp:406] Convolution51 <- Convolution50
I0925 11:03:16.686753  2600 net.cpp:380] Convolution51 -> Convolution51
I0925 11:03:16.689613  2600 net.cpp:122] Setting up Convolution51
I0925 11:03:16.689622  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.689625  2600 net.cpp:137] Memory required for data: 862714000
I0925 11:03:16.689630  2600 layer_factory.hpp:77] Creating layer BatchNorm51
I0925 11:03:16.689636  2600 net.cpp:84] Creating Layer BatchNorm51
I0925 11:03:16.689640  2600 net.cpp:406] BatchNorm51 <- Convolution51
I0925 11:03:16.689643  2600 net.cpp:367] BatchNorm51 -> Convolution51 (in-place)
I0925 11:03:16.689834  2600 net.cpp:122] Setting up BatchNorm51
I0925 11:03:16.689852  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.689857  2600 net.cpp:137] Memory required for data: 863968400
I0925 11:03:16.689864  2600 layer_factory.hpp:77] Creating layer Scale51
I0925 11:03:16.689872  2600 net.cpp:84] Creating Layer Scale51
I0925 11:03:16.689877  2600 net.cpp:406] Scale51 <- Convolution51
I0925 11:03:16.689882  2600 net.cpp:367] Scale51 -> Convolution51 (in-place)
I0925 11:03:16.689935  2600 layer_factory.hpp:77] Creating layer Scale51
I0925 11:03:16.690027  2600 net.cpp:122] Setting up Scale51
I0925 11:03:16.690033  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.690035  2600 net.cpp:137] Memory required for data: 865222800
I0925 11:03:16.690039  2600 layer_factory.hpp:77] Creating layer Eltwise24
I0925 11:03:16.690043  2600 net.cpp:84] Creating Layer Eltwise24
I0925 11:03:16.690047  2600 net.cpp:406] Eltwise24 <- Eltwise23_penlu47_0_split_1
I0925 11:03:16.690049  2600 net.cpp:406] Eltwise24 <- Convolution51
I0925 11:03:16.690054  2600 net.cpp:380] Eltwise24 -> Eltwise24
I0925 11:03:16.690073  2600 net.cpp:122] Setting up Eltwise24
I0925 11:03:16.690076  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.690078  2600 net.cpp:137] Memory required for data: 866477200
I0925 11:03:16.690080  2600 layer_factory.hpp:77] Creating layer penlu49
I0925 11:03:16.690086  2600 net.cpp:84] Creating Layer penlu49
I0925 11:03:16.690088  2600 net.cpp:406] penlu49 <- Eltwise24
I0925 11:03:16.690091  2600 net.cpp:367] penlu49 -> Eltwise24 (in-place)
I0925 11:03:16.690214  2600 net.cpp:122] Setting up penlu49
I0925 11:03:16.690225  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.690228  2600 net.cpp:137] Memory required for data: 867731600
I0925 11:03:16.690233  2600 layer_factory.hpp:77] Creating layer Eltwise24_penlu49_0_split
I0925 11:03:16.690237  2600 net.cpp:84] Creating Layer Eltwise24_penlu49_0_split
I0925 11:03:16.690239  2600 net.cpp:406] Eltwise24_penlu49_0_split <- Eltwise24
I0925 11:03:16.690243  2600 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_0
I0925 11:03:16.690248  2600 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_1
I0925 11:03:16.690274  2600 net.cpp:122] Setting up Eltwise24_penlu49_0_split
I0925 11:03:16.690279  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.690281  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.690284  2600 net.cpp:137] Memory required for data: 870240400
I0925 11:03:16.690286  2600 layer_factory.hpp:77] Creating layer Convolution52
I0925 11:03:16.690292  2600 net.cpp:84] Creating Layer Convolution52
I0925 11:03:16.690295  2600 net.cpp:406] Convolution52 <- Eltwise24_penlu49_0_split_0
I0925 11:03:16.690299  2600 net.cpp:380] Convolution52 -> Convolution52
I0925 11:03:16.692107  2600 net.cpp:122] Setting up Convolution52
I0925 11:03:16.692116  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.692119  2600 net.cpp:137] Memory required for data: 871494800
I0925 11:03:16.692123  2600 layer_factory.hpp:77] Creating layer BatchNorm52
I0925 11:03:16.692129  2600 net.cpp:84] Creating Layer BatchNorm52
I0925 11:03:16.692131  2600 net.cpp:406] BatchNorm52 <- Convolution52
I0925 11:03:16.692137  2600 net.cpp:367] BatchNorm52 -> Convolution52 (in-place)
I0925 11:03:16.692288  2600 net.cpp:122] Setting up BatchNorm52
I0925 11:03:16.692293  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.692296  2600 net.cpp:137] Memory required for data: 872749200
I0925 11:03:16.692301  2600 layer_factory.hpp:77] Creating layer Scale52
I0925 11:03:16.692306  2600 net.cpp:84] Creating Layer Scale52
I0925 11:03:16.692308  2600 net.cpp:406] Scale52 <- Convolution52
I0925 11:03:16.692312  2600 net.cpp:367] Scale52 -> Convolution52 (in-place)
I0925 11:03:16.692342  2600 layer_factory.hpp:77] Creating layer Scale52
I0925 11:03:16.692431  2600 net.cpp:122] Setting up Scale52
I0925 11:03:16.692436  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.692438  2600 net.cpp:137] Memory required for data: 874003600
I0925 11:03:16.692442  2600 layer_factory.hpp:77] Creating layer penlu50
I0925 11:03:16.692447  2600 net.cpp:84] Creating Layer penlu50
I0925 11:03:16.692451  2600 net.cpp:406] penlu50 <- Convolution52
I0925 11:03:16.692453  2600 net.cpp:367] penlu50 -> Convolution52 (in-place)
I0925 11:03:16.692587  2600 net.cpp:122] Setting up penlu50
I0925 11:03:16.692593  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.692595  2600 net.cpp:137] Memory required for data: 875258000
I0925 11:03:16.692639  2600 layer_factory.hpp:77] Creating layer Convolution53
I0925 11:03:16.692662  2600 net.cpp:84] Creating Layer Convolution53
I0925 11:03:16.692664  2600 net.cpp:406] Convolution53 <- Convolution52
I0925 11:03:16.692668  2600 net.cpp:380] Convolution53 -> Convolution53
I0925 11:03:16.694701  2600 net.cpp:122] Setting up Convolution53
I0925 11:03:16.694711  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.694713  2600 net.cpp:137] Memory required for data: 876512400
I0925 11:03:16.694718  2600 layer_factory.hpp:77] Creating layer BatchNorm53
I0925 11:03:16.694725  2600 net.cpp:84] Creating Layer BatchNorm53
I0925 11:03:16.694728  2600 net.cpp:406] BatchNorm53 <- Convolution53
I0925 11:03:16.694732  2600 net.cpp:367] BatchNorm53 -> Convolution53 (in-place)
I0925 11:03:16.694890  2600 net.cpp:122] Setting up BatchNorm53
I0925 11:03:16.694895  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.694896  2600 net.cpp:137] Memory required for data: 877766800
I0925 11:03:16.694901  2600 layer_factory.hpp:77] Creating layer Scale53
I0925 11:03:16.694906  2600 net.cpp:84] Creating Layer Scale53
I0925 11:03:16.694916  2600 net.cpp:406] Scale53 <- Convolution53
I0925 11:03:16.694921  2600 net.cpp:367] Scale53 -> Convolution53 (in-place)
I0925 11:03:16.694952  2600 layer_factory.hpp:77] Creating layer Scale53
I0925 11:03:16.695044  2600 net.cpp:122] Setting up Scale53
I0925 11:03:16.695049  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.695050  2600 net.cpp:137] Memory required for data: 879021200
I0925 11:03:16.695055  2600 layer_factory.hpp:77] Creating layer Eltwise25
I0925 11:03:16.695058  2600 net.cpp:84] Creating Layer Eltwise25
I0925 11:03:16.695061  2600 net.cpp:406] Eltwise25 <- Eltwise24_penlu49_0_split_1
I0925 11:03:16.695065  2600 net.cpp:406] Eltwise25 <- Convolution53
I0925 11:03:16.695068  2600 net.cpp:380] Eltwise25 -> Eltwise25
I0925 11:03:16.695086  2600 net.cpp:122] Setting up Eltwise25
I0925 11:03:16.695091  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.695092  2600 net.cpp:137] Memory required for data: 880275600
I0925 11:03:16.695094  2600 layer_factory.hpp:77] Creating layer penlu51
I0925 11:03:16.695101  2600 net.cpp:84] Creating Layer penlu51
I0925 11:03:16.695102  2600 net.cpp:406] penlu51 <- Eltwise25
I0925 11:03:16.695106  2600 net.cpp:367] penlu51 -> Eltwise25 (in-place)
I0925 11:03:16.695227  2600 net.cpp:122] Setting up penlu51
I0925 11:03:16.695231  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.695233  2600 net.cpp:137] Memory required for data: 881530000
I0925 11:03:16.695238  2600 layer_factory.hpp:77] Creating layer Eltwise25_penlu51_0_split
I0925 11:03:16.695241  2600 net.cpp:84] Creating Layer Eltwise25_penlu51_0_split
I0925 11:03:16.695245  2600 net.cpp:406] Eltwise25_penlu51_0_split <- Eltwise25
I0925 11:03:16.695248  2600 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_0
I0925 11:03:16.695252  2600 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_1
I0925 11:03:16.695277  2600 net.cpp:122] Setting up Eltwise25_penlu51_0_split
I0925 11:03:16.695281  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.695284  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.695286  2600 net.cpp:137] Memory required for data: 884038800
I0925 11:03:16.695288  2600 layer_factory.hpp:77] Creating layer Convolution54
I0925 11:03:16.695296  2600 net.cpp:84] Creating Layer Convolution54
I0925 11:03:16.695297  2600 net.cpp:406] Convolution54 <- Eltwise25_penlu51_0_split_0
I0925 11:03:16.695302  2600 net.cpp:380] Convolution54 -> Convolution54
I0925 11:03:16.697564  2600 net.cpp:122] Setting up Convolution54
I0925 11:03:16.697574  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.697576  2600 net.cpp:137] Memory required for data: 885293200
I0925 11:03:16.697582  2600 layer_factory.hpp:77] Creating layer BatchNorm54
I0925 11:03:16.697587  2600 net.cpp:84] Creating Layer BatchNorm54
I0925 11:03:16.697590  2600 net.cpp:406] BatchNorm54 <- Convolution54
I0925 11:03:16.697593  2600 net.cpp:367] BatchNorm54 -> Convolution54 (in-place)
I0925 11:03:16.697748  2600 net.cpp:122] Setting up BatchNorm54
I0925 11:03:16.697752  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.697755  2600 net.cpp:137] Memory required for data: 886547600
I0925 11:03:16.697760  2600 layer_factory.hpp:77] Creating layer Scale54
I0925 11:03:16.697764  2600 net.cpp:84] Creating Layer Scale54
I0925 11:03:16.697767  2600 net.cpp:406] Scale54 <- Convolution54
I0925 11:03:16.697772  2600 net.cpp:367] Scale54 -> Convolution54 (in-place)
I0925 11:03:16.697803  2600 layer_factory.hpp:77] Creating layer Scale54
I0925 11:03:16.697890  2600 net.cpp:122] Setting up Scale54
I0925 11:03:16.697896  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.697898  2600 net.cpp:137] Memory required for data: 887802000
I0925 11:03:16.697901  2600 layer_factory.hpp:77] Creating layer penlu52
I0925 11:03:16.697907  2600 net.cpp:84] Creating Layer penlu52
I0925 11:03:16.697911  2600 net.cpp:406] penlu52 <- Convolution54
I0925 11:03:16.697914  2600 net.cpp:367] penlu52 -> Convolution54 (in-place)
I0925 11:03:16.698045  2600 net.cpp:122] Setting up penlu52
I0925 11:03:16.698050  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.698052  2600 net.cpp:137] Memory required for data: 889056400
I0925 11:03:16.698057  2600 layer_factory.hpp:77] Creating layer Convolution55
I0925 11:03:16.698063  2600 net.cpp:84] Creating Layer Convolution55
I0925 11:03:16.698066  2600 net.cpp:406] Convolution55 <- Convolution54
I0925 11:03:16.698071  2600 net.cpp:380] Convolution55 -> Convolution55
I0925 11:03:16.700095  2600 net.cpp:122] Setting up Convolution55
I0925 11:03:16.700104  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.700107  2600 net.cpp:137] Memory required for data: 890310800
I0925 11:03:16.700112  2600 layer_factory.hpp:77] Creating layer BatchNorm55
I0925 11:03:16.700119  2600 net.cpp:84] Creating Layer BatchNorm55
I0925 11:03:16.700121  2600 net.cpp:406] BatchNorm55 <- Convolution55
I0925 11:03:16.700124  2600 net.cpp:367] BatchNorm55 -> Convolution55 (in-place)
I0925 11:03:16.700284  2600 net.cpp:122] Setting up BatchNorm55
I0925 11:03:16.700289  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.700290  2600 net.cpp:137] Memory required for data: 891565200
I0925 11:03:16.700295  2600 layer_factory.hpp:77] Creating layer Scale55
I0925 11:03:16.700299  2600 net.cpp:84] Creating Layer Scale55
I0925 11:03:16.700302  2600 net.cpp:406] Scale55 <- Convolution55
I0925 11:03:16.700305  2600 net.cpp:367] Scale55 -> Convolution55 (in-place)
I0925 11:03:16.700336  2600 layer_factory.hpp:77] Creating layer Scale55
I0925 11:03:16.700425  2600 net.cpp:122] Setting up Scale55
I0925 11:03:16.700430  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.700433  2600 net.cpp:137] Memory required for data: 892819600
I0925 11:03:16.700436  2600 layer_factory.hpp:77] Creating layer Eltwise26
I0925 11:03:16.700441  2600 net.cpp:84] Creating Layer Eltwise26
I0925 11:03:16.700444  2600 net.cpp:406] Eltwise26 <- Eltwise25_penlu51_0_split_1
I0925 11:03:16.700446  2600 net.cpp:406] Eltwise26 <- Convolution55
I0925 11:03:16.700450  2600 net.cpp:380] Eltwise26 -> Eltwise26
I0925 11:03:16.700469  2600 net.cpp:122] Setting up Eltwise26
I0925 11:03:16.700474  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.700475  2600 net.cpp:137] Memory required for data: 894074000
I0925 11:03:16.700477  2600 layer_factory.hpp:77] Creating layer penlu53
I0925 11:03:16.700484  2600 net.cpp:84] Creating Layer penlu53
I0925 11:03:16.700485  2600 net.cpp:406] penlu53 <- Eltwise26
I0925 11:03:16.700489  2600 net.cpp:367] penlu53 -> Eltwise26 (in-place)
I0925 11:03:16.700640  2600 net.cpp:122] Setting up penlu53
I0925 11:03:16.700646  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.700649  2600 net.cpp:137] Memory required for data: 895328400
I0925 11:03:16.700654  2600 layer_factory.hpp:77] Creating layer Eltwise26_penlu53_0_split
I0925 11:03:16.700656  2600 net.cpp:84] Creating Layer Eltwise26_penlu53_0_split
I0925 11:03:16.700659  2600 net.cpp:406] Eltwise26_penlu53_0_split <- Eltwise26
I0925 11:03:16.700662  2600 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_0
I0925 11:03:16.700667  2600 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_1
I0925 11:03:16.700695  2600 net.cpp:122] Setting up Eltwise26_penlu53_0_split
I0925 11:03:16.700698  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.700701  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.700703  2600 net.cpp:137] Memory required for data: 897837200
I0925 11:03:16.700706  2600 layer_factory.hpp:77] Creating layer Convolution56
I0925 11:03:16.700711  2600 net.cpp:84] Creating Layer Convolution56
I0925 11:03:16.700713  2600 net.cpp:406] Convolution56 <- Eltwise26_penlu53_0_split_0
I0925 11:03:16.700718  2600 net.cpp:380] Convolution56 -> Convolution56
I0925 11:03:16.702425  2600 net.cpp:122] Setting up Convolution56
I0925 11:03:16.702435  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.702438  2600 net.cpp:137] Memory required for data: 899091600
I0925 11:03:16.702448  2600 layer_factory.hpp:77] Creating layer BatchNorm56
I0925 11:03:16.702453  2600 net.cpp:84] Creating Layer BatchNorm56
I0925 11:03:16.702456  2600 net.cpp:406] BatchNorm56 <- Convolution56
I0925 11:03:16.702461  2600 net.cpp:367] BatchNorm56 -> Convolution56 (in-place)
I0925 11:03:16.702615  2600 net.cpp:122] Setting up BatchNorm56
I0925 11:03:16.702620  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.702621  2600 net.cpp:137] Memory required for data: 900346000
I0925 11:03:16.702626  2600 layer_factory.hpp:77] Creating layer Scale56
I0925 11:03:16.702630  2600 net.cpp:84] Creating Layer Scale56
I0925 11:03:16.702632  2600 net.cpp:406] Scale56 <- Convolution56
I0925 11:03:16.702636  2600 net.cpp:367] Scale56 -> Convolution56 (in-place)
I0925 11:03:16.702666  2600 layer_factory.hpp:77] Creating layer Scale56
I0925 11:03:16.717200  2600 net.cpp:122] Setting up Scale56
I0925 11:03:16.717211  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.717216  2600 net.cpp:137] Memory required for data: 901600400
I0925 11:03:16.717224  2600 layer_factory.hpp:77] Creating layer penlu54
I0925 11:03:16.717233  2600 net.cpp:84] Creating Layer penlu54
I0925 11:03:16.717237  2600 net.cpp:406] penlu54 <- Convolution56
I0925 11:03:16.717247  2600 net.cpp:367] penlu54 -> Convolution56 (in-place)
I0925 11:03:16.717442  2600 net.cpp:122] Setting up penlu54
I0925 11:03:16.717452  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.717455  2600 net.cpp:137] Memory required for data: 902854800
I0925 11:03:16.717464  2600 layer_factory.hpp:77] Creating layer Convolution57
I0925 11:03:16.717474  2600 net.cpp:84] Creating Layer Convolution57
I0925 11:03:16.717478  2600 net.cpp:406] Convolution57 <- Convolution56
I0925 11:03:16.717486  2600 net.cpp:380] Convolution57 -> Convolution57
I0925 11:03:16.719987  2600 net.cpp:122] Setting up Convolution57
I0925 11:03:16.719997  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.720001  2600 net.cpp:137] Memory required for data: 904109200
I0925 11:03:16.720006  2600 layer_factory.hpp:77] Creating layer BatchNorm57
I0925 11:03:16.720011  2600 net.cpp:84] Creating Layer BatchNorm57
I0925 11:03:16.720015  2600 net.cpp:406] BatchNorm57 <- Convolution57
I0925 11:03:16.720018  2600 net.cpp:367] BatchNorm57 -> Convolution57 (in-place)
I0925 11:03:16.720188  2600 net.cpp:122] Setting up BatchNorm57
I0925 11:03:16.720194  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.720196  2600 net.cpp:137] Memory required for data: 905363600
I0925 11:03:16.720201  2600 layer_factory.hpp:77] Creating layer Scale57
I0925 11:03:16.720206  2600 net.cpp:84] Creating Layer Scale57
I0925 11:03:16.720208  2600 net.cpp:406] Scale57 <- Convolution57
I0925 11:03:16.720211  2600 net.cpp:367] Scale57 -> Convolution57 (in-place)
I0925 11:03:16.720244  2600 layer_factory.hpp:77] Creating layer Scale57
I0925 11:03:16.720376  2600 net.cpp:122] Setting up Scale57
I0925 11:03:16.720386  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.720389  2600 net.cpp:137] Memory required for data: 906618000
I0925 11:03:16.720396  2600 layer_factory.hpp:77] Creating layer Eltwise27
I0925 11:03:16.720402  2600 net.cpp:84] Creating Layer Eltwise27
I0925 11:03:16.720417  2600 net.cpp:406] Eltwise27 <- Eltwise26_penlu53_0_split_1
I0925 11:03:16.720422  2600 net.cpp:406] Eltwise27 <- Convolution57
I0925 11:03:16.720428  2600 net.cpp:380] Eltwise27 -> Eltwise27
I0925 11:03:16.720455  2600 net.cpp:122] Setting up Eltwise27
I0925 11:03:16.720460  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.720463  2600 net.cpp:137] Memory required for data: 907872400
I0925 11:03:16.720465  2600 layer_factory.hpp:77] Creating layer penlu55
I0925 11:03:16.720470  2600 net.cpp:84] Creating Layer penlu55
I0925 11:03:16.720474  2600 net.cpp:406] penlu55 <- Eltwise27
I0925 11:03:16.720477  2600 net.cpp:367] penlu55 -> Eltwise27 (in-place)
I0925 11:03:16.720621  2600 net.cpp:122] Setting up penlu55
I0925 11:03:16.720628  2600 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0925 11:03:16.720636  2600 net.cpp:137] Memory required for data: 909126800
I0925 11:03:16.720641  2600 layer_factory.hpp:77] Creating layer Pooling1
I0925 11:03:16.720646  2600 net.cpp:84] Creating Layer Pooling1
I0925 11:03:16.720649  2600 net.cpp:406] Pooling1 <- Eltwise27
I0925 11:03:16.720652  2600 net.cpp:380] Pooling1 -> Pooling1
I0925 11:03:16.721132  2600 net.cpp:122] Setting up Pooling1
I0925 11:03:16.721141  2600 net.cpp:129] Top shape: 100 64 1 1 (6400)
I0925 11:03:16.721143  2600 net.cpp:137] Memory required for data: 909152400
I0925 11:03:16.721146  2600 layer_factory.hpp:77] Creating layer InnerProduct1
I0925 11:03:16.721155  2600 net.cpp:84] Creating Layer InnerProduct1
I0925 11:03:16.721158  2600 net.cpp:406] InnerProduct1 <- Pooling1
I0925 11:03:16.721163  2600 net.cpp:380] InnerProduct1 -> InnerProduct1
I0925 11:03:16.721271  2600 net.cpp:122] Setting up InnerProduct1
I0925 11:03:16.721276  2600 net.cpp:129] Top shape: 100 10 (1000)
I0925 11:03:16.721278  2600 net.cpp:137] Memory required for data: 909156400
I0925 11:03:16.721282  2600 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0925 11:03:16.721287  2600 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0925 11:03:16.721288  2600 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I0925 11:03:16.721292  2600 net.cpp:406] SoftmaxWithLoss1 <- Data2
I0925 11:03:16.721297  2600 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0925 11:03:16.721302  2600 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0925 11:03:16.721561  2600 net.cpp:122] Setting up SoftmaxWithLoss1
I0925 11:03:16.721568  2600 net.cpp:129] Top shape: (1)
I0925 11:03:16.721571  2600 net.cpp:132]     with loss weight 1
I0925 11:03:16.721583  2600 net.cpp:137] Memory required for data: 909156404
I0925 11:03:16.721586  2600 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0925 11:03:16.721598  2600 net.cpp:198] InnerProduct1 needs backward computation.
I0925 11:03:16.721601  2600 net.cpp:198] Pooling1 needs backward computation.
I0925 11:03:16.721602  2600 net.cpp:198] penlu55 needs backward computation.
I0925 11:03:16.721604  2600 net.cpp:198] Eltwise27 needs backward computation.
I0925 11:03:16.721606  2600 net.cpp:198] Scale57 needs backward computation.
I0925 11:03:16.721608  2600 net.cpp:198] BatchNorm57 needs backward computation.
I0925 11:03:16.721611  2600 net.cpp:198] Convolution57 needs backward computation.
I0925 11:03:16.721613  2600 net.cpp:198] penlu54 needs backward computation.
I0925 11:03:16.721616  2600 net.cpp:198] Scale56 needs backward computation.
I0925 11:03:16.721616  2600 net.cpp:198] BatchNorm56 needs backward computation.
I0925 11:03:16.721618  2600 net.cpp:198] Convolution56 needs backward computation.
I0925 11:03:16.721621  2600 net.cpp:198] Eltwise26_penlu53_0_split needs backward computation.
I0925 11:03:16.721623  2600 net.cpp:198] penlu53 needs backward computation.
I0925 11:03:16.721626  2600 net.cpp:198] Eltwise26 needs backward computation.
I0925 11:03:16.721627  2600 net.cpp:198] Scale55 needs backward computation.
I0925 11:03:16.721629  2600 net.cpp:198] BatchNorm55 needs backward computation.
I0925 11:03:16.721632  2600 net.cpp:198] Convolution55 needs backward computation.
I0925 11:03:16.721633  2600 net.cpp:198] penlu52 needs backward computation.
I0925 11:03:16.721635  2600 net.cpp:198] Scale54 needs backward computation.
I0925 11:03:16.721637  2600 net.cpp:198] BatchNorm54 needs backward computation.
I0925 11:03:16.721639  2600 net.cpp:198] Convolution54 needs backward computation.
I0925 11:03:16.721642  2600 net.cpp:198] Eltwise25_penlu51_0_split needs backward computation.
I0925 11:03:16.721644  2600 net.cpp:198] penlu51 needs backward computation.
I0925 11:03:16.721647  2600 net.cpp:198] Eltwise25 needs backward computation.
I0925 11:03:16.721648  2600 net.cpp:198] Scale53 needs backward computation.
I0925 11:03:16.721650  2600 net.cpp:198] BatchNorm53 needs backward computation.
I0925 11:03:16.721653  2600 net.cpp:198] Convolution53 needs backward computation.
I0925 11:03:16.721655  2600 net.cpp:198] penlu50 needs backward computation.
I0925 11:03:16.721663  2600 net.cpp:198] Scale52 needs backward computation.
I0925 11:03:16.721665  2600 net.cpp:198] BatchNorm52 needs backward computation.
I0925 11:03:16.721668  2600 net.cpp:198] Convolution52 needs backward computation.
I0925 11:03:16.721670  2600 net.cpp:198] Eltwise24_penlu49_0_split needs backward computation.
I0925 11:03:16.721673  2600 net.cpp:198] penlu49 needs backward computation.
I0925 11:03:16.721674  2600 net.cpp:198] Eltwise24 needs backward computation.
I0925 11:03:16.721678  2600 net.cpp:198] Scale51 needs backward computation.
I0925 11:03:16.721679  2600 net.cpp:198] BatchNorm51 needs backward computation.
I0925 11:03:16.721681  2600 net.cpp:198] Convolution51 needs backward computation.
I0925 11:03:16.721684  2600 net.cpp:198] penlu48 needs backward computation.
I0925 11:03:16.721686  2600 net.cpp:198] Scale50 needs backward computation.
I0925 11:03:16.721688  2600 net.cpp:198] BatchNorm50 needs backward computation.
I0925 11:03:16.721690  2600 net.cpp:198] Convolution50 needs backward computation.
I0925 11:03:16.721693  2600 net.cpp:198] Eltwise23_penlu47_0_split needs backward computation.
I0925 11:03:16.721695  2600 net.cpp:198] penlu47 needs backward computation.
I0925 11:03:16.721698  2600 net.cpp:198] Eltwise23 needs backward computation.
I0925 11:03:16.721700  2600 net.cpp:198] Scale49 needs backward computation.
I0925 11:03:16.721702  2600 net.cpp:198] BatchNorm49 needs backward computation.
I0925 11:03:16.721705  2600 net.cpp:198] Convolution49 needs backward computation.
I0925 11:03:16.721707  2600 net.cpp:198] penlu46 needs backward computation.
I0925 11:03:16.721709  2600 net.cpp:198] Scale48 needs backward computation.
I0925 11:03:16.721711  2600 net.cpp:198] BatchNorm48 needs backward computation.
I0925 11:03:16.721714  2600 net.cpp:198] Convolution48 needs backward computation.
I0925 11:03:16.721716  2600 net.cpp:198] Eltwise22_penlu45_0_split needs backward computation.
I0925 11:03:16.721719  2600 net.cpp:198] penlu45 needs backward computation.
I0925 11:03:16.721720  2600 net.cpp:198] Eltwise22 needs backward computation.
I0925 11:03:16.721724  2600 net.cpp:198] Scale47 needs backward computation.
I0925 11:03:16.721725  2600 net.cpp:198] BatchNorm47 needs backward computation.
I0925 11:03:16.721727  2600 net.cpp:198] Convolution47 needs backward computation.
I0925 11:03:16.721730  2600 net.cpp:198] penlu44 needs backward computation.
I0925 11:03:16.721732  2600 net.cpp:198] Scale46 needs backward computation.
I0925 11:03:16.721735  2600 net.cpp:198] BatchNorm46 needs backward computation.
I0925 11:03:16.721746  2600 net.cpp:198] Convolution46 needs backward computation.
I0925 11:03:16.721750  2600 net.cpp:198] Eltwise21_penlu43_0_split needs backward computation.
I0925 11:03:16.721751  2600 net.cpp:198] penlu43 needs backward computation.
I0925 11:03:16.721753  2600 net.cpp:198] Eltwise21 needs backward computation.
I0925 11:03:16.721756  2600 net.cpp:198] Scale45 needs backward computation.
I0925 11:03:16.721767  2600 net.cpp:198] BatchNorm45 needs backward computation.
I0925 11:03:16.721771  2600 net.cpp:198] Convolution45 needs backward computation.
I0925 11:03:16.721772  2600 net.cpp:198] penlu42 needs backward computation.
I0925 11:03:16.721774  2600 net.cpp:198] Scale44 needs backward computation.
I0925 11:03:16.721777  2600 net.cpp:198] BatchNorm44 needs backward computation.
I0925 11:03:16.721779  2600 net.cpp:198] Convolution44 needs backward computation.
I0925 11:03:16.721781  2600 net.cpp:198] Eltwise20_penlu41_0_split needs backward computation.
I0925 11:03:16.721794  2600 net.cpp:198] penlu41 needs backward computation.
I0925 11:03:16.721796  2600 net.cpp:198] Eltwise20 needs backward computation.
I0925 11:03:16.721799  2600 net.cpp:198] Scale43 needs backward computation.
I0925 11:03:16.721801  2600 net.cpp:198] BatchNorm43 needs backward computation.
I0925 11:03:16.721803  2600 net.cpp:198] Convolution43 needs backward computation.
I0925 11:03:16.721807  2600 net.cpp:198] penlu40 needs backward computation.
I0925 11:03:16.721819  2600 net.cpp:198] Scale42 needs backward computation.
I0925 11:03:16.721825  2600 net.cpp:198] BatchNorm42 needs backward computation.
I0925 11:03:16.721837  2600 net.cpp:198] Convolution42 needs backward computation.
I0925 11:03:16.721842  2600 net.cpp:198] Eltwise19_penlu39_0_split needs backward computation.
I0925 11:03:16.721843  2600 net.cpp:198] penlu39 needs backward computation.
I0925 11:03:16.721846  2600 net.cpp:198] Eltwise19 needs backward computation.
I0925 11:03:16.721858  2600 net.cpp:198] Scale41 needs backward computation.
I0925 11:03:16.721860  2600 net.cpp:198] BatchNorm41 needs backward computation.
I0925 11:03:16.721863  2600 net.cpp:198] Convolution41 needs backward computation.
I0925 11:03:16.721865  2600 net.cpp:198] penlu38 needs backward computation.
I0925 11:03:16.721868  2600 net.cpp:198] Scale40 needs backward computation.
I0925 11:03:16.721869  2600 net.cpp:198] BatchNorm40 needs backward computation.
I0925 11:03:16.721882  2600 net.cpp:198] Convolution40 needs backward computation.
I0925 11:03:16.721884  2600 net.cpp:198] Scale39 needs backward computation.
I0925 11:03:16.721887  2600 net.cpp:198] BatchNorm39 needs backward computation.
I0925 11:03:16.721889  2600 net.cpp:198] Convolution39 needs backward computation.
I0925 11:03:16.721891  2600 net.cpp:198] Eltwise18_penlu37_0_split needs backward computation.
I0925 11:03:16.721894  2600 net.cpp:198] penlu37 needs backward computation.
I0925 11:03:16.721905  2600 net.cpp:198] Eltwise18 needs backward computation.
I0925 11:03:16.721909  2600 net.cpp:198] Scale38 needs backward computation.
I0925 11:03:16.721910  2600 net.cpp:198] BatchNorm38 needs backward computation.
I0925 11:03:16.721912  2600 net.cpp:198] Convolution38 needs backward computation.
I0925 11:03:16.721915  2600 net.cpp:198] penlu36 needs backward computation.
I0925 11:03:16.721917  2600 net.cpp:198] Scale37 needs backward computation.
I0925 11:03:16.721920  2600 net.cpp:198] BatchNorm37 needs backward computation.
I0925 11:03:16.721921  2600 net.cpp:198] Convolution37 needs backward computation.
I0925 11:03:16.721925  2600 net.cpp:198] Eltwise17_penlu35_0_split needs backward computation.
I0925 11:03:16.721926  2600 net.cpp:198] penlu35 needs backward computation.
I0925 11:03:16.721928  2600 net.cpp:198] Eltwise17 needs backward computation.
I0925 11:03:16.721931  2600 net.cpp:198] Scale36 needs backward computation.
I0925 11:03:16.721933  2600 net.cpp:198] BatchNorm36 needs backward computation.
I0925 11:03:16.721935  2600 net.cpp:198] Convolution36 needs backward computation.
I0925 11:03:16.721938  2600 net.cpp:198] penlu34 needs backward computation.
I0925 11:03:16.721940  2600 net.cpp:198] Scale35 needs backward computation.
I0925 11:03:16.721942  2600 net.cpp:198] BatchNorm35 needs backward computation.
I0925 11:03:16.721945  2600 net.cpp:198] Convolution35 needs backward computation.
I0925 11:03:16.721947  2600 net.cpp:198] Eltwise16_penlu33_0_split needs backward computation.
I0925 11:03:16.721949  2600 net.cpp:198] penlu33 needs backward computation.
I0925 11:03:16.721951  2600 net.cpp:198] Eltwise16 needs backward computation.
I0925 11:03:16.721954  2600 net.cpp:198] Scale34 needs backward computation.
I0925 11:03:16.721956  2600 net.cpp:198] BatchNorm34 needs backward computation.
I0925 11:03:16.721958  2600 net.cpp:198] Convolution34 needs backward computation.
I0925 11:03:16.721961  2600 net.cpp:198] penlu32 needs backward computation.
I0925 11:03:16.721963  2600 net.cpp:198] Scale33 needs backward computation.
I0925 11:03:16.721966  2600 net.cpp:198] BatchNorm33 needs backward computation.
I0925 11:03:16.721967  2600 net.cpp:198] Convolution33 needs backward computation.
I0925 11:03:16.721971  2600 net.cpp:198] Eltwise15_penlu31_0_split needs backward computation.
I0925 11:03:16.721973  2600 net.cpp:198] penlu31 needs backward computation.
I0925 11:03:16.721976  2600 net.cpp:198] Eltwise15 needs backward computation.
I0925 11:03:16.721978  2600 net.cpp:198] Scale32 needs backward computation.
I0925 11:03:16.721981  2600 net.cpp:198] BatchNorm32 needs backward computation.
I0925 11:03:16.721985  2600 net.cpp:198] Convolution32 needs backward computation.
I0925 11:03:16.721988  2600 net.cpp:198] penlu30 needs backward computation.
I0925 11:03:16.721990  2600 net.cpp:198] Scale31 needs backward computation.
I0925 11:03:16.721992  2600 net.cpp:198] BatchNorm31 needs backward computation.
I0925 11:03:16.721995  2600 net.cpp:198] Convolution31 needs backward computation.
I0925 11:03:16.721997  2600 net.cpp:198] Eltwise14_penlu29_0_split needs backward computation.
I0925 11:03:16.721999  2600 net.cpp:198] penlu29 needs backward computation.
I0925 11:03:16.722002  2600 net.cpp:198] Eltwise14 needs backward computation.
I0925 11:03:16.722004  2600 net.cpp:198] Scale30 needs backward computation.
I0925 11:03:16.747704  2600 net.cpp:198] BatchNorm30 needs backward computation.
I0925 11:03:16.747714  2600 net.cpp:198] Convolution30 needs backward computation.
I0925 11:03:16.747719  2600 net.cpp:198] penlu28 needs backward computation.
I0925 11:03:16.747723  2600 net.cpp:198] Scale29 needs backward computation.
I0925 11:03:16.747727  2600 net.cpp:198] BatchNorm29 needs backward computation.
I0925 11:03:16.747731  2600 net.cpp:198] Convolution29 needs backward computation.
I0925 11:03:16.747736  2600 net.cpp:198] Eltwise13_penlu27_0_split needs backward computation.
I0925 11:03:16.747740  2600 net.cpp:198] penlu27 needs backward computation.
I0925 11:03:16.747745  2600 net.cpp:198] Eltwise13 needs backward computation.
I0925 11:03:16.747750  2600 net.cpp:198] Scale28 needs backward computation.
I0925 11:03:16.747756  2600 net.cpp:198] BatchNorm28 needs backward computation.
I0925 11:03:16.747761  2600 net.cpp:198] Convolution28 needs backward computation.
I0925 11:03:16.747766  2600 net.cpp:198] penlu26 needs backward computation.
I0925 11:03:16.747769  2600 net.cpp:198] Scale27 needs backward computation.
I0925 11:03:16.747773  2600 net.cpp:198] BatchNorm27 needs backward computation.
I0925 11:03:16.747777  2600 net.cpp:198] Convolution27 needs backward computation.
I0925 11:03:16.747782  2600 net.cpp:198] Eltwise12_penlu25_0_split needs backward computation.
I0925 11:03:16.747786  2600 net.cpp:198] penlu25 needs backward computation.
I0925 11:03:16.747790  2600 net.cpp:198] Eltwise12 needs backward computation.
I0925 11:03:16.747795  2600 net.cpp:198] Scale26 needs backward computation.
I0925 11:03:16.747799  2600 net.cpp:198] BatchNorm26 needs backward computation.
I0925 11:03:16.747803  2600 net.cpp:198] Convolution26 needs backward computation.
I0925 11:03:16.747808  2600 net.cpp:198] penlu24 needs backward computation.
I0925 11:03:16.747812  2600 net.cpp:198] Scale25 needs backward computation.
I0925 11:03:16.747817  2600 net.cpp:198] BatchNorm25 needs backward computation.
I0925 11:03:16.747820  2600 net.cpp:198] Convolution25 needs backward computation.
I0925 11:03:16.747824  2600 net.cpp:198] Eltwise11_penlu23_0_split needs backward computation.
I0925 11:03:16.747829  2600 net.cpp:198] penlu23 needs backward computation.
I0925 11:03:16.747833  2600 net.cpp:198] Eltwise11 needs backward computation.
I0925 11:03:16.747838  2600 net.cpp:198] Scale24 needs backward computation.
I0925 11:03:16.747841  2600 net.cpp:198] BatchNorm24 needs backward computation.
I0925 11:03:16.747845  2600 net.cpp:198] Convolution24 needs backward computation.
I0925 11:03:16.747850  2600 net.cpp:198] penlu22 needs backward computation.
I0925 11:03:16.747854  2600 net.cpp:198] Scale23 needs backward computation.
I0925 11:03:16.747859  2600 net.cpp:198] BatchNorm23 needs backward computation.
I0925 11:03:16.747862  2600 net.cpp:198] Convolution23 needs backward computation.
I0925 11:03:16.747867  2600 net.cpp:198] Eltwise10_penlu21_0_split needs backward computation.
I0925 11:03:16.747872  2600 net.cpp:198] penlu21 needs backward computation.
I0925 11:03:16.747876  2600 net.cpp:198] Eltwise10 needs backward computation.
I0925 11:03:16.747881  2600 net.cpp:198] Scale22 needs backward computation.
I0925 11:03:16.747885  2600 net.cpp:198] BatchNorm22 needs backward computation.
I0925 11:03:16.747890  2600 net.cpp:198] Convolution22 needs backward computation.
I0925 11:03:16.747905  2600 net.cpp:198] penlu20 needs backward computation.
I0925 11:03:16.747910  2600 net.cpp:198] Scale21 needs backward computation.
I0925 11:03:16.747915  2600 net.cpp:198] BatchNorm21 needs backward computation.
I0925 11:03:16.747918  2600 net.cpp:198] Convolution21 needs backward computation.
I0925 11:03:16.747923  2600 net.cpp:198] Scale20 needs backward computation.
I0925 11:03:16.747927  2600 net.cpp:198] BatchNorm20 needs backward computation.
I0925 11:03:16.747931  2600 net.cpp:198] Convolution20 needs backward computation.
I0925 11:03:16.747936  2600 net.cpp:198] Eltwise9_penlu19_0_split needs backward computation.
I0925 11:03:16.747941  2600 net.cpp:198] penlu19 needs backward computation.
I0925 11:03:16.747946  2600 net.cpp:198] Eltwise9 needs backward computation.
I0925 11:03:16.747951  2600 net.cpp:198] Scale19 needs backward computation.
I0925 11:03:16.747956  2600 net.cpp:198] BatchNorm19 needs backward computation.
I0925 11:03:16.747959  2600 net.cpp:198] Convolution19 needs backward computation.
I0925 11:03:16.747963  2600 net.cpp:198] penlu18 needs backward computation.
I0925 11:03:16.747968  2600 net.cpp:198] Scale18 needs backward computation.
I0925 11:03:16.747972  2600 net.cpp:198] BatchNorm18 needs backward computation.
I0925 11:03:16.747977  2600 net.cpp:198] Convolution18 needs backward computation.
I0925 11:03:16.747982  2600 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I0925 11:03:16.747985  2600 net.cpp:198] penlu17 needs backward computation.
I0925 11:03:16.747990  2600 net.cpp:198] Eltwise8 needs backward computation.
I0925 11:03:16.747995  2600 net.cpp:198] Scale17 needs backward computation.
I0925 11:03:16.747999  2600 net.cpp:198] BatchNorm17 needs backward computation.
I0925 11:03:16.748003  2600 net.cpp:198] Convolution17 needs backward computation.
I0925 11:03:16.748008  2600 net.cpp:198] penlu16 needs backward computation.
I0925 11:03:16.748013  2600 net.cpp:198] Scale16 needs backward computation.
I0925 11:03:16.748016  2600 net.cpp:198] BatchNorm16 needs backward computation.
I0925 11:03:16.748020  2600 net.cpp:198] Convolution16 needs backward computation.
I0925 11:03:16.748025  2600 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I0925 11:03:16.748029  2600 net.cpp:198] penlu15 needs backward computation.
I0925 11:03:16.748034  2600 net.cpp:198] Eltwise7 needs backward computation.
I0925 11:03:16.748039  2600 net.cpp:198] Scale15 needs backward computation.
I0925 11:03:16.748044  2600 net.cpp:198] BatchNorm15 needs backward computation.
I0925 11:03:16.748047  2600 net.cpp:198] Convolution15 needs backward computation.
I0925 11:03:16.748052  2600 net.cpp:198] penlu14 needs backward computation.
I0925 11:03:16.748056  2600 net.cpp:198] Scale14 needs backward computation.
I0925 11:03:16.748060  2600 net.cpp:198] BatchNorm14 needs backward computation.
I0925 11:03:16.748064  2600 net.cpp:198] Convolution14 needs backward computation.
I0925 11:03:16.748070  2600 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I0925 11:03:16.748075  2600 net.cpp:198] penlu13 needs backward computation.
I0925 11:03:16.748078  2600 net.cpp:198] Eltwise6 needs backward computation.
I0925 11:03:16.748083  2600 net.cpp:198] Scale13 needs backward computation.
I0925 11:03:16.748087  2600 net.cpp:198] BatchNorm13 needs backward computation.
I0925 11:03:16.748091  2600 net.cpp:198] Convolution13 needs backward computation.
I0925 11:03:16.748096  2600 net.cpp:198] penlu12 needs backward computation.
I0925 11:03:16.748100  2600 net.cpp:198] Scale12 needs backward computation.
I0925 11:03:16.748105  2600 net.cpp:198] BatchNorm12 needs backward computation.
I0925 11:03:16.748108  2600 net.cpp:198] Convolution12 needs backward computation.
I0925 11:03:16.748113  2600 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I0925 11:03:16.748117  2600 net.cpp:198] penlu11 needs backward computation.
I0925 11:03:16.748122  2600 net.cpp:198] Eltwise5 needs backward computation.
I0925 11:03:16.748132  2600 net.cpp:198] Scale11 needs backward computation.
I0925 11:03:16.748136  2600 net.cpp:198] BatchNorm11 needs backward computation.
I0925 11:03:16.748142  2600 net.cpp:198] Convolution11 needs backward computation.
I0925 11:03:16.748145  2600 net.cpp:198] penlu10 needs backward computation.
I0925 11:03:16.748149  2600 net.cpp:198] Scale10 needs backward computation.
I0925 11:03:16.748153  2600 net.cpp:198] BatchNorm10 needs backward computation.
I0925 11:03:16.748158  2600 net.cpp:198] Convolution10 needs backward computation.
I0925 11:03:16.748162  2600 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I0925 11:03:16.750155  2600 net.cpp:198] penlu9 needs backward computation.
I0925 11:03:16.750164  2600 net.cpp:198] Eltwise4 needs backward computation.
I0925 11:03:16.750167  2600 net.cpp:198] Scale9 needs backward computation.
I0925 11:03:16.750170  2600 net.cpp:198] BatchNorm9 needs backward computation.
I0925 11:03:16.750172  2600 net.cpp:198] Convolution9 needs backward computation.
I0925 11:03:16.750175  2600 net.cpp:198] penlu8 needs backward computation.
I0925 11:03:16.750178  2600 net.cpp:198] Scale8 needs backward computation.
I0925 11:03:16.750180  2600 net.cpp:198] BatchNorm8 needs backward computation.
I0925 11:03:16.750183  2600 net.cpp:198] Convolution8 needs backward computation.
I0925 11:03:16.750186  2600 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I0925 11:03:16.750188  2600 net.cpp:198] penlu7 needs backward computation.
I0925 11:03:16.750191  2600 net.cpp:198] Eltwise3 needs backward computation.
I0925 11:03:16.750197  2600 net.cpp:198] Scale7 needs backward computation.
I0925 11:03:16.750200  2600 net.cpp:198] BatchNorm7 needs backward computation.
I0925 11:03:16.750202  2600 net.cpp:198] Convolution7 needs backward computation.
I0925 11:03:16.750205  2600 net.cpp:198] penlu6 needs backward computation.
I0925 11:03:16.750206  2600 net.cpp:198] Scale6 needs backward computation.
I0925 11:03:16.750210  2600 net.cpp:198] BatchNorm6 needs backward computation.
I0925 11:03:16.750211  2600 net.cpp:198] Convolution6 needs backward computation.
I0925 11:03:16.750214  2600 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I0925 11:03:16.750217  2600 net.cpp:198] penlu5 needs backward computation.
I0925 11:03:16.750219  2600 net.cpp:198] Eltwise2 needs backward computation.
I0925 11:03:16.750223  2600 net.cpp:198] Scale5 needs backward computation.
I0925 11:03:16.750226  2600 net.cpp:198] BatchNorm5 needs backward computation.
I0925 11:03:16.750228  2600 net.cpp:198] Convolution5 needs backward computation.
I0925 11:03:16.750231  2600 net.cpp:198] penlu4 needs backward computation.
I0925 11:03:16.750233  2600 net.cpp:198] Scale4 needs backward computation.
I0925 11:03:16.750236  2600 net.cpp:198] BatchNorm4 needs backward computation.
I0925 11:03:16.750237  2600 net.cpp:198] Convolution4 needs backward computation.
I0925 11:03:16.750241  2600 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I0925 11:03:16.750243  2600 net.cpp:198] penlu3 needs backward computation.
I0925 11:03:16.750246  2600 net.cpp:198] Eltwise1 needs backward computation.
I0925 11:03:16.750249  2600 net.cpp:198] Scale3 needs backward computation.
I0925 11:03:16.750252  2600 net.cpp:198] BatchNorm3 needs backward computation.
I0925 11:03:16.750254  2600 net.cpp:198] Convolution3 needs backward computation.
I0925 11:03:16.750257  2600 net.cpp:198] penlu2 needs backward computation.
I0925 11:03:16.750259  2600 net.cpp:198] Scale2 needs backward computation.
I0925 11:03:16.750262  2600 net.cpp:198] BatchNorm2 needs backward computation.
I0925 11:03:16.750263  2600 net.cpp:198] Convolution2 needs backward computation.
I0925 11:03:16.750267  2600 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I0925 11:03:16.750269  2600 net.cpp:198] penlu1 needs backward computation.
I0925 11:03:16.750272  2600 net.cpp:198] Scale1 needs backward computation.
I0925 11:03:16.750274  2600 net.cpp:198] BatchNorm1 needs backward computation.
I0925 11:03:16.750283  2600 net.cpp:198] Convolution1 needs backward computation.
I0925 11:03:16.750286  2600 net.cpp:200] Data1 does not need backward computation.
I0925 11:03:16.750289  2600 net.cpp:242] This network produces output SoftmaxWithLoss1
I0925 11:03:16.750385  2600 net.cpp:255] Network initialization done.
I0925 11:03:16.755297  2600 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0925 11:03:16.755312  2600 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0925 11:03:16.755317  2600 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0925 11:03:16.755515  2600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I0925 11:03:16.756889  2600 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution17"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution19"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution20"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu22"
  type: "PENLU"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      
I0925 11:03:16.811342  2600 layer_factory.hpp:77] Creating layer Data1
I0925 11:03:16.811390  2600 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb
I0925 11:03:16.811401  2600 net.cpp:84] Creating Layer Data1
I0925 11:03:16.811406  2600 net.cpp:380] Data1 -> Data1
I0925 11:03:16.811415  2600 net.cpp:380] Data1 -> Data2
I0925 11:03:16.811421  2600 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I0925 11:03:16.811595  2600 data_layer.cpp:45] output data size: 100,3,32,32
I0925 11:03:16.815698  2600 net.cpp:122] Setting up Data1
I0925 11:03:16.815721  2600 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0925 11:03:16.815726  2600 net.cpp:129] Top shape: 100 (100)
I0925 11:03:16.815727  2600 net.cpp:137] Memory required for data: 1229200
I0925 11:03:16.815732  2600 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0925 11:03:16.815742  2600 net.cpp:84] Creating Layer Data2_Data1_1_split
I0925 11:03:16.815745  2600 net.cpp:406] Data2_Data1_1_split <- Data2
I0925 11:03:16.815752  2600 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0925 11:03:16.815758  2600 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0925 11:03:16.815805  2600 net.cpp:122] Setting up Data2_Data1_1_split
I0925 11:03:16.815811  2600 net.cpp:129] Top shape: 100 (100)
I0925 11:03:16.815814  2600 net.cpp:129] Top shape: 100 (100)
I0925 11:03:16.815816  2600 net.cpp:137] Memory required for data: 1230000
I0925 11:03:16.815819  2600 layer_factory.hpp:77] Creating layer Convolution1
I0925 11:03:16.815829  2600 net.cpp:84] Creating Layer Convolution1
I0925 11:03:16.815831  2600 net.cpp:406] Convolution1 <- Data1
I0925 11:03:16.815835  2600 net.cpp:380] Convolution1 -> Convolution1
I0925 11:03:16.817102  2600 net.cpp:122] Setting up Convolution1
I0925 11:03:16.817113  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.817129  2600 net.cpp:137] Memory required for data: 7783600
I0925 11:03:16.817142  2600 layer_factory.hpp:77] Creating layer BatchNorm1
I0925 11:03:16.817148  2600 net.cpp:84] Creating Layer BatchNorm1
I0925 11:03:16.817150  2600 net.cpp:406] BatchNorm1 <- Convolution1
I0925 11:03:16.817154  2600 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0925 11:03:16.817313  2600 net.cpp:122] Setting up BatchNorm1
I0925 11:03:16.817318  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.817322  2600 net.cpp:137] Memory required for data: 14337200
I0925 11:03:16.817328  2600 layer_factory.hpp:77] Creating layer Scale1
I0925 11:03:16.817334  2600 net.cpp:84] Creating Layer Scale1
I0925 11:03:16.817338  2600 net.cpp:406] Scale1 <- Convolution1
I0925 11:03:16.817342  2600 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0925 11:03:16.817378  2600 layer_factory.hpp:77] Creating layer Scale1
I0925 11:03:16.817466  2600 net.cpp:122] Setting up Scale1
I0925 11:03:16.817473  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.817476  2600 net.cpp:137] Memory required for data: 20890800
I0925 11:03:16.817481  2600 layer_factory.hpp:77] Creating layer penlu1
I0925 11:03:16.817487  2600 net.cpp:84] Creating Layer penlu1
I0925 11:03:16.817489  2600 net.cpp:406] penlu1 <- Convolution1
I0925 11:03:16.817493  2600 net.cpp:367] penlu1 -> Convolution1 (in-place)
I0925 11:03:16.818145  2600 net.cpp:122] Setting up penlu1
I0925 11:03:16.818155  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.818157  2600 net.cpp:137] Memory required for data: 27444400
I0925 11:03:16.818164  2600 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I0925 11:03:16.818169  2600 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I0925 11:03:16.818172  2600 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I0925 11:03:16.818176  2600 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I0925 11:03:16.818183  2600 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I0925 11:03:16.818212  2600 net.cpp:122] Setting up Convolution1_penlu1_0_split
I0925 11:03:16.818217  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.838773  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.838783  2600 net.cpp:137] Memory required for data: 40551600
I0925 11:03:16.838788  2600 layer_factory.hpp:77] Creating layer Convolution2
I0925 11:03:16.838802  2600 net.cpp:84] Creating Layer Convolution2
I0925 11:03:16.838806  2600 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I0925 11:03:16.838814  2600 net.cpp:380] Convolution2 -> Convolution2
I0925 11:03:16.840389  2600 net.cpp:122] Setting up Convolution2
I0925 11:03:16.840401  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.840406  2600 net.cpp:137] Memory required for data: 47105200
I0925 11:03:16.840414  2600 layer_factory.hpp:77] Creating layer BatchNorm2
I0925 11:03:16.840423  2600 net.cpp:84] Creating Layer BatchNorm2
I0925 11:03:16.840427  2600 net.cpp:406] BatchNorm2 <- Convolution2
I0925 11:03:16.840433  2600 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0925 11:03:16.840664  2600 net.cpp:122] Setting up BatchNorm2
I0925 11:03:16.840674  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.840678  2600 net.cpp:137] Memory required for data: 53658800
I0925 11:03:16.840687  2600 layer_factory.hpp:77] Creating layer Scale2
I0925 11:03:16.840693  2600 net.cpp:84] Creating Layer Scale2
I0925 11:03:16.840698  2600 net.cpp:406] Scale2 <- Convolution2
I0925 11:03:16.840704  2600 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0925 11:03:16.840754  2600 layer_factory.hpp:77] Creating layer Scale2
I0925 11:03:16.840893  2600 net.cpp:122] Setting up Scale2
I0925 11:03:16.840903  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.840908  2600 net.cpp:137] Memory required for data: 60212400
I0925 11:03:16.840917  2600 layer_factory.hpp:77] Creating layer penlu2
I0925 11:03:16.840937  2600 net.cpp:84] Creating Layer penlu2
I0925 11:03:16.840942  2600 net.cpp:406] penlu2 <- Convolution2
I0925 11:03:16.840950  2600 net.cpp:367] penlu2 -> Convolution2 (in-place)
I0925 11:03:16.841146  2600 net.cpp:122] Setting up penlu2
I0925 11:03:16.841156  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.841161  2600 net.cpp:137] Memory required for data: 66766000
I0925 11:03:16.841167  2600 layer_factory.hpp:77] Creating layer Convolution3
I0925 11:03:16.841177  2600 net.cpp:84] Creating Layer Convolution3
I0925 11:03:16.841181  2600 net.cpp:406] Convolution3 <- Convolution2
I0925 11:03:16.841189  2600 net.cpp:380] Convolution3 -> Convolution3
I0925 11:03:16.842434  2600 net.cpp:122] Setting up Convolution3
I0925 11:03:16.842444  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.842447  2600 net.cpp:137] Memory required for data: 73319600
I0925 11:03:16.842452  2600 layer_factory.hpp:77] Creating layer BatchNorm3
I0925 11:03:16.842458  2600 net.cpp:84] Creating Layer BatchNorm3
I0925 11:03:16.842460  2600 net.cpp:406] BatchNorm3 <- Convolution3
I0925 11:03:16.842465  2600 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0925 11:03:16.842622  2600 net.cpp:122] Setting up BatchNorm3
I0925 11:03:16.842627  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.842628  2600 net.cpp:137] Memory required for data: 79873200
I0925 11:03:16.842633  2600 layer_factory.hpp:77] Creating layer Scale3
I0925 11:03:16.842638  2600 net.cpp:84] Creating Layer Scale3
I0925 11:03:16.842640  2600 net.cpp:406] Scale3 <- Convolution3
I0925 11:03:16.842643  2600 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0925 11:03:16.842674  2600 layer_factory.hpp:77] Creating layer Scale3
I0925 11:03:16.842761  2600 net.cpp:122] Setting up Scale3
I0925 11:03:16.842766  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.842767  2600 net.cpp:137] Memory required for data: 86426800
I0925 11:03:16.842772  2600 layer_factory.hpp:77] Creating layer Eltwise1
I0925 11:03:16.842777  2600 net.cpp:84] Creating Layer Eltwise1
I0925 11:03:16.842778  2600 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I0925 11:03:16.842782  2600 net.cpp:406] Eltwise1 <- Convolution3
I0925 11:03:16.842787  2600 net.cpp:380] Eltwise1 -> Eltwise1
I0925 11:03:16.842804  2600 net.cpp:122] Setting up Eltwise1
I0925 11:03:16.842808  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.842810  2600 net.cpp:137] Memory required for data: 92980400
I0925 11:03:16.842813  2600 layer_factory.hpp:77] Creating layer penlu3
I0925 11:03:16.842819  2600 net.cpp:84] Creating Layer penlu3
I0925 11:03:16.842821  2600 net.cpp:406] penlu3 <- Eltwise1
I0925 11:03:16.842825  2600 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I0925 11:03:16.842954  2600 net.cpp:122] Setting up penlu3
I0925 11:03:16.842959  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.842962  2600 net.cpp:137] Memory required for data: 99534000
I0925 11:03:16.842965  2600 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I0925 11:03:16.842970  2600 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I0925 11:03:16.842973  2600 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I0925 11:03:16.842978  2600 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I0925 11:03:16.842981  2600 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I0925 11:03:16.843008  2600 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I0925 11:03:16.843011  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.843014  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.843016  2600 net.cpp:137] Memory required for data: 112641200
I0925 11:03:16.843019  2600 layer_factory.hpp:77] Creating layer Convolution4
I0925 11:03:16.843025  2600 net.cpp:84] Creating Layer Convolution4
I0925 11:03:16.843027  2600 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I0925 11:03:16.843031  2600 net.cpp:380] Convolution4 -> Convolution4
I0925 11:03:16.844115  2600 net.cpp:122] Setting up Convolution4
I0925 11:03:16.844132  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.844135  2600 net.cpp:137] Memory required for data: 119194800
I0925 11:03:16.844141  2600 layer_factory.hpp:77] Creating layer BatchNorm4
I0925 11:03:16.844146  2600 net.cpp:84] Creating Layer BatchNorm4
I0925 11:03:16.844148  2600 net.cpp:406] BatchNorm4 <- Convolution4
I0925 11:03:16.844152  2600 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0925 11:03:16.844311  2600 net.cpp:122] Setting up BatchNorm4
I0925 11:03:16.844316  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.844318  2600 net.cpp:137] Memory required for data: 125748400
I0925 11:03:16.844327  2600 layer_factory.hpp:77] Creating layer Scale4
I0925 11:03:16.844331  2600 net.cpp:84] Creating Layer Scale4
I0925 11:03:16.844334  2600 net.cpp:406] Scale4 <- Convolution4
I0925 11:03:16.844337  2600 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0925 11:03:16.844370  2600 layer_factory.hpp:77] Creating layer Scale4
I0925 11:03:16.844460  2600 net.cpp:122] Setting up Scale4
I0925 11:03:16.844465  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.844466  2600 net.cpp:137] Memory required for data: 132302000
I0925 11:03:16.844470  2600 layer_factory.hpp:77] Creating layer penlu4
I0925 11:03:16.844477  2600 net.cpp:84] Creating Layer penlu4
I0925 11:03:16.844480  2600 net.cpp:406] penlu4 <- Convolution4
I0925 11:03:16.844483  2600 net.cpp:367] penlu4 -> Convolution4 (in-place)
I0925 11:03:16.844631  2600 net.cpp:122] Setting up penlu4
I0925 11:03:16.844637  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.844640  2600 net.cpp:137] Memory required for data: 138855600
I0925 11:03:16.844645  2600 layer_factory.hpp:77] Creating layer Convolution5
I0925 11:03:16.844651  2600 net.cpp:84] Creating Layer Convolution5
I0925 11:03:16.844653  2600 net.cpp:406] Convolution5 <- Convolution4
I0925 11:03:16.844657  2600 net.cpp:380] Convolution5 -> Convolution5
I0925 11:03:16.845607  2600 net.cpp:122] Setting up Convolution5
I0925 11:03:16.845615  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.845618  2600 net.cpp:137] Memory required for data: 145409200
I0925 11:03:16.845623  2600 layer_factory.hpp:77] Creating layer BatchNorm5
I0925 11:03:16.845628  2600 net.cpp:84] Creating Layer BatchNorm5
I0925 11:03:16.845629  2600 net.cpp:406] BatchNorm5 <- Convolution5
I0925 11:03:16.845634  2600 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0925 11:03:16.845788  2600 net.cpp:122] Setting up BatchNorm5
I0925 11:03:16.845793  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.845795  2600 net.cpp:137] Memory required for data: 151962800
I0925 11:03:16.845800  2600 layer_factory.hpp:77] Creating layer Scale5
I0925 11:03:16.845804  2600 net.cpp:84] Creating Layer Scale5
I0925 11:03:16.845808  2600 net.cpp:406] Scale5 <- Convolution5
I0925 11:03:16.845810  2600 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0925 11:03:16.845840  2600 layer_factory.hpp:77] Creating layer Scale5
I0925 11:03:16.845927  2600 net.cpp:122] Setting up Scale5
I0925 11:03:16.845932  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.845934  2600 net.cpp:137] Memory required for data: 158516400
I0925 11:03:16.845938  2600 layer_factory.hpp:77] Creating layer Eltwise2
I0925 11:03:16.845942  2600 net.cpp:84] Creating Layer Eltwise2
I0925 11:03:16.845945  2600 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I0925 11:03:16.845948  2600 net.cpp:406] Eltwise2 <- Convolution5
I0925 11:03:16.845952  2600 net.cpp:380] Eltwise2 -> Eltwise2
I0925 11:03:16.845968  2600 net.cpp:122] Setting up Eltwise2
I0925 11:03:16.845973  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.845974  2600 net.cpp:137] Memory required for data: 165070000
I0925 11:03:16.845978  2600 layer_factory.hpp:77] Creating layer penlu5
I0925 11:03:16.845983  2600 net.cpp:84] Creating Layer penlu5
I0925 11:03:16.845986  2600 net.cpp:406] penlu5 <- Eltwise2
I0925 11:03:16.845989  2600 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I0925 11:03:16.846127  2600 net.cpp:122] Setting up penlu5
I0925 11:03:16.846132  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.846134  2600 net.cpp:137] Memory required for data: 171623600
I0925 11:03:16.846139  2600 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I0925 11:03:16.846143  2600 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I0925 11:03:16.846145  2600 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I0925 11:03:16.846149  2600 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I0925 11:03:16.846153  2600 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I0925 11:03:16.846181  2600 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I0925 11:03:16.846185  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.846189  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.846190  2600 net.cpp:137] Memory required for data: 184730800
I0925 11:03:16.846192  2600 layer_factory.hpp:77] Creating layer Convolution6
I0925 11:03:16.846199  2600 net.cpp:84] Creating Layer Convolution6
I0925 11:03:16.846200  2600 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I0925 11:03:16.846205  2600 net.cpp:380] Convolution6 -> Convolution6
I0925 11:03:16.847151  2600 net.cpp:122] Setting up Convolution6
I0925 11:03:16.847159  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.847162  2600 net.cpp:137] Memory required for data: 191284400
I0925 11:03:16.847167  2600 layer_factory.hpp:77] Creating layer BatchNorm6
I0925 11:03:16.847172  2600 net.cpp:84] Creating Layer BatchNorm6
I0925 11:03:16.847175  2600 net.cpp:406] BatchNorm6 <- Convolution6
I0925 11:03:16.847179  2600 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0925 11:03:16.847331  2600 net.cpp:122] Setting up BatchNorm6
I0925 11:03:16.847335  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.847337  2600 net.cpp:137] Memory required for data: 197838000
I0925 11:03:16.847342  2600 layer_factory.hpp:77] Creating layer Scale6
I0925 11:03:16.847347  2600 net.cpp:84] Creating Layer Scale6
I0925 11:03:16.847349  2600 net.cpp:406] Scale6 <- Convolution6
I0925 11:03:16.847352  2600 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0925 11:03:16.847383  2600 layer_factory.hpp:77] Creating layer Scale6
I0925 11:03:16.847470  2600 net.cpp:122] Setting up Scale6
I0925 11:03:16.847473  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.847476  2600 net.cpp:137] Memory required for data: 204391600
I0925 11:03:16.847479  2600 layer_factory.hpp:77] Creating layer penlu6
I0925 11:03:16.847486  2600 net.cpp:84] Creating Layer penlu6
I0925 11:03:16.847488  2600 net.cpp:406] penlu6 <- Convolution6
I0925 11:03:16.847492  2600 net.cpp:367] penlu6 -> Convolution6 (in-place)
I0925 11:03:16.847620  2600 net.cpp:122] Setting up penlu6
I0925 11:03:16.847625  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.847626  2600 net.cpp:137] Memory required for data: 210945200
I0925 11:03:16.847630  2600 layer_factory.hpp:77] Creating layer Convolution7
I0925 11:03:16.847637  2600 net.cpp:84] Creating Layer Convolution7
I0925 11:03:16.847640  2600 net.cpp:406] Convolution7 <- Convolution6
I0925 11:03:16.847643  2600 net.cpp:380] Convolution7 -> Convolution7
I0925 11:03:16.848618  2600 net.cpp:122] Setting up Convolution7
I0925 11:03:16.848626  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.848630  2600 net.cpp:137] Memory required for data: 217498800
I0925 11:03:16.848634  2600 layer_factory.hpp:77] Creating layer BatchNorm7
I0925 11:03:16.848641  2600 net.cpp:84] Creating Layer BatchNorm7
I0925 11:03:16.848644  2600 net.cpp:406] BatchNorm7 <- Convolution7
I0925 11:03:16.848649  2600 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0925 11:03:16.848799  2600 net.cpp:122] Setting up BatchNorm7
I0925 11:03:16.848804  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.848808  2600 net.cpp:137] Memory required for data: 224052400
I0925 11:03:16.848816  2600 layer_factory.hpp:77] Creating layer Scale7
I0925 11:03:16.848821  2600 net.cpp:84] Creating Layer Scale7
I0925 11:03:16.848829  2600 net.cpp:406] Scale7 <- Convolution7
I0925 11:03:16.848834  2600 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0925 11:03:16.848877  2600 layer_factory.hpp:77] Creating layer Scale7
I0925 11:03:16.848975  2600 net.cpp:122] Setting up Scale7
I0925 11:03:16.848980  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.848983  2600 net.cpp:137] Memory required for data: 230606000
I0925 11:03:16.848986  2600 layer_factory.hpp:77] Creating layer Eltwise3
I0925 11:03:16.848990  2600 net.cpp:84] Creating Layer Eltwise3
I0925 11:03:16.848992  2600 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I0925 11:03:16.848995  2600 net.cpp:406] Eltwise3 <- Convolution7
I0925 11:03:16.848999  2600 net.cpp:380] Eltwise3 -> Eltwise3
I0925 11:03:16.849017  2600 net.cpp:122] Setting up Eltwise3
I0925 11:03:16.849021  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.849023  2600 net.cpp:137] Memory required for data: 237159600
I0925 11:03:16.849025  2600 layer_factory.hpp:77] Creating layer penlu7
I0925 11:03:16.849030  2600 net.cpp:84] Creating Layer penlu7
I0925 11:03:16.849033  2600 net.cpp:406] penlu7 <- Eltwise3
I0925 11:03:16.849036  2600 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I0925 11:03:16.849164  2600 net.cpp:122] Setting up penlu7
I0925 11:03:16.849169  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.849170  2600 net.cpp:137] Memory required for data: 243713200
I0925 11:03:16.849174  2600 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I0925 11:03:16.849179  2600 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I0925 11:03:16.849180  2600 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I0925 11:03:16.849185  2600 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I0925 11:03:16.849189  2600 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I0925 11:03:16.849215  2600 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I0925 11:03:16.849220  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.849221  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.869438  2600 net.cpp:137] Memory required for data: 256820400
I0925 11:03:16.869447  2600 layer_factory.hpp:77] Creating layer Convolution8
I0925 11:03:16.869460  2600 net.cpp:84] Creating Layer Convolution8
I0925 11:03:16.869465  2600 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I0925 11:03:16.869473  2600 net.cpp:380] Convolution8 -> Convolution8
I0925 11:03:16.870944  2600 net.cpp:122] Setting up Convolution8
I0925 11:03:16.870956  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.870960  2600 net.cpp:137] Memory required for data: 263374000
I0925 11:03:16.870968  2600 layer_factory.hpp:77] Creating layer BatchNorm8
I0925 11:03:16.870976  2600 net.cpp:84] Creating Layer BatchNorm8
I0925 11:03:16.870981  2600 net.cpp:406] BatchNorm8 <- Convolution8
I0925 11:03:16.870987  2600 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0925 11:03:16.871201  2600 net.cpp:122] Setting up BatchNorm8
I0925 11:03:16.871208  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.871212  2600 net.cpp:137] Memory required for data: 269927600
I0925 11:03:16.871220  2600 layer_factory.hpp:77] Creating layer Scale8
I0925 11:03:16.871227  2600 net.cpp:84] Creating Layer Scale8
I0925 11:03:16.871232  2600 net.cpp:406] Scale8 <- Convolution8
I0925 11:03:16.871237  2600 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0925 11:03:16.871282  2600 layer_factory.hpp:77] Creating layer Scale8
I0925 11:03:16.871419  2600 net.cpp:122] Setting up Scale8
I0925 11:03:16.871433  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.871436  2600 net.cpp:137] Memory required for data: 276481200
I0925 11:03:16.871443  2600 layer_factory.hpp:77] Creating layer penlu8
I0925 11:03:16.871451  2600 net.cpp:84] Creating Layer penlu8
I0925 11:03:16.871455  2600 net.cpp:406] penlu8 <- Convolution8
I0925 11:03:16.871462  2600 net.cpp:367] penlu8 -> Convolution8 (in-place)
I0925 11:03:16.871671  2600 net.cpp:122] Setting up penlu8
I0925 11:03:16.871691  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.871695  2600 net.cpp:137] Memory required for data: 283034800
I0925 11:03:16.871703  2600 layer_factory.hpp:77] Creating layer Convolution9
I0925 11:03:16.871714  2600 net.cpp:84] Creating Layer Convolution9
I0925 11:03:16.871718  2600 net.cpp:406] Convolution9 <- Convolution8
I0925 11:03:16.871726  2600 net.cpp:380] Convolution9 -> Convolution9
I0925 11:03:16.873046  2600 net.cpp:122] Setting up Convolution9
I0925 11:03:16.873056  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.873059  2600 net.cpp:137] Memory required for data: 289588400
I0925 11:03:16.873064  2600 layer_factory.hpp:77] Creating layer BatchNorm9
I0925 11:03:16.873071  2600 net.cpp:84] Creating Layer BatchNorm9
I0925 11:03:16.873075  2600 net.cpp:406] BatchNorm9 <- Convolution9
I0925 11:03:16.873077  2600 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0925 11:03:16.873235  2600 net.cpp:122] Setting up BatchNorm9
I0925 11:03:16.873240  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.873242  2600 net.cpp:137] Memory required for data: 296142000
I0925 11:03:16.873247  2600 layer_factory.hpp:77] Creating layer Scale9
I0925 11:03:16.873252  2600 net.cpp:84] Creating Layer Scale9
I0925 11:03:16.873255  2600 net.cpp:406] Scale9 <- Convolution9
I0925 11:03:16.873258  2600 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0925 11:03:16.873291  2600 layer_factory.hpp:77] Creating layer Scale9
I0925 11:03:16.873380  2600 net.cpp:122] Setting up Scale9
I0925 11:03:16.873384  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.873386  2600 net.cpp:137] Memory required for data: 302695600
I0925 11:03:16.873390  2600 layer_factory.hpp:77] Creating layer Eltwise4
I0925 11:03:16.873395  2600 net.cpp:84] Creating Layer Eltwise4
I0925 11:03:16.873399  2600 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I0925 11:03:16.873401  2600 net.cpp:406] Eltwise4 <- Convolution9
I0925 11:03:16.873405  2600 net.cpp:380] Eltwise4 -> Eltwise4
I0925 11:03:16.873425  2600 net.cpp:122] Setting up Eltwise4
I0925 11:03:16.873428  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.873430  2600 net.cpp:137] Memory required for data: 309249200
I0925 11:03:16.873432  2600 layer_factory.hpp:77] Creating layer penlu9
I0925 11:03:16.873438  2600 net.cpp:84] Creating Layer penlu9
I0925 11:03:16.873440  2600 net.cpp:406] penlu9 <- Eltwise4
I0925 11:03:16.873445  2600 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I0925 11:03:16.873575  2600 net.cpp:122] Setting up penlu9
I0925 11:03:16.873580  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.873582  2600 net.cpp:137] Memory required for data: 315802800
I0925 11:03:16.873586  2600 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I0925 11:03:16.873590  2600 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I0925 11:03:16.873592  2600 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I0925 11:03:16.873596  2600 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I0925 11:03:16.873600  2600 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I0925 11:03:16.873627  2600 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I0925 11:03:16.873631  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.873634  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.873636  2600 net.cpp:137] Memory required for data: 328910000
I0925 11:03:16.873638  2600 layer_factory.hpp:77] Creating layer Convolution10
I0925 11:03:16.873646  2600 net.cpp:84] Creating Layer Convolution10
I0925 11:03:16.873647  2600 net.cpp:406] Convolution10 <- Eltwise4_penlu9_0_split_0
I0925 11:03:16.873651  2600 net.cpp:380] Convolution10 -> Convolution10
I0925 11:03:16.874752  2600 net.cpp:122] Setting up Convolution10
I0925 11:03:16.874761  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.874764  2600 net.cpp:137] Memory required for data: 335463600
I0925 11:03:16.874769  2600 layer_factory.hpp:77] Creating layer BatchNorm10
I0925 11:03:16.874781  2600 net.cpp:84] Creating Layer BatchNorm10
I0925 11:03:16.874784  2600 net.cpp:406] BatchNorm10 <- Convolution10
I0925 11:03:16.874789  2600 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0925 11:03:16.874951  2600 net.cpp:122] Setting up BatchNorm10
I0925 11:03:16.874956  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.874959  2600 net.cpp:137] Memory required for data: 342017200
I0925 11:03:16.874964  2600 layer_factory.hpp:77] Creating layer Scale10
I0925 11:03:16.874969  2600 net.cpp:84] Creating Layer Scale10
I0925 11:03:16.874970  2600 net.cpp:406] Scale10 <- Convolution10
I0925 11:03:16.874974  2600 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0925 11:03:16.875007  2600 layer_factory.hpp:77] Creating layer Scale10
I0925 11:03:16.875098  2600 net.cpp:122] Setting up Scale10
I0925 11:03:16.875103  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.875105  2600 net.cpp:137] Memory required for data: 348570800
I0925 11:03:16.875109  2600 layer_factory.hpp:77] Creating layer penlu10
I0925 11:03:16.875114  2600 net.cpp:84] Creating Layer penlu10
I0925 11:03:16.875118  2600 net.cpp:406] penlu10 <- Convolution10
I0925 11:03:16.875120  2600 net.cpp:367] penlu10 -> Convolution10 (in-place)
I0925 11:03:16.875253  2600 net.cpp:122] Setting up penlu10
I0925 11:03:16.875258  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.875260  2600 net.cpp:137] Memory required for data: 355124400
I0925 11:03:16.875264  2600 layer_factory.hpp:77] Creating layer Convolution11
I0925 11:03:16.875272  2600 net.cpp:84] Creating Layer Convolution11
I0925 11:03:16.875274  2600 net.cpp:406] Convolution11 <- Convolution10
I0925 11:03:16.875277  2600 net.cpp:380] Convolution11 -> Convolution11
I0925 11:03:16.876579  2600 net.cpp:122] Setting up Convolution11
I0925 11:03:16.876588  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.876591  2600 net.cpp:137] Memory required for data: 361678000
I0925 11:03:16.876596  2600 layer_factory.hpp:77] Creating layer BatchNorm11
I0925 11:03:16.876601  2600 net.cpp:84] Creating Layer BatchNorm11
I0925 11:03:16.876605  2600 net.cpp:406] BatchNorm11 <- Convolution11
I0925 11:03:16.876608  2600 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0925 11:03:16.876772  2600 net.cpp:122] Setting up BatchNorm11
I0925 11:03:16.876777  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.876780  2600 net.cpp:137] Memory required for data: 368231600
I0925 11:03:16.876785  2600 layer_factory.hpp:77] Creating layer Scale11
I0925 11:03:16.876788  2600 net.cpp:84] Creating Layer Scale11
I0925 11:03:16.876791  2600 net.cpp:406] Scale11 <- Convolution11
I0925 11:03:16.876794  2600 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0925 11:03:16.876827  2600 layer_factory.hpp:77] Creating layer Scale11
I0925 11:03:16.876919  2600 net.cpp:122] Setting up Scale11
I0925 11:03:16.876924  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.876925  2600 net.cpp:137] Memory required for data: 374785200
I0925 11:03:16.876929  2600 layer_factory.hpp:77] Creating layer Eltwise5
I0925 11:03:16.876933  2600 net.cpp:84] Creating Layer Eltwise5
I0925 11:03:16.876936  2600 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I0925 11:03:16.876940  2600 net.cpp:406] Eltwise5 <- Convolution11
I0925 11:03:16.876943  2600 net.cpp:380] Eltwise5 -> Eltwise5
I0925 11:03:16.876961  2600 net.cpp:122] Setting up Eltwise5
I0925 11:03:16.876966  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.876968  2600 net.cpp:137] Memory required for data: 381338800
I0925 11:03:16.876971  2600 layer_factory.hpp:77] Creating layer penlu11
I0925 11:03:16.876976  2600 net.cpp:84] Creating Layer penlu11
I0925 11:03:16.876978  2600 net.cpp:406] penlu11 <- Eltwise5
I0925 11:03:16.876982  2600 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I0925 11:03:16.877117  2600 net.cpp:122] Setting up penlu11
I0925 11:03:16.877122  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.877125  2600 net.cpp:137] Memory required for data: 387892400
I0925 11:03:16.877135  2600 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I0925 11:03:16.877140  2600 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I0925 11:03:16.877142  2600 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I0925 11:03:16.877146  2600 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I0925 11:03:16.877151  2600 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I0925 11:03:16.877180  2600 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I0925 11:03:16.877185  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.877188  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.877190  2600 net.cpp:137] Memory required for data: 400999600
I0925 11:03:16.877192  2600 layer_factory.hpp:77] Creating layer Convolution12
I0925 11:03:16.877198  2600 net.cpp:84] Creating Layer Convolution12
I0925 11:03:16.877202  2600 net.cpp:406] Convolution12 <- Eltwise5_penlu11_0_split_0
I0925 11:03:16.877205  2600 net.cpp:380] Convolution12 -> Convolution12
I0925 11:03:16.878191  2600 net.cpp:122] Setting up Convolution12
I0925 11:03:16.878201  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.878203  2600 net.cpp:137] Memory required for data: 407553200
I0925 11:03:16.878207  2600 layer_factory.hpp:77] Creating layer BatchNorm12
I0925 11:03:16.878212  2600 net.cpp:84] Creating Layer BatchNorm12
I0925 11:03:16.878216  2600 net.cpp:406] BatchNorm12 <- Convolution12
I0925 11:03:16.878219  2600 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0925 11:03:16.878381  2600 net.cpp:122] Setting up BatchNorm12
I0925 11:03:16.878386  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.878387  2600 net.cpp:137] Memory required for data: 414106800
I0925 11:03:16.878392  2600 layer_factory.hpp:77] Creating layer Scale12
I0925 11:03:16.878397  2600 net.cpp:84] Creating Layer Scale12
I0925 11:03:16.878401  2600 net.cpp:406] Scale12 <- Convolution12
I0925 11:03:16.878403  2600 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0925 11:03:16.878435  2600 layer_factory.hpp:77] Creating layer Scale12
I0925 11:03:16.878527  2600 net.cpp:122] Setting up Scale12
I0925 11:03:16.878531  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.878535  2600 net.cpp:137] Memory required for data: 420660400
I0925 11:03:16.878538  2600 layer_factory.hpp:77] Creating layer penlu12
I0925 11:03:16.878543  2600 net.cpp:84] Creating Layer penlu12
I0925 11:03:16.878545  2600 net.cpp:406] penlu12 <- Convolution12
I0925 11:03:16.878551  2600 net.cpp:367] penlu12 -> Convolution12 (in-place)
I0925 11:03:16.878684  2600 net.cpp:122] Setting up penlu12
I0925 11:03:16.878690  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.878691  2600 net.cpp:137] Memory required for data: 427214000
I0925 11:03:16.878695  2600 layer_factory.hpp:77] Creating layer Convolution13
I0925 11:03:16.878701  2600 net.cpp:84] Creating Layer Convolution13
I0925 11:03:16.878705  2600 net.cpp:406] Convolution13 <- Convolution12
I0925 11:03:16.878708  2600 net.cpp:380] Convolution13 -> Convolution13
I0925 11:03:16.879709  2600 net.cpp:122] Setting up Convolution13
I0925 11:03:16.879719  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.879720  2600 net.cpp:137] Memory required for data: 433767600
I0925 11:03:16.879725  2600 layer_factory.hpp:77] Creating layer BatchNorm13
I0925 11:03:16.879730  2600 net.cpp:84] Creating Layer BatchNorm13
I0925 11:03:16.879734  2600 net.cpp:406] BatchNorm13 <- Convolution13
I0925 11:03:16.879736  2600 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0925 11:03:16.879897  2600 net.cpp:122] Setting up BatchNorm13
I0925 11:03:16.879902  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.879904  2600 net.cpp:137] Memory required for data: 440321200
I0925 11:03:16.879909  2600 layer_factory.hpp:77] Creating layer Scale13
I0925 11:03:16.879914  2600 net.cpp:84] Creating Layer Scale13
I0925 11:03:16.879916  2600 net.cpp:406] Scale13 <- Convolution13
I0925 11:03:16.879926  2600 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0925 11:03:16.879961  2600 layer_factory.hpp:77] Creating layer Scale13
I0925 11:03:16.880053  2600 net.cpp:122] Setting up Scale13
I0925 11:03:16.880059  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.880060  2600 net.cpp:137] Memory required for data: 446874800
I0925 11:03:16.880064  2600 layer_factory.hpp:77] Creating layer Eltwise6
I0925 11:03:16.880074  2600 net.cpp:84] Creating Layer Eltwise6
I0925 11:03:16.880076  2600 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I0925 11:03:16.880079  2600 net.cpp:406] Eltwise6 <- Convolution13
I0925 11:03:16.880084  2600 net.cpp:380] Eltwise6 -> Eltwise6
I0925 11:03:16.880103  2600 net.cpp:122] Setting up Eltwise6
I0925 11:03:16.880108  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.880110  2600 net.cpp:137] Memory required for data: 453428400
I0925 11:03:16.880112  2600 layer_factory.hpp:77] Creating layer penlu13
I0925 11:03:16.880117  2600 net.cpp:84] Creating Layer penlu13
I0925 11:03:16.880120  2600 net.cpp:406] penlu13 <- Eltwise6
I0925 11:03:16.880123  2600 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I0925 11:03:16.880259  2600 net.cpp:122] Setting up penlu13
I0925 11:03:16.880264  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.880265  2600 net.cpp:137] Memory required for data: 459982000
I0925 11:03:16.880278  2600 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I0925 11:03:16.880282  2600 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I0925 11:03:16.880285  2600 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I0925 11:03:16.880290  2600 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I0925 11:03:16.880295  2600 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I0925 11:03:16.880323  2600 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I0925 11:03:16.880328  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.880331  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.899957  2600 net.cpp:137] Memory required for data: 473089200
I0925 11:03:16.899967  2600 layer_factory.hpp:77] Creating layer Convolution14
I0925 11:03:16.899981  2600 net.cpp:84] Creating Layer Convolution14
I0925 11:03:16.899986  2600 net.cpp:406] Convolution14 <- Eltwise6_penlu13_0_split_0
I0925 11:03:16.899994  2600 net.cpp:380] Convolution14 -> Convolution14
I0925 11:03:16.901504  2600 net.cpp:122] Setting up Convolution14
I0925 11:03:16.901516  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.901521  2600 net.cpp:137] Memory required for data: 479642800
I0925 11:03:16.901528  2600 layer_factory.hpp:77] Creating layer BatchNorm14
I0925 11:03:16.901536  2600 net.cpp:84] Creating Layer BatchNorm14
I0925 11:03:16.901541  2600 net.cpp:406] BatchNorm14 <- Convolution14
I0925 11:03:16.901547  2600 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0925 11:03:16.901778  2600 net.cpp:122] Setting up BatchNorm14
I0925 11:03:16.901788  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.901793  2600 net.cpp:137] Memory required for data: 486196400
I0925 11:03:16.901801  2600 layer_factory.hpp:77] Creating layer Scale14
I0925 11:03:16.901808  2600 net.cpp:84] Creating Layer Scale14
I0925 11:03:16.901813  2600 net.cpp:406] Scale14 <- Convolution14
I0925 11:03:16.901819  2600 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0925 11:03:16.901868  2600 layer_factory.hpp:77] Creating layer Scale14
I0925 11:03:16.902012  2600 net.cpp:122] Setting up Scale14
I0925 11:03:16.902022  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.902026  2600 net.cpp:137] Memory required for data: 492750000
I0925 11:03:16.902034  2600 layer_factory.hpp:77] Creating layer penlu14
I0925 11:03:16.902041  2600 net.cpp:84] Creating Layer penlu14
I0925 11:03:16.902045  2600 net.cpp:406] penlu14 <- Convolution14
I0925 11:03:16.902052  2600 net.cpp:367] penlu14 -> Convolution14 (in-place)
I0925 11:03:16.902273  2600 net.cpp:122] Setting up penlu14
I0925 11:03:16.902292  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.902298  2600 net.cpp:137] Memory required for data: 499303600
I0925 11:03:16.902307  2600 layer_factory.hpp:77] Creating layer Convolution15
I0925 11:03:16.902318  2600 net.cpp:84] Creating Layer Convolution15
I0925 11:03:16.902323  2600 net.cpp:406] Convolution15 <- Convolution14
I0925 11:03:16.902331  2600 net.cpp:380] Convolution15 -> Convolution15
I0925 11:03:16.903872  2600 net.cpp:122] Setting up Convolution15
I0925 11:03:16.903882  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.903884  2600 net.cpp:137] Memory required for data: 505857200
I0925 11:03:16.903889  2600 layer_factory.hpp:77] Creating layer BatchNorm15
I0925 11:03:16.903895  2600 net.cpp:84] Creating Layer BatchNorm15
I0925 11:03:16.903898  2600 net.cpp:406] BatchNorm15 <- Convolution15
I0925 11:03:16.903903  2600 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0925 11:03:16.904063  2600 net.cpp:122] Setting up BatchNorm15
I0925 11:03:16.904068  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.904070  2600 net.cpp:137] Memory required for data: 512410800
I0925 11:03:16.904075  2600 layer_factory.hpp:77] Creating layer Scale15
I0925 11:03:16.904080  2600 net.cpp:84] Creating Layer Scale15
I0925 11:03:16.904083  2600 net.cpp:406] Scale15 <- Convolution15
I0925 11:03:16.904086  2600 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0925 11:03:16.904119  2600 layer_factory.hpp:77] Creating layer Scale15
I0925 11:03:16.904208  2600 net.cpp:122] Setting up Scale15
I0925 11:03:16.904213  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.904215  2600 net.cpp:137] Memory required for data: 518964400
I0925 11:03:16.904219  2600 layer_factory.hpp:77] Creating layer Eltwise7
I0925 11:03:16.904224  2600 net.cpp:84] Creating Layer Eltwise7
I0925 11:03:16.904227  2600 net.cpp:406] Eltwise7 <- Eltwise6_penlu13_0_split_1
I0925 11:03:16.904230  2600 net.cpp:406] Eltwise7 <- Convolution15
I0925 11:03:16.904233  2600 net.cpp:380] Eltwise7 -> Eltwise7
I0925 11:03:16.904253  2600 net.cpp:122] Setting up Eltwise7
I0925 11:03:16.904256  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.904258  2600 net.cpp:137] Memory required for data: 525518000
I0925 11:03:16.904271  2600 layer_factory.hpp:77] Creating layer penlu15
I0925 11:03:16.904276  2600 net.cpp:84] Creating Layer penlu15
I0925 11:03:16.904279  2600 net.cpp:406] penlu15 <- Eltwise7
I0925 11:03:16.904283  2600 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I0925 11:03:16.904451  2600 net.cpp:122] Setting up penlu15
I0925 11:03:16.904455  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.904458  2600 net.cpp:137] Memory required for data: 532071600
I0925 11:03:16.904462  2600 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I0925 11:03:16.904466  2600 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I0925 11:03:16.904469  2600 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I0925 11:03:16.904471  2600 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I0925 11:03:16.904475  2600 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I0925 11:03:16.904521  2600 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I0925 11:03:16.904528  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.904531  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.904533  2600 net.cpp:137] Memory required for data: 545178800
I0925 11:03:16.904536  2600 layer_factory.hpp:77] Creating layer Convolution16
I0925 11:03:16.904542  2600 net.cpp:84] Creating Layer Convolution16
I0925 11:03:16.904544  2600 net.cpp:406] Convolution16 <- Eltwise7_penlu15_0_split_0
I0925 11:03:16.904549  2600 net.cpp:380] Convolution16 -> Convolution16
I0925 11:03:16.905215  2600 net.cpp:122] Setting up Convolution16
I0925 11:03:16.905223  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.905225  2600 net.cpp:137] Memory required for data: 551732400
I0925 11:03:16.905230  2600 layer_factory.hpp:77] Creating layer BatchNorm16
I0925 11:03:16.905242  2600 net.cpp:84] Creating Layer BatchNorm16
I0925 11:03:16.905246  2600 net.cpp:406] BatchNorm16 <- Convolution16
I0925 11:03:16.905251  2600 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0925 11:03:16.905417  2600 net.cpp:122] Setting up BatchNorm16
I0925 11:03:16.905422  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.905424  2600 net.cpp:137] Memory required for data: 558286000
I0925 11:03:16.905429  2600 layer_factory.hpp:77] Creating layer Scale16
I0925 11:03:16.905434  2600 net.cpp:84] Creating Layer Scale16
I0925 11:03:16.905437  2600 net.cpp:406] Scale16 <- Convolution16
I0925 11:03:16.905441  2600 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0925 11:03:16.905475  2600 layer_factory.hpp:77] Creating layer Scale16
I0925 11:03:16.905568  2600 net.cpp:122] Setting up Scale16
I0925 11:03:16.905573  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.905576  2600 net.cpp:137] Memory required for data: 564839600
I0925 11:03:16.905580  2600 layer_factory.hpp:77] Creating layer penlu16
I0925 11:03:16.905586  2600 net.cpp:84] Creating Layer penlu16
I0925 11:03:16.905588  2600 net.cpp:406] penlu16 <- Convolution16
I0925 11:03:16.905592  2600 net.cpp:367] penlu16 -> Convolution16 (in-place)
I0925 11:03:16.905730  2600 net.cpp:122] Setting up penlu16
I0925 11:03:16.905735  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.905737  2600 net.cpp:137] Memory required for data: 571393200
I0925 11:03:16.905742  2600 layer_factory.hpp:77] Creating layer Convolution17
I0925 11:03:16.905748  2600 net.cpp:84] Creating Layer Convolution17
I0925 11:03:16.905751  2600 net.cpp:406] Convolution17 <- Convolution16
I0925 11:03:16.905755  2600 net.cpp:380] Convolution17 -> Convolution17
I0925 11:03:16.906764  2600 net.cpp:122] Setting up Convolution17
I0925 11:03:16.906774  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.906776  2600 net.cpp:137] Memory required for data: 577946800
I0925 11:03:16.906781  2600 layer_factory.hpp:77] Creating layer BatchNorm17
I0925 11:03:16.906786  2600 net.cpp:84] Creating Layer BatchNorm17
I0925 11:03:16.906790  2600 net.cpp:406] BatchNorm17 <- Convolution17
I0925 11:03:16.906793  2600 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0925 11:03:16.906960  2600 net.cpp:122] Setting up BatchNorm17
I0925 11:03:16.906965  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.906967  2600 net.cpp:137] Memory required for data: 584500400
I0925 11:03:16.906971  2600 layer_factory.hpp:77] Creating layer Scale17
I0925 11:03:16.906976  2600 net.cpp:84] Creating Layer Scale17
I0925 11:03:16.906978  2600 net.cpp:406] Scale17 <- Convolution17
I0925 11:03:16.906981  2600 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0925 11:03:16.907016  2600 layer_factory.hpp:77] Creating layer Scale17
I0925 11:03:16.907109  2600 net.cpp:122] Setting up Scale17
I0925 11:03:16.907114  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.907117  2600 net.cpp:137] Memory required for data: 591054000
I0925 11:03:16.907121  2600 layer_factory.hpp:77] Creating layer Eltwise8
I0925 11:03:16.907125  2600 net.cpp:84] Creating Layer Eltwise8
I0925 11:03:16.907129  2600 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I0925 11:03:16.907131  2600 net.cpp:406] Eltwise8 <- Convolution17
I0925 11:03:16.907135  2600 net.cpp:380] Eltwise8 -> Eltwise8
I0925 11:03:16.907155  2600 net.cpp:122] Setting up Eltwise8
I0925 11:03:16.907160  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.907161  2600 net.cpp:137] Memory required for data: 597607600
I0925 11:03:16.907163  2600 layer_factory.hpp:77] Creating layer penlu17
I0925 11:03:16.907168  2600 net.cpp:84] Creating Layer penlu17
I0925 11:03:16.907171  2600 net.cpp:406] penlu17 <- Eltwise8
I0925 11:03:16.907174  2600 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I0925 11:03:16.907317  2600 net.cpp:122] Setting up penlu17
I0925 11:03:16.907322  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.907325  2600 net.cpp:137] Memory required for data: 604161200
I0925 11:03:16.907335  2600 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I0925 11:03:16.907341  2600 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I0925 11:03:16.907342  2600 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I0925 11:03:16.907346  2600 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I0925 11:03:16.907351  2600 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I0925 11:03:16.907380  2600 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I0925 11:03:16.907385  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.907388  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.907390  2600 net.cpp:137] Memory required for data: 617268400
I0925 11:03:16.907392  2600 layer_factory.hpp:77] Creating layer Convolution18
I0925 11:03:16.907400  2600 net.cpp:84] Creating Layer Convolution18
I0925 11:03:16.907403  2600 net.cpp:406] Convolution18 <- Eltwise8_penlu17_0_split_0
I0925 11:03:16.907407  2600 net.cpp:380] Convolution18 -> Convolution18
I0925 11:03:16.908450  2600 net.cpp:122] Setting up Convolution18
I0925 11:03:16.908459  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.908463  2600 net.cpp:137] Memory required for data: 623822000
I0925 11:03:16.908468  2600 layer_factory.hpp:77] Creating layer BatchNorm18
I0925 11:03:16.908473  2600 net.cpp:84] Creating Layer BatchNorm18
I0925 11:03:16.908474  2600 net.cpp:406] BatchNorm18 <- Convolution18
I0925 11:03:16.908479  2600 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0925 11:03:16.908653  2600 net.cpp:122] Setting up BatchNorm18
I0925 11:03:16.908659  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.908663  2600 net.cpp:137] Memory required for data: 630375600
I0925 11:03:16.908668  2600 layer_factory.hpp:77] Creating layer Scale18
I0925 11:03:16.908671  2600 net.cpp:84] Creating Layer Scale18
I0925 11:03:16.908674  2600 net.cpp:406] Scale18 <- Convolution18
I0925 11:03:16.908679  2600 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0925 11:03:16.908711  2600 layer_factory.hpp:77] Creating layer Scale18
I0925 11:03:16.908804  2600 net.cpp:122] Setting up Scale18
I0925 11:03:16.908809  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.908812  2600 net.cpp:137] Memory required for data: 636929200
I0925 11:03:16.908815  2600 layer_factory.hpp:77] Creating layer penlu18
I0925 11:03:16.908821  2600 net.cpp:84] Creating Layer penlu18
I0925 11:03:16.908823  2600 net.cpp:406] penlu18 <- Convolution18
I0925 11:03:16.908828  2600 net.cpp:367] penlu18 -> Convolution18 (in-place)
I0925 11:03:16.908972  2600 net.cpp:122] Setting up penlu18
I0925 11:03:16.908977  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.908978  2600 net.cpp:137] Memory required for data: 643482800
I0925 11:03:16.908983  2600 layer_factory.hpp:77] Creating layer Convolution19
I0925 11:03:16.908990  2600 net.cpp:84] Creating Layer Convolution19
I0925 11:03:16.908993  2600 net.cpp:406] Convolution19 <- Convolution18
I0925 11:03:16.908998  2600 net.cpp:380] Convolution19 -> Convolution19
I0925 11:03:16.910001  2600 net.cpp:122] Setting up Convolution19
I0925 11:03:16.910010  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.910012  2600 net.cpp:137] Memory required for data: 650036400
I0925 11:03:16.910017  2600 layer_factory.hpp:77] Creating layer BatchNorm19
I0925 11:03:16.910022  2600 net.cpp:84] Creating Layer BatchNorm19
I0925 11:03:16.910025  2600 net.cpp:406] BatchNorm19 <- Convolution19
I0925 11:03:16.910030  2600 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0925 11:03:16.910198  2600 net.cpp:122] Setting up BatchNorm19
I0925 11:03:16.910203  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.910205  2600 net.cpp:137] Memory required for data: 656590000
I0925 11:03:16.910210  2600 layer_factory.hpp:77] Creating layer Scale19
I0925 11:03:16.910217  2600 net.cpp:84] Creating Layer Scale19
I0925 11:03:16.910219  2600 net.cpp:406] Scale19 <- Convolution19
I0925 11:03:16.910229  2600 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0925 11:03:16.910265  2600 layer_factory.hpp:77] Creating layer Scale19
I0925 11:03:16.910360  2600 net.cpp:122] Setting up Scale19
I0925 11:03:16.910365  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.910368  2600 net.cpp:137] Memory required for data: 663143600
I0925 11:03:16.910372  2600 layer_factory.hpp:77] Creating layer Eltwise9
I0925 11:03:16.910377  2600 net.cpp:84] Creating Layer Eltwise9
I0925 11:03:16.910379  2600 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I0925 11:03:16.910382  2600 net.cpp:406] Eltwise9 <- Convolution19
I0925 11:03:16.910387  2600 net.cpp:380] Eltwise9 -> Eltwise9
I0925 11:03:16.910405  2600 net.cpp:122] Setting up Eltwise9
I0925 11:03:16.910410  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.910413  2600 net.cpp:137] Memory required for data: 669697200
I0925 11:03:16.910415  2600 layer_factory.hpp:77] Creating layer penlu19
I0925 11:03:16.910419  2600 net.cpp:84] Creating Layer penlu19
I0925 11:03:16.910423  2600 net.cpp:406] penlu19 <- Eltwise9
I0925 11:03:16.910426  2600 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I0925 11:03:16.910569  2600 net.cpp:122] Setting up penlu19
I0925 11:03:16.910574  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.910576  2600 net.cpp:137] Memory required for data: 676250800
I0925 11:03:16.910581  2600 layer_factory.hpp:77] Creating layer Eltwise9_penlu19_0_split
I0925 11:03:16.910584  2600 net.cpp:84] Creating Layer Eltwise9_penlu19_0_split
I0925 11:03:16.910588  2600 net.cpp:406] Eltwise9_penlu19_0_split <- Eltwise9
I0925 11:03:16.910590  2600 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_0
I0925 11:03:16.910594  2600 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_1
I0925 11:03:16.910624  2600 net.cpp:122] Setting up Eltwise9_penlu19_0_split
I0925 11:03:16.930493  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.930503  2600 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0925 11:03:16.930508  2600 net.cpp:137] Memory required for data: 689358000
I0925 11:03:16.930513  2600 layer_factory.hpp:77] Creating layer Convolution20
I0925 11:03:16.930523  2600 net.cpp:84] Creating Layer Convolution20
I0925 11:03:16.930528  2600 net.cpp:406] Convolution20 <- Eltwise9_penlu19_0_split_0
I0925 11:03:16.930539  2600 net.cpp:380] Convolution20 -> Convolution20
I0925 11:03:16.932785  2600 net.cpp:122] Setting up Convolution20
I0925 11:03:16.932798  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.932802  2600 net.cpp:137] Memory required for data: 692634800
I0925 11:03:16.932811  2600 layer_factory.hpp:77] Creating layer BatchNorm20
I0925 11:03:16.932818  2600 net.cpp:84] Creating Layer BatchNorm20
I0925 11:03:16.932824  2600 net.cpp:406] BatchNorm20 <- Convolution20
I0925 11:03:16.932831  2600 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0925 11:03:16.933017  2600 net.cpp:122] Setting up BatchNorm20
I0925 11:03:16.933025  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.933027  2600 net.cpp:137] Memory required for data: 695911600
I0925 11:03:16.933033  2600 layer_factory.hpp:77] Creating layer Scale20
I0925 11:03:16.933039  2600 net.cpp:84] Creating Layer Scale20
I0925 11:03:16.933043  2600 net.cpp:406] Scale20 <- Convolution20
I0925 11:03:16.933045  2600 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0925 11:03:16.933075  2600 layer_factory.hpp:77] Creating layer Scale20
I0925 11:03:16.933161  2600 net.cpp:122] Setting up Scale20
I0925 11:03:16.933167  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.933169  2600 net.cpp:137] Memory required for data: 699188400
I0925 11:03:16.933174  2600 layer_factory.hpp:77] Creating layer Convolution21
I0925 11:03:16.933184  2600 net.cpp:84] Creating Layer Convolution21
I0925 11:03:16.933188  2600 net.cpp:406] Convolution21 <- Eltwise9_penlu19_0_split_1
I0925 11:03:16.933197  2600 net.cpp:380] Convolution21 -> Convolution21
I0925 11:03:16.934253  2600 net.cpp:122] Setting up Convolution21
I0925 11:03:16.934270  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.934273  2600 net.cpp:137] Memory required for data: 702465200
I0925 11:03:16.934278  2600 layer_factory.hpp:77] Creating layer BatchNorm21
I0925 11:03:16.934284  2600 net.cpp:84] Creating Layer BatchNorm21
I0925 11:03:16.934288  2600 net.cpp:406] BatchNorm21 <- Convolution21
I0925 11:03:16.934290  2600 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0925 11:03:16.934422  2600 net.cpp:122] Setting up BatchNorm21
I0925 11:03:16.934427  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.934430  2600 net.cpp:137] Memory required for data: 705742000
I0925 11:03:16.934435  2600 layer_factory.hpp:77] Creating layer Scale21
I0925 11:03:16.934440  2600 net.cpp:84] Creating Layer Scale21
I0925 11:03:16.934442  2600 net.cpp:406] Scale21 <- Convolution21
I0925 11:03:16.934445  2600 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0925 11:03:16.934473  2600 layer_factory.hpp:77] Creating layer Scale21
I0925 11:03:16.934548  2600 net.cpp:122] Setting up Scale21
I0925 11:03:16.934553  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.934556  2600 net.cpp:137] Memory required for data: 709018800
I0925 11:03:16.934559  2600 layer_factory.hpp:77] Creating layer penlu20
I0925 11:03:16.934566  2600 net.cpp:84] Creating Layer penlu20
I0925 11:03:16.934567  2600 net.cpp:406] penlu20 <- Convolution21
I0925 11:03:16.934571  2600 net.cpp:367] penlu20 -> Convolution21 (in-place)
I0925 11:03:16.934684  2600 net.cpp:122] Setting up penlu20
I0925 11:03:16.934689  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.934691  2600 net.cpp:137] Memory required for data: 712295600
I0925 11:03:16.934696  2600 layer_factory.hpp:77] Creating layer Convolution22
I0925 11:03:16.934705  2600 net.cpp:84] Creating Layer Convolution22
I0925 11:03:16.934707  2600 net.cpp:406] Convolution22 <- Convolution21
I0925 11:03:16.934711  2600 net.cpp:380] Convolution22 -> Convolution22
I0925 11:03:16.935849  2600 net.cpp:122] Setting up Convolution22
I0925 11:03:16.935858  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.935861  2600 net.cpp:137] Memory required for data: 715572400
I0925 11:03:16.935865  2600 layer_factory.hpp:77] Creating layer BatchNorm22
I0925 11:03:16.935871  2600 net.cpp:84] Creating Layer BatchNorm22
I0925 11:03:16.935874  2600 net.cpp:406] BatchNorm22 <- Convolution22
I0925 11:03:16.935878  2600 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I0925 11:03:16.936008  2600 net.cpp:122] Setting up BatchNorm22
I0925 11:03:16.936013  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.936015  2600 net.cpp:137] Memory required for data: 718849200
I0925 11:03:16.936020  2600 layer_factory.hpp:77] Creating layer Scale22
I0925 11:03:16.936025  2600 net.cpp:84] Creating Layer Scale22
I0925 11:03:16.936028  2600 net.cpp:406] Scale22 <- Convolution22
I0925 11:03:16.936031  2600 net.cpp:367] Scale22 -> Convolution22 (in-place)
I0925 11:03:16.936058  2600 layer_factory.hpp:77] Creating layer Scale22
I0925 11:03:16.936136  2600 net.cpp:122] Setting up Scale22
I0925 11:03:16.936141  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.936143  2600 net.cpp:137] Memory required for data: 722126000
I0925 11:03:16.936147  2600 layer_factory.hpp:77] Creating layer Eltwise10
I0925 11:03:16.936151  2600 net.cpp:84] Creating Layer Eltwise10
I0925 11:03:16.936153  2600 net.cpp:406] Eltwise10 <- Convolution20
I0925 11:03:16.936156  2600 net.cpp:406] Eltwise10 <- Convolution22
I0925 11:03:16.936162  2600 net.cpp:380] Eltwise10 -> Eltwise10
I0925 11:03:16.936174  2600 net.cpp:122] Setting up Eltwise10
I0925 11:03:16.936178  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.936180  2600 net.cpp:137] Memory required for data: 725402800
I0925 11:03:16.936182  2600 layer_factory.hpp:77] Creating layer penlu21
I0925 11:03:16.936188  2600 net.cpp:84] Creating Layer penlu21
I0925 11:03:16.936192  2600 net.cpp:406] penlu21 <- Eltwise10
I0925 11:03:16.936195  2600 net.cpp:367] penlu21 -> Eltwise10 (in-place)
I0925 11:03:16.936314  2600 net.cpp:122] Setting up penlu21
I0925 11:03:16.936321  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.936322  2600 net.cpp:137] Memory required for data: 728679600
I0925 11:03:16.936327  2600 layer_factory.hpp:77] Creating layer Eltwise10_penlu21_0_split
I0925 11:03:16.936331  2600 net.cpp:84] Creating Layer Eltwise10_penlu21_0_split
I0925 11:03:16.936333  2600 net.cpp:406] Eltwise10_penlu21_0_split <- Eltwise10
I0925 11:03:16.936336  2600 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_0
I0925 11:03:16.936342  2600 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_1
I0925 11:03:16.936365  2600 net.cpp:122] Setting up Eltwise10_penlu21_0_split
I0925 11:03:16.936369  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.936372  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.936374  2600 net.cpp:137] Memory required for data: 735233200
I0925 11:03:16.936377  2600 layer_factory.hpp:77] Creating layer Convolution23
I0925 11:03:16.936383  2600 net.cpp:84] Creating Layer Convolution23
I0925 11:03:16.936384  2600 net.cpp:406] Convolution23 <- Eltwise10_penlu21_0_split_0
I0925 11:03:16.936389  2600 net.cpp:380] Convolution23 -> Convolution23
I0925 11:03:16.937532  2600 net.cpp:122] Setting up Convolution23
I0925 11:03:16.937542  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.937546  2600 net.cpp:137] Memory required for data: 738510000
I0925 11:03:16.937551  2600 layer_factory.hpp:77] Creating layer BatchNorm23
I0925 11:03:16.937556  2600 net.cpp:84] Creating Layer BatchNorm23
I0925 11:03:16.937558  2600 net.cpp:406] BatchNorm23 <- Convolution23
I0925 11:03:16.937562  2600 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I0925 11:03:16.937691  2600 net.cpp:122] Setting up BatchNorm23
I0925 11:03:16.937697  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.937700  2600 net.cpp:137] Memory required for data: 741786800
I0925 11:03:16.937705  2600 layer_factory.hpp:77] Creating layer Scale23
I0925 11:03:16.937708  2600 net.cpp:84] Creating Layer Scale23
I0925 11:03:16.937711  2600 net.cpp:406] Scale23 <- Convolution23
I0925 11:03:16.937714  2600 net.cpp:367] Scale23 -> Convolution23 (in-place)
I0925 11:03:16.937741  2600 layer_factory.hpp:77] Creating layer Scale23
I0925 11:03:16.937819  2600 net.cpp:122] Setting up Scale23
I0925 11:03:16.937824  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.937825  2600 net.cpp:137] Memory required for data: 745063600
I0925 11:03:16.937829  2600 layer_factory.hpp:77] Creating layer penlu22
I0925 11:03:16.937835  2600 net.cpp:84] Creating Layer penlu22
I0925 11:03:16.937839  2600 net.cpp:406] penlu22 <- Convolution23
I0925 11:03:16.937841  2600 net.cpp:367] penlu22 -> Convolution23 (in-place)
I0925 11:03:16.937949  2600 net.cpp:122] Setting up penlu22
I0925 11:03:16.937954  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.937957  2600 net.cpp:137] Memory required for data: 748340400
I0925 11:03:16.937961  2600 layer_factory.hpp:77] Creating layer Convolution24
I0925 11:03:16.937968  2600 net.cpp:84] Creating Layer Convolution24
I0925 11:03:16.937971  2600 net.cpp:406] Convolution24 <- Convolution23
I0925 11:03:16.937975  2600 net.cpp:380] Convolution24 -> Convolution24
I0925 11:03:16.939105  2600 net.cpp:122] Setting up Convolution24
I0925 11:03:16.939113  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.939116  2600 net.cpp:137] Memory required for data: 751617200
I0925 11:03:16.939121  2600 layer_factory.hpp:77] Creating layer BatchNorm24
I0925 11:03:16.939126  2600 net.cpp:84] Creating Layer BatchNorm24
I0925 11:03:16.939128  2600 net.cpp:406] BatchNorm24 <- Convolution24
I0925 11:03:16.939132  2600 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I0925 11:03:16.939265  2600 net.cpp:122] Setting up BatchNorm24
I0925 11:03:16.939270  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.939271  2600 net.cpp:137] Memory required for data: 754894000
I0925 11:03:16.939283  2600 layer_factory.hpp:77] Creating layer Scale24
I0925 11:03:16.939288  2600 net.cpp:84] Creating Layer Scale24
I0925 11:03:16.939291  2600 net.cpp:406] Scale24 <- Convolution24
I0925 11:03:16.939294  2600 net.cpp:367] Scale24 -> Convolution24 (in-place)
I0925 11:03:16.939322  2600 layer_factory.hpp:77] Creating layer Scale24
I0925 11:03:16.939399  2600 net.cpp:122] Setting up Scale24
I0925 11:03:16.939404  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.939405  2600 net.cpp:137] Memory required for data: 758170800
I0925 11:03:16.939409  2600 layer_factory.hpp:77] Creating layer Eltwise11
I0925 11:03:16.939414  2600 net.cpp:84] Creating Layer Eltwise11
I0925 11:03:16.939416  2600 net.cpp:406] Eltwise11 <- Eltwise10_penlu21_0_split_1
I0925 11:03:16.939419  2600 net.cpp:406] Eltwise11 <- Convolution24
I0925 11:03:16.939424  2600 net.cpp:380] Eltwise11 -> Eltwise11
I0925 11:03:16.939435  2600 net.cpp:122] Setting up Eltwise11
I0925 11:03:16.939440  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.939441  2600 net.cpp:137] Memory required for data: 761447600
I0925 11:03:16.939443  2600 layer_factory.hpp:77] Creating layer penlu23
I0925 11:03:16.939450  2600 net.cpp:84] Creating Layer penlu23
I0925 11:03:16.939451  2600 net.cpp:406] penlu23 <- Eltwise11
I0925 11:03:16.939455  2600 net.cpp:367] penlu23 -> Eltwise11 (in-place)
I0925 11:03:16.939568  2600 net.cpp:122] Setting up penlu23
I0925 11:03:16.939571  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.939574  2600 net.cpp:137] Memory required for data: 764724400
I0925 11:03:16.939579  2600 layer_factory.hpp:77] Creating layer Eltwise11_penlu23_0_split
I0925 11:03:16.939582  2600 net.cpp:84] Creating Layer Eltwise11_penlu23_0_split
I0925 11:03:16.939585  2600 net.cpp:406] Eltwise11_penlu23_0_split <- Eltwise11
I0925 11:03:16.939589  2600 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_0
I0925 11:03:16.939594  2600 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_1
I0925 11:03:16.939616  2600 net.cpp:122] Setting up Eltwise11_penlu23_0_split
I0925 11:03:16.939620  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.939623  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.939625  2600 net.cpp:137] Memory required for data: 771278000
I0925 11:03:16.939627  2600 layer_factory.hpp:77] Creating layer Convolution25
I0925 11:03:16.939633  2600 net.cpp:84] Creating Layer Convolution25
I0925 11:03:16.939636  2600 net.cpp:406] Convolution25 <- Eltwise11_penlu23_0_split_0
I0925 11:03:16.939640  2600 net.cpp:380] Convolution25 -> Convolution25
I0925 11:03:16.940775  2600 net.cpp:122] Setting up Convolution25
I0925 11:03:16.940785  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.940788  2600 net.cpp:137] Memory required for data: 774554800
I0925 11:03:16.940793  2600 layer_factory.hpp:77] Creating layer BatchNorm25
I0925 11:03:16.940799  2600 net.cpp:84] Creating Layer BatchNorm25
I0925 11:03:16.940803  2600 net.cpp:406] BatchNorm25 <- Convolution25
I0925 11:03:16.940806  2600 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I0925 11:03:16.940942  2600 net.cpp:122] Setting up BatchNorm25
I0925 11:03:16.940946  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.940949  2600 net.cpp:137] Memory required for data: 777831600
I0925 11:03:16.940954  2600 layer_factory.hpp:77] Creating layer Scale25
I0925 11:03:16.940959  2600 net.cpp:84] Creating Layer Scale25
I0925 11:03:16.940961  2600 net.cpp:406] Scale25 <- Convolution25
I0925 11:03:16.940964  2600 net.cpp:367] Scale25 -> Convolution25 (in-place)
I0925 11:03:16.940994  2600 layer_factory.hpp:77] Creating layer Scale25
I0925 11:03:16.941071  2600 net.cpp:122] Setting up Scale25
I0925 11:03:16.941076  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.941077  2600 net.cpp:137] Memory required for data: 781108400
I0925 11:03:16.941082  2600 layer_factory.hpp:77] Creating layer penlu24
I0925 11:03:16.941087  2600 net.cpp:84] Creating Layer penlu24
I0925 11:03:16.941097  2600 net.cpp:406] penlu24 <- Convolution25
I0925 11:03:16.941102  2600 net.cpp:367] penlu24 -> Convolution25 (in-place)
I0925 11:03:16.941212  2600 net.cpp:122] Setting up penlu24
I0925 11:03:16.941217  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.941220  2600 net.cpp:137] Memory required for data: 784385200
I0925 11:03:16.941224  2600 layer_factory.hpp:77] Creating layer Convolution26
I0925 11:03:16.941231  2600 net.cpp:84] Creating Layer Convolution26
I0925 11:03:16.941234  2600 net.cpp:406] Convolution26 <- Convolution25
I0925 11:03:16.941239  2600 net.cpp:380] Convolution26 -> Convolution26
I0925 11:03:16.942029  2600 net.cpp:122] Setting up Convolution26
I0925 11:03:16.942037  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.942039  2600 net.cpp:137] Memory required for data: 787662000
I0925 11:03:16.942044  2600 layer_factory.hpp:77] Creating layer BatchNorm26
I0925 11:03:16.942049  2600 net.cpp:84] Creating Layer BatchNorm26
I0925 11:03:16.942052  2600 net.cpp:406] BatchNorm26 <- Convolution26
I0925 11:03:16.942055  2600 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I0925 11:03:16.942188  2600 net.cpp:122] Setting up BatchNorm26
I0925 11:03:16.942191  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.942194  2600 net.cpp:137] Memory required for data: 790938800
I0925 11:03:16.942199  2600 layer_factory.hpp:77] Creating layer Scale26
I0925 11:03:16.942203  2600 net.cpp:84] Creating Layer Scale26
I0925 11:03:16.942206  2600 net.cpp:406] Scale26 <- Convolution26
I0925 11:03:16.942209  2600 net.cpp:367] Scale26 -> Convolution26 (in-place)
I0925 11:03:16.942236  2600 layer_factory.hpp:77] Creating layer Scale26
I0925 11:03:16.961527  2600 net.cpp:122] Setting up Scale26
I0925 11:03:16.961539  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.961544  2600 net.cpp:137] Memory required for data: 794215600
I0925 11:03:16.961552  2600 layer_factory.hpp:77] Creating layer Eltwise12
I0925 11:03:16.961560  2600 net.cpp:84] Creating Layer Eltwise12
I0925 11:03:16.961565  2600 net.cpp:406] Eltwise12 <- Eltwise11_penlu23_0_split_1
I0925 11:03:16.961571  2600 net.cpp:406] Eltwise12 <- Convolution26
I0925 11:03:16.961577  2600 net.cpp:380] Eltwise12 -> Eltwise12
I0925 11:03:16.961598  2600 net.cpp:122] Setting up Eltwise12
I0925 11:03:16.961606  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.961611  2600 net.cpp:137] Memory required for data: 797492400
I0925 11:03:16.961614  2600 layer_factory.hpp:77] Creating layer penlu25
I0925 11:03:16.961633  2600 net.cpp:84] Creating Layer penlu25
I0925 11:03:16.961638  2600 net.cpp:406] penlu25 <- Eltwise12
I0925 11:03:16.961647  2600 net.cpp:367] penlu25 -> Eltwise12 (in-place)
I0925 11:03:16.961819  2600 net.cpp:122] Setting up penlu25
I0925 11:03:16.961829  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.961833  2600 net.cpp:137] Memory required for data: 800769200
I0925 11:03:16.961868  2600 layer_factory.hpp:77] Creating layer Eltwise12_penlu25_0_split
I0925 11:03:16.961875  2600 net.cpp:84] Creating Layer Eltwise12_penlu25_0_split
I0925 11:03:16.961879  2600 net.cpp:406] Eltwise12_penlu25_0_split <- Eltwise12
I0925 11:03:16.961885  2600 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_0
I0925 11:03:16.961894  2600 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_1
I0925 11:03:16.961932  2600 net.cpp:122] Setting up Eltwise12_penlu25_0_split
I0925 11:03:16.961940  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.961946  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.961948  2600 net.cpp:137] Memory required for data: 807322800
I0925 11:03:16.961953  2600 layer_factory.hpp:77] Creating layer Convolution27
I0925 11:03:16.961963  2600 net.cpp:84] Creating Layer Convolution27
I0925 11:03:16.961969  2600 net.cpp:406] Convolution27 <- Eltwise12_penlu25_0_split_0
I0925 11:03:16.961977  2600 net.cpp:380] Convolution27 -> Convolution27
I0925 11:03:16.964027  2600 net.cpp:122] Setting up Convolution27
I0925 11:03:16.964048  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.964052  2600 net.cpp:137] Memory required for data: 810599600
I0925 11:03:16.964057  2600 layer_factory.hpp:77] Creating layer BatchNorm27
I0925 11:03:16.964063  2600 net.cpp:84] Creating Layer BatchNorm27
I0925 11:03:16.964066  2600 net.cpp:406] BatchNorm27 <- Convolution27
I0925 11:03:16.964082  2600 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I0925 11:03:16.964264  2600 net.cpp:122] Setting up BatchNorm27
I0925 11:03:16.964274  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.964278  2600 net.cpp:137] Memory required for data: 813876400
I0925 11:03:16.964287  2600 layer_factory.hpp:77] Creating layer Scale27
I0925 11:03:16.964294  2600 net.cpp:84] Creating Layer Scale27
I0925 11:03:16.964298  2600 net.cpp:406] Scale27 <- Convolution27
I0925 11:03:16.964304  2600 net.cpp:367] Scale27 -> Convolution27 (in-place)
I0925 11:03:16.964337  2600 layer_factory.hpp:77] Creating layer Scale27
I0925 11:03:16.964414  2600 net.cpp:122] Setting up Scale27
I0925 11:03:16.964419  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.964421  2600 net.cpp:137] Memory required for data: 817153200
I0925 11:03:16.964426  2600 layer_factory.hpp:77] Creating layer penlu26
I0925 11:03:16.964432  2600 net.cpp:84] Creating Layer penlu26
I0925 11:03:16.964433  2600 net.cpp:406] penlu26 <- Convolution27
I0925 11:03:16.964437  2600 net.cpp:367] penlu26 -> Convolution27 (in-place)
I0925 11:03:16.964557  2600 net.cpp:122] Setting up penlu26
I0925 11:03:16.964565  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.964566  2600 net.cpp:137] Memory required for data: 820430000
I0925 11:03:16.964571  2600 layer_factory.hpp:77] Creating layer Convolution28
I0925 11:03:16.964578  2600 net.cpp:84] Creating Layer Convolution28
I0925 11:03:16.964581  2600 net.cpp:406] Convolution28 <- Convolution27
I0925 11:03:16.964586  2600 net.cpp:380] Convolution28 -> Convolution28
I0925 11:03:16.966126  2600 net.cpp:122] Setting up Convolution28
I0925 11:03:16.966135  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.966138  2600 net.cpp:137] Memory required for data: 823706800
I0925 11:03:16.966143  2600 layer_factory.hpp:77] Creating layer BatchNorm28
I0925 11:03:16.966148  2600 net.cpp:84] Creating Layer BatchNorm28
I0925 11:03:16.966151  2600 net.cpp:406] BatchNorm28 <- Convolution28
I0925 11:03:16.966156  2600 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I0925 11:03:16.966289  2600 net.cpp:122] Setting up BatchNorm28
I0925 11:03:16.966295  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.966296  2600 net.cpp:137] Memory required for data: 826983600
I0925 11:03:16.966301  2600 layer_factory.hpp:77] Creating layer Scale28
I0925 11:03:16.966306  2600 net.cpp:84] Creating Layer Scale28
I0925 11:03:16.966308  2600 net.cpp:406] Scale28 <- Convolution28
I0925 11:03:16.966311  2600 net.cpp:367] Scale28 -> Convolution28 (in-place)
I0925 11:03:16.966339  2600 layer_factory.hpp:77] Creating layer Scale28
I0925 11:03:16.966415  2600 net.cpp:122] Setting up Scale28
I0925 11:03:16.966420  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.966423  2600 net.cpp:137] Memory required for data: 830260400
I0925 11:03:16.966426  2600 layer_factory.hpp:77] Creating layer Eltwise13
I0925 11:03:16.966430  2600 net.cpp:84] Creating Layer Eltwise13
I0925 11:03:16.966434  2600 net.cpp:406] Eltwise13 <- Eltwise12_penlu25_0_split_1
I0925 11:03:16.966436  2600 net.cpp:406] Eltwise13 <- Convolution28
I0925 11:03:16.966440  2600 net.cpp:380] Eltwise13 -> Eltwise13
I0925 11:03:16.966452  2600 net.cpp:122] Setting up Eltwise13
I0925 11:03:16.966456  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.966459  2600 net.cpp:137] Memory required for data: 833537200
I0925 11:03:16.966460  2600 layer_factory.hpp:77] Creating layer penlu27
I0925 11:03:16.966465  2600 net.cpp:84] Creating Layer penlu27
I0925 11:03:16.966469  2600 net.cpp:406] penlu27 <- Eltwise13
I0925 11:03:16.966480  2600 net.cpp:367] penlu27 -> Eltwise13 (in-place)
I0925 11:03:16.966593  2600 net.cpp:122] Setting up penlu27
I0925 11:03:16.966598  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.966600  2600 net.cpp:137] Memory required for data: 836814000
I0925 11:03:16.966604  2600 layer_factory.hpp:77] Creating layer Eltwise13_penlu27_0_split
I0925 11:03:16.966609  2600 net.cpp:84] Creating Layer Eltwise13_penlu27_0_split
I0925 11:03:16.966611  2600 net.cpp:406] Eltwise13_penlu27_0_split <- Eltwise13
I0925 11:03:16.966614  2600 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_0
I0925 11:03:16.966619  2600 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_1
I0925 11:03:16.966641  2600 net.cpp:122] Setting up Eltwise13_penlu27_0_split
I0925 11:03:16.966645  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.966648  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.966650  2600 net.cpp:137] Memory required for data: 843367600
I0925 11:03:16.966652  2600 layer_factory.hpp:77] Creating layer Convolution29
I0925 11:03:16.966660  2600 net.cpp:84] Creating Layer Convolution29
I0925 11:03:16.966661  2600 net.cpp:406] Convolution29 <- Eltwise13_penlu27_0_split_0
I0925 11:03:16.966665  2600 net.cpp:380] Convolution29 -> Convolution29
I0925 11:03:16.967773  2600 net.cpp:122] Setting up Convolution29
I0925 11:03:16.967782  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.967785  2600 net.cpp:137] Memory required for data: 846644400
I0925 11:03:16.967789  2600 layer_factory.hpp:77] Creating layer BatchNorm29
I0925 11:03:16.967794  2600 net.cpp:84] Creating Layer BatchNorm29
I0925 11:03:16.967797  2600 net.cpp:406] BatchNorm29 <- Convolution29
I0925 11:03:16.967803  2600 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I0925 11:03:16.967932  2600 net.cpp:122] Setting up BatchNorm29
I0925 11:03:16.967937  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.967939  2600 net.cpp:137] Memory required for data: 849921200
I0925 11:03:16.967944  2600 layer_factory.hpp:77] Creating layer Scale29
I0925 11:03:16.967948  2600 net.cpp:84] Creating Layer Scale29
I0925 11:03:16.967950  2600 net.cpp:406] Scale29 <- Convolution29
I0925 11:03:16.967953  2600 net.cpp:367] Scale29 -> Convolution29 (in-place)
I0925 11:03:16.967981  2600 layer_factory.hpp:77] Creating layer Scale29
I0925 11:03:16.968056  2600 net.cpp:122] Setting up Scale29
I0925 11:03:16.968061  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.968063  2600 net.cpp:137] Memory required for data: 853198000
I0925 11:03:16.968067  2600 layer_factory.hpp:77] Creating layer penlu28
I0925 11:03:16.968073  2600 net.cpp:84] Creating Layer penlu28
I0925 11:03:16.968075  2600 net.cpp:406] penlu28 <- Convolution29
I0925 11:03:16.968080  2600 net.cpp:367] penlu28 -> Convolution29 (in-place)
I0925 11:03:16.968190  2600 net.cpp:122] Setting up penlu28
I0925 11:03:16.968195  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.968197  2600 net.cpp:137] Memory required for data: 856474800
I0925 11:03:16.968201  2600 layer_factory.hpp:77] Creating layer Convolution30
I0925 11:03:16.968209  2600 net.cpp:84] Creating Layer Convolution30
I0925 11:03:16.968211  2600 net.cpp:406] Convolution30 <- Convolution29
I0925 11:03:16.968215  2600 net.cpp:380] Convolution30 -> Convolution30
I0925 11:03:16.969338  2600 net.cpp:122] Setting up Convolution30
I0925 11:03:16.969347  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.969350  2600 net.cpp:137] Memory required for data: 859751600
I0925 11:03:16.969355  2600 layer_factory.hpp:77] Creating layer BatchNorm30
I0925 11:03:16.969360  2600 net.cpp:84] Creating Layer BatchNorm30
I0925 11:03:16.969363  2600 net.cpp:406] BatchNorm30 <- Convolution30
I0925 11:03:16.969367  2600 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I0925 11:03:16.969496  2600 net.cpp:122] Setting up BatchNorm30
I0925 11:03:16.969501  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.969503  2600 net.cpp:137] Memory required for data: 863028400
I0925 11:03:16.969516  2600 layer_factory.hpp:77] Creating layer Scale30
I0925 11:03:16.969521  2600 net.cpp:84] Creating Layer Scale30
I0925 11:03:16.969523  2600 net.cpp:406] Scale30 <- Convolution30
I0925 11:03:16.969527  2600 net.cpp:367] Scale30 -> Convolution30 (in-place)
I0925 11:03:16.969554  2600 layer_factory.hpp:77] Creating layer Scale30
I0925 11:03:16.969631  2600 net.cpp:122] Setting up Scale30
I0925 11:03:16.969636  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.969638  2600 net.cpp:137] Memory required for data: 866305200
I0925 11:03:16.969642  2600 layer_factory.hpp:77] Creating layer Eltwise14
I0925 11:03:16.969646  2600 net.cpp:84] Creating Layer Eltwise14
I0925 11:03:16.969648  2600 net.cpp:406] Eltwise14 <- Eltwise13_penlu27_0_split_1
I0925 11:03:16.969651  2600 net.cpp:406] Eltwise14 <- Convolution30
I0925 11:03:16.969656  2600 net.cpp:380] Eltwise14 -> Eltwise14
I0925 11:03:16.969668  2600 net.cpp:122] Setting up Eltwise14
I0925 11:03:16.969672  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.969674  2600 net.cpp:137] Memory required for data: 869582000
I0925 11:03:16.969676  2600 layer_factory.hpp:77] Creating layer penlu29
I0925 11:03:16.969682  2600 net.cpp:84] Creating Layer penlu29
I0925 11:03:16.969684  2600 net.cpp:406] penlu29 <- Eltwise14
I0925 11:03:16.969688  2600 net.cpp:367] penlu29 -> Eltwise14 (in-place)
I0925 11:03:16.969800  2600 net.cpp:122] Setting up penlu29
I0925 11:03:16.969805  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.969807  2600 net.cpp:137] Memory required for data: 872858800
I0925 11:03:16.969812  2600 layer_factory.hpp:77] Creating layer Eltwise14_penlu29_0_split
I0925 11:03:16.969816  2600 net.cpp:84] Creating Layer Eltwise14_penlu29_0_split
I0925 11:03:16.969818  2600 net.cpp:406] Eltwise14_penlu29_0_split <- Eltwise14
I0925 11:03:16.969822  2600 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_0
I0925 11:03:16.969826  2600 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_1
I0925 11:03:16.969851  2600 net.cpp:122] Setting up Eltwise14_penlu29_0_split
I0925 11:03:16.969854  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.969856  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.969859  2600 net.cpp:137] Memory required for data: 879412400
I0925 11:03:16.969861  2600 layer_factory.hpp:77] Creating layer Convolution31
I0925 11:03:16.969866  2600 net.cpp:84] Creating Layer Convolution31
I0925 11:03:16.969869  2600 net.cpp:406] Convolution31 <- Eltwise14_penlu29_0_split_0
I0925 11:03:16.969874  2600 net.cpp:380] Convolution31 -> Convolution31
I0925 11:03:16.970995  2600 net.cpp:122] Setting up Convolution31
I0925 11:03:16.971004  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.971007  2600 net.cpp:137] Memory required for data: 882689200
I0925 11:03:16.971011  2600 layer_factory.hpp:77] Creating layer BatchNorm31
I0925 11:03:16.971016  2600 net.cpp:84] Creating Layer BatchNorm31
I0925 11:03:16.971019  2600 net.cpp:406] BatchNorm31 <- Convolution31
I0925 11:03:16.971024  2600 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I0925 11:03:16.971150  2600 net.cpp:122] Setting up BatchNorm31
I0925 11:03:16.971155  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.971158  2600 net.cpp:137] Memory required for data: 885966000
I0925 11:03:16.971163  2600 layer_factory.hpp:77] Creating layer Scale31
I0925 11:03:16.971166  2600 net.cpp:84] Creating Layer Scale31
I0925 11:03:16.971168  2600 net.cpp:406] Scale31 <- Convolution31
I0925 11:03:16.971171  2600 net.cpp:367] Scale31 -> Convolution31 (in-place)
I0925 11:03:16.971199  2600 layer_factory.hpp:77] Creating layer Scale31
I0925 11:03:16.971272  2600 net.cpp:122] Setting up Scale31
I0925 11:03:16.971276  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.971279  2600 net.cpp:137] Memory required for data: 889242800
I0925 11:03:16.971282  2600 layer_factory.hpp:77] Creating layer penlu30
I0925 11:03:16.971288  2600 net.cpp:84] Creating Layer penlu30
I0925 11:03:16.971297  2600 net.cpp:406] penlu30 <- Convolution31
I0925 11:03:16.971300  2600 net.cpp:367] penlu30 -> Convolution31 (in-place)
I0925 11:03:16.971405  2600 net.cpp:122] Setting up penlu30
I0925 11:03:16.971410  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.971412  2600 net.cpp:137] Memory required for data: 892519600
I0925 11:03:16.971416  2600 layer_factory.hpp:77] Creating layer Convolution32
I0925 11:03:16.971423  2600 net.cpp:84] Creating Layer Convolution32
I0925 11:03:16.971426  2600 net.cpp:406] Convolution32 <- Convolution31
I0925 11:03:16.971429  2600 net.cpp:380] Convolution32 -> Convolution32
I0925 11:03:16.972556  2600 net.cpp:122] Setting up Convolution32
I0925 11:03:16.972566  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.972568  2600 net.cpp:137] Memory required for data: 895796400
I0925 11:03:16.972573  2600 layer_factory.hpp:77] Creating layer BatchNorm32
I0925 11:03:16.972579  2600 net.cpp:84] Creating Layer BatchNorm32
I0925 11:03:16.972580  2600 net.cpp:406] BatchNorm32 <- Convolution32
I0925 11:03:16.972585  2600 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I0925 11:03:16.972714  2600 net.cpp:122] Setting up BatchNorm32
I0925 11:03:16.972719  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.972721  2600 net.cpp:137] Memory required for data: 899073200
I0925 11:03:16.972725  2600 layer_factory.hpp:77] Creating layer Scale32
I0925 11:03:16.972729  2600 net.cpp:84] Creating Layer Scale32
I0925 11:03:16.972733  2600 net.cpp:406] Scale32 <- Convolution32
I0925 11:03:16.992471  2600 net.cpp:367] Scale32 -> Convolution32 (in-place)
I0925 11:03:16.992537  2600 layer_factory.hpp:77] Creating layer Scale32
I0925 11:03:16.992658  2600 net.cpp:122] Setting up Scale32
I0925 11:03:16.992668  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.992672  2600 net.cpp:137] Memory required for data: 902350000
I0925 11:03:16.992681  2600 layer_factory.hpp:77] Creating layer Eltwise15
I0925 11:03:16.992688  2600 net.cpp:84] Creating Layer Eltwise15
I0925 11:03:16.992693  2600 net.cpp:406] Eltwise15 <- Eltwise14_penlu29_0_split_1
I0925 11:03:16.992699  2600 net.cpp:406] Eltwise15 <- Convolution32
I0925 11:03:16.992707  2600 net.cpp:380] Eltwise15 -> Eltwise15
I0925 11:03:16.992725  2600 net.cpp:122] Setting up Eltwise15
I0925 11:03:16.992733  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.992736  2600 net.cpp:137] Memory required for data: 905626800
I0925 11:03:16.992740  2600 layer_factory.hpp:77] Creating layer penlu31
I0925 11:03:16.992749  2600 net.cpp:84] Creating Layer penlu31
I0925 11:03:16.992754  2600 net.cpp:406] penlu31 <- Eltwise15
I0925 11:03:16.992761  2600 net.cpp:367] penlu31 -> Eltwise15 (in-place)
I0925 11:03:16.992933  2600 net.cpp:122] Setting up penlu31
I0925 11:03:16.992941  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.992945  2600 net.cpp:137] Memory required for data: 908903600
I0925 11:03:16.992954  2600 layer_factory.hpp:77] Creating layer Eltwise15_penlu31_0_split
I0925 11:03:16.992960  2600 net.cpp:84] Creating Layer Eltwise15_penlu31_0_split
I0925 11:03:16.992964  2600 net.cpp:406] Eltwise15_penlu31_0_split <- Eltwise15
I0925 11:03:16.992970  2600 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_0
I0925 11:03:16.992980  2600 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_1
I0925 11:03:16.993018  2600 net.cpp:122] Setting up Eltwise15_penlu31_0_split
I0925 11:03:16.993027  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.993033  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.993037  2600 net.cpp:137] Memory required for data: 915457200
I0925 11:03:16.993041  2600 layer_factory.hpp:77] Creating layer Convolution33
I0925 11:03:16.993053  2600 net.cpp:84] Creating Layer Convolution33
I0925 11:03:16.993058  2600 net.cpp:406] Convolution33 <- Eltwise15_penlu31_0_split_0
I0925 11:03:16.993065  2600 net.cpp:380] Convolution33 -> Convolution33
I0925 11:03:16.994706  2600 net.cpp:122] Setting up Convolution33
I0925 11:03:16.994719  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.994722  2600 net.cpp:137] Memory required for data: 918734000
I0925 11:03:16.994729  2600 layer_factory.hpp:77] Creating layer BatchNorm33
I0925 11:03:16.994738  2600 net.cpp:84] Creating Layer BatchNorm33
I0925 11:03:16.994743  2600 net.cpp:406] BatchNorm33 <- Convolution33
I0925 11:03:16.994750  2600 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I0925 11:03:16.994949  2600 net.cpp:122] Setting up BatchNorm33
I0925 11:03:16.994961  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.994964  2600 net.cpp:137] Memory required for data: 922010800
I0925 11:03:16.994973  2600 layer_factory.hpp:77] Creating layer Scale33
I0925 11:03:16.994983  2600 net.cpp:84] Creating Layer Scale33
I0925 11:03:16.994987  2600 net.cpp:406] Scale33 <- Convolution33
I0925 11:03:16.994994  2600 net.cpp:367] Scale33 -> Convolution33 (in-place)
I0925 11:03:16.995035  2600 layer_factory.hpp:77] Creating layer Scale33
I0925 11:03:16.995208  2600 net.cpp:122] Setting up Scale33
I0925 11:03:16.995218  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.995223  2600 net.cpp:137] Memory required for data: 925287600
I0925 11:03:16.995230  2600 layer_factory.hpp:77] Creating layer penlu32
I0925 11:03:16.995241  2600 net.cpp:84] Creating Layer penlu32
I0925 11:03:16.995246  2600 net.cpp:406] penlu32 <- Convolution33
I0925 11:03:16.995250  2600 net.cpp:367] penlu32 -> Convolution33 (in-place)
I0925 11:03:16.995364  2600 net.cpp:122] Setting up penlu32
I0925 11:03:16.995369  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.995371  2600 net.cpp:137] Memory required for data: 928564400
I0925 11:03:16.995376  2600 layer_factory.hpp:77] Creating layer Convolution34
I0925 11:03:16.995383  2600 net.cpp:84] Creating Layer Convolution34
I0925 11:03:16.995385  2600 net.cpp:406] Convolution34 <- Convolution33
I0925 11:03:16.995390  2600 net.cpp:380] Convolution34 -> Convolution34
I0925 11:03:16.997457  2600 net.cpp:122] Setting up Convolution34
I0925 11:03:16.997467  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.997470  2600 net.cpp:137] Memory required for data: 931841200
I0925 11:03:16.997475  2600 layer_factory.hpp:77] Creating layer BatchNorm34
I0925 11:03:16.997481  2600 net.cpp:84] Creating Layer BatchNorm34
I0925 11:03:16.997485  2600 net.cpp:406] BatchNorm34 <- Convolution34
I0925 11:03:16.997489  2600 net.cpp:367] BatchNorm34 -> Convolution34 (in-place)
I0925 11:03:16.997684  2600 net.cpp:122] Setting up BatchNorm34
I0925 11:03:16.997689  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.997690  2600 net.cpp:137] Memory required for data: 935118000
I0925 11:03:16.997695  2600 layer_factory.hpp:77] Creating layer Scale34
I0925 11:03:16.997699  2600 net.cpp:84] Creating Layer Scale34
I0925 11:03:16.997702  2600 net.cpp:406] Scale34 <- Convolution34
I0925 11:03:16.997706  2600 net.cpp:367] Scale34 -> Convolution34 (in-place)
I0925 11:03:16.997742  2600 layer_factory.hpp:77] Creating layer Scale34
I0925 11:03:16.997826  2600 net.cpp:122] Setting up Scale34
I0925 11:03:16.997831  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.997834  2600 net.cpp:137] Memory required for data: 938394800
I0925 11:03:16.997838  2600 layer_factory.hpp:77] Creating layer Eltwise16
I0925 11:03:16.997843  2600 net.cpp:84] Creating Layer Eltwise16
I0925 11:03:16.997846  2600 net.cpp:406] Eltwise16 <- Eltwise15_penlu31_0_split_1
I0925 11:03:16.997849  2600 net.cpp:406] Eltwise16 <- Convolution34
I0925 11:03:16.997853  2600 net.cpp:380] Eltwise16 -> Eltwise16
I0925 11:03:16.997867  2600 net.cpp:122] Setting up Eltwise16
I0925 11:03:16.997872  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.997874  2600 net.cpp:137] Memory required for data: 941671600
I0925 11:03:16.997876  2600 layer_factory.hpp:77] Creating layer penlu33
I0925 11:03:16.997882  2600 net.cpp:84] Creating Layer penlu33
I0925 11:03:16.997885  2600 net.cpp:406] penlu33 <- Eltwise16
I0925 11:03:16.997898  2600 net.cpp:367] penlu33 -> Eltwise16 (in-place)
I0925 11:03:16.998025  2600 net.cpp:122] Setting up penlu33
I0925 11:03:16.998030  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.998034  2600 net.cpp:137] Memory required for data: 944948400
I0925 11:03:16.998039  2600 layer_factory.hpp:77] Creating layer Eltwise16_penlu33_0_split
I0925 11:03:16.998044  2600 net.cpp:84] Creating Layer Eltwise16_penlu33_0_split
I0925 11:03:16.998045  2600 net.cpp:406] Eltwise16_penlu33_0_split <- Eltwise16
I0925 11:03:16.998049  2600 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_0
I0925 11:03:16.998054  2600 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_1
I0925 11:03:16.998080  2600 net.cpp:122] Setting up Eltwise16_penlu33_0_split
I0925 11:03:16.998085  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.998087  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.998090  2600 net.cpp:137] Memory required for data: 951502000
I0925 11:03:16.998092  2600 layer_factory.hpp:77] Creating layer Convolution35
I0925 11:03:16.998098  2600 net.cpp:84] Creating Layer Convolution35
I0925 11:03:16.998101  2600 net.cpp:406] Convolution35 <- Eltwise16_penlu33_0_split_0
I0925 11:03:16.998106  2600 net.cpp:380] Convolution35 -> Convolution35
I0925 11:03:16.999282  2600 net.cpp:122] Setting up Convolution35
I0925 11:03:16.999291  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.999294  2600 net.cpp:137] Memory required for data: 954778800
I0925 11:03:16.999299  2600 layer_factory.hpp:77] Creating layer BatchNorm35
I0925 11:03:16.999311  2600 net.cpp:84] Creating Layer BatchNorm35
I0925 11:03:16.999315  2600 net.cpp:406] BatchNorm35 <- Convolution35
I0925 11:03:16.999318  2600 net.cpp:367] BatchNorm35 -> Convolution35 (in-place)
I0925 11:03:16.999456  2600 net.cpp:122] Setting up BatchNorm35
I0925 11:03:16.999461  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.999464  2600 net.cpp:137] Memory required for data: 958055600
I0925 11:03:16.999469  2600 layer_factory.hpp:77] Creating layer Scale35
I0925 11:03:16.999474  2600 net.cpp:84] Creating Layer Scale35
I0925 11:03:16.999476  2600 net.cpp:406] Scale35 <- Convolution35
I0925 11:03:16.999480  2600 net.cpp:367] Scale35 -> Convolution35 (in-place)
I0925 11:03:16.999507  2600 layer_factory.hpp:77] Creating layer Scale35
I0925 11:03:16.999588  2600 net.cpp:122] Setting up Scale35
I0925 11:03:16.999593  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.999595  2600 net.cpp:137] Memory required for data: 961332400
I0925 11:03:16.999599  2600 layer_factory.hpp:77] Creating layer penlu34
I0925 11:03:16.999605  2600 net.cpp:84] Creating Layer penlu34
I0925 11:03:16.999608  2600 net.cpp:406] penlu34 <- Convolution35
I0925 11:03:16.999611  2600 net.cpp:367] penlu34 -> Convolution35 (in-place)
I0925 11:03:16.999722  2600 net.cpp:122] Setting up penlu34
I0925 11:03:16.999727  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:16.999728  2600 net.cpp:137] Memory required for data: 964609200
I0925 11:03:16.999733  2600 layer_factory.hpp:77] Creating layer Convolution36
I0925 11:03:16.999740  2600 net.cpp:84] Creating Layer Convolution36
I0925 11:03:16.999743  2600 net.cpp:406] Convolution36 <- Convolution35
I0925 11:03:16.999747  2600 net.cpp:380] Convolution36 -> Convolution36
I0925 11:03:17.000555  2600 net.cpp:122] Setting up Convolution36
I0925 11:03:17.000563  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.000566  2600 net.cpp:137] Memory required for data: 967886000
I0925 11:03:17.000571  2600 layer_factory.hpp:77] Creating layer BatchNorm36
I0925 11:03:17.000576  2600 net.cpp:84] Creating Layer BatchNorm36
I0925 11:03:17.000578  2600 net.cpp:406] BatchNorm36 <- Convolution36
I0925 11:03:17.000581  2600 net.cpp:367] BatchNorm36 -> Convolution36 (in-place)
I0925 11:03:17.000715  2600 net.cpp:122] Setting up BatchNorm36
I0925 11:03:17.000720  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.000721  2600 net.cpp:137] Memory required for data: 971162800
I0925 11:03:17.000733  2600 layer_factory.hpp:77] Creating layer Scale36
I0925 11:03:17.000739  2600 net.cpp:84] Creating Layer Scale36
I0925 11:03:17.000741  2600 net.cpp:406] Scale36 <- Convolution36
I0925 11:03:17.000746  2600 net.cpp:367] Scale36 -> Convolution36 (in-place)
I0925 11:03:17.000773  2600 layer_factory.hpp:77] Creating layer Scale36
I0925 11:03:17.000852  2600 net.cpp:122] Setting up Scale36
I0925 11:03:17.000856  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.000859  2600 net.cpp:137] Memory required for data: 974439600
I0925 11:03:17.000862  2600 layer_factory.hpp:77] Creating layer Eltwise17
I0925 11:03:17.000866  2600 net.cpp:84] Creating Layer Eltwise17
I0925 11:03:17.000869  2600 net.cpp:406] Eltwise17 <- Eltwise16_penlu33_0_split_1
I0925 11:03:17.000874  2600 net.cpp:406] Eltwise17 <- Convolution36
I0925 11:03:17.000876  2600 net.cpp:380] Eltwise17 -> Eltwise17
I0925 11:03:17.000888  2600 net.cpp:122] Setting up Eltwise17
I0925 11:03:17.000893  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.000895  2600 net.cpp:137] Memory required for data: 977716400
I0925 11:03:17.000897  2600 layer_factory.hpp:77] Creating layer penlu35
I0925 11:03:17.000902  2600 net.cpp:84] Creating Layer penlu35
I0925 11:03:17.000905  2600 net.cpp:406] penlu35 <- Eltwise17
I0925 11:03:17.000908  2600 net.cpp:367] penlu35 -> Eltwise17 (in-place)
I0925 11:03:17.001020  2600 net.cpp:122] Setting up penlu35
I0925 11:03:17.001025  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.001027  2600 net.cpp:137] Memory required for data: 980993200
I0925 11:03:17.001032  2600 layer_factory.hpp:77] Creating layer Eltwise17_penlu35_0_split
I0925 11:03:17.001035  2600 net.cpp:84] Creating Layer Eltwise17_penlu35_0_split
I0925 11:03:17.001039  2600 net.cpp:406] Eltwise17_penlu35_0_split <- Eltwise17
I0925 11:03:17.001041  2600 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_0
I0925 11:03:17.001045  2600 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_1
I0925 11:03:17.001070  2600 net.cpp:122] Setting up Eltwise17_penlu35_0_split
I0925 11:03:17.001075  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.001077  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.001080  2600 net.cpp:137] Memory required for data: 987546800
I0925 11:03:17.001081  2600 layer_factory.hpp:77] Creating layer Convolution37
I0925 11:03:17.001088  2600 net.cpp:84] Creating Layer Convolution37
I0925 11:03:17.001091  2600 net.cpp:406] Convolution37 <- Eltwise17_penlu35_0_split_0
I0925 11:03:17.001094  2600 net.cpp:380] Convolution37 -> Convolution37
I0925 11:03:17.002241  2600 net.cpp:122] Setting up Convolution37
I0925 11:03:17.002250  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.002254  2600 net.cpp:137] Memory required for data: 990823600
I0925 11:03:17.002259  2600 layer_factory.hpp:77] Creating layer BatchNorm37
I0925 11:03:17.002264  2600 net.cpp:84] Creating Layer BatchNorm37
I0925 11:03:17.002266  2600 net.cpp:406] BatchNorm37 <- Convolution37
I0925 11:03:17.002271  2600 net.cpp:367] BatchNorm37 -> Convolution37 (in-place)
I0925 11:03:17.002416  2600 net.cpp:122] Setting up BatchNorm37
I0925 11:03:17.002421  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.002424  2600 net.cpp:137] Memory required for data: 994100400
I0925 11:03:17.002429  2600 layer_factory.hpp:77] Creating layer Scale37
I0925 11:03:17.002431  2600 net.cpp:84] Creating Layer Scale37
I0925 11:03:17.002434  2600 net.cpp:406] Scale37 <- Convolution37
I0925 11:03:17.002437  2600 net.cpp:367] Scale37 -> Convolution37 (in-place)
I0925 11:03:17.002465  2600 layer_factory.hpp:77] Creating layer Scale37
I0925 11:03:17.002545  2600 net.cpp:122] Setting up Scale37
I0925 11:03:17.002549  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.002552  2600 net.cpp:137] Memory required for data: 997377200
I0925 11:03:17.002555  2600 layer_factory.hpp:77] Creating layer penlu36
I0925 11:03:17.002568  2600 net.cpp:84] Creating Layer penlu36
I0925 11:03:17.002571  2600 net.cpp:406] penlu36 <- Convolution37
I0925 11:03:17.002574  2600 net.cpp:367] penlu36 -> Convolution37 (in-place)
I0925 11:03:17.002684  2600 net.cpp:122] Setting up penlu36
I0925 11:03:17.002689  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.002691  2600 net.cpp:137] Memory required for data: 1000654000
I0925 11:03:17.002696  2600 layer_factory.hpp:77] Creating layer Convolution38
I0925 11:03:17.002702  2600 net.cpp:84] Creating Layer Convolution38
I0925 11:03:17.002706  2600 net.cpp:406] Convolution38 <- Convolution37
I0925 11:03:17.002709  2600 net.cpp:380] Convolution38 -> Convolution38
I0925 11:03:17.004155  2600 net.cpp:122] Setting up Convolution38
I0925 11:03:17.004164  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.004166  2600 net.cpp:137] Memory required for data: 1003930800
I0925 11:03:17.004173  2600 layer_factory.hpp:77] Creating layer BatchNorm38
I0925 11:03:17.004176  2600 net.cpp:84] Creating Layer BatchNorm38
I0925 11:03:17.004179  2600 net.cpp:406] BatchNorm38 <- Convolution38
I0925 11:03:17.004184  2600 net.cpp:367] BatchNorm38 -> Convolution38 (in-place)
I0925 11:03:17.004321  2600 net.cpp:122] Setting up BatchNorm38
I0925 11:03:17.004326  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.004328  2600 net.cpp:137] Memory required for data: 1007207600
I0925 11:03:17.023069  2600 layer_factory.hpp:77] Creating layer Scale38
I0925 11:03:17.023084  2600 net.cpp:84] Creating Layer Scale38
I0925 11:03:17.023089  2600 net.cpp:406] Scale38 <- Convolution38
I0925 11:03:17.023095  2600 net.cpp:367] Scale38 -> Convolution38 (in-place)
I0925 11:03:17.023149  2600 layer_factory.hpp:77] Creating layer Scale38
I0925 11:03:17.023277  2600 net.cpp:122] Setting up Scale38
I0925 11:03:17.023288  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.023291  2600 net.cpp:137] Memory required for data: 1010484400
I0925 11:03:17.023299  2600 layer_factory.hpp:77] Creating layer Eltwise18
I0925 11:03:17.023308  2600 net.cpp:84] Creating Layer Eltwise18
I0925 11:03:17.023313  2600 net.cpp:406] Eltwise18 <- Eltwise17_penlu35_0_split_1
I0925 11:03:17.023319  2600 net.cpp:406] Eltwise18 <- Convolution38
I0925 11:03:17.023325  2600 net.cpp:380] Eltwise18 -> Eltwise18
I0925 11:03:17.023345  2600 net.cpp:122] Setting up Eltwise18
I0925 11:03:17.023352  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.023356  2600 net.cpp:137] Memory required for data: 1013761200
I0925 11:03:17.023360  2600 layer_factory.hpp:77] Creating layer penlu37
I0925 11:03:17.023370  2600 net.cpp:84] Creating Layer penlu37
I0925 11:03:17.023375  2600 net.cpp:406] penlu37 <- Eltwise18
I0925 11:03:17.023380  2600 net.cpp:367] penlu37 -> Eltwise18 (in-place)
I0925 11:03:17.023551  2600 net.cpp:122] Setting up penlu37
I0925 11:03:17.023560  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.023563  2600 net.cpp:137] Memory required for data: 1017038000
I0925 11:03:17.023572  2600 layer_factory.hpp:77] Creating layer Eltwise18_penlu37_0_split
I0925 11:03:17.023578  2600 net.cpp:84] Creating Layer Eltwise18_penlu37_0_split
I0925 11:03:17.023583  2600 net.cpp:406] Eltwise18_penlu37_0_split <- Eltwise18
I0925 11:03:17.023589  2600 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_0
I0925 11:03:17.023597  2600 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_1
I0925 11:03:17.023633  2600 net.cpp:122] Setting up Eltwise18_penlu37_0_split
I0925 11:03:17.023639  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.023645  2600 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0925 11:03:17.023649  2600 net.cpp:137] Memory required for data: 1023591600
I0925 11:03:17.023653  2600 layer_factory.hpp:77] Creating layer Convolution39
I0925 11:03:17.023663  2600 net.cpp:84] Creating Layer Convolution39
I0925 11:03:17.023669  2600 net.cpp:406] Convolution39 <- Eltwise18_penlu37_0_split_0
I0925 11:03:17.023675  2600 net.cpp:380] Convolution39 -> Convolution39
I0925 11:03:17.025142  2600 net.cpp:122] Setting up Convolution39
I0925 11:03:17.025152  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.025156  2600 net.cpp:137] Memory required for data: 1025230000
I0925 11:03:17.025161  2600 layer_factory.hpp:77] Creating layer BatchNorm39
I0925 11:03:17.025166  2600 net.cpp:84] Creating Layer BatchNorm39
I0925 11:03:17.025168  2600 net.cpp:406] BatchNorm39 <- Convolution39
I0925 11:03:17.025173  2600 net.cpp:367] BatchNorm39 -> Convolution39 (in-place)
I0925 11:03:17.025373  2600 net.cpp:122] Setting up BatchNorm39
I0925 11:03:17.025384  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.025388  2600 net.cpp:137] Memory required for data: 1026868400
I0925 11:03:17.025398  2600 layer_factory.hpp:77] Creating layer Scale39
I0925 11:03:17.025405  2600 net.cpp:84] Creating Layer Scale39
I0925 11:03:17.025409  2600 net.cpp:406] Scale39 <- Convolution39
I0925 11:03:17.025415  2600 net.cpp:367] Scale39 -> Convolution39 (in-place)
I0925 11:03:17.025457  2600 layer_factory.hpp:77] Creating layer Scale39
I0925 11:03:17.025598  2600 net.cpp:122] Setting up Scale39
I0925 11:03:17.025605  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.025609  2600 net.cpp:137] Memory required for data: 1028506800
I0925 11:03:17.025616  2600 layer_factory.hpp:77] Creating layer Convolution40
I0925 11:03:17.025638  2600 net.cpp:84] Creating Layer Convolution40
I0925 11:03:17.025645  2600 net.cpp:406] Convolution40 <- Eltwise18_penlu37_0_split_1
I0925 11:03:17.025660  2600 net.cpp:380] Convolution40 -> Convolution40
I0925 11:03:17.027115  2600 net.cpp:122] Setting up Convolution40
I0925 11:03:17.027125  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.027128  2600 net.cpp:137] Memory required for data: 1030145200
I0925 11:03:17.027133  2600 layer_factory.hpp:77] Creating layer BatchNorm40
I0925 11:03:17.027139  2600 net.cpp:84] Creating Layer BatchNorm40
I0925 11:03:17.027143  2600 net.cpp:406] BatchNorm40 <- Convolution40
I0925 11:03:17.027145  2600 net.cpp:367] BatchNorm40 -> Convolution40 (in-place)
I0925 11:03:17.027287  2600 net.cpp:122] Setting up BatchNorm40
I0925 11:03:17.027292  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.027295  2600 net.cpp:137] Memory required for data: 1031783600
I0925 11:03:17.027312  2600 layer_factory.hpp:77] Creating layer Scale40
I0925 11:03:17.027315  2600 net.cpp:84] Creating Layer Scale40
I0925 11:03:17.027318  2600 net.cpp:406] Scale40 <- Convolution40
I0925 11:03:17.027321  2600 net.cpp:367] Scale40 -> Convolution40 (in-place)
I0925 11:03:17.027360  2600 layer_factory.hpp:77] Creating layer Scale40
I0925 11:03:17.027463  2600 net.cpp:122] Setting up Scale40
I0925 11:03:17.027468  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.027470  2600 net.cpp:137] Memory required for data: 1033422000
I0925 11:03:17.027474  2600 layer_factory.hpp:77] Creating layer penlu38
I0925 11:03:17.027480  2600 net.cpp:84] Creating Layer penlu38
I0925 11:03:17.027482  2600 net.cpp:406] penlu38 <- Convolution40
I0925 11:03:17.027485  2600 net.cpp:367] penlu38 -> Convolution40 (in-place)
I0925 11:03:17.027626  2600 net.cpp:122] Setting up penlu38
I0925 11:03:17.027631  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.027632  2600 net.cpp:137] Memory required for data: 1035060400
I0925 11:03:17.027637  2600 layer_factory.hpp:77] Creating layer Convolution41
I0925 11:03:17.027644  2600 net.cpp:84] Creating Layer Convolution41
I0925 11:03:17.027647  2600 net.cpp:406] Convolution41 <- Convolution40
I0925 11:03:17.027650  2600 net.cpp:380] Convolution41 -> Convolution41
I0925 11:03:17.029456  2600 net.cpp:122] Setting up Convolution41
I0925 11:03:17.029465  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.029469  2600 net.cpp:137] Memory required for data: 1036698800
I0925 11:03:17.029472  2600 layer_factory.hpp:77] Creating layer BatchNorm41
I0925 11:03:17.029479  2600 net.cpp:84] Creating Layer BatchNorm41
I0925 11:03:17.029481  2600 net.cpp:406] BatchNorm41 <- Convolution41
I0925 11:03:17.029494  2600 net.cpp:367] BatchNorm41 -> Convolution41 (in-place)
I0925 11:03:17.029636  2600 net.cpp:122] Setting up BatchNorm41
I0925 11:03:17.029641  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.029644  2600 net.cpp:137] Memory required for data: 1038337200
I0925 11:03:17.029649  2600 layer_factory.hpp:77] Creating layer Scale41
I0925 11:03:17.029654  2600 net.cpp:84] Creating Layer Scale41
I0925 11:03:17.029657  2600 net.cpp:406] Scale41 <- Convolution41
I0925 11:03:17.029660  2600 net.cpp:367] Scale41 -> Convolution41 (in-place)
I0925 11:03:17.029690  2600 layer_factory.hpp:77] Creating layer Scale41
I0925 11:03:17.029774  2600 net.cpp:122] Setting up Scale41
I0925 11:03:17.029779  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.029781  2600 net.cpp:137] Memory required for data: 1039975600
I0925 11:03:17.029785  2600 layer_factory.hpp:77] Creating layer Eltwise19
I0925 11:03:17.029790  2600 net.cpp:84] Creating Layer Eltwise19
I0925 11:03:17.029793  2600 net.cpp:406] Eltwise19 <- Convolution39
I0925 11:03:17.029795  2600 net.cpp:406] Eltwise19 <- Convolution41
I0925 11:03:17.029799  2600 net.cpp:380] Eltwise19 -> Eltwise19
I0925 11:03:17.029816  2600 net.cpp:122] Setting up Eltwise19
I0925 11:03:17.029820  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.029822  2600 net.cpp:137] Memory required for data: 1041614000
I0925 11:03:17.029825  2600 layer_factory.hpp:77] Creating layer penlu39
I0925 11:03:17.029830  2600 net.cpp:84] Creating Layer penlu39
I0925 11:03:17.029834  2600 net.cpp:406] penlu39 <- Eltwise19
I0925 11:03:17.029836  2600 net.cpp:367] penlu39 -> Eltwise19 (in-place)
I0925 11:03:17.029958  2600 net.cpp:122] Setting up penlu39
I0925 11:03:17.029963  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.029965  2600 net.cpp:137] Memory required for data: 1043252400
I0925 11:03:17.029970  2600 layer_factory.hpp:77] Creating layer Eltwise19_penlu39_0_split
I0925 11:03:17.029973  2600 net.cpp:84] Creating Layer Eltwise19_penlu39_0_split
I0925 11:03:17.029976  2600 net.cpp:406] Eltwise19_penlu39_0_split <- Eltwise19
I0925 11:03:17.029979  2600 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_0
I0925 11:03:17.029985  2600 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_1
I0925 11:03:17.030009  2600 net.cpp:122] Setting up Eltwise19_penlu39_0_split
I0925 11:03:17.030014  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.030016  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.030019  2600 net.cpp:137] Memory required for data: 1046529200
I0925 11:03:17.030020  2600 layer_factory.hpp:77] Creating layer Convolution42
I0925 11:03:17.030028  2600 net.cpp:84] Creating Layer Convolution42
I0925 11:03:17.030030  2600 net.cpp:406] Convolution42 <- Eltwise19_penlu39_0_split_0
I0925 11:03:17.030035  2600 net.cpp:380] Convolution42 -> Convolution42
I0925 11:03:17.031792  2600 net.cpp:122] Setting up Convolution42
I0925 11:03:17.031801  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.031805  2600 net.cpp:137] Memory required for data: 1048167600
I0925 11:03:17.031808  2600 layer_factory.hpp:77] Creating layer BatchNorm42
I0925 11:03:17.031814  2600 net.cpp:84] Creating Layer BatchNorm42
I0925 11:03:17.031817  2600 net.cpp:406] BatchNorm42 <- Convolution42
I0925 11:03:17.031821  2600 net.cpp:367] BatchNorm42 -> Convolution42 (in-place)
I0925 11:03:17.031958  2600 net.cpp:122] Setting up BatchNorm42
I0925 11:03:17.031963  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.031965  2600 net.cpp:137] Memory required for data: 1049806000
I0925 11:03:17.031970  2600 layer_factory.hpp:77] Creating layer Scale42
I0925 11:03:17.031975  2600 net.cpp:84] Creating Layer Scale42
I0925 11:03:17.031977  2600 net.cpp:406] Scale42 <- Convolution42
I0925 11:03:17.031980  2600 net.cpp:367] Scale42 -> Convolution42 (in-place)
I0925 11:03:17.032011  2600 layer_factory.hpp:77] Creating layer Scale42
I0925 11:03:17.032090  2600 net.cpp:122] Setting up Scale42
I0925 11:03:17.032101  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.032104  2600 net.cpp:137] Memory required for data: 1051444400
I0925 11:03:17.032107  2600 layer_factory.hpp:77] Creating layer penlu40
I0925 11:03:17.032114  2600 net.cpp:84] Creating Layer penlu40
I0925 11:03:17.032116  2600 net.cpp:406] penlu40 <- Convolution42
I0925 11:03:17.032120  2600 net.cpp:367] penlu40 -> Convolution42 (in-place)
I0925 11:03:17.032238  2600 net.cpp:122] Setting up penlu40
I0925 11:03:17.032243  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.032245  2600 net.cpp:137] Memory required for data: 1053082800
I0925 11:03:17.032250  2600 layer_factory.hpp:77] Creating layer Convolution43
I0925 11:03:17.032258  2600 net.cpp:84] Creating Layer Convolution43
I0925 11:03:17.032259  2600 net.cpp:406] Convolution43 <- Convolution42
I0925 11:03:17.032263  2600 net.cpp:380] Convolution43 -> Convolution43
I0925 11:03:17.034008  2600 net.cpp:122] Setting up Convolution43
I0925 11:03:17.034018  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.034020  2600 net.cpp:137] Memory required for data: 1054721200
I0925 11:03:17.034025  2600 layer_factory.hpp:77] Creating layer BatchNorm43
I0925 11:03:17.034030  2600 net.cpp:84] Creating Layer BatchNorm43
I0925 11:03:17.034034  2600 net.cpp:406] BatchNorm43 <- Convolution43
I0925 11:03:17.034037  2600 net.cpp:367] BatchNorm43 -> Convolution43 (in-place)
I0925 11:03:17.034178  2600 net.cpp:122] Setting up BatchNorm43
I0925 11:03:17.034183  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.034186  2600 net.cpp:137] Memory required for data: 1056359600
I0925 11:03:17.034191  2600 layer_factory.hpp:77] Creating layer Scale43
I0925 11:03:17.034195  2600 net.cpp:84] Creating Layer Scale43
I0925 11:03:17.034198  2600 net.cpp:406] Scale43 <- Convolution43
I0925 11:03:17.034201  2600 net.cpp:367] Scale43 -> Convolution43 (in-place)
I0925 11:03:17.034229  2600 layer_factory.hpp:77] Creating layer Scale43
I0925 11:03:17.034310  2600 net.cpp:122] Setting up Scale43
I0925 11:03:17.034315  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.034318  2600 net.cpp:137] Memory required for data: 1057998000
I0925 11:03:17.034322  2600 layer_factory.hpp:77] Creating layer Eltwise20
I0925 11:03:17.034327  2600 net.cpp:84] Creating Layer Eltwise20
I0925 11:03:17.034330  2600 net.cpp:406] Eltwise20 <- Eltwise19_penlu39_0_split_1
I0925 11:03:17.034333  2600 net.cpp:406] Eltwise20 <- Convolution43
I0925 11:03:17.034337  2600 net.cpp:380] Eltwise20 -> Eltwise20
I0925 11:03:17.034354  2600 net.cpp:122] Setting up Eltwise20
I0925 11:03:17.034358  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.034360  2600 net.cpp:137] Memory required for data: 1059636400
I0925 11:03:17.034363  2600 layer_factory.hpp:77] Creating layer penlu41
I0925 11:03:17.034368  2600 net.cpp:84] Creating Layer penlu41
I0925 11:03:17.034370  2600 net.cpp:406] penlu41 <- Eltwise20
I0925 11:03:17.034373  2600 net.cpp:367] penlu41 -> Eltwise20 (in-place)
I0925 11:03:17.034489  2600 net.cpp:122] Setting up penlu41
I0925 11:03:17.034494  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.034497  2600 net.cpp:137] Memory required for data: 1061274800
I0925 11:03:17.034500  2600 layer_factory.hpp:77] Creating layer Eltwise20_penlu41_0_split
I0925 11:03:17.034504  2600 net.cpp:84] Creating Layer Eltwise20_penlu41_0_split
I0925 11:03:17.034507  2600 net.cpp:406] Eltwise20_penlu41_0_split <- Eltwise20
I0925 11:03:17.034510  2600 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_0
I0925 11:03:17.034514  2600 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_1
I0925 11:03:17.034538  2600 net.cpp:122] Setting up Eltwise20_penlu41_0_split
I0925 11:03:17.034543  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.034545  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.034548  2600 net.cpp:137] Memory required for data: 1064551600
I0925 11:03:17.034549  2600 layer_factory.hpp:77] Creating layer Convolution44
I0925 11:03:17.034564  2600 net.cpp:84] Creating Layer Convolution44
I0925 11:03:17.034566  2600 net.cpp:406] Convolution44 <- Eltwise20_penlu41_0_split_0
I0925 11:03:17.034570  2600 net.cpp:380] Convolution44 -> Convolution44
I0925 11:03:17.036639  2600 net.cpp:122] Setting up Convolution44
I0925 11:03:17.036648  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.036651  2600 net.cpp:137] Memory required for data: 1066190000
I0925 11:03:17.036658  2600 layer_factory.hpp:77] Creating layer BatchNorm44
I0925 11:03:17.036661  2600 net.cpp:84] Creating Layer BatchNorm44
I0925 11:03:17.036664  2600 net.cpp:406] BatchNorm44 <- Convolution44
I0925 11:03:17.036669  2600 net.cpp:367] BatchNorm44 -> Convolution44 (in-place)
I0925 11:03:17.036815  2600 net.cpp:122] Setting up BatchNorm44
I0925 11:03:17.036820  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.036823  2600 net.cpp:137] Memory required for data: 1067828400
I0925 11:03:17.036828  2600 layer_factory.hpp:77] Creating layer Scale44
I0925 11:03:17.036833  2600 net.cpp:84] Creating Layer Scale44
I0925 11:03:17.036834  2600 net.cpp:406] Scale44 <- Convolution44
I0925 11:03:17.036837  2600 net.cpp:367] Scale44 -> Convolution44 (in-place)
I0925 11:03:17.036869  2600 layer_factory.hpp:77] Creating layer Scale44
I0925 11:03:17.036949  2600 net.cpp:122] Setting up Scale44
I0925 11:03:17.036954  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.036957  2600 net.cpp:137] Memory required for data: 1069466800
I0925 11:03:17.036960  2600 layer_factory.hpp:77] Creating layer penlu42
I0925 11:03:17.036965  2600 net.cpp:84] Creating Layer penlu42
I0925 11:03:17.036968  2600 net.cpp:406] penlu42 <- Convolution44
I0925 11:03:17.036972  2600 net.cpp:367] penlu42 -> Convolution44 (in-place)
I0925 11:03:17.037089  2600 net.cpp:122] Setting up penlu42
I0925 11:03:17.037094  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.037096  2600 net.cpp:137] Memory required for data: 1071105200
I0925 11:03:17.037101  2600 layer_factory.hpp:77] Creating layer Convolution45
I0925 11:03:17.037107  2600 net.cpp:84] Creating Layer Convolution45
I0925 11:03:17.037111  2600 net.cpp:406] Convolution45 <- Convolution44
I0925 11:03:17.037114  2600 net.cpp:380] Convolution45 -> Convolution45
I0925 11:03:17.038863  2600 net.cpp:122] Setting up Convolution45
I0925 11:03:17.038872  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.038874  2600 net.cpp:137] Memory required for data: 1072743600
I0925 11:03:17.038879  2600 layer_factory.hpp:77] Creating layer BatchNorm45
I0925 11:03:17.038885  2600 net.cpp:84] Creating Layer BatchNorm45
I0925 11:03:17.038888  2600 net.cpp:406] BatchNorm45 <- Convolution45
I0925 11:03:17.038892  2600 net.cpp:367] BatchNorm45 -> Convolution45 (in-place)
I0925 11:03:17.039032  2600 net.cpp:122] Setting up BatchNorm45
I0925 11:03:17.039036  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.039039  2600 net.cpp:137] Memory required for data: 1074382000
I0925 11:03:17.039044  2600 layer_factory.hpp:77] Creating layer Scale45
I0925 11:03:17.039048  2600 net.cpp:84] Creating Layer Scale45
I0925 11:03:17.039052  2600 net.cpp:406] Scale45 <- Convolution45
I0925 11:03:17.039054  2600 net.cpp:367] Scale45 -> Convolution45 (in-place)
I0925 11:03:17.039084  2600 layer_factory.hpp:77] Creating layer Scale45
I0925 11:03:17.039165  2600 net.cpp:122] Setting up Scale45
I0925 11:03:17.039170  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.039171  2600 net.cpp:137] Memory required for data: 1076020400
I0925 11:03:17.039175  2600 layer_factory.hpp:77] Creating layer Eltwise21
I0925 11:03:17.039180  2600 net.cpp:84] Creating Layer Eltwise21
I0925 11:03:17.039181  2600 net.cpp:406] Eltwise21 <- Eltwise20_penlu41_0_split_1
I0925 11:03:17.039185  2600 net.cpp:406] Eltwise21 <- Convolution45
I0925 11:03:17.039189  2600 net.cpp:380] Eltwise21 -> Eltwise21
I0925 11:03:17.039206  2600 net.cpp:122] Setting up Eltwise21
I0925 11:03:17.039209  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.039211  2600 net.cpp:137] Memory required for data: 1077658800
I0925 11:03:17.039221  2600 layer_factory.hpp:77] Creating layer penlu43
I0925 11:03:17.039227  2600 net.cpp:84] Creating Layer penlu43
I0925 11:03:17.039229  2600 net.cpp:406] penlu43 <- Eltwise21
I0925 11:03:17.039232  2600 net.cpp:367] penlu43 -> Eltwise21 (in-place)
I0925 11:03:17.039351  2600 net.cpp:122] Setting up penlu43
I0925 11:03:17.039356  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.039358  2600 net.cpp:137] Memory required for data: 1079297200
I0925 11:03:17.039363  2600 layer_factory.hpp:77] Creating layer Eltwise21_penlu43_0_split
I0925 11:03:17.039366  2600 net.cpp:84] Creating Layer Eltwise21_penlu43_0_split
I0925 11:03:17.039369  2600 net.cpp:406] Eltwise21_penlu43_0_split <- Eltwise21
I0925 11:03:17.039373  2600 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_0
I0925 11:03:17.039378  2600 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_1
I0925 11:03:17.039402  2600 net.cpp:122] Setting up Eltwise21_penlu43_0_split
I0925 11:03:17.039407  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.039409  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.053500  2600 net.cpp:137] Memory required for data: 1082574000
I0925 11:03:17.053510  2600 layer_factory.hpp:77] Creating layer Convolution46
I0925 11:03:17.053524  2600 net.cpp:84] Creating Layer Convolution46
I0925 11:03:17.053529  2600 net.cpp:406] Convolution46 <- Eltwise21_penlu43_0_split_0
I0925 11:03:17.053536  2600 net.cpp:380] Convolution46 -> Convolution46
I0925 11:03:17.056174  2600 net.cpp:122] Setting up Convolution46
I0925 11:03:17.056185  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.056187  2600 net.cpp:137] Memory required for data: 1084212400
I0925 11:03:17.056201  2600 layer_factory.hpp:77] Creating layer BatchNorm46
I0925 11:03:17.056206  2600 net.cpp:84] Creating Layer BatchNorm46
I0925 11:03:17.056210  2600 net.cpp:406] BatchNorm46 <- Convolution46
I0925 11:03:17.056213  2600 net.cpp:367] BatchNorm46 -> Convolution46 (in-place)
I0925 11:03:17.056361  2600 net.cpp:122] Setting up BatchNorm46
I0925 11:03:17.056366  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.056368  2600 net.cpp:137] Memory required for data: 1085850800
I0925 11:03:17.056373  2600 layer_factory.hpp:77] Creating layer Scale46
I0925 11:03:17.056377  2600 net.cpp:84] Creating Layer Scale46
I0925 11:03:17.056380  2600 net.cpp:406] Scale46 <- Convolution46
I0925 11:03:17.056383  2600 net.cpp:367] Scale46 -> Convolution46 (in-place)
I0925 11:03:17.056414  2600 layer_factory.hpp:77] Creating layer Scale46
I0925 11:03:17.056506  2600 net.cpp:122] Setting up Scale46
I0925 11:03:17.056514  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.056515  2600 net.cpp:137] Memory required for data: 1087489200
I0925 11:03:17.056519  2600 layer_factory.hpp:77] Creating layer penlu44
I0925 11:03:17.056525  2600 net.cpp:84] Creating Layer penlu44
I0925 11:03:17.056529  2600 net.cpp:406] penlu44 <- Convolution46
I0925 11:03:17.056532  2600 net.cpp:367] penlu44 -> Convolution46 (in-place)
I0925 11:03:17.056706  2600 net.cpp:122] Setting up penlu44
I0925 11:03:17.056726  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.056730  2600 net.cpp:137] Memory required for data: 1089127600
I0925 11:03:17.056738  2600 layer_factory.hpp:77] Creating layer Convolution47
I0925 11:03:17.056746  2600 net.cpp:84] Creating Layer Convolution47
I0925 11:03:17.056749  2600 net.cpp:406] Convolution47 <- Convolution46
I0925 11:03:17.056753  2600 net.cpp:380] Convolution47 -> Convolution47
I0925 11:03:17.058627  2600 net.cpp:122] Setting up Convolution47
I0925 11:03:17.058636  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.058640  2600 net.cpp:137] Memory required for data: 1090766000
I0925 11:03:17.058643  2600 layer_factory.hpp:77] Creating layer BatchNorm47
I0925 11:03:17.058650  2600 net.cpp:84] Creating Layer BatchNorm47
I0925 11:03:17.058652  2600 net.cpp:406] BatchNorm47 <- Convolution47
I0925 11:03:17.058656  2600 net.cpp:367] BatchNorm47 -> Convolution47 (in-place)
I0925 11:03:17.058805  2600 net.cpp:122] Setting up BatchNorm47
I0925 11:03:17.058811  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.058814  2600 net.cpp:137] Memory required for data: 1092404400
I0925 11:03:17.058818  2600 layer_factory.hpp:77] Creating layer Scale47
I0925 11:03:17.058822  2600 net.cpp:84] Creating Layer Scale47
I0925 11:03:17.058825  2600 net.cpp:406] Scale47 <- Convolution47
I0925 11:03:17.058828  2600 net.cpp:367] Scale47 -> Convolution47 (in-place)
I0925 11:03:17.058857  2600 layer_factory.hpp:77] Creating layer Scale47
I0925 11:03:17.058939  2600 net.cpp:122] Setting up Scale47
I0925 11:03:17.058944  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.058946  2600 net.cpp:137] Memory required for data: 1094042800
I0925 11:03:17.058950  2600 layer_factory.hpp:77] Creating layer Eltwise22
I0925 11:03:17.058954  2600 net.cpp:84] Creating Layer Eltwise22
I0925 11:03:17.058957  2600 net.cpp:406] Eltwise22 <- Eltwise21_penlu43_0_split_1
I0925 11:03:17.058960  2600 net.cpp:406] Eltwise22 <- Convolution47
I0925 11:03:17.058964  2600 net.cpp:380] Eltwise22 -> Eltwise22
I0925 11:03:17.058981  2600 net.cpp:122] Setting up Eltwise22
I0925 11:03:17.058985  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.058987  2600 net.cpp:137] Memory required for data: 1095681200
I0925 11:03:17.058990  2600 layer_factory.hpp:77] Creating layer penlu45
I0925 11:03:17.058995  2600 net.cpp:84] Creating Layer penlu45
I0925 11:03:17.058997  2600 net.cpp:406] penlu45 <- Eltwise22
I0925 11:03:17.059001  2600 net.cpp:367] penlu45 -> Eltwise22 (in-place)
I0925 11:03:17.059120  2600 net.cpp:122] Setting up penlu45
I0925 11:03:17.059124  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.059126  2600 net.cpp:137] Memory required for data: 1097319600
I0925 11:03:17.059131  2600 layer_factory.hpp:77] Creating layer Eltwise22_penlu45_0_split
I0925 11:03:17.059135  2600 net.cpp:84] Creating Layer Eltwise22_penlu45_0_split
I0925 11:03:17.059137  2600 net.cpp:406] Eltwise22_penlu45_0_split <- Eltwise22
I0925 11:03:17.059144  2600 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_0
I0925 11:03:17.059147  2600 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_1
I0925 11:03:17.059171  2600 net.cpp:122] Setting up Eltwise22_penlu45_0_split
I0925 11:03:17.059175  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.059178  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.059180  2600 net.cpp:137] Memory required for data: 1100596400
I0925 11:03:17.059182  2600 layer_factory.hpp:77] Creating layer Convolution48
I0925 11:03:17.059190  2600 net.cpp:84] Creating Layer Convolution48
I0925 11:03:17.059191  2600 net.cpp:406] Convolution48 <- Eltwise22_penlu45_0_split_0
I0925 11:03:17.059195  2600 net.cpp:380] Convolution48 -> Convolution48
I0925 11:03:17.061264  2600 net.cpp:122] Setting up Convolution48
I0925 11:03:17.061272  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.061275  2600 net.cpp:137] Memory required for data: 1102234800
I0925 11:03:17.061281  2600 layer_factory.hpp:77] Creating layer BatchNorm48
I0925 11:03:17.061286  2600 net.cpp:84] Creating Layer BatchNorm48
I0925 11:03:17.061290  2600 net.cpp:406] BatchNorm48 <- Convolution48
I0925 11:03:17.061293  2600 net.cpp:367] BatchNorm48 -> Convolution48 (in-place)
I0925 11:03:17.061439  2600 net.cpp:122] Setting up BatchNorm48
I0925 11:03:17.061444  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.061446  2600 net.cpp:137] Memory required for data: 1103873200
I0925 11:03:17.061451  2600 layer_factory.hpp:77] Creating layer Scale48
I0925 11:03:17.061455  2600 net.cpp:84] Creating Layer Scale48
I0925 11:03:17.061458  2600 net.cpp:406] Scale48 <- Convolution48
I0925 11:03:17.061461  2600 net.cpp:367] Scale48 -> Convolution48 (in-place)
I0925 11:03:17.061491  2600 layer_factory.hpp:77] Creating layer Scale48
I0925 11:03:17.061575  2600 net.cpp:122] Setting up Scale48
I0925 11:03:17.061580  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.061589  2600 net.cpp:137] Memory required for data: 1105511600
I0925 11:03:17.061594  2600 layer_factory.hpp:77] Creating layer penlu46
I0925 11:03:17.061599  2600 net.cpp:84] Creating Layer penlu46
I0925 11:03:17.061600  2600 net.cpp:406] penlu46 <- Convolution48
I0925 11:03:17.061605  2600 net.cpp:367] penlu46 -> Convolution48 (in-place)
I0925 11:03:17.061725  2600 net.cpp:122] Setting up penlu46
I0925 11:03:17.061730  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.061733  2600 net.cpp:137] Memory required for data: 1107150000
I0925 11:03:17.061738  2600 layer_factory.hpp:77] Creating layer Convolution49
I0925 11:03:17.061743  2600 net.cpp:84] Creating Layer Convolution49
I0925 11:03:17.061746  2600 net.cpp:406] Convolution49 <- Convolution48
I0925 11:03:17.061750  2600 net.cpp:380] Convolution49 -> Convolution49
I0925 11:03:17.063844  2600 net.cpp:122] Setting up Convolution49
I0925 11:03:17.063853  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.063856  2600 net.cpp:137] Memory required for data: 1108788400
I0925 11:03:17.063861  2600 layer_factory.hpp:77] Creating layer BatchNorm49
I0925 11:03:17.063866  2600 net.cpp:84] Creating Layer BatchNorm49
I0925 11:03:17.063869  2600 net.cpp:406] BatchNorm49 <- Convolution49
I0925 11:03:17.063874  2600 net.cpp:367] BatchNorm49 -> Convolution49 (in-place)
I0925 11:03:17.064016  2600 net.cpp:122] Setting up BatchNorm49
I0925 11:03:17.064021  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.064023  2600 net.cpp:137] Memory required for data: 1110426800
I0925 11:03:17.064028  2600 layer_factory.hpp:77] Creating layer Scale49
I0925 11:03:17.064033  2600 net.cpp:84] Creating Layer Scale49
I0925 11:03:17.064035  2600 net.cpp:406] Scale49 <- Convolution49
I0925 11:03:17.064039  2600 net.cpp:367] Scale49 -> Convolution49 (in-place)
I0925 11:03:17.064067  2600 layer_factory.hpp:77] Creating layer Scale49
I0925 11:03:17.064151  2600 net.cpp:122] Setting up Scale49
I0925 11:03:17.064155  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.064157  2600 net.cpp:137] Memory required for data: 1112065200
I0925 11:03:17.064162  2600 layer_factory.hpp:77] Creating layer Eltwise23
I0925 11:03:17.064167  2600 net.cpp:84] Creating Layer Eltwise23
I0925 11:03:17.064168  2600 net.cpp:406] Eltwise23 <- Eltwise22_penlu45_0_split_1
I0925 11:03:17.064172  2600 net.cpp:406] Eltwise23 <- Convolution49
I0925 11:03:17.064175  2600 net.cpp:380] Eltwise23 -> Eltwise23
I0925 11:03:17.064193  2600 net.cpp:122] Setting up Eltwise23
I0925 11:03:17.064196  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.064198  2600 net.cpp:137] Memory required for data: 1113703600
I0925 11:03:17.064200  2600 layer_factory.hpp:77] Creating layer penlu47
I0925 11:03:17.064206  2600 net.cpp:84] Creating Layer penlu47
I0925 11:03:17.064208  2600 net.cpp:406] penlu47 <- Eltwise23
I0925 11:03:17.064213  2600 net.cpp:367] penlu47 -> Eltwise23 (in-place)
I0925 11:03:17.064330  2600 net.cpp:122] Setting up penlu47
I0925 11:03:17.064335  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.064337  2600 net.cpp:137] Memory required for data: 1115342000
I0925 11:03:17.064342  2600 layer_factory.hpp:77] Creating layer Eltwise23_penlu47_0_split
I0925 11:03:17.064345  2600 net.cpp:84] Creating Layer Eltwise23_penlu47_0_split
I0925 11:03:17.064348  2600 net.cpp:406] Eltwise23_penlu47_0_split <- Eltwise23
I0925 11:03:17.064352  2600 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_0
I0925 11:03:17.064355  2600 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_1
I0925 11:03:17.064381  2600 net.cpp:122] Setting up Eltwise23_penlu47_0_split
I0925 11:03:17.064385  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.064388  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.064390  2600 net.cpp:137] Memory required for data: 1118618800
I0925 11:03:17.064393  2600 layer_factory.hpp:77] Creating layer Convolution50
I0925 11:03:17.064399  2600 net.cpp:84] Creating Layer Convolution50
I0925 11:03:17.064407  2600 net.cpp:406] Convolution50 <- Eltwise23_penlu47_0_split_0
I0925 11:03:17.064412  2600 net.cpp:380] Convolution50 -> Convolution50
I0925 11:03:17.067004  2600 net.cpp:122] Setting up Convolution50
I0925 11:03:17.067014  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.067018  2600 net.cpp:137] Memory required for data: 1120257200
I0925 11:03:17.067021  2600 layer_factory.hpp:77] Creating layer BatchNorm50
I0925 11:03:17.067028  2600 net.cpp:84] Creating Layer BatchNorm50
I0925 11:03:17.067030  2600 net.cpp:406] BatchNorm50 <- Convolution50
I0925 11:03:17.067034  2600 net.cpp:367] BatchNorm50 -> Convolution50 (in-place)
I0925 11:03:17.067181  2600 net.cpp:122] Setting up BatchNorm50
I0925 11:03:17.067186  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.067188  2600 net.cpp:137] Memory required for data: 1121895600
I0925 11:03:17.067193  2600 layer_factory.hpp:77] Creating layer Scale50
I0925 11:03:17.067198  2600 net.cpp:84] Creating Layer Scale50
I0925 11:03:17.067201  2600 net.cpp:406] Scale50 <- Convolution50
I0925 11:03:17.067205  2600 net.cpp:367] Scale50 -> Convolution50 (in-place)
I0925 11:03:17.067235  2600 layer_factory.hpp:77] Creating layer Scale50
I0925 11:03:17.067318  2600 net.cpp:122] Setting up Scale50
I0925 11:03:17.067323  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.067325  2600 net.cpp:137] Memory required for data: 1123534000
I0925 11:03:17.067329  2600 layer_factory.hpp:77] Creating layer penlu48
I0925 11:03:17.067334  2600 net.cpp:84] Creating Layer penlu48
I0925 11:03:17.067337  2600 net.cpp:406] penlu48 <- Convolution50
I0925 11:03:17.067340  2600 net.cpp:367] penlu48 -> Convolution50 (in-place)
I0925 11:03:17.067457  2600 net.cpp:122] Setting up penlu48
I0925 11:03:17.067462  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.067464  2600 net.cpp:137] Memory required for data: 1125172400
I0925 11:03:17.067469  2600 layer_factory.hpp:77] Creating layer Convolution51
I0925 11:03:17.067477  2600 net.cpp:84] Creating Layer Convolution51
I0925 11:03:17.067479  2600 net.cpp:406] Convolution51 <- Convolution50
I0925 11:03:17.067483  2600 net.cpp:380] Convolution51 -> Convolution51
I0925 11:03:17.069226  2600 net.cpp:122] Setting up Convolution51
I0925 11:03:17.069236  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.069238  2600 net.cpp:137] Memory required for data: 1126810800
I0925 11:03:17.069242  2600 layer_factory.hpp:77] Creating layer BatchNorm51
I0925 11:03:17.069248  2600 net.cpp:84] Creating Layer BatchNorm51
I0925 11:03:17.069252  2600 net.cpp:406] BatchNorm51 <- Convolution51
I0925 11:03:17.069255  2600 net.cpp:367] BatchNorm51 -> Convolution51 (in-place)
I0925 11:03:17.069396  2600 net.cpp:122] Setting up BatchNorm51
I0925 11:03:17.069401  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.069403  2600 net.cpp:137] Memory required for data: 1128449200
I0925 11:03:17.069408  2600 layer_factory.hpp:77] Creating layer Scale51
I0925 11:03:17.069413  2600 net.cpp:84] Creating Layer Scale51
I0925 11:03:17.069416  2600 net.cpp:406] Scale51 <- Convolution51
I0925 11:03:17.069419  2600 net.cpp:367] Scale51 -> Convolution51 (in-place)
I0925 11:03:17.069448  2600 layer_factory.hpp:77] Creating layer Scale51
I0925 11:03:17.069531  2600 net.cpp:122] Setting up Scale51
I0925 11:03:17.069535  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.069538  2600 net.cpp:137] Memory required for data: 1130087600
I0925 11:03:17.069541  2600 layer_factory.hpp:77] Creating layer Eltwise24
I0925 11:03:17.069546  2600 net.cpp:84] Creating Layer Eltwise24
I0925 11:03:17.069550  2600 net.cpp:406] Eltwise24 <- Eltwise23_penlu47_0_split_1
I0925 11:03:17.069552  2600 net.cpp:406] Eltwise24 <- Convolution51
I0925 11:03:17.069556  2600 net.cpp:380] Eltwise24 -> Eltwise24
I0925 11:03:17.069573  2600 net.cpp:122] Setting up Eltwise24
I0925 11:03:17.069577  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.069579  2600 net.cpp:137] Memory required for data: 1131726000
I0925 11:03:17.069588  2600 layer_factory.hpp:77] Creating layer penlu49
I0925 11:03:17.069594  2600 net.cpp:84] Creating Layer penlu49
I0925 11:03:17.069597  2600 net.cpp:406] penlu49 <- Eltwise24
I0925 11:03:17.069600  2600 net.cpp:367] penlu49 -> Eltwise24 (in-place)
I0925 11:03:17.069720  2600 net.cpp:122] Setting up penlu49
I0925 11:03:17.069725  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.069726  2600 net.cpp:137] Memory required for data: 1133364400
I0925 11:03:17.069730  2600 layer_factory.hpp:77] Creating layer Eltwise24_penlu49_0_split
I0925 11:03:17.069735  2600 net.cpp:84] Creating Layer Eltwise24_penlu49_0_split
I0925 11:03:17.069736  2600 net.cpp:406] Eltwise24_penlu49_0_split <- Eltwise24
I0925 11:03:17.069741  2600 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_0
I0925 11:03:17.069744  2600 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_1
I0925 11:03:17.069780  2600 net.cpp:122] Setting up Eltwise24_penlu49_0_split
I0925 11:03:17.083916  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.083927  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.083932  2600 net.cpp:137] Memory required for data: 1136641200
I0925 11:03:17.083937  2600 layer_factory.hpp:77] Creating layer Convolution52
I0925 11:03:17.083950  2600 net.cpp:84] Creating Layer Convolution52
I0925 11:03:17.083955  2600 net.cpp:406] Convolution52 <- Eltwise24_penlu49_0_split_0
I0925 11:03:17.083963  2600 net.cpp:380] Convolution52 -> Convolution52
I0925 11:03:17.086920  2600 net.cpp:122] Setting up Convolution52
I0925 11:03:17.086930  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.086932  2600 net.cpp:137] Memory required for data: 1138279600
I0925 11:03:17.086938  2600 layer_factory.hpp:77] Creating layer BatchNorm52
I0925 11:03:17.086943  2600 net.cpp:84] Creating Layer BatchNorm52
I0925 11:03:17.086946  2600 net.cpp:406] BatchNorm52 <- Convolution52
I0925 11:03:17.086951  2600 net.cpp:367] BatchNorm52 -> Convolution52 (in-place)
I0925 11:03:17.087133  2600 net.cpp:122] Setting up BatchNorm52
I0925 11:03:17.087143  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.087148  2600 net.cpp:137] Memory required for data: 1139918000
I0925 11:03:17.087157  2600 layer_factory.hpp:77] Creating layer Scale52
I0925 11:03:17.087163  2600 net.cpp:84] Creating Layer Scale52
I0925 11:03:17.087168  2600 net.cpp:406] Scale52 <- Convolution52
I0925 11:03:17.087173  2600 net.cpp:367] Scale52 -> Convolution52 (in-place)
I0925 11:03:17.087208  2600 layer_factory.hpp:77] Creating layer Scale52
I0925 11:03:17.087293  2600 net.cpp:122] Setting up Scale52
I0925 11:03:17.087298  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.087301  2600 net.cpp:137] Memory required for data: 1141556400
I0925 11:03:17.087304  2600 layer_factory.hpp:77] Creating layer penlu50
I0925 11:03:17.087327  2600 net.cpp:84] Creating Layer penlu50
I0925 11:03:17.087330  2600 net.cpp:406] penlu50 <- Convolution52
I0925 11:03:17.087334  2600 net.cpp:367] penlu50 -> Convolution52 (in-place)
I0925 11:03:17.087457  2600 net.cpp:122] Setting up penlu50
I0925 11:03:17.087462  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.087465  2600 net.cpp:137] Memory required for data: 1143194800
I0925 11:03:17.087504  2600 layer_factory.hpp:77] Creating layer Convolution53
I0925 11:03:17.087512  2600 net.cpp:84] Creating Layer Convolution53
I0925 11:03:17.087514  2600 net.cpp:406] Convolution53 <- Convolution52
I0925 11:03:17.087518  2600 net.cpp:380] Convolution53 -> Convolution53
I0925 11:03:17.089376  2600 net.cpp:122] Setting up Convolution53
I0925 11:03:17.089385  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.089388  2600 net.cpp:137] Memory required for data: 1144833200
I0925 11:03:17.089392  2600 layer_factory.hpp:77] Creating layer BatchNorm53
I0925 11:03:17.089398  2600 net.cpp:84] Creating Layer BatchNorm53
I0925 11:03:17.089401  2600 net.cpp:406] BatchNorm53 <- Convolution53
I0925 11:03:17.089406  2600 net.cpp:367] BatchNorm53 -> Convolution53 (in-place)
I0925 11:03:17.089560  2600 net.cpp:122] Setting up BatchNorm53
I0925 11:03:17.089565  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.089566  2600 net.cpp:137] Memory required for data: 1146471600
I0925 11:03:17.089571  2600 layer_factory.hpp:77] Creating layer Scale53
I0925 11:03:17.089576  2600 net.cpp:84] Creating Layer Scale53
I0925 11:03:17.089578  2600 net.cpp:406] Scale53 <- Convolution53
I0925 11:03:17.089583  2600 net.cpp:367] Scale53 -> Convolution53 (in-place)
I0925 11:03:17.089612  2600 layer_factory.hpp:77] Creating layer Scale53
I0925 11:03:17.089696  2600 net.cpp:122] Setting up Scale53
I0925 11:03:17.089701  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.089704  2600 net.cpp:137] Memory required for data: 1148110000
I0925 11:03:17.089709  2600 layer_factory.hpp:77] Creating layer Eltwise25
I0925 11:03:17.089712  2600 net.cpp:84] Creating Layer Eltwise25
I0925 11:03:17.089715  2600 net.cpp:406] Eltwise25 <- Eltwise24_penlu49_0_split_1
I0925 11:03:17.089718  2600 net.cpp:406] Eltwise25 <- Convolution53
I0925 11:03:17.089723  2600 net.cpp:380] Eltwise25 -> Eltwise25
I0925 11:03:17.089741  2600 net.cpp:122] Setting up Eltwise25
I0925 11:03:17.089746  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.089748  2600 net.cpp:137] Memory required for data: 1149748400
I0925 11:03:17.089751  2600 layer_factory.hpp:77] Creating layer penlu51
I0925 11:03:17.089756  2600 net.cpp:84] Creating Layer penlu51
I0925 11:03:17.089757  2600 net.cpp:406] penlu51 <- Eltwise25
I0925 11:03:17.089761  2600 net.cpp:367] penlu51 -> Eltwise25 (in-place)
I0925 11:03:17.089882  2600 net.cpp:122] Setting up penlu51
I0925 11:03:17.089887  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.089890  2600 net.cpp:137] Memory required for data: 1151386800
I0925 11:03:17.089895  2600 layer_factory.hpp:77] Creating layer Eltwise25_penlu51_0_split
I0925 11:03:17.089897  2600 net.cpp:84] Creating Layer Eltwise25_penlu51_0_split
I0925 11:03:17.089900  2600 net.cpp:406] Eltwise25_penlu51_0_split <- Eltwise25
I0925 11:03:17.089903  2600 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_0
I0925 11:03:17.089907  2600 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_1
I0925 11:03:17.089932  2600 net.cpp:122] Setting up Eltwise25_penlu51_0_split
I0925 11:03:17.089937  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.089939  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.089942  2600 net.cpp:137] Memory required for data: 1154663600
I0925 11:03:17.089944  2600 layer_factory.hpp:77] Creating layer Convolution54
I0925 11:03:17.089951  2600 net.cpp:84] Creating Layer Convolution54
I0925 11:03:17.089953  2600 net.cpp:406] Convolution54 <- Eltwise25_penlu51_0_split_0
I0925 11:03:17.089957  2600 net.cpp:380] Convolution54 -> Convolution54
I0925 11:03:17.092027  2600 net.cpp:122] Setting up Convolution54
I0925 11:03:17.092036  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.092038  2600 net.cpp:137] Memory required for data: 1156302000
I0925 11:03:17.092043  2600 layer_factory.hpp:77] Creating layer BatchNorm54
I0925 11:03:17.092049  2600 net.cpp:84] Creating Layer BatchNorm54
I0925 11:03:17.092052  2600 net.cpp:406] BatchNorm54 <- Convolution54
I0925 11:03:17.092056  2600 net.cpp:367] BatchNorm54 -> Convolution54 (in-place)
I0925 11:03:17.092206  2600 net.cpp:122] Setting up BatchNorm54
I0925 11:03:17.092211  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.092214  2600 net.cpp:137] Memory required for data: 1157940400
I0925 11:03:17.092219  2600 layer_factory.hpp:77] Creating layer Scale54
I0925 11:03:17.092223  2600 net.cpp:84] Creating Layer Scale54
I0925 11:03:17.092226  2600 net.cpp:406] Scale54 <- Convolution54
I0925 11:03:17.092228  2600 net.cpp:367] Scale54 -> Convolution54 (in-place)
I0925 11:03:17.092260  2600 layer_factory.hpp:77] Creating layer Scale54
I0925 11:03:17.092342  2600 net.cpp:122] Setting up Scale54
I0925 11:03:17.092347  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.092355  2600 net.cpp:137] Memory required for data: 1159578800
I0925 11:03:17.092360  2600 layer_factory.hpp:77] Creating layer penlu52
I0925 11:03:17.092366  2600 net.cpp:84] Creating Layer penlu52
I0925 11:03:17.092368  2600 net.cpp:406] penlu52 <- Convolution54
I0925 11:03:17.092372  2600 net.cpp:367] penlu52 -> Convolution54 (in-place)
I0925 11:03:17.092504  2600 net.cpp:122] Setting up penlu52
I0925 11:03:17.092511  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.092514  2600 net.cpp:137] Memory required for data: 1161217200
I0925 11:03:17.092519  2600 layer_factory.hpp:77] Creating layer Convolution55
I0925 11:03:17.092525  2600 net.cpp:84] Creating Layer Convolution55
I0925 11:03:17.092527  2600 net.cpp:406] Convolution55 <- Convolution54
I0925 11:03:17.092540  2600 net.cpp:380] Convolution55 -> Convolution55
I0925 11:03:17.094339  2600 net.cpp:122] Setting up Convolution55
I0925 11:03:17.094348  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.094352  2600 net.cpp:137] Memory required for data: 1162855600
I0925 11:03:17.094357  2600 layer_factory.hpp:77] Creating layer BatchNorm55
I0925 11:03:17.094362  2600 net.cpp:84] Creating Layer BatchNorm55
I0925 11:03:17.094364  2600 net.cpp:406] BatchNorm55 <- Convolution55
I0925 11:03:17.094368  2600 net.cpp:367] BatchNorm55 -> Convolution55 (in-place)
I0925 11:03:17.094527  2600 net.cpp:122] Setting up BatchNorm55
I0925 11:03:17.094532  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.094533  2600 net.cpp:137] Memory required for data: 1164494000
I0925 11:03:17.094538  2600 layer_factory.hpp:77] Creating layer Scale55
I0925 11:03:17.094542  2600 net.cpp:84] Creating Layer Scale55
I0925 11:03:17.094544  2600 net.cpp:406] Scale55 <- Convolution55
I0925 11:03:17.094548  2600 net.cpp:367] Scale55 -> Convolution55 (in-place)
I0925 11:03:17.094578  2600 layer_factory.hpp:77] Creating layer Scale55
I0925 11:03:17.094661  2600 net.cpp:122] Setting up Scale55
I0925 11:03:17.094666  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.094668  2600 net.cpp:137] Memory required for data: 1166132400
I0925 11:03:17.094672  2600 layer_factory.hpp:77] Creating layer Eltwise26
I0925 11:03:17.094677  2600 net.cpp:84] Creating Layer Eltwise26
I0925 11:03:17.094681  2600 net.cpp:406] Eltwise26 <- Eltwise25_penlu51_0_split_1
I0925 11:03:17.094683  2600 net.cpp:406] Eltwise26 <- Convolution55
I0925 11:03:17.094687  2600 net.cpp:380] Eltwise26 -> Eltwise26
I0925 11:03:17.094704  2600 net.cpp:122] Setting up Eltwise26
I0925 11:03:17.094709  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.094712  2600 net.cpp:137] Memory required for data: 1167770800
I0925 11:03:17.094713  2600 layer_factory.hpp:77] Creating layer penlu53
I0925 11:03:17.094717  2600 net.cpp:84] Creating Layer penlu53
I0925 11:03:17.094720  2600 net.cpp:406] penlu53 <- Eltwise26
I0925 11:03:17.094723  2600 net.cpp:367] penlu53 -> Eltwise26 (in-place)
I0925 11:03:17.094846  2600 net.cpp:122] Setting up penlu53
I0925 11:03:17.094851  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.094852  2600 net.cpp:137] Memory required for data: 1169409200
I0925 11:03:17.094856  2600 layer_factory.hpp:77] Creating layer Eltwise26_penlu53_0_split
I0925 11:03:17.094861  2600 net.cpp:84] Creating Layer Eltwise26_penlu53_0_split
I0925 11:03:17.094862  2600 net.cpp:406] Eltwise26_penlu53_0_split <- Eltwise26
I0925 11:03:17.094866  2600 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_0
I0925 11:03:17.094871  2600 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_1
I0925 11:03:17.094894  2600 net.cpp:122] Setting up Eltwise26_penlu53_0_split
I0925 11:03:17.094898  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.094902  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.094903  2600 net.cpp:137] Memory required for data: 1172686000
I0925 11:03:17.094905  2600 layer_factory.hpp:77] Creating layer Convolution56
I0925 11:03:17.094913  2600 net.cpp:84] Creating Layer Convolution56
I0925 11:03:17.094923  2600 net.cpp:406] Convolution56 <- Eltwise26_penlu53_0_split_0
I0925 11:03:17.094928  2600 net.cpp:380] Convolution56 -> Convolution56
I0925 11:03:17.096662  2600 net.cpp:122] Setting up Convolution56
I0925 11:03:17.096671  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.096673  2600 net.cpp:137] Memory required for data: 1174324400
I0925 11:03:17.096679  2600 layer_factory.hpp:77] Creating layer BatchNorm56
I0925 11:03:17.096684  2600 net.cpp:84] Creating Layer BatchNorm56
I0925 11:03:17.096688  2600 net.cpp:406] BatchNorm56 <- Convolution56
I0925 11:03:17.096691  2600 net.cpp:367] BatchNorm56 -> Convolution56 (in-place)
I0925 11:03:17.096842  2600 net.cpp:122] Setting up BatchNorm56
I0925 11:03:17.096848  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.096849  2600 net.cpp:137] Memory required for data: 1175962800
I0925 11:03:17.096854  2600 layer_factory.hpp:77] Creating layer Scale56
I0925 11:03:17.096858  2600 net.cpp:84] Creating Layer Scale56
I0925 11:03:17.096861  2600 net.cpp:406] Scale56 <- Convolution56
I0925 11:03:17.096864  2600 net.cpp:367] Scale56 -> Convolution56 (in-place)
I0925 11:03:17.096895  2600 layer_factory.hpp:77] Creating layer Scale56
I0925 11:03:17.096977  2600 net.cpp:122] Setting up Scale56
I0925 11:03:17.096982  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.096984  2600 net.cpp:137] Memory required for data: 1177601200
I0925 11:03:17.096988  2600 layer_factory.hpp:77] Creating layer penlu54
I0925 11:03:17.096993  2600 net.cpp:84] Creating Layer penlu54
I0925 11:03:17.096997  2600 net.cpp:406] penlu54 <- Convolution56
I0925 11:03:17.096999  2600 net.cpp:367] penlu54 -> Convolution56 (in-place)
I0925 11:03:17.097122  2600 net.cpp:122] Setting up penlu54
I0925 11:03:17.097127  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.097129  2600 net.cpp:137] Memory required for data: 1179239600
I0925 11:03:17.097133  2600 layer_factory.hpp:77] Creating layer Convolution57
I0925 11:03:17.097141  2600 net.cpp:84] Creating Layer Convolution57
I0925 11:03:17.097143  2600 net.cpp:406] Convolution57 <- Convolution56
I0925 11:03:17.097147  2600 net.cpp:380] Convolution57 -> Convolution57
I0925 11:03:17.098882  2600 net.cpp:122] Setting up Convolution57
I0925 11:03:17.098891  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.098894  2600 net.cpp:137] Memory required for data: 1180878000
I0925 11:03:17.098898  2600 layer_factory.hpp:77] Creating layer BatchNorm57
I0925 11:03:17.098904  2600 net.cpp:84] Creating Layer BatchNorm57
I0925 11:03:17.098907  2600 net.cpp:406] BatchNorm57 <- Convolution57
I0925 11:03:17.098912  2600 net.cpp:367] BatchNorm57 -> Convolution57 (in-place)
I0925 11:03:17.099058  2600 net.cpp:122] Setting up BatchNorm57
I0925 11:03:17.099063  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.099066  2600 net.cpp:137] Memory required for data: 1182516400
I0925 11:03:17.099071  2600 layer_factory.hpp:77] Creating layer Scale57
I0925 11:03:17.099076  2600 net.cpp:84] Creating Layer Scale57
I0925 11:03:17.099077  2600 net.cpp:406] Scale57 <- Convolution57
I0925 11:03:17.099081  2600 net.cpp:367] Scale57 -> Convolution57 (in-place)
I0925 11:03:17.099112  2600 layer_factory.hpp:77] Creating layer Scale57
I0925 11:03:17.099196  2600 net.cpp:122] Setting up Scale57
I0925 11:03:17.099201  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.099203  2600 net.cpp:137] Memory required for data: 1184154800
I0925 11:03:17.099207  2600 layer_factory.hpp:77] Creating layer Eltwise27
I0925 11:03:17.099211  2600 net.cpp:84] Creating Layer Eltwise27
I0925 11:03:17.099215  2600 net.cpp:406] Eltwise27 <- Eltwise26_penlu53_0_split_1
I0925 11:03:17.099217  2600 net.cpp:406] Eltwise27 <- Convolution57
I0925 11:03:17.099221  2600 net.cpp:380] Eltwise27 -> Eltwise27
I0925 11:03:17.099238  2600 net.cpp:122] Setting up Eltwise27
I0925 11:03:17.099242  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.099244  2600 net.cpp:137] Memory required for data: 1185793200
I0925 11:03:17.099253  2600 layer_factory.hpp:77] Creating layer penlu55
I0925 11:03:17.099259  2600 net.cpp:84] Creating Layer penlu55
I0925 11:03:17.099262  2600 net.cpp:406] penlu55 <- Eltwise27
I0925 11:03:17.099265  2600 net.cpp:367] penlu55 -> Eltwise27 (in-place)
I0925 11:03:17.099390  2600 net.cpp:122] Setting up penlu55
I0925 11:03:17.099395  2600 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0925 11:03:17.099397  2600 net.cpp:137] Memory required for data: 1187431600
I0925 11:03:17.099401  2600 layer_factory.hpp:77] Creating layer Pooling1
I0925 11:03:17.099406  2600 net.cpp:84] Creating Layer Pooling1
I0925 11:03:17.099409  2600 net.cpp:406] Pooling1 <- Eltwise27
I0925 11:03:17.099413  2600 net.cpp:380] Pooling1 -> Pooling1
I0925 11:03:17.099903  2600 net.cpp:122] Setting up Pooling1
I0925 11:03:17.099912  2600 net.cpp:129] Top shape: 100 64 1 1 (6400)
I0925 11:03:17.116574  2600 net.cpp:137] Memory required for data: 1187457200
I0925 11:03:17.116581  2600 layer_factory.hpp:77] Creating layer InnerProduct1
I0925 11:03:17.116600  2600 net.cpp:84] Creating Layer InnerProduct1
I0925 11:03:17.116602  2600 net.cpp:406] InnerProduct1 <- Pooling1
I0925 11:03:17.116606  2600 net.cpp:380] InnerProduct1 -> InnerProduct1
I0925 11:03:17.116739  2600 net.cpp:122] Setting up InnerProduct1
I0925 11:03:17.116744  2600 net.cpp:129] Top shape: 100 10 (1000)
I0925 11:03:17.116746  2600 net.cpp:137] Memory required for data: 1187461200
I0925 11:03:17.116750  2600 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0925 11:03:17.116755  2600 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I0925 11:03:17.116757  2600 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0925 11:03:17.116761  2600 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0925 11:03:17.116765  2600 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0925 11:03:17.116792  2600 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I0925 11:03:17.116797  2600 net.cpp:129] Top shape: 100 10 (1000)
I0925 11:03:17.116799  2600 net.cpp:129] Top shape: 100 10 (1000)
I0925 11:03:17.116801  2600 net.cpp:137] Memory required for data: 1187469200
I0925 11:03:17.116803  2600 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0925 11:03:17.116807  2600 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0925 11:03:17.116811  2600 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0925 11:03:17.116813  2600 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0925 11:03:17.116816  2600 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0925 11:03:17.116829  2600 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0925 11:03:17.117055  2600 net.cpp:122] Setting up SoftmaxWithLoss1
I0925 11:03:17.117063  2600 net.cpp:129] Top shape: (1)
I0925 11:03:17.117064  2600 net.cpp:132]     with loss weight 1
I0925 11:03:17.117072  2600 net.cpp:137] Memory required for data: 1187469204
I0925 11:03:17.117074  2600 layer_factory.hpp:77] Creating layer Accuracy1
I0925 11:03:17.117080  2600 net.cpp:84] Creating Layer Accuracy1
I0925 11:03:17.117084  2600 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0925 11:03:17.117087  2600 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I0925 11:03:17.117091  2600 net.cpp:380] Accuracy1 -> Accuracy1
I0925 11:03:17.117097  2600 net.cpp:122] Setting up Accuracy1
I0925 11:03:17.117100  2600 net.cpp:129] Top shape: (1)
I0925 11:03:17.117102  2600 net.cpp:137] Memory required for data: 1187469208
I0925 11:03:17.117105  2600 net.cpp:200] Accuracy1 does not need backward computation.
I0925 11:03:17.117107  2600 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0925 11:03:17.117110  2600 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0925 11:03:17.117113  2600 net.cpp:198] InnerProduct1 needs backward computation.
I0925 11:03:17.117115  2600 net.cpp:198] Pooling1 needs backward computation.
I0925 11:03:17.117118  2600 net.cpp:198] penlu55 needs backward computation.
I0925 11:03:17.117127  2600 net.cpp:198] Eltwise27 needs backward computation.
I0925 11:03:17.117130  2600 net.cpp:198] Scale57 needs backward computation.
I0925 11:03:17.117133  2600 net.cpp:198] BatchNorm57 needs backward computation.
I0925 11:03:17.117136  2600 net.cpp:198] Convolution57 needs backward computation.
I0925 11:03:17.117137  2600 net.cpp:198] penlu54 needs backward computation.
I0925 11:03:17.117139  2600 net.cpp:198] Scale56 needs backward computation.
I0925 11:03:17.117142  2600 net.cpp:198] BatchNorm56 needs backward computation.
I0925 11:03:17.117144  2600 net.cpp:198] Convolution56 needs backward computation.
I0925 11:03:17.117146  2600 net.cpp:198] Eltwise26_penlu53_0_split needs backward computation.
I0925 11:03:17.117149  2600 net.cpp:198] penlu53 needs backward computation.
I0925 11:03:17.117151  2600 net.cpp:198] Eltwise26 needs backward computation.
I0925 11:03:17.117154  2600 net.cpp:198] Scale55 needs backward computation.
I0925 11:03:17.117156  2600 net.cpp:198] BatchNorm55 needs backward computation.
I0925 11:03:17.117158  2600 net.cpp:198] Convolution55 needs backward computation.
I0925 11:03:17.117161  2600 net.cpp:198] penlu52 needs backward computation.
I0925 11:03:17.117162  2600 net.cpp:198] Scale54 needs backward computation.
I0925 11:03:17.117164  2600 net.cpp:198] BatchNorm54 needs backward computation.
I0925 11:03:17.117166  2600 net.cpp:198] Convolution54 needs backward computation.
I0925 11:03:17.117169  2600 net.cpp:198] Eltwise25_penlu51_0_split needs backward computation.
I0925 11:03:17.117172  2600 net.cpp:198] penlu51 needs backward computation.
I0925 11:03:17.117174  2600 net.cpp:198] Eltwise25 needs backward computation.
I0925 11:03:17.117177  2600 net.cpp:198] Scale53 needs backward computation.
I0925 11:03:17.117178  2600 net.cpp:198] BatchNorm53 needs backward computation.
I0925 11:03:17.117182  2600 net.cpp:198] Convolution53 needs backward computation.
I0925 11:03:17.117183  2600 net.cpp:198] penlu50 needs backward computation.
I0925 11:03:17.117185  2600 net.cpp:198] Scale52 needs backward computation.
I0925 11:03:17.117187  2600 net.cpp:198] BatchNorm52 needs backward computation.
I0925 11:03:17.117190  2600 net.cpp:198] Convolution52 needs backward computation.
I0925 11:03:17.117192  2600 net.cpp:198] Eltwise24_penlu49_0_split needs backward computation.
I0925 11:03:17.117195  2600 net.cpp:198] penlu49 needs backward computation.
I0925 11:03:17.117197  2600 net.cpp:198] Eltwise24 needs backward computation.
I0925 11:03:17.117200  2600 net.cpp:198] Scale51 needs backward computation.
I0925 11:03:17.117202  2600 net.cpp:198] BatchNorm51 needs backward computation.
I0925 11:03:17.117205  2600 net.cpp:198] Convolution51 needs backward computation.
I0925 11:03:17.117208  2600 net.cpp:198] penlu48 needs backward computation.
I0925 11:03:17.117210  2600 net.cpp:198] Scale50 needs backward computation.
I0925 11:03:17.117213  2600 net.cpp:198] BatchNorm50 needs backward computation.
I0925 11:03:17.117214  2600 net.cpp:198] Convolution50 needs backward computation.
I0925 11:03:17.117218  2600 net.cpp:198] Eltwise23_penlu47_0_split needs backward computation.
I0925 11:03:17.117219  2600 net.cpp:198] penlu47 needs backward computation.
I0925 11:03:17.117221  2600 net.cpp:198] Eltwise23 needs backward computation.
I0925 11:03:17.117225  2600 net.cpp:198] Scale49 needs backward computation.
I0925 11:03:17.117228  2600 net.cpp:198] BatchNorm49 needs backward computation.
I0925 11:03:17.117230  2600 net.cpp:198] Convolution49 needs backward computation.
I0925 11:03:17.117233  2600 net.cpp:198] penlu46 needs backward computation.
I0925 11:03:17.117235  2600 net.cpp:198] Scale48 needs backward computation.
I0925 11:03:17.117238  2600 net.cpp:198] BatchNorm48 needs backward computation.
I0925 11:03:17.117239  2600 net.cpp:198] Convolution48 needs backward computation.
I0925 11:03:17.117242  2600 net.cpp:198] Eltwise22_penlu45_0_split needs backward computation.
I0925 11:03:17.117244  2600 net.cpp:198] penlu45 needs backward computation.
I0925 11:03:17.117246  2600 net.cpp:198] Eltwise22 needs backward computation.
I0925 11:03:17.117254  2600 net.cpp:198] Scale47 needs backward computation.
I0925 11:03:17.117255  2600 net.cpp:198] BatchNorm47 needs backward computation.
I0925 11:03:17.117259  2600 net.cpp:198] Convolution47 needs backward computation.
I0925 11:03:17.117260  2600 net.cpp:198] penlu44 needs backward computation.
I0925 11:03:17.117264  2600 net.cpp:198] Scale46 needs backward computation.
I0925 11:03:17.117265  2600 net.cpp:198] BatchNorm46 needs backward computation.
I0925 11:03:17.117267  2600 net.cpp:198] Convolution46 needs backward computation.
I0925 11:03:17.117270  2600 net.cpp:198] Eltwise21_penlu43_0_split needs backward computation.
I0925 11:03:17.117272  2600 net.cpp:198] penlu43 needs backward computation.
I0925 11:03:17.117275  2600 net.cpp:198] Eltwise21 needs backward computation.
I0925 11:03:17.117278  2600 net.cpp:198] Scale45 needs backward computation.
I0925 11:03:17.117280  2600 net.cpp:198] BatchNorm45 needs backward computation.
I0925 11:03:17.117283  2600 net.cpp:198] Convolution45 needs backward computation.
I0925 11:03:17.117285  2600 net.cpp:198] penlu42 needs backward computation.
I0925 11:03:17.117288  2600 net.cpp:198] Scale44 needs backward computation.
I0925 11:03:17.117290  2600 net.cpp:198] BatchNorm44 needs backward computation.
I0925 11:03:17.117292  2600 net.cpp:198] Convolution44 needs backward computation.
I0925 11:03:17.117295  2600 net.cpp:198] Eltwise20_penlu41_0_split needs backward computation.
I0925 11:03:17.117297  2600 net.cpp:198] penlu41 needs backward computation.
I0925 11:03:17.117300  2600 net.cpp:198] Eltwise20 needs backward computation.
I0925 11:03:17.117302  2600 net.cpp:198] Scale43 needs backward computation.
I0925 11:03:17.117305  2600 net.cpp:198] BatchNorm43 needs backward computation.
I0925 11:03:17.117307  2600 net.cpp:198] Convolution43 needs backward computation.
I0925 11:03:17.117311  2600 net.cpp:198] penlu40 needs backward computation.
I0925 11:03:17.117312  2600 net.cpp:198] Scale42 needs backward computation.
I0925 11:03:17.117314  2600 net.cpp:198] BatchNorm42 needs backward computation.
I0925 11:03:17.117317  2600 net.cpp:198] Convolution42 needs backward computation.
I0925 11:03:17.117319  2600 net.cpp:198] Eltwise19_penlu39_0_split needs backward computation.
I0925 11:03:17.117322  2600 net.cpp:198] penlu39 needs backward computation.
I0925 11:03:17.117326  2600 net.cpp:198] Eltwise19 needs backward computation.
I0925 11:03:17.117327  2600 net.cpp:198] Scale41 needs backward computation.
I0925 11:03:17.117331  2600 net.cpp:198] BatchNorm41 needs backward computation.
I0925 11:03:17.117332  2600 net.cpp:198] Convolution41 needs backward computation.
I0925 11:03:17.117336  2600 net.cpp:198] penlu38 needs backward computation.
I0925 11:03:17.117337  2600 net.cpp:198] Scale40 needs backward computation.
I0925 11:03:17.117339  2600 net.cpp:198] BatchNorm40 needs backward computation.
I0925 11:03:17.117342  2600 net.cpp:198] Convolution40 needs backward computation.
I0925 11:03:17.117344  2600 net.cpp:198] Scale39 needs backward computation.
I0925 11:03:17.117347  2600 net.cpp:198] BatchNorm39 needs backward computation.
I0925 11:03:17.117349  2600 net.cpp:198] Convolution39 needs backward computation.
I0925 11:03:17.117352  2600 net.cpp:198] Eltwise18_penlu37_0_split needs backward computation.
I0925 11:03:17.117355  2600 net.cpp:198] penlu37 needs backward computation.
I0925 11:03:17.117357  2600 net.cpp:198] Eltwise18 needs backward computation.
I0925 11:03:17.117360  2600 net.cpp:198] Scale38 needs backward computation.
I0925 11:03:17.117362  2600 net.cpp:198] BatchNorm38 needs backward computation.
I0925 11:03:17.117365  2600 net.cpp:198] Convolution38 needs backward computation.
I0925 11:03:17.117367  2600 net.cpp:198] penlu36 needs backward computation.
I0925 11:03:17.117369  2600 net.cpp:198] Scale37 needs backward computation.
I0925 11:03:17.117372  2600 net.cpp:198] BatchNorm37 needs backward computation.
I0925 11:03:17.117374  2600 net.cpp:198] Convolution37 needs backward computation.
I0925 11:03:17.117380  2600 net.cpp:198] Eltwise17_penlu35_0_split needs backward computation.
I0925 11:03:17.117383  2600 net.cpp:198] penlu35 needs backward computation.
I0925 11:03:17.117385  2600 net.cpp:198] Eltwise17 needs backward computation.
I0925 11:03:17.117388  2600 net.cpp:198] Scale36 needs backward computation.
I0925 11:03:17.117390  2600 net.cpp:198] BatchNorm36 needs backward computation.
I0925 11:03:17.117393  2600 net.cpp:198] Convolution36 needs backward computation.
I0925 11:03:17.117395  2600 net.cpp:198] penlu34 needs backward computation.
I0925 11:03:17.117398  2600 net.cpp:198] Scale35 needs backward computation.
I0925 11:03:17.117400  2600 net.cpp:198] BatchNorm35 needs backward computation.
I0925 11:03:17.117403  2600 net.cpp:198] Convolution35 needs backward computation.
I0925 11:03:17.117405  2600 net.cpp:198] Eltwise16_penlu33_0_split needs backward computation.
I0925 11:03:17.117408  2600 net.cpp:198] penlu33 needs backward computation.
I0925 11:03:17.117411  2600 net.cpp:198] Eltwise16 needs backward computation.
I0925 11:03:17.117414  2600 net.cpp:198] Scale34 needs backward computation.
I0925 11:03:17.117416  2600 net.cpp:198] BatchNorm34 needs backward computation.
I0925 11:03:17.117419  2600 net.cpp:198] Convolution34 needs backward computation.
I0925 11:03:17.117421  2600 net.cpp:198] penlu32 needs backward computation.
I0925 11:03:17.117424  2600 net.cpp:198] Scale33 needs backward computation.
I0925 11:03:17.117426  2600 net.cpp:198] BatchNorm33 needs backward computation.
I0925 11:03:17.117429  2600 net.cpp:198] Convolution33 needs backward computation.
I0925 11:03:17.117431  2600 net.cpp:198] Eltwise15_penlu31_0_split needs backward computation.
I0925 11:03:17.117434  2600 net.cpp:198] penlu31 needs backward computation.
I0925 11:03:17.117436  2600 net.cpp:198] Eltwise15 needs backward computation.
I0925 11:03:17.117439  2600 net.cpp:198] Scale32 needs backward computation.
I0925 11:03:17.117441  2600 net.cpp:198] BatchNorm32 needs backward computation.
I0925 11:03:17.117444  2600 net.cpp:198] Convolution32 needs backward computation.
I0925 11:03:17.117446  2600 net.cpp:198] penlu30 needs backward computation.
I0925 11:03:17.117449  2600 net.cpp:198] Scale31 needs backward computation.
I0925 11:03:17.117451  2600 net.cpp:198] BatchNorm31 needs backward computation.
I0925 11:03:17.117453  2600 net.cpp:198] Convolution31 needs backward computation.
I0925 11:03:17.117456  2600 net.cpp:198] Eltwise14_penlu29_0_split needs backward computation.
I0925 11:03:17.117458  2600 net.cpp:198] penlu29 needs backward computation.
I0925 11:03:17.117461  2600 net.cpp:198] Eltwise14 needs backward computation.
I0925 11:03:17.117465  2600 net.cpp:198] Scale30 needs backward computation.
I0925 11:03:17.117466  2600 net.cpp:198] BatchNorm30 needs backward computation.
I0925 11:03:17.117468  2600 net.cpp:198] Convolution30 needs backward computation.
I0925 11:03:17.117471  2600 net.cpp:198] penlu28 needs backward computation.
I0925 11:03:17.117473  2600 net.cpp:198] Scale29 needs backward computation.
I0925 11:03:17.117476  2600 net.cpp:198] BatchNorm29 needs backward computation.
I0925 11:03:17.117478  2600 net.cpp:198] Convolution29 needs backward computation.
I0925 11:03:17.117480  2600 net.cpp:198] Eltwise13_penlu27_0_split needs backward computation.
I0925 11:03:17.117483  2600 net.cpp:198] penlu27 needs backward computation.
I0925 11:03:17.117486  2600 net.cpp:198] Eltwise13 needs backward computation.
I0925 11:03:17.117488  2600 net.cpp:198] Scale28 needs backward computation.
I0925 11:03:17.117491  2600 net.cpp:198] BatchNorm28 needs backward computation.
I0925 11:03:17.117493  2600 net.cpp:198] Convolution28 needs backward computation.
I0925 11:03:17.117496  2600 net.cpp:198] penlu26 needs backward computation.
I0925 11:03:17.117498  2600 net.cpp:198] Scale27 needs backward computation.
I0925 11:03:17.117501  2600 net.cpp:198] BatchNorm27 needs backward computation.
I0925 11:03:17.117503  2600 net.cpp:198] Convolution27 needs backward computation.
I0925 11:03:17.117506  2600 net.cpp:198] Eltwise12_penlu25_0_split needs backward computation.
I0925 11:03:17.117511  2600 net.cpp:198] penlu25 needs backward computation.
I0925 11:03:17.117514  2600 net.cpp:198] Eltwise12 needs backward computation.
I0925 11:03:17.117517  2600 net.cpp:198] Scale26 needs backward computation.
I0925 11:03:17.117519  2600 net.cpp:198] BatchNorm26 needs backward computation.
I0925 11:03:17.117522  2600 net.cpp:198] Convolution26 needs backward computation.
I0925 11:03:17.117524  2600 net.cpp:198] penlu24 needs backward computation.
I0925 11:03:17.117527  2600 net.cpp:198] Scale25 needs backward computation.
I0925 11:03:17.117529  2600 net.cpp:198] BatchNorm25 needs backward computation.
I0925 11:03:17.144686  2600 net.cpp:198] Convolution25 needs backward computation.
I0925 11:03:17.144697  2600 net.cpp:198] Eltwise11_penlu23_0_split needs backward computation.
I0925 11:03:17.144702  2600 net.cpp:198] penlu23 needs backward computation.
I0925 11:03:17.144706  2600 net.cpp:198] Eltwise11 needs backward computation.
I0925 11:03:17.144711  2600 net.cpp:198] Scale24 needs backward computation.
I0925 11:03:17.144716  2600 net.cpp:198] BatchNorm24 needs backward computation.
I0925 11:03:17.144721  2600 net.cpp:198] Convolution24 needs backward computation.
I0925 11:03:17.144724  2600 net.cpp:198] penlu22 needs backward computation.
I0925 11:03:17.144728  2600 net.cpp:198] Scale23 needs backward computation.
I0925 11:03:17.144732  2600 net.cpp:198] BatchNorm23 needs backward computation.
I0925 11:03:17.144737  2600 net.cpp:198] Convolution23 needs backward computation.
I0925 11:03:17.144742  2600 net.cpp:198] Eltwise10_penlu21_0_split needs backward computation.
I0925 11:03:17.144747  2600 net.cpp:198] penlu21 needs backward computation.
I0925 11:03:17.144752  2600 net.cpp:198] Eltwise10 needs backward computation.
I0925 11:03:17.144757  2600 net.cpp:198] Scale22 needs backward computation.
I0925 11:03:17.144760  2600 net.cpp:198] BatchNorm22 needs backward computation.
I0925 11:03:17.144765  2600 net.cpp:198] Convolution22 needs backward computation.
I0925 11:03:17.144769  2600 net.cpp:198] penlu20 needs backward computation.
I0925 11:03:17.144773  2600 net.cpp:198] Scale21 needs backward computation.
I0925 11:03:17.144778  2600 net.cpp:198] BatchNorm21 needs backward computation.
I0925 11:03:17.144783  2600 net.cpp:198] Convolution21 needs backward computation.
I0925 11:03:17.144786  2600 net.cpp:198] Scale20 needs backward computation.
I0925 11:03:17.144791  2600 net.cpp:198] BatchNorm20 needs backward computation.
I0925 11:03:17.144795  2600 net.cpp:198] Convolution20 needs backward computation.
I0925 11:03:17.144800  2600 net.cpp:198] Eltwise9_penlu19_0_split needs backward computation.
I0925 11:03:17.144805  2600 net.cpp:198] penlu19 needs backward computation.
I0925 11:03:17.144810  2600 net.cpp:198] Eltwise9 needs backward computation.
I0925 11:03:17.144815  2600 net.cpp:198] Scale19 needs backward computation.
I0925 11:03:17.144819  2600 net.cpp:198] BatchNorm19 needs backward computation.
I0925 11:03:17.144824  2600 net.cpp:198] Convolution19 needs backward computation.
I0925 11:03:17.144829  2600 net.cpp:198] penlu18 needs backward computation.
I0925 11:03:17.144832  2600 net.cpp:198] Scale18 needs backward computation.
I0925 11:03:17.144836  2600 net.cpp:198] BatchNorm18 needs backward computation.
I0925 11:03:17.144840  2600 net.cpp:198] Convolution18 needs backward computation.
I0925 11:03:17.144845  2600 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I0925 11:03:17.144850  2600 net.cpp:198] penlu17 needs backward computation.
I0925 11:03:17.144855  2600 net.cpp:198] Eltwise8 needs backward computation.
I0925 11:03:17.144860  2600 net.cpp:198] Scale17 needs backward computation.
I0925 11:03:17.144863  2600 net.cpp:198] BatchNorm17 needs backward computation.
I0925 11:03:17.144868  2600 net.cpp:198] Convolution17 needs backward computation.
I0925 11:03:17.144872  2600 net.cpp:198] penlu16 needs backward computation.
I0925 11:03:17.144876  2600 net.cpp:198] Scale16 needs backward computation.
I0925 11:03:17.144889  2600 net.cpp:198] BatchNorm16 needs backward computation.
I0925 11:03:17.144894  2600 net.cpp:198] Convolution16 needs backward computation.
I0925 11:03:17.144899  2600 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I0925 11:03:17.144903  2600 net.cpp:198] penlu15 needs backward computation.
I0925 11:03:17.144908  2600 net.cpp:198] Eltwise7 needs backward computation.
I0925 11:03:17.144913  2600 net.cpp:198] Scale15 needs backward computation.
I0925 11:03:17.144917  2600 net.cpp:198] BatchNorm15 needs backward computation.
I0925 11:03:17.144922  2600 net.cpp:198] Convolution15 needs backward computation.
I0925 11:03:17.144927  2600 net.cpp:198] penlu14 needs backward computation.
I0925 11:03:17.144930  2600 net.cpp:198] Scale14 needs backward computation.
I0925 11:03:17.144935  2600 net.cpp:198] BatchNorm14 needs backward computation.
I0925 11:03:17.144939  2600 net.cpp:198] Convolution14 needs backward computation.
I0925 11:03:17.144944  2600 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I0925 11:03:17.144951  2600 net.cpp:198] penlu13 needs backward computation.
I0925 11:03:17.144955  2600 net.cpp:198] Eltwise6 needs backward computation.
I0925 11:03:17.144960  2600 net.cpp:198] Scale13 needs backward computation.
I0925 11:03:17.144964  2600 net.cpp:198] BatchNorm13 needs backward computation.
I0925 11:03:17.144969  2600 net.cpp:198] Convolution13 needs backward computation.
I0925 11:03:17.144973  2600 net.cpp:198] penlu12 needs backward computation.
I0925 11:03:17.144978  2600 net.cpp:198] Scale12 needs backward computation.
I0925 11:03:17.144982  2600 net.cpp:198] BatchNorm12 needs backward computation.
I0925 11:03:17.144986  2600 net.cpp:198] Convolution12 needs backward computation.
I0925 11:03:17.144990  2600 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I0925 11:03:17.144995  2600 net.cpp:198] penlu11 needs backward computation.
I0925 11:03:17.144999  2600 net.cpp:198] Eltwise5 needs backward computation.
I0925 11:03:17.145004  2600 net.cpp:198] Scale11 needs backward computation.
I0925 11:03:17.145009  2600 net.cpp:198] BatchNorm11 needs backward computation.
I0925 11:03:17.145014  2600 net.cpp:198] Convolution11 needs backward computation.
I0925 11:03:17.145018  2600 net.cpp:198] penlu10 needs backward computation.
I0925 11:03:17.145022  2600 net.cpp:198] Scale10 needs backward computation.
I0925 11:03:17.145026  2600 net.cpp:198] BatchNorm10 needs backward computation.
I0925 11:03:17.145030  2600 net.cpp:198] Convolution10 needs backward computation.
I0925 11:03:17.145035  2600 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I0925 11:03:17.145040  2600 net.cpp:198] penlu9 needs backward computation.
I0925 11:03:17.145045  2600 net.cpp:198] Eltwise4 needs backward computation.
I0925 11:03:17.145051  2600 net.cpp:198] Scale9 needs backward computation.
I0925 11:03:17.145054  2600 net.cpp:198] BatchNorm9 needs backward computation.
I0925 11:03:17.145059  2600 net.cpp:198] Convolution9 needs backward computation.
I0925 11:03:17.145063  2600 net.cpp:198] penlu8 needs backward computation.
I0925 11:03:17.145068  2600 net.cpp:198] Scale8 needs backward computation.
I0925 11:03:17.145072  2600 net.cpp:198] BatchNorm8 needs backward computation.
I0925 11:03:17.145076  2600 net.cpp:198] Convolution8 needs backward computation.
I0925 11:03:17.145081  2600 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I0925 11:03:17.145087  2600 net.cpp:198] penlu7 needs backward computation.
I0925 11:03:17.145090  2600 net.cpp:198] Eltwise3 needs backward computation.
I0925 11:03:17.145095  2600 net.cpp:198] Scale7 needs backward computation.
I0925 11:03:17.145100  2600 net.cpp:198] BatchNorm7 needs backward computation.
I0925 11:03:17.145104  2600 net.cpp:198] Convolution7 needs backward computation.
I0925 11:03:17.145109  2600 net.cpp:198] penlu6 needs backward computation.
I0925 11:03:17.145113  2600 net.cpp:198] Scale6 needs backward computation.
I0925 11:03:17.145117  2600 net.cpp:198] BatchNorm6 needs backward computation.
I0925 11:03:17.145126  2600 net.cpp:198] Convolution6 needs backward computation.
I0925 11:03:17.145133  2600 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I0925 11:03:17.145136  2600 net.cpp:198] penlu5 needs backward computation.
I0925 11:03:17.145140  2600 net.cpp:198] Eltwise2 needs backward computation.
I0925 11:03:17.145146  2600 net.cpp:198] Scale5 needs backward computation.
I0925 11:03:17.145150  2600 net.cpp:198] BatchNorm5 needs backward computation.
I0925 11:03:17.145154  2600 net.cpp:198] Convolution5 needs backward computation.
I0925 11:03:17.146947  2600 net.cpp:198] penlu4 needs backward computation.
I0925 11:03:17.146953  2600 net.cpp:198] Scale4 needs backward computation.
I0925 11:03:17.146956  2600 net.cpp:198] BatchNorm4 needs backward computation.
I0925 11:03:17.146958  2600 net.cpp:198] Convolution4 needs backward computation.
I0925 11:03:17.146961  2600 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I0925 11:03:17.146965  2600 net.cpp:198] penlu3 needs backward computation.
I0925 11:03:17.146966  2600 net.cpp:198] Eltwise1 needs backward computation.
I0925 11:03:17.146970  2600 net.cpp:198] Scale3 needs backward computation.
I0925 11:03:17.146971  2600 net.cpp:198] BatchNorm3 needs backward computation.
I0925 11:03:17.146975  2600 net.cpp:198] Convolution3 needs backward computation.
I0925 11:03:17.146976  2600 net.cpp:198] penlu2 needs backward computation.
I0925 11:03:17.146978  2600 net.cpp:198] Scale2 needs backward computation.
I0925 11:03:17.146981  2600 net.cpp:198] BatchNorm2 needs backward computation.
I0925 11:03:17.146983  2600 net.cpp:198] Convolution2 needs backward computation.
I0925 11:03:17.146986  2600 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I0925 11:03:17.146988  2600 net.cpp:198] penlu1 needs backward computation.
I0925 11:03:17.146991  2600 net.cpp:198] Scale1 needs backward computation.
I0925 11:03:17.146993  2600 net.cpp:198] BatchNorm1 needs backward computation.
I0925 11:03:17.146996  2600 net.cpp:198] Convolution1 needs backward computation.
I0925 11:03:17.146998  2600 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I0925 11:03:17.147001  2600 net.cpp:200] Data1 does not need backward computation.
I0925 11:03:17.147006  2600 net.cpp:242] This network produces output Accuracy1
I0925 11:03:17.147008  2600 net.cpp:242] This network produces output SoftmaxWithLoss1
I0925 11:03:17.147100  2600 net.cpp:255] Network initialization done.
I0925 11:03:17.147881  2600 solver.cpp:56] Solver scaffolding done.
I0925 11:03:17.162495  2600 caffe.cpp:248] Starting Optimization
I0925 11:03:17.162504  2600 solver.cpp:272] Solving resnet_cifar10
I0925 11:03:17.162508  2600 solver.cpp:273] Learning Rate Policy: multistep
I0925 11:03:17.169425  2600 solver.cpp:330] Iteration 0, Testing net (#0)
I0925 11:03:20.643928  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:03:20.784780  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.1
I0925 11:03:20.784807  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I0925 11:03:20.985868  2600 solver.cpp:218] Iteration 0 (0 iter/s, 3.82328s/100 iters), loss = 2.30144
I0925 11:03:20.985899  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.30144 (* 1 = 2.30144 loss)
I0925 11:03:20.985915  2600 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I0925 11:03:35.576277  2600 solver.cpp:218] Iteration 100 (6.85386 iter/s, 14.5903s/100 iters), loss = 1.6808
I0925 11:03:35.576318  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.6808 (* 1 = 1.6808 loss)
I0925 11:03:35.576324  2600 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I0925 11:03:50.147315  2600 solver.cpp:218] Iteration 200 (6.86297 iter/s, 14.5709s/100 iters), loss = 1.76214
I0925 11:03:50.147398  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.76214 (* 1 = 1.76214 loss)
I0925 11:03:50.147405  2600 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I0925 11:04:04.721629  2600 solver.cpp:218] Iteration 300 (6.86145 iter/s, 14.5742s/100 iters), loss = 1.46011
I0925 11:04:04.721669  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.46011 (* 1 = 1.46011 loss)
I0925 11:04:04.721675  2600 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I0925 11:04:19.301216  2600 solver.cpp:218] Iteration 400 (6.85895 iter/s, 14.5795s/100 iters), loss = 1.14396
I0925 11:04:19.301259  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.14396 (* 1 = 1.14396 loss)
I0925 11:04:19.301265  2600 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I0925 11:04:33.158489  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:04:33.741058  2600 solver.cpp:330] Iteration 500, Testing net (#0)
I0925 11:04:37.160910  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:04:37.303303  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.407
I0925 11:04:37.303339  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.82528 (* 1 = 1.82528 loss)
I0925 11:04:37.448071  2600 solver.cpp:218] Iteration 500 (5.51063 iter/s, 18.1467s/100 iters), loss = 1.36023
I0925 11:04:37.448099  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.36023 (* 1 = 1.36023 loss)
I0925 11:04:37.448106  2600 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I0925 11:04:52.025784  2600 solver.cpp:218] Iteration 600 (6.85982 iter/s, 14.5776s/100 iters), loss = 1.24332
I0925 11:04:52.025813  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.24332 (* 1 = 1.24332 loss)
I0925 11:04:52.025820  2600 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I0925 11:05:06.608363  2600 solver.cpp:218] Iteration 700 (6.85754 iter/s, 14.5825s/100 iters), loss = 1.23095
I0925 11:05:06.608435  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.23095 (* 1 = 1.23095 loss)
I0925 11:05:06.608443  2600 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I0925 11:05:21.190995  2600 solver.cpp:218] Iteration 800 (6.85753 iter/s, 14.5825s/100 iters), loss = 1.26769
I0925 11:05:21.191037  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.26769 (* 1 = 1.26769 loss)
I0925 11:05:21.191043  2600 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I0925 11:05:35.769723  2600 solver.cpp:218] Iteration 900 (6.85935 iter/s, 14.5786s/100 iters), loss = 1.03004
I0925 11:05:35.769754  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03004 (* 1 = 1.03004 loss)
I0925 11:05:35.769759  2600 sgd_solver.cpp:46] MultiStep Status: Iteration 900, step = 1
I0925 11:05:35.769762  2600 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I0925 11:05:49.621335  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:05:50.203583  2600 solver.cpp:330] Iteration 1000, Testing net (#0)
I0925 11:05:53.629295  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:05:53.772331  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5121
I0925 11:05:53.772368  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.42112 (* 1 = 1.42112 loss)
I0925 11:05:53.917498  2600 solver.cpp:218] Iteration 1000 (5.51035 iter/s, 18.1477s/100 iters), loss = 0.980376
I0925 11:05:53.917528  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.980376 (* 1 = 0.980376 loss)
I0925 11:05:53.917536  2600 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I0925 11:06:08.509866  2600 solver.cpp:218] Iteration 1100 (6.85294 iter/s, 14.5923s/100 iters), loss = 0.928501
I0925 11:06:08.509899  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.928501 (* 1 = 0.928501 loss)
I0925 11:06:08.509907  2600 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I0925 11:06:23.445991  2600 solver.cpp:218] Iteration 1200 (6.69522 iter/s, 14.936s/100 iters), loss = 0.962792
I0925 11:06:23.446123  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.962792 (* 1 = 0.962792 loss)
I0925 11:06:23.446143  2600 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I0925 11:06:38.126359  2600 solver.cpp:218] Iteration 1300 (6.81192 iter/s, 14.6802s/100 iters), loss = 0.950792
I0925 11:06:38.126416  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.950792 (* 1 = 0.950792 loss)
I0925 11:06:38.126426  2600 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I0925 11:06:52.875285  2600 solver.cpp:218] Iteration 1400 (6.78022 iter/s, 14.7488s/100 iters), loss = 0.851125
I0925 11:06:52.875324  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.851125 (* 1 = 0.851125 loss)
I0925 11:06:52.875330  2600 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I0925 11:07:07.166431  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:07:07.774107  2600 solver.cpp:330] Iteration 1500, Testing net (#0)
I0925 11:07:11.211902  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:07:11.354984  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6084
I0925 11:07:11.355007  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.10675 (* 1 = 1.10675 loss)
I0925 11:07:11.499357  2600 solver.cpp:218] Iteration 1500 (5.36943 iter/s, 18.624s/100 iters), loss = 0.909861
I0925 11:07:11.499402  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.909861 (* 1 = 0.909861 loss)
I0925 11:07:11.499410  2600 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I0925 11:07:26.449936  2600 solver.cpp:218] Iteration 1600 (6.68875 iter/s, 14.9505s/100 iters), loss = 0.881897
I0925 11:07:26.450002  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.881897 (* 1 = 0.881897 loss)
I0925 11:07:26.450017  2600 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I0925 11:07:41.131391  2600 solver.cpp:218] Iteration 1700 (6.81138 iter/s, 14.6813s/100 iters), loss = 0.929649
I0925 11:07:41.131765  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.929649 (* 1 = 0.929649 loss)
I0925 11:07:41.131799  2600 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I0925 11:07:55.820873  2600 solver.cpp:218] Iteration 1800 (6.80778 iter/s, 14.6891s/100 iters), loss = 0.829931
I0925 11:07:55.820904  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.829931 (* 1 = 0.829931 loss)
I0925 11:07:55.820914  2600 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I0925 11:08:10.548806  2600 solver.cpp:218] Iteration 1900 (6.78986 iter/s, 14.7279s/100 iters), loss = 0.784701
I0925 11:08:10.548838  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.784701 (* 1 = 0.784701 loss)
I0925 11:08:10.548847  2600 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I0925 11:08:24.703999  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:08:25.292363  2600 solver.cpp:330] Iteration 2000, Testing net (#0)
I0925 11:08:28.819653  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:08:28.962880  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6351
I0925 11:08:28.962906  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.02565 (* 1 = 1.02565 loss)
I0925 11:08:29.108232  2600 solver.cpp:218] Iteration 2000 (5.38813 iter/s, 18.5593s/100 iters), loss = 0.820091
I0925 11:08:29.108263  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.820091 (* 1 = 0.820091 loss)
I0925 11:08:29.108269  2600 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I0925 11:08:43.809979  2600 solver.cpp:218] Iteration 2100 (6.80195 iter/s, 14.7017s/100 iters), loss = 0.872205
I0925 11:08:43.810012  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.872205 (* 1 = 0.872205 loss)
I0925 11:08:43.810019  2600 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I0925 11:08:58.512109  2600 solver.cpp:218] Iteration 2200 (6.80177 iter/s, 14.702s/100 iters), loss = 0.82947
I0925 11:08:58.512274  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.82947 (* 1 = 0.82947 loss)
I0925 11:08:58.512284  2600 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I0925 11:09:13.211990  2600 solver.cpp:218] Iteration 2300 (6.80287 iter/s, 14.6997s/100 iters), loss = 0.911814
I0925 11:09:13.212023  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.911814 (* 1 = 0.911814 loss)
I0925 11:09:13.212030  2600 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I0925 11:09:27.917632  2600 solver.cpp:218] Iteration 2400 (6.80015 iter/s, 14.7056s/100 iters), loss = 0.770832
I0925 11:09:27.917665  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.770832 (* 1 = 0.770832 loss)
I0925 11:09:27.917671  2600 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I0925 11:09:41.897256  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:09:42.486222  2600 solver.cpp:330] Iteration 2500, Testing net (#0)
I0925 11:09:45.935359  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:09:46.079190  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6411
I0925 11:09:46.079216  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.00106 (* 1 = 1.00106 loss)
I0925 11:09:46.225587  2600 solver.cpp:218] Iteration 2500 (5.46213 iter/s, 18.3079s/100 iters), loss = 0.863639
I0925 11:09:46.225620  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.863639 (* 1 = 0.863639 loss)
I0925 11:09:46.225626  2600 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I0925 11:10:01.168078  2600 solver.cpp:218] Iteration 2600 (6.69236 iter/s, 14.9424s/100 iters), loss = 0.698105
I0925 11:10:01.168123  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.698105 (* 1 = 0.698105 loss)
I0925 11:10:01.168130  2600 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I0925 11:10:16.263098  2600 solver.cpp:218] Iteration 2700 (6.62474 iter/s, 15.0949s/100 iters), loss = 0.819299
I0925 11:10:16.263237  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.819299 (* 1 = 0.819299 loss)
I0925 11:10:16.263245  2600 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I0925 11:10:30.860114  2600 solver.cpp:218] Iteration 2800 (6.8508 iter/s, 14.5968s/100 iters), loss = 0.816044
I0925 11:10:30.860154  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.816044 (* 1 = 0.816044 loss)
I0925 11:10:30.860162  2600 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I0925 11:10:45.454507  2600 solver.cpp:218] Iteration 2900 (6.85199 iter/s, 14.5943s/100 iters), loss = 0.818046
I0925 11:10:45.454547  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.818046 (* 1 = 0.818046 loss)
I0925 11:10:45.454553  2600 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I0925 11:10:59.326187  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:10:59.911092  2600 solver.cpp:330] Iteration 3000, Testing net (#0)
I0925 11:11:03.333344  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:11:03.476125  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6602
I0925 11:11:03.476161  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.941071 (* 1 = 0.941071 loss)
I0925 11:11:03.620764  2600 solver.cpp:218] Iteration 3000 (5.50474 iter/s, 18.1662s/100 iters), loss = 0.821789
I0925 11:11:03.620793  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.821789 (* 1 = 0.821789 loss)
I0925 11:11:03.620800  2600 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I0925 11:11:18.234313  2600 solver.cpp:218] Iteration 3100 (6.843 iter/s, 14.6135s/100 iters), loss = 0.680788
I0925 11:11:18.234354  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.680788 (* 1 = 0.680788 loss)
I0925 11:11:18.234360  2600 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I0925 11:11:32.847159  2600 solver.cpp:218] Iteration 3200 (6.84333 iter/s, 14.6128s/100 iters), loss = 0.797307
I0925 11:11:32.847267  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.797307 (* 1 = 0.797307 loss)
I0925 11:11:32.847275  2600 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I0925 11:11:47.462101  2600 solver.cpp:218] Iteration 3300 (6.84238 iter/s, 14.6148s/100 iters), loss = 0.814569
I0925 11:11:47.462142  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.814569 (* 1 = 0.814569 loss)
I0925 11:11:47.462149  2600 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I0925 11:12:02.282305  2600 solver.cpp:218] Iteration 3400 (6.74759 iter/s, 14.8201s/100 iters), loss = 0.735151
I0925 11:12:02.282361  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.735151 (* 1 = 0.735151 loss)
I0925 11:12:02.282371  2600 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I0925 11:12:16.342281  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:12:16.929005  2600 solver.cpp:330] Iteration 3500, Testing net (#0)
I0925 11:12:20.382910  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:12:20.525796  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.623
I0925 11:12:20.525831  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.01738 (* 1 = 1.01738 loss)
I0925 11:12:20.673269  2600 solver.cpp:218] Iteration 3500 (5.4375 iter/s, 18.3908s/100 iters), loss = 0.769219
I0925 11:12:20.673305  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.769219 (* 1 = 0.769219 loss)
I0925 11:12:20.673322  2600 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I0925 11:12:35.376063  2600 solver.cpp:218] Iteration 3600 (6.80147 iter/s, 14.7027s/100 iters), loss = 0.654655
I0925 11:12:35.376096  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.654655 (* 1 = 0.654655 loss)
I0925 11:12:35.376101  2600 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I0925 11:12:49.963052  2600 solver.cpp:218] Iteration 3700 (6.85546 iter/s, 14.5869s/100 iters), loss = 0.788874
I0925 11:12:49.963135  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.788874 (* 1 = 0.788874 loss)
I0925 11:12:49.963142  2600 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I0925 11:13:04.557845  2600 solver.cpp:218] Iteration 3800 (6.85182 iter/s, 14.5947s/100 iters), loss = 0.745218
I0925 11:13:04.557878  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.745218 (* 1 = 0.745218 loss)
I0925 11:13:04.557885  2600 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I0925 11:13:19.144614  2600 solver.cpp:218] Iteration 3900 (6.85556 iter/s, 14.5867s/100 iters), loss = 0.700398
I0925 11:13:19.144647  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.700398 (* 1 = 0.700398 loss)
I0925 11:13:19.144655  2600 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I0925 11:13:33.004132  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:13:33.587610  2600 solver.cpp:330] Iteration 4000, Testing net (#0)
I0925 11:13:37.011840  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:13:37.154266  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6447
I0925 11:13:37.154301  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.990775 (* 1 = 0.990775 loss)
I0925 11:13:37.299033  2600 solver.cpp:218] Iteration 4000 (5.50833 iter/s, 18.1543s/100 iters), loss = 0.79603
I0925 11:13:37.299060  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.79603 (* 1 = 0.79603 loss)
I0925 11:13:37.299067  2600 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I0925 11:13:51.889780  2600 solver.cpp:218] Iteration 4100 (6.85369 iter/s, 14.5907s/100 iters), loss = 0.611231
I0925 11:13:51.889809  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.611231 (* 1 = 0.611231 loss)
I0925 11:13:51.889816  2600 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I0925 11:14:06.479348  2600 solver.cpp:218] Iteration 4200 (6.85425 iter/s, 14.5895s/100 iters), loss = 0.748914
I0925 11:14:06.479424  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.748914 (* 1 = 0.748914 loss)
I0925 11:14:06.479432  2600 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I0925 11:14:21.078789  2600 solver.cpp:218] Iteration 4300 (6.84963 iter/s, 14.5993s/100 iters), loss = 0.692067
I0925 11:14:21.078824  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.692067 (* 1 = 0.692067 loss)
I0925 11:14:21.078832  2600 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I0925 11:14:35.667763  2600 solver.cpp:218] Iteration 4400 (6.85453 iter/s, 14.5889s/100 iters), loss = 0.704798
I0925 11:14:35.667794  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.704798 (* 1 = 0.704798 loss)
I0925 11:14:35.667800  2600 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I0925 11:14:49.530102  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:14:50.115453  2600 solver.cpp:330] Iteration 4500, Testing net (#0)
I0925 11:14:53.540444  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:14:53.682942  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6674
I0925 11:14:53.682970  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.912668 (* 1 = 0.912668 loss)
I0925 11:14:53.827571  2600 solver.cpp:218] Iteration 4500 (5.50669 iter/s, 18.1597s/100 iters), loss = 0.711626
I0925 11:14:53.827605  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.711626 (* 1 = 0.711626 loss)
I0925 11:14:53.827611  2600 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I0925 11:15:08.422811  2600 solver.cpp:218] Iteration 4600 (6.85159 iter/s, 14.5952s/100 iters), loss = 0.589669
I0925 11:15:08.422842  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.589669 (* 1 = 0.589669 loss)
I0925 11:15:08.422848  2600 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I0925 11:15:23.016361  2600 solver.cpp:218] Iteration 4700 (6.85238 iter/s, 14.5935s/100 iters), loss = 0.659437
I0925 11:15:23.016484  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.659437 (* 1 = 0.659437 loss)
I0925 11:15:23.016492  2600 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I0925 11:15:37.613046  2600 solver.cpp:218] Iteration 4800 (6.85095 iter/s, 14.5965s/100 iters), loss = 0.669785
I0925 11:15:37.613078  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.669785 (* 1 = 0.669785 loss)
I0925 11:15:37.613087  2600 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I0925 11:15:52.213357  2600 solver.cpp:218] Iteration 4900 (6.84921 iter/s, 14.6002s/100 iters), loss = 0.707653
I0925 11:15:52.213388  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.707653 (* 1 = 0.707653 loss)
I0925 11:15:52.213395  2600 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I0925 11:16:06.090694  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:16:06.675330  2600 solver.cpp:330] Iteration 5000, Testing net (#0)
I0925 11:16:10.099848  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:16:10.242584  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6522
I0925 11:16:10.242620  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.945663 (* 1 = 0.945663 loss)
I0925 11:16:10.386952  2600 solver.cpp:218] Iteration 5000 (5.50251 iter/s, 18.1735s/100 iters), loss = 0.6673
I0925 11:16:10.386981  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.6673 (* 1 = 0.6673 loss)
I0925 11:16:10.386987  2600 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I0925 11:16:24.978986  2600 solver.cpp:218] Iteration 5100 (6.85309 iter/s, 14.592s/100 iters), loss = 0.621422
I0925 11:16:24.979025  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.621422 (* 1 = 0.621422 loss)
I0925 11:16:24.979032  2600 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I0925 11:16:39.566934  2600 solver.cpp:218] Iteration 5200 (6.85501 iter/s, 14.5879s/100 iters), loss = 0.694092
I0925 11:16:39.567050  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.694092 (* 1 = 0.694092 loss)
I0925 11:16:39.567067  2600 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I0925 11:16:54.161578  2600 solver.cpp:218] Iteration 5300 (6.8519 iter/s, 14.5945s/100 iters), loss = 0.710365
I0925 11:16:54.161608  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.710365 (* 1 = 0.710365 loss)
I0925 11:16:54.161614  2600 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I0925 11:17:08.757498  2600 solver.cpp:218] Iteration 5400 (6.85127 iter/s, 14.5958s/100 iters), loss = 0.58854
I0925 11:17:08.757527  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.58854 (* 1 = 0.58854 loss)
I0925 11:17:08.757534  2600 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I0925 11:17:22.623922  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:17:23.208956  2600 solver.cpp:330] Iteration 5500, Testing net (#0)
I0925 11:17:26.632102  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:17:26.774971  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6942
I0925 11:17:26.774996  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.845363 (* 1 = 0.845363 loss)
I0925 11:17:26.920351  2600 solver.cpp:218] Iteration 5500 (5.50577 iter/s, 18.1628s/100 iters), loss = 0.653379
I0925 11:17:26.920382  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.653379 (* 1 = 0.653379 loss)
I0925 11:17:26.920388  2600 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I0925 11:17:41.530210  2600 solver.cpp:218] Iteration 5600 (6.84473 iter/s, 14.6098s/100 iters), loss = 0.502681
I0925 11:17:41.530241  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.502681 (* 1 = 0.502681 loss)
I0925 11:17:41.530246  2600 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I0925 11:17:56.138166  2600 solver.cpp:218] Iteration 5700 (6.84562 iter/s, 14.6079s/100 iters), loss = 0.568277
I0925 11:17:56.138295  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.568277 (* 1 = 0.568277 loss)
I0925 11:17:56.138303  2600 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I0925 11:18:10.750622  2600 solver.cpp:218] Iteration 5800 (6.84356 iter/s, 14.6123s/100 iters), loss = 0.651732
I0925 11:18:10.750653  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.651732 (* 1 = 0.651732 loss)
I0925 11:18:10.750660  2600 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I0925 11:18:25.358667  2600 solver.cpp:218] Iteration 5900 (6.84558 iter/s, 14.608s/100 iters), loss = 0.566687
I0925 11:18:25.358697  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.566687 (* 1 = 0.566687 loss)
I0925 11:18:25.358705  2600 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I0925 11:18:39.241269  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:18:39.826550  2600 solver.cpp:330] Iteration 6000, Testing net (#0)
I0925 11:18:43.250349  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:18:43.392932  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7212
I0925 11:18:43.392957  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.784878 (* 1 = 0.784878 loss)
I0925 11:18:43.537422  2600 solver.cpp:218] Iteration 6000 (5.50095 iter/s, 18.1787s/100 iters), loss = 0.774101
I0925 11:18:43.537451  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.774101 (* 1 = 0.774101 loss)
I0925 11:18:43.537457  2600 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I0925 11:18:58.125165  2600 solver.cpp:218] Iteration 6100 (6.8551 iter/s, 14.5877s/100 iters), loss = 0.471882
I0925 11:18:58.125195  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.471882 (* 1 = 0.471882 loss)
I0925 11:18:58.125200  2600 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I0925 11:19:12.718646  2600 solver.cpp:218] Iteration 6200 (6.85241 iter/s, 14.5934s/100 iters), loss = 0.618174
I0925 11:19:12.718791  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.618174 (* 1 = 0.618174 loss)
I0925 11:19:12.718799  2600 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I0925 11:19:27.306195  2600 solver.cpp:218] Iteration 6300 (6.85525 iter/s, 14.5874s/100 iters), loss = 0.595033
I0925 11:19:27.306236  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.595033 (* 1 = 0.595033 loss)
I0925 11:19:27.306241  2600 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I0925 11:19:41.887989  2600 solver.cpp:218] Iteration 6400 (6.85791 iter/s, 14.5817s/100 iters), loss = 0.575199
I0925 11:19:41.888025  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.575199 (* 1 = 0.575199 loss)
I0925 11:19:41.888031  2600 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I0925 11:19:55.745746  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:19:56.330456  2600 solver.cpp:330] Iteration 6500, Testing net (#0)
I0925 11:19:59.755120  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:19:59.898409  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7363
I0925 11:19:59.898434  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.725877 (* 1 = 0.725877 loss)
I0925 11:20:00.042399  2600 solver.cpp:218] Iteration 6500 (5.50833 iter/s, 18.1543s/100 iters), loss = 0.758308
I0925 11:20:00.042428  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.758308 (* 1 = 0.758308 loss)
I0925 11:20:00.042435  2600 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I0925 11:20:14.636376  2600 solver.cpp:218] Iteration 6600 (6.85218 iter/s, 14.5939s/100 iters), loss = 0.443578
I0925 11:20:14.636417  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.443578 (* 1 = 0.443578 loss)
I0925 11:20:14.636425  2600 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I0925 11:20:29.229490  2600 solver.cpp:218] Iteration 6700 (6.85259 iter/s, 14.593s/100 iters), loss = 0.613271
I0925 11:20:29.229631  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.613271 (* 1 = 0.613271 loss)
I0925 11:20:29.229640  2600 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I0925 11:20:43.829916  2600 solver.cpp:218] Iteration 6800 (6.8492 iter/s, 14.6002s/100 iters), loss = 0.612614
I0925 11:20:43.829952  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.612614 (* 1 = 0.612614 loss)
I0925 11:20:43.829959  2600 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I0925 11:20:58.570574  2600 solver.cpp:218] Iteration 6900 (6.78399 iter/s, 14.7406s/100 iters), loss = 0.525458
I0925 11:20:58.570603  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.525458 (* 1 = 0.525458 loss)
I0925 11:20:58.570611  2600 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I0925 11:21:12.485031  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:21:13.069028  2600 solver.cpp:330] Iteration 7000, Testing net (#0)
I0925 11:21:16.500432  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:21:16.642918  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.749
I0925 11:21:16.642953  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.724944 (* 1 = 0.724944 loss)
I0925 11:21:16.787362  2600 solver.cpp:218] Iteration 7000 (5.48947 iter/s, 18.2167s/100 iters), loss = 0.762149
I0925 11:21:16.787405  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.762149 (* 1 = 0.762149 loss)
I0925 11:21:16.787411  2600 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I0925 11:21:31.435101  2600 solver.cpp:218] Iteration 7100 (6.82703 iter/s, 14.6477s/100 iters), loss = 0.52502
I0925 11:21:31.435137  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.52502 (* 1 = 0.52502 loss)
I0925 11:21:31.435145  2600 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I0925 11:21:46.394927  2600 solver.cpp:218] Iteration 7200 (6.68461 iter/s, 14.9597s/100 iters), loss = 0.537751
I0925 11:21:46.395016  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.537751 (* 1 = 0.537751 loss)
I0925 11:21:46.395025  2600 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I0925 11:22:01.109566  2600 solver.cpp:218] Iteration 7300 (6.79601 iter/s, 14.7145s/100 iters), loss = 0.570268
I0925 11:22:01.109599  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.570268 (* 1 = 0.570268 loss)
I0925 11:22:01.109606  2600 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I0925 11:22:15.815807  2600 solver.cpp:218] Iteration 7400 (6.79987 iter/s, 14.7062s/100 iters), loss = 0.527712
I0925 11:22:15.815841  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.527712 (* 1 = 0.527712 loss)
I0925 11:22:15.815850  2600 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I0925 11:22:29.733732  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:22:30.317890  2600 solver.cpp:330] Iteration 7500, Testing net (#0)
I0925 11:22:33.765007  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:22:33.917459  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7531
I0925 11:22:33.917491  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.702593 (* 1 = 0.702593 loss)
I0925 11:22:34.067989  2600 solver.cpp:218] Iteration 7500 (5.47882 iter/s, 18.2521s/100 iters), loss = 0.593894
I0925 11:22:34.068023  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.593894 (* 1 = 0.593894 loss)
I0925 11:22:34.068030  2600 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I0925 11:22:48.847839  2600 solver.cpp:218] Iteration 7600 (6.76602 iter/s, 14.7797s/100 iters), loss = 0.45836
I0925 11:22:48.847889  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.45836 (* 1 = 0.45836 loss)
I0925 11:22:48.847898  2600 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I0925 11:23:03.536862  2600 solver.cpp:218] Iteration 7700 (6.80786 iter/s, 14.6889s/100 iters), loss = 0.54816
I0925 11:23:03.536988  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.54816 (* 1 = 0.54816 loss)
I0925 11:23:03.536995  2600 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I0925 11:23:18.372967  2600 solver.cpp:218] Iteration 7800 (6.74039 iter/s, 14.8359s/100 iters), loss = 0.520381
I0925 11:23:18.372999  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.520381 (* 1 = 0.520381 loss)
I0925 11:23:18.373006  2600 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I0925 11:23:32.965931  2600 solver.cpp:218] Iteration 7900 (6.85265 iter/s, 14.5929s/100 iters), loss = 0.51553
I0925 11:23:32.965960  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.51553 (* 1 = 0.51553 loss)
I0925 11:23:32.965967  2600 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I0925 11:23:46.837666  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:23:47.422544  2600 solver.cpp:330] Iteration 8000, Testing net (#0)
I0925 11:23:50.846290  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:23:50.989053  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7551
I0925 11:23:50.989089  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.703561 (* 1 = 0.703561 loss)
I0925 11:23:51.134011  2600 solver.cpp:218] Iteration 8000 (5.50418 iter/s, 18.168s/100 iters), loss = 0.597445
I0925 11:23:51.134042  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.597445 (* 1 = 0.597445 loss)
I0925 11:23:51.134049  2600 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I0925 11:24:05.739581  2600 solver.cpp:218] Iteration 8100 (6.84674 iter/s, 14.6055s/100 iters), loss = 0.496847
I0925 11:24:05.739622  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.496847 (* 1 = 0.496847 loss)
I0925 11:24:05.739629  2600 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I0925 11:24:20.344796  2600 solver.cpp:218] Iteration 8200 (6.84691 iter/s, 14.6051s/100 iters), loss = 0.578716
I0925 11:24:20.344874  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.578716 (* 1 = 0.578716 loss)
I0925 11:24:20.344882  2600 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I0925 11:24:35.067574  2600 solver.cpp:218] Iteration 8300 (6.79225 iter/s, 14.7227s/100 iters), loss = 0.519868
I0925 11:24:35.067620  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.519868 (* 1 = 0.519868 loss)
I0925 11:24:35.067627  2600 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I0925 11:24:50.041273  2600 solver.cpp:218] Iteration 8400 (6.67842 iter/s, 14.9736s/100 iters), loss = 0.528746
I0925 11:24:50.041313  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.528746 (* 1 = 0.528746 loss)
I0925 11:24:50.041322  2600 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I0925 11:25:04.371461  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:25:04.956826  2600 solver.cpp:330] Iteration 8500, Testing net (#0)
I0925 11:25:08.382094  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:25:08.524984  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7756
I0925 11:25:08.525010  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.63145 (* 1 = 0.63145 loss)
I0925 11:25:08.669211  2600 solver.cpp:218] Iteration 8500 (5.36831 iter/s, 18.6278s/100 iters), loss = 0.605673
I0925 11:25:08.669239  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.605673 (* 1 = 0.605673 loss)
I0925 11:25:08.669246  2600 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I0925 11:25:23.258903  2600 solver.cpp:218] Iteration 8600 (6.85419 iter/s, 14.5896s/100 iters), loss = 0.470018
I0925 11:25:23.258944  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.470018 (* 1 = 0.470018 loss)
I0925 11:25:23.258950  2600 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I0925 11:25:37.852314  2600 solver.cpp:218] Iteration 8700 (6.85245 iter/s, 14.5933s/100 iters), loss = 0.47192
I0925 11:25:37.852442  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.47192 (* 1 = 0.47192 loss)
I0925 11:25:37.852459  2600 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I0925 11:25:52.441350  2600 solver.cpp:218] Iteration 8800 (6.85454 iter/s, 14.5889s/100 iters), loss = 0.544558
I0925 11:25:52.441391  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.544558 (* 1 = 0.544558 loss)
I0925 11:25:52.441398  2600 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I0925 11:26:07.027614  2600 solver.cpp:218] Iteration 8900 (6.8558 iter/s, 14.5862s/100 iters), loss = 0.501137
I0925 11:26:07.027645  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.501137 (* 1 = 0.501137 loss)
I0925 11:26:07.027652  2600 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I0925 11:26:20.892469  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:26:21.476887  2600 solver.cpp:330] Iteration 9000, Testing net (#0)
I0925 11:26:24.901533  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:26:25.044009  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7685
I0925 11:26:25.044046  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.663627 (* 1 = 0.663627 loss)
I0925 11:26:25.188668  2600 solver.cpp:218] Iteration 9000 (5.50631 iter/s, 18.161s/100 iters), loss = 0.550002
I0925 11:26:25.188696  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.550002 (* 1 = 0.550002 loss)
I0925 11:26:25.188702  2600 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I0925 11:26:39.776690  2600 solver.cpp:218] Iteration 9100 (6.85497 iter/s, 14.588s/100 iters), loss = 0.48272
I0925 11:26:39.776721  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.48272 (* 1 = 0.48272 loss)
I0925 11:26:39.776726  2600 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I0925 11:26:54.369770  2600 solver.cpp:218] Iteration 9200 (6.8526 iter/s, 14.593s/100 iters), loss = 0.525143
I0925 11:26:54.369849  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.525143 (* 1 = 0.525143 loss)
I0925 11:26:54.369868  2600 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I0925 11:27:08.957695  2600 solver.cpp:218] Iteration 9300 (6.85504 iter/s, 14.5878s/100 iters), loss = 0.538892
I0925 11:27:08.957737  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.538892 (* 1 = 0.538892 loss)
I0925 11:27:08.957743  2600 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I0925 11:27:23.545470  2600 solver.cpp:218] Iteration 9400 (6.85509 iter/s, 14.5877s/100 iters), loss = 0.481575
I0925 11:27:23.545501  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.481575 (* 1 = 0.481575 loss)
I0925 11:27:23.545507  2600 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I0925 11:27:37.408118  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:27:37.991961  2600 solver.cpp:330] Iteration 9500, Testing net (#0)
I0925 11:27:41.414916  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:27:41.557737  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7816
I0925 11:27:41.557772  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.626776 (* 1 = 0.626776 loss)
I0925 11:27:41.702206  2600 solver.cpp:218] Iteration 9500 (5.50762 iter/s, 18.1567s/100 iters), loss = 0.480026
I0925 11:27:41.702232  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.480026 (* 1 = 0.480026 loss)
I0925 11:27:41.702239  2600 sgd_solver.cpp:105] Iteration 9500, lr = 0.01
I0925 11:27:56.295559  2600 solver.cpp:218] Iteration 9600 (6.85247 iter/s, 14.5933s/100 iters), loss = 0.495922
I0925 11:27:56.295589  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.495922 (* 1 = 0.495922 loss)
I0925 11:27:56.295595  2600 sgd_solver.cpp:105] Iteration 9600, lr = 0.01
I0925 11:28:10.896600  2600 solver.cpp:218] Iteration 9700 (6.84886 iter/s, 14.601s/100 iters), loss = 0.476987
I0925 11:28:10.896750  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.476987 (* 1 = 0.476987 loss)
I0925 11:28:10.896760  2600 sgd_solver.cpp:105] Iteration 9700, lr = 0.01
I0925 11:28:25.486403  2600 solver.cpp:218] Iteration 9800 (6.85419 iter/s, 14.5896s/100 iters), loss = 0.509555
I0925 11:28:25.486445  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.509555 (* 1 = 0.509555 loss)
I0925 11:28:25.486451  2600 sgd_solver.cpp:105] Iteration 9800, lr = 0.01
I0925 11:28:40.077520  2600 solver.cpp:218] Iteration 9900 (6.85352 iter/s, 14.591s/100 iters), loss = 0.453115
I0925 11:28:40.077561  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.453115 (* 1 = 0.453115 loss)
I0925 11:28:40.077567  2600 sgd_solver.cpp:105] Iteration 9900, lr = 0.01
I0925 11:28:53.951776  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:28:54.536857  2600 solver.cpp:330] Iteration 10000, Testing net (#0)
I0925 11:28:57.960283  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:28:58.102852  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7769
I0925 11:28:58.102877  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.635771 (* 1 = 0.635771 loss)
I0925 11:28:58.247674  2600 solver.cpp:218] Iteration 10000 (5.50356 iter/s, 18.1701s/100 iters), loss = 0.471823
I0925 11:28:58.247704  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.471823 (* 1 = 0.471823 loss)
I0925 11:28:58.247710  2600 sgd_solver.cpp:105] Iteration 10000, lr = 0.01
I0925 11:29:12.841009  2600 solver.cpp:218] Iteration 10100 (6.85248 iter/s, 14.5933s/100 iters), loss = 0.329409
I0925 11:29:12.841051  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329409 (* 1 = 0.329409 loss)
I0925 11:29:12.841058  2600 sgd_solver.cpp:105] Iteration 10100, lr = 0.01
I0925 11:29:27.431594  2600 solver.cpp:218] Iteration 10200 (6.85377 iter/s, 14.5905s/100 iters), loss = 0.499983
I0925 11:29:27.431674  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.499983 (* 1 = 0.499983 loss)
I0925 11:29:27.431682  2600 sgd_solver.cpp:105] Iteration 10200, lr = 0.01
I0925 11:29:42.022740  2600 solver.cpp:218] Iteration 10300 (6.85353 iter/s, 14.591s/100 iters), loss = 0.502756
I0925 11:29:42.022775  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.502756 (* 1 = 0.502756 loss)
I0925 11:29:42.022783  2600 sgd_solver.cpp:105] Iteration 10300, lr = 0.01
I0925 11:29:56.615859  2600 solver.cpp:218] Iteration 10400 (6.85258 iter/s, 14.593s/100 iters), loss = 0.456577
I0925 11:29:56.615890  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.456577 (* 1 = 0.456577 loss)
I0925 11:29:56.615897  2600 sgd_solver.cpp:105] Iteration 10400, lr = 0.01
I0925 11:30:10.480834  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:30:11.064760  2600 solver.cpp:330] Iteration 10500, Testing net (#0)
I0925 11:30:14.489543  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:30:14.632499  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7796
I0925 11:30:14.632534  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.631297 (* 1 = 0.631297 loss)
I0925 11:30:14.777037  2600 solver.cpp:218] Iteration 10500 (5.50627 iter/s, 18.1611s/100 iters), loss = 0.52055
I0925 11:30:14.777071  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.52055 (* 1 = 0.52055 loss)
I0925 11:30:14.777077  2600 sgd_solver.cpp:105] Iteration 10500, lr = 0.01
I0925 11:30:29.387338  2600 solver.cpp:218] Iteration 10600 (6.84452 iter/s, 14.6102s/100 iters), loss = 0.478263
I0925 11:30:29.387378  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.478263 (* 1 = 0.478263 loss)
I0925 11:30:29.387385  2600 sgd_solver.cpp:105] Iteration 10600, lr = 0.01
I0925 11:30:43.995344  2600 solver.cpp:218] Iteration 10700 (6.8456 iter/s, 14.6079s/100 iters), loss = 0.468339
I0925 11:30:43.995517  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.468339 (* 1 = 0.468339 loss)
I0925 11:30:43.995539  2600 sgd_solver.cpp:105] Iteration 10700, lr = 0.01
I0925 11:30:58.600322  2600 solver.cpp:218] Iteration 10800 (6.84708 iter/s, 14.6048s/100 iters), loss = 0.511714
I0925 11:30:58.600363  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.511714 (* 1 = 0.511714 loss)
I0925 11:30:58.600369  2600 sgd_solver.cpp:105] Iteration 10800, lr = 0.01
I0925 11:31:13.210798  2600 solver.cpp:218] Iteration 10900 (6.84444 iter/s, 14.6104s/100 iters), loss = 0.354747
I0925 11:31:13.210829  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.354747 (* 1 = 0.354747 loss)
I0925 11:31:13.210834  2600 sgd_solver.cpp:105] Iteration 10900, lr = 0.01
I0925 11:31:27.095485  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:31:27.680402  2600 solver.cpp:330] Iteration 11000, Testing net (#0)
I0925 11:31:31.104594  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:31:31.247597  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7965
I0925 11:31:31.247632  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.57747 (* 1 = 0.57747 loss)
I0925 11:31:31.392051  2600 solver.cpp:218] Iteration 11000 (5.50019 iter/s, 18.1812s/100 iters), loss = 0.54588
I0925 11:31:31.392078  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.54588 (* 1 = 0.54588 loss)
I0925 11:31:31.392084  2600 sgd_solver.cpp:105] Iteration 11000, lr = 0.01
I0925 11:31:45.975186  2600 solver.cpp:218] Iteration 11100 (6.85727 iter/s, 14.5831s/100 iters), loss = 0.446028
I0925 11:31:45.975215  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.446028 (* 1 = 0.446028 loss)
I0925 11:31:45.975221  2600 sgd_solver.cpp:105] Iteration 11100, lr = 0.01
I0925 11:32:00.555058  2600 solver.cpp:218] Iteration 11200 (6.8588 iter/s, 14.5798s/100 iters), loss = 0.48545
I0925 11:32:00.555156  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.48545 (* 1 = 0.48545 loss)
I0925 11:32:00.555172  2600 sgd_solver.cpp:105] Iteration 11200, lr = 0.01
I0925 11:32:15.136227  2600 solver.cpp:218] Iteration 11300 (6.85822 iter/s, 14.581s/100 iters), loss = 0.548599
I0925 11:32:15.136260  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.548599 (* 1 = 0.548599 loss)
I0925 11:32:15.136265  2600 sgd_solver.cpp:105] Iteration 11300, lr = 0.01
I0925 11:32:29.714978  2600 solver.cpp:218] Iteration 11400 (6.85933 iter/s, 14.5787s/100 iters), loss = 0.414078
I0925 11:32:29.715009  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.414078 (* 1 = 0.414078 loss)
I0925 11:32:29.715025  2600 sgd_solver.cpp:105] Iteration 11400, lr = 0.01
I0925 11:32:43.573148  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:32:44.156404  2600 solver.cpp:330] Iteration 11500, Testing net (#0)
I0925 11:32:47.581516  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:32:47.724303  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7753
I0925 11:32:47.724328  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.641842 (* 1 = 0.641842 loss)
I0925 11:32:47.869277  2600 solver.cpp:218] Iteration 11500 (5.50836 iter/s, 18.1542s/100 iters), loss = 0.415385
I0925 11:32:47.869305  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.415385 (* 1 = 0.415385 loss)
I0925 11:32:47.869313  2600 sgd_solver.cpp:105] Iteration 11500, lr = 0.01
I0925 11:33:02.463001  2600 solver.cpp:218] Iteration 11600 (6.85229 iter/s, 14.5937s/100 iters), loss = 0.476986
I0925 11:33:02.463042  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.476986 (* 1 = 0.476986 loss)
I0925 11:33:02.463047  2600 sgd_solver.cpp:105] Iteration 11600, lr = 0.01
I0925 11:33:17.054718  2600 solver.cpp:218] Iteration 11700 (6.85324 iter/s, 14.5916s/100 iters), loss = 0.488931
I0925 11:33:17.054854  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.488931 (* 1 = 0.488931 loss)
I0925 11:33:17.054863  2600 sgd_solver.cpp:105] Iteration 11700, lr = 0.01
I0925 11:33:31.653594  2600 solver.cpp:218] Iteration 11800 (6.84992 iter/s, 14.5987s/100 iters), loss = 0.509853
I0925 11:33:31.653635  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.509853 (* 1 = 0.509853 loss)
I0925 11:33:31.653641  2600 sgd_solver.cpp:105] Iteration 11800, lr = 0.01
I0925 11:33:46.245676  2600 solver.cpp:218] Iteration 11900 (6.85307 iter/s, 14.592s/100 iters), loss = 0.407366
I0925 11:33:46.245705  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.407366 (* 1 = 0.407366 loss)
I0925 11:33:46.245712  2600 sgd_solver.cpp:105] Iteration 11900, lr = 0.01
I0925 11:34:00.176692  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:34:00.765694  2600 solver.cpp:330] Iteration 12000, Testing net (#0)
I0925 11:34:04.209794  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:34:04.351757  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.805
I0925 11:34:04.351793  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.57174 (* 1 = 0.57174 loss)
I0925 11:34:04.495015  2600 solver.cpp:218] Iteration 12000 (5.47967 iter/s, 18.2493s/100 iters), loss = 0.411118
I0925 11:34:04.495046  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.411118 (* 1 = 0.411118 loss)
I0925 11:34:04.495054  2600 sgd_solver.cpp:105] Iteration 12000, lr = 0.01
I0925 11:34:19.234608  2600 solver.cpp:218] Iteration 12100 (6.78448 iter/s, 14.7395s/100 iters), loss = 0.411758
I0925 11:34:19.234640  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.411758 (* 1 = 0.411758 loss)
I0925 11:34:19.234647  2600 sgd_solver.cpp:105] Iteration 12100, lr = 0.01
I0925 11:34:33.872360  2600 solver.cpp:218] Iteration 12200 (6.83168 iter/s, 14.6377s/100 iters), loss = 0.415827
I0925 11:34:33.872464  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.415827 (* 1 = 0.415827 loss)
I0925 11:34:33.872473  2600 sgd_solver.cpp:105] Iteration 12200, lr = 0.01
I0925 11:34:48.525081  2600 solver.cpp:218] Iteration 12300 (6.82474 iter/s, 14.6526s/100 iters), loss = 0.534268
I0925 11:34:48.525112  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.534268 (* 1 = 0.534268 loss)
I0925 11:34:48.525118  2600 sgd_solver.cpp:105] Iteration 12300, lr = 0.01
I0925 11:35:03.133203  2600 solver.cpp:218] Iteration 12400 (6.84554 iter/s, 14.608s/100 iters), loss = 0.42576
I0925 11:35:03.133235  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.42576 (* 1 = 0.42576 loss)
I0925 11:35:03.133242  2600 sgd_solver.cpp:105] Iteration 12400, lr = 0.01
I0925 11:35:17.135399  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:35:17.720448  2600 solver.cpp:330] Iteration 12500, Testing net (#0)
I0925 11:35:21.149202  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:35:21.292157  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8035
I0925 11:35:21.292194  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.561065 (* 1 = 0.561065 loss)
I0925 11:35:21.436545  2600 solver.cpp:218] Iteration 12500 (5.46351 iter/s, 18.3033s/100 iters), loss = 0.420372
I0925 11:35:21.436574  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.420372 (* 1 = 0.420372 loss)
I0925 11:35:21.436581  2600 sgd_solver.cpp:105] Iteration 12500, lr = 0.01
I0925 11:35:36.055101  2600 solver.cpp:218] Iteration 12600 (6.84065 iter/s, 14.6185s/100 iters), loss = 0.394546
I0925 11:35:36.055133  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394546 (* 1 = 0.394546 loss)
I0925 11:35:36.055140  2600 sgd_solver.cpp:105] Iteration 12600, lr = 0.01
I0925 11:35:50.726891  2600 solver.cpp:218] Iteration 12700 (6.81583 iter/s, 14.6717s/100 iters), loss = 0.397449
I0925 11:35:50.727067  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.397449 (* 1 = 0.397449 loss)
I0925 11:35:50.727077  2600 sgd_solver.cpp:105] Iteration 12700, lr = 0.01
I0925 11:36:05.333652  2600 solver.cpp:218] Iteration 12800 (6.84624 iter/s, 14.6066s/100 iters), loss = 0.462232
I0925 11:36:05.333693  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.462232 (* 1 = 0.462232 loss)
I0925 11:36:05.333699  2600 sgd_solver.cpp:105] Iteration 12800, lr = 0.01
I0925 11:36:19.937383  2600 solver.cpp:218] Iteration 12900 (6.8476 iter/s, 14.6037s/100 iters), loss = 0.370869
I0925 11:36:19.937414  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.370869 (* 1 = 0.370869 loss)
I0925 11:36:19.937420  2600 sgd_solver.cpp:105] Iteration 12900, lr = 0.01
I0925 11:36:33.818574  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:36:34.403223  2600 solver.cpp:330] Iteration 13000, Testing net (#0)
I0925 11:36:37.831022  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:36:37.973430  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7804
I0925 11:36:37.973456  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.62417 (* 1 = 0.62417 loss)
I0925 11:36:38.118528  2600 solver.cpp:218] Iteration 13000 (5.50023 iter/s, 18.1811s/100 iters), loss = 0.420418
I0925 11:36:38.118558  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.420418 (* 1 = 0.420418 loss)
I0925 11:36:38.118566  2600 sgd_solver.cpp:105] Iteration 13000, lr = 0.01
I0925 11:36:52.726589  2600 solver.cpp:218] Iteration 13100 (6.84557 iter/s, 14.608s/100 iters), loss = 0.390536
I0925 11:36:52.726630  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390536 (* 1 = 0.390536 loss)
I0925 11:36:52.726636  2600 sgd_solver.cpp:105] Iteration 13100, lr = 0.01
I0925 11:37:07.413231  2600 solver.cpp:218] Iteration 13200 (6.80895 iter/s, 14.6866s/100 iters), loss = 0.409647
I0925 11:37:07.413355  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.409647 (* 1 = 0.409647 loss)
I0925 11:37:07.413367  2600 sgd_solver.cpp:105] Iteration 13200, lr = 0.01
I0925 11:37:22.073824  2600 solver.cpp:218] Iteration 13300 (6.82108 iter/s, 14.6604s/100 iters), loss = 0.494063
I0925 11:37:22.073855  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.494063 (* 1 = 0.494063 loss)
I0925 11:37:22.073861  2600 sgd_solver.cpp:105] Iteration 13300, lr = 0.01
I0925 11:37:36.685436  2600 solver.cpp:218] Iteration 13400 (6.84391 iter/s, 14.6115s/100 iters), loss = 0.364839
I0925 11:37:36.685465  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364839 (* 1 = 0.364839 loss)
I0925 11:37:36.685472  2600 sgd_solver.cpp:105] Iteration 13400, lr = 0.01
I0925 11:37:50.574811  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:37:51.158633  2600 solver.cpp:330] Iteration 13500, Testing net (#0)
I0925 11:37:54.587891  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:37:54.730891  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7929
I0925 11:37:54.730926  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.601459 (* 1 = 0.601459 loss)
I0925 11:37:54.876231  2600 solver.cpp:218] Iteration 13500 (5.49731 iter/s, 18.1907s/100 iters), loss = 0.394895
I0925 11:37:54.876261  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394895 (* 1 = 0.394895 loss)
I0925 11:37:54.876268  2600 sgd_solver.cpp:105] Iteration 13500, lr = 0.01
I0925 11:38:09.472957  2600 solver.cpp:218] Iteration 13600 (6.85088 iter/s, 14.5967s/100 iters), loss = 0.405334
I0925 11:38:09.472998  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.405334 (* 1 = 0.405334 loss)
I0925 11:38:09.473004  2600 sgd_solver.cpp:105] Iteration 13600, lr = 0.01
I0925 11:38:24.071303  2600 solver.cpp:218] Iteration 13700 (6.85013 iter/s, 14.5983s/100 iters), loss = 0.415558
I0925 11:38:24.071435  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.415558 (* 1 = 0.415558 loss)
I0925 11:38:24.071444  2600 sgd_solver.cpp:105] Iteration 13700, lr = 0.01
I0925 11:38:38.662981  2600 solver.cpp:218] Iteration 13800 (6.8533 iter/s, 14.5915s/100 iters), loss = 0.475951
I0925 11:38:38.663022  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.475951 (* 1 = 0.475951 loss)
I0925 11:38:38.663028  2600 sgd_solver.cpp:105] Iteration 13800, lr = 0.01
I0925 11:38:53.260120  2600 solver.cpp:218] Iteration 13900 (6.8507 iter/s, 14.5971s/100 iters), loss = 0.455923
I0925 11:38:53.260160  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.455923 (* 1 = 0.455923 loss)
I0925 11:38:53.260166  2600 sgd_solver.cpp:105] Iteration 13900, lr = 0.01
I0925 11:39:07.134613  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:39:07.719044  2600 solver.cpp:330] Iteration 14000, Testing net (#0)
I0925 11:39:11.146633  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:39:11.289532  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.79
I0925 11:39:11.289568  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.60054 (* 1 = 0.60054 loss)
I0925 11:39:11.434695  2600 solver.cpp:218] Iteration 14000 (5.50222 iter/s, 18.1745s/100 iters), loss = 0.413535
I0925 11:39:11.434725  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.413535 (* 1 = 0.413535 loss)
I0925 11:39:11.434731  2600 sgd_solver.cpp:105] Iteration 14000, lr = 0.01
I0925 11:39:26.038610  2600 solver.cpp:218] Iteration 14100 (6.84751 iter/s, 14.6038s/100 iters), loss = 0.389552
I0925 11:39:26.038650  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.389552 (* 1 = 0.389552 loss)
I0925 11:39:26.038657  2600 sgd_solver.cpp:105] Iteration 14100, lr = 0.01
I0925 11:39:40.639534  2600 solver.cpp:218] Iteration 14200 (6.84892 iter/s, 14.6008s/100 iters), loss = 0.404581
I0925 11:39:40.639650  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.404581 (* 1 = 0.404581 loss)
I0925 11:39:40.639667  2600 sgd_solver.cpp:105] Iteration 14200, lr = 0.01
I0925 11:39:55.236919  2600 solver.cpp:218] Iteration 14300 (6.85061 iter/s, 14.5972s/100 iters), loss = 0.42769
I0925 11:39:55.236949  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.42769 (* 1 = 0.42769 loss)
I0925 11:39:55.236955  2600 sgd_solver.cpp:105] Iteration 14300, lr = 0.01
I0925 11:40:09.836087  2600 solver.cpp:218] Iteration 14400 (6.84974 iter/s, 14.5991s/100 iters), loss = 0.409546
I0925 11:40:09.836119  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.409546 (* 1 = 0.409546 loss)
I0925 11:40:09.836127  2600 sgd_solver.cpp:105] Iteration 14400, lr = 0.01
I0925 11:40:23.715577  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:40:24.299726  2600 solver.cpp:330] Iteration 14500, Testing net (#0)
I0925 11:40:27.726871  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:40:27.869642  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.783
I0925 11:40:27.869668  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.638183 (* 1 = 0.638183 loss)
I0925 11:40:28.014574  2600 solver.cpp:218] Iteration 14500 (5.50103 iter/s, 18.1784s/100 iters), loss = 0.401679
I0925 11:40:28.014603  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.401679 (* 1 = 0.401679 loss)
I0925 11:40:28.014611  2600 sgd_solver.cpp:105] Iteration 14500, lr = 0.01
I0925 11:40:42.611237  2600 solver.cpp:218] Iteration 14600 (6.85091 iter/s, 14.5966s/100 iters), loss = 0.44601
I0925 11:40:42.611268  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44601 (* 1 = 0.44601 loss)
I0925 11:40:42.611284  2600 sgd_solver.cpp:105] Iteration 14600, lr = 0.01
I0925 11:40:57.200525  2600 solver.cpp:218] Iteration 14700 (6.85438 iter/s, 14.5892s/100 iters), loss = 0.386065
I0925 11:40:57.200682  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.386065 (* 1 = 0.386065 loss)
I0925 11:40:57.200691  2600 sgd_solver.cpp:105] Iteration 14700, lr = 0.01
I0925 11:41:11.799185  2600 solver.cpp:218] Iteration 14800 (6.85004 iter/s, 14.5985s/100 iters), loss = 0.490712
I0925 11:41:11.799213  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.490712 (* 1 = 0.490712 loss)
I0925 11:41:11.799219  2600 sgd_solver.cpp:105] Iteration 14800, lr = 0.01
I0925 11:41:26.398315  2600 solver.cpp:218] Iteration 14900 (6.84975 iter/s, 14.5991s/100 iters), loss = 0.253303
I0925 11:41:26.398345  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253303 (* 1 = 0.253303 loss)
I0925 11:41:26.398351  2600 sgd_solver.cpp:105] Iteration 14900, lr = 0.01
I0925 11:41:40.268923  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:41:40.852447  2600 solver.cpp:330] Iteration 15000, Testing net (#0)
I0925 11:41:44.281509  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:41:44.423964  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8069
I0925 11:41:44.424000  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.561893 (* 1 = 0.561893 loss)
I0925 11:41:44.569217  2600 solver.cpp:218] Iteration 15000 (5.50333 iter/s, 18.1708s/100 iters), loss = 0.356191
I0925 11:41:44.569247  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.356191 (* 1 = 0.356191 loss)
I0925 11:41:44.569253  2600 sgd_solver.cpp:105] Iteration 15000, lr = 0.01
I0925 11:41:59.174156  2600 solver.cpp:218] Iteration 15100 (6.84703 iter/s, 14.6049s/100 iters), loss = 0.371099
I0925 11:41:59.174186  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.371099 (* 1 = 0.371099 loss)
I0925 11:41:59.174192  2600 sgd_solver.cpp:105] Iteration 15100, lr = 0.01
I0925 11:42:13.778190  2600 solver.cpp:218] Iteration 15200 (6.84746 iter/s, 14.604s/100 iters), loss = 0.434192
I0925 11:42:13.778306  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.434192 (* 1 = 0.434192 loss)
I0925 11:42:13.778312  2600 sgd_solver.cpp:105] Iteration 15200, lr = 0.01
I0925 11:42:28.375234  2600 solver.cpp:218] Iteration 15300 (6.85077 iter/s, 14.5969s/100 iters), loss = 0.372649
I0925 11:42:28.375263  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372649 (* 1 = 0.372649 loss)
I0925 11:42:28.375269  2600 sgd_solver.cpp:105] Iteration 15300, lr = 0.01
I0925 11:42:42.975528  2600 solver.cpp:218] Iteration 15400 (6.84921 iter/s, 14.6002s/100 iters), loss = 0.324189
I0925 11:42:42.975558  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.324189 (* 1 = 0.324189 loss)
I0925 11:42:42.975564  2600 sgd_solver.cpp:105] Iteration 15400, lr = 0.01
I0925 11:42:56.844540  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:42:57.428951  2600 solver.cpp:330] Iteration 15500, Testing net (#0)
I0925 11:43:00.856768  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:43:00.998941  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8142
I0925 11:43:00.998977  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.540644 (* 1 = 0.540644 loss)
I0925 11:43:01.143597  2600 solver.cpp:218] Iteration 15500 (5.50419 iter/s, 18.168s/100 iters), loss = 0.345445
I0925 11:43:01.143628  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.345445 (* 1 = 0.345445 loss)
I0925 11:43:01.143635  2600 sgd_solver.cpp:105] Iteration 15500, lr = 0.01
I0925 11:43:15.743957  2600 solver.cpp:218] Iteration 15600 (6.84918 iter/s, 14.6003s/100 iters), loss = 0.315597
I0925 11:43:15.743998  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.315597 (* 1 = 0.315597 loss)
I0925 11:43:15.744004  2600 sgd_solver.cpp:105] Iteration 15600, lr = 0.01
I0925 11:43:30.347244  2600 solver.cpp:218] Iteration 15700 (6.84781 iter/s, 14.6032s/100 iters), loss = 0.424874
I0925 11:43:30.347389  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.424874 (* 1 = 0.424874 loss)
I0925 11:43:30.347398  2600 sgd_solver.cpp:105] Iteration 15700, lr = 0.01
I0925 11:43:44.955077  2600 solver.cpp:218] Iteration 15800 (6.84573 iter/s, 14.6077s/100 iters), loss = 0.455525
I0925 11:43:44.955108  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.455525 (* 1 = 0.455525 loss)
I0925 11:43:44.955114  2600 sgd_solver.cpp:105] Iteration 15800, lr = 0.01
I0925 11:43:59.874570  2600 solver.cpp:218] Iteration 15900 (6.70267 iter/s, 14.9194s/100 iters), loss = 0.413676
I0925 11:43:59.874605  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.413676 (* 1 = 0.413676 loss)
I0925 11:43:59.874622  2600 sgd_solver.cpp:105] Iteration 15900, lr = 0.01
I0925 11:44:13.803100  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:44:14.389397  2600 solver.cpp:330] Iteration 16000, Testing net (#0)
I0925 11:44:17.859527  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:44:18.002610  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8145
I0925 11:44:18.002645  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535742 (* 1 = 0.535742 loss)
I0925 11:44:18.147104  2600 solver.cpp:218] Iteration 16000 (5.47272 iter/s, 18.2724s/100 iters), loss = 0.366719
I0925 11:44:18.147140  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.366719 (* 1 = 0.366719 loss)
I0925 11:44:18.147147  2600 sgd_solver.cpp:105] Iteration 16000, lr = 0.01
I0925 11:44:32.795215  2600 solver.cpp:218] Iteration 16100 (6.82685 iter/s, 14.648s/100 iters), loss = 0.35722
I0925 11:44:32.795246  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.35722 (* 1 = 0.35722 loss)
I0925 11:44:32.795253  2600 sgd_solver.cpp:105] Iteration 16100, lr = 0.01
I0925 11:44:47.401767  2600 solver.cpp:218] Iteration 16200 (6.84628 iter/s, 14.6065s/100 iters), loss = 0.370155
I0925 11:44:47.401882  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.370155 (* 1 = 0.370155 loss)
I0925 11:44:47.401899  2600 sgd_solver.cpp:105] Iteration 16200, lr = 0.01
I0925 11:45:02.017354  2600 solver.cpp:218] Iteration 16300 (6.84208 iter/s, 14.6154s/100 iters), loss = 0.393898
I0925 11:45:02.017387  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.393898 (* 1 = 0.393898 loss)
I0925 11:45:02.017393  2600 sgd_solver.cpp:105] Iteration 16300, lr = 0.01
I0925 11:45:16.640856  2600 solver.cpp:218] Iteration 16400 (6.83834 iter/s, 14.6234s/100 iters), loss = 0.307298
I0925 11:45:16.640887  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.307298 (* 1 = 0.307298 loss)
I0925 11:45:16.640892  2600 sgd_solver.cpp:105] Iteration 16400, lr = 0.01
I0925 11:45:30.524624  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:45:31.107615  2600 solver.cpp:330] Iteration 16500, Testing net (#0)
I0925 11:45:34.544879  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:45:34.692510  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8168
I0925 11:45:34.692545  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.533778 (* 1 = 0.533778 loss)
I0925 11:45:34.838430  2600 solver.cpp:218] Iteration 16500 (5.49526 iter/s, 18.1975s/100 iters), loss = 0.407705
I0925 11:45:34.838465  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.407705 (* 1 = 0.407705 loss)
I0925 11:45:34.838472  2600 sgd_solver.cpp:105] Iteration 16500, lr = 0.01
I0925 11:45:49.486153  2600 solver.cpp:218] Iteration 16600 (6.82703 iter/s, 14.6476s/100 iters), loss = 0.403369
I0925 11:45:49.486186  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.403369 (* 1 = 0.403369 loss)
I0925 11:45:49.486192  2600 sgd_solver.cpp:105] Iteration 16600, lr = 0.01
I0925 11:46:04.156033  2600 solver.cpp:218] Iteration 16700 (6.81672 iter/s, 14.6698s/100 iters), loss = 0.351166
I0925 11:46:04.156183  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.351166 (* 1 = 0.351166 loss)
I0925 11:46:04.156200  2600 sgd_solver.cpp:105] Iteration 16700, lr = 0.01
I0925 11:46:18.828346  2600 solver.cpp:218] Iteration 16800 (6.81564 iter/s, 14.6721s/100 iters), loss = 0.431568
I0925 11:46:18.828375  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.431568 (* 1 = 0.431568 loss)
I0925 11:46:18.828382  2600 sgd_solver.cpp:105] Iteration 16800, lr = 0.01
I0925 11:46:33.580492  2600 solver.cpp:218] Iteration 16900 (6.77871 iter/s, 14.7521s/100 iters), loss = 0.349432
I0925 11:46:33.580523  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.349432 (* 1 = 0.349432 loss)
I0925 11:46:33.580529  2600 sgd_solver.cpp:105] Iteration 16900, lr = 0.01
I0925 11:46:47.613164  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:46:48.203745  2600 solver.cpp:330] Iteration 17000, Testing net (#0)
I0925 11:46:51.646785  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:46:51.789850  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8202
I0925 11:46:51.789876  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.519218 (* 1 = 0.519218 loss)
I0925 11:46:51.934692  2600 solver.cpp:218] Iteration 17000 (5.44837 iter/s, 18.3541s/100 iters), loss = 0.36804
I0925 11:46:51.934722  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.36804 (* 1 = 0.36804 loss)
I0925 11:46:51.934729  2600 sgd_solver.cpp:105] Iteration 17000, lr = 0.01
I0925 11:47:06.651891  2600 solver.cpp:218] Iteration 17100 (6.7948 iter/s, 14.7171s/100 iters), loss = 0.376519
I0925 11:47:06.651922  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376519 (* 1 = 0.376519 loss)
I0925 11:47:06.651928  2600 sgd_solver.cpp:105] Iteration 17100, lr = 0.01
I0925 11:47:21.423290  2600 solver.cpp:218] Iteration 17200 (6.76987 iter/s, 14.7713s/100 iters), loss = 0.303816
I0925 11:47:21.423373  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303816 (* 1 = 0.303816 loss)
I0925 11:47:21.423390  2600 sgd_solver.cpp:105] Iteration 17200, lr = 0.01
I0925 11:47:36.227504  2600 solver.cpp:218] Iteration 17300 (6.75488 iter/s, 14.8041s/100 iters), loss = 0.369491
I0925 11:47:36.227537  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.369491 (* 1 = 0.369491 loss)
I0925 11:47:36.227545  2600 sgd_solver.cpp:105] Iteration 17300, lr = 0.01
I0925 11:47:50.963224  2600 solver.cpp:218] Iteration 17400 (6.78626 iter/s, 14.7356s/100 iters), loss = 0.298444
I0925 11:47:50.963253  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.298444 (* 1 = 0.298444 loss)
I0925 11:47:50.963269  2600 sgd_solver.cpp:105] Iteration 17400, lr = 0.01
I0925 11:48:04.923058  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:48:05.508497  2600 solver.cpp:330] Iteration 17500, Testing net (#0)
I0925 11:48:08.935804  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:48:09.079170  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8182
I0925 11:48:09.079205  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.529599 (* 1 = 0.529599 loss)
I0925 11:48:09.223546  2600 solver.cpp:218] Iteration 17500 (5.47638 iter/s, 18.2602s/100 iters), loss = 0.335463
I0925 11:48:09.223580  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.335463 (* 1 = 0.335463 loss)
I0925 11:48:09.223587  2600 sgd_solver.cpp:105] Iteration 17500, lr = 0.01
I0925 11:48:23.816339  2600 solver.cpp:218] Iteration 17600 (6.85273 iter/s, 14.5927s/100 iters), loss = 0.308127
I0925 11:48:23.816378  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.308127 (* 1 = 0.308127 loss)
I0925 11:48:23.816385  2600 sgd_solver.cpp:105] Iteration 17600, lr = 0.01
I0925 11:48:38.417485  2600 solver.cpp:218] Iteration 17700 (6.84881 iter/s, 14.6011s/100 iters), loss = 0.359149
I0925 11:48:38.417589  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.359149 (* 1 = 0.359149 loss)
I0925 11:48:38.417599  2600 sgd_solver.cpp:105] Iteration 17700, lr = 0.01
I0925 11:48:53.020058  2600 solver.cpp:218] Iteration 17800 (6.84817 iter/s, 14.6024s/100 iters), loss = 0.399622
I0925 11:48:53.020099  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.399622 (* 1 = 0.399622 loss)
I0925 11:48:53.020105  2600 sgd_solver.cpp:105] Iteration 17800, lr = 0.01
I0925 11:49:07.621757  2600 solver.cpp:218] Iteration 17900 (6.84856 iter/s, 14.6016s/100 iters), loss = 0.366515
I0925 11:49:07.621786  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.366515 (* 1 = 0.366515 loss)
I0925 11:49:07.621793  2600 sgd_solver.cpp:105] Iteration 17900, lr = 0.01
I0925 11:49:21.500525  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:49:22.083914  2600 solver.cpp:330] Iteration 18000, Testing net (#0)
I0925 11:49:25.512830  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:49:25.655458  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8288
I0925 11:49:25.655494  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.494523 (* 1 = 0.494523 loss)
I0925 11:49:25.800755  2600 solver.cpp:218] Iteration 18000 (5.50088 iter/s, 18.1789s/100 iters), loss = 0.350901
I0925 11:49:25.800784  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.350901 (* 1 = 0.350901 loss)
I0925 11:49:25.800791  2600 sgd_solver.cpp:105] Iteration 18000, lr = 0.01
I0925 11:49:40.392138  2600 solver.cpp:218] Iteration 18100 (6.85339 iter/s, 14.5913s/100 iters), loss = 0.330603
I0925 11:49:40.392179  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.330603 (* 1 = 0.330603 loss)
I0925 11:49:40.392185  2600 sgd_solver.cpp:105] Iteration 18100, lr = 0.01
I0925 11:49:54.989270  2600 solver.cpp:218] Iteration 18200 (6.8507 iter/s, 14.5971s/100 iters), loss = 0.36844
I0925 11:49:54.989415  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.36844 (* 1 = 0.36844 loss)
I0925 11:49:54.989424  2600 sgd_solver.cpp:105] Iteration 18200, lr = 0.01
I0925 11:50:09.584499  2600 solver.cpp:218] Iteration 18300 (6.85164 iter/s, 14.595s/100 iters), loss = 0.438135
I0925 11:50:09.584530  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.438135 (* 1 = 0.438135 loss)
I0925 11:50:09.584537  2600 sgd_solver.cpp:105] Iteration 18300, lr = 0.01
I0925 11:50:24.184975  2600 solver.cpp:218] Iteration 18400 (6.84912 iter/s, 14.6004s/100 iters), loss = 0.348144
I0925 11:50:24.185006  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348144 (* 1 = 0.348144 loss)
I0925 11:50:24.185012  2600 sgd_solver.cpp:105] Iteration 18400, lr = 0.01
I0925 11:50:38.056852  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:50:38.640086  2600 solver.cpp:330] Iteration 18500, Testing net (#0)
I0925 11:50:42.066861  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:50:42.210155  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8244
I0925 11:50:42.210191  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.519049 (* 1 = 0.519049 loss)
I0925 11:50:42.354465  2600 solver.cpp:218] Iteration 18500 (5.50376 iter/s, 18.1694s/100 iters), loss = 0.326793
I0925 11:50:42.354527  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.326793 (* 1 = 0.326793 loss)
I0925 11:50:42.354534  2600 sgd_solver.cpp:105] Iteration 18500, lr = 0.01
I0925 11:50:56.946456  2600 solver.cpp:218] Iteration 18600 (6.85312 iter/s, 14.5919s/100 iters), loss = 0.313501
I0925 11:50:56.946497  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313501 (* 1 = 0.313501 loss)
I0925 11:50:56.946504  2600 sgd_solver.cpp:105] Iteration 18600, lr = 0.01
I0925 11:51:11.541321  2600 solver.cpp:218] Iteration 18700 (6.85176 iter/s, 14.5948s/100 iters), loss = 0.367722
I0925 11:51:11.541416  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.367722 (* 1 = 0.367722 loss)
I0925 11:51:11.541434  2600 sgd_solver.cpp:105] Iteration 18700, lr = 0.01
I0925 11:51:26.125742  2600 solver.cpp:218] Iteration 18800 (6.85669 iter/s, 14.5843s/100 iters), loss = 0.425387
I0925 11:51:26.125771  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.425387 (* 1 = 0.425387 loss)
I0925 11:51:26.125777  2600 sgd_solver.cpp:105] Iteration 18800, lr = 0.01
I0925 11:51:40.720736  2600 solver.cpp:218] Iteration 18900 (6.8517 iter/s, 14.5949s/100 iters), loss = 0.339913
I0925 11:51:40.720767  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339913 (* 1 = 0.339913 loss)
I0925 11:51:40.720783  2600 sgd_solver.cpp:105] Iteration 18900, lr = 0.01
I0925 11:51:54.590353  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:51:55.174507  2600 solver.cpp:330] Iteration 19000, Testing net (#0)
I0925 11:51:58.602077  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:51:58.744905  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8161
I0925 11:51:58.744940  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.528707 (* 1 = 0.528707 loss)
I0925 11:51:58.890174  2600 solver.cpp:218] Iteration 19000 (5.50377 iter/s, 18.1694s/100 iters), loss = 0.393097
I0925 11:51:58.890204  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.393096 (* 1 = 0.393096 loss)
I0925 11:51:58.890210  2600 sgd_solver.cpp:105] Iteration 19000, lr = 0.01
I0925 11:52:13.488504  2600 solver.cpp:218] Iteration 19100 (6.85013 iter/s, 14.5983s/100 iters), loss = 0.283481
I0925 11:52:13.488546  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283481 (* 1 = 0.283481 loss)
I0925 11:52:13.488553  2600 sgd_solver.cpp:105] Iteration 19100, lr = 0.01
I0925 11:52:28.094581  2600 solver.cpp:218] Iteration 19200 (6.8465 iter/s, 14.606s/100 iters), loss = 0.386384
I0925 11:52:28.094697  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.386384 (* 1 = 0.386384 loss)
I0925 11:52:28.094715  2600 sgd_solver.cpp:105] Iteration 19200, lr = 0.01
I0925 11:52:42.693377  2600 solver.cpp:218] Iteration 19300 (6.84995 iter/s, 14.5986s/100 iters), loss = 0.414996
I0925 11:52:42.693418  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.414996 (* 1 = 0.414996 loss)
I0925 11:52:42.693424  2600 sgd_solver.cpp:105] Iteration 19300, lr = 0.01
I0925 11:52:57.290977  2600 solver.cpp:218] Iteration 19400 (6.85048 iter/s, 14.5975s/100 iters), loss = 0.303575
I0925 11:52:57.291007  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303575 (* 1 = 0.303575 loss)
I0925 11:52:57.291013  2600 sgd_solver.cpp:105] Iteration 19400, lr = 0.01
I0925 11:53:11.166036  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:53:11.749065  2600 solver.cpp:330] Iteration 19500, Testing net (#0)
I0925 11:53:15.176236  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:53:15.318964  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8013
I0925 11:53:15.319000  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.593178 (* 1 = 0.593178 loss)
I0925 11:53:15.463989  2600 solver.cpp:218] Iteration 19500 (5.50269 iter/s, 18.1729s/100 iters), loss = 0.353414
I0925 11:53:15.464022  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353414 (* 1 = 0.353414 loss)
I0925 11:53:15.464030  2600 sgd_solver.cpp:105] Iteration 19500, lr = 0.01
I0925 11:53:30.058881  2600 solver.cpp:218] Iteration 19600 (6.85175 iter/s, 14.5948s/100 iters), loss = 0.344871
I0925 11:53:30.058912  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.344871 (* 1 = 0.344871 loss)
I0925 11:53:30.058917  2600 sgd_solver.cpp:105] Iteration 19600, lr = 0.01
I0925 11:53:44.658416  2600 solver.cpp:218] Iteration 19700 (6.84957 iter/s, 14.5995s/100 iters), loss = 0.395856
I0925 11:53:44.658526  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.395856 (* 1 = 0.395856 loss)
I0925 11:53:44.658534  2600 sgd_solver.cpp:105] Iteration 19700, lr = 0.01
I0925 11:53:59.264755  2600 solver.cpp:218] Iteration 19800 (6.84641 iter/s, 14.6062s/100 iters), loss = 0.368678
I0925 11:53:59.264783  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.368678 (* 1 = 0.368678 loss)
I0925 11:53:59.264789  2600 sgd_solver.cpp:105] Iteration 19800, lr = 0.01
I0925 11:54:13.858574  2600 solver.cpp:218] Iteration 19900 (6.85225 iter/s, 14.5938s/100 iters), loss = 0.311638
I0925 11:54:13.858604  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.311638 (* 1 = 0.311638 loss)
I0925 11:54:13.858610  2600 sgd_solver.cpp:105] Iteration 19900, lr = 0.01
I0925 11:54:27.732815  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:54:28.317373  2600 solver.cpp:330] Iteration 20000, Testing net (#0)
I0925 11:54:31.745952  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:54:31.888638  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8248
I0925 11:54:31.888674  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.517613 (* 1 = 0.517613 loss)
I0925 11:54:32.033665  2600 solver.cpp:218] Iteration 20000 (5.50206 iter/s, 18.175s/100 iters), loss = 0.283033
I0925 11:54:32.033694  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283032 (* 1 = 0.283032 loss)
I0925 11:54:32.033701  2600 sgd_solver.cpp:105] Iteration 20000, lr = 0.01
I0925 11:54:46.630676  2600 solver.cpp:218] Iteration 20100 (6.85075 iter/s, 14.5969s/100 iters), loss = 0.33454
I0925 11:54:46.630717  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.33454 (* 1 = 0.33454 loss)
I0925 11:54:46.630723  2600 sgd_solver.cpp:105] Iteration 20100, lr = 0.01
I0925 11:55:01.231021  2600 solver.cpp:218] Iteration 20200 (6.84919 iter/s, 14.6003s/100 iters), loss = 0.396713
I0925 11:55:01.231166  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.396713 (* 1 = 0.396713 loss)
I0925 11:55:01.231173  2600 sgd_solver.cpp:105] Iteration 20200, lr = 0.01
I0925 11:55:15.831348  2600 solver.cpp:218] Iteration 20300 (6.84925 iter/s, 14.6001s/100 iters), loss = 0.358381
I0925 11:55:15.831378  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.358381 (* 1 = 0.358381 loss)
I0925 11:55:15.831384  2600 sgd_solver.cpp:105] Iteration 20300, lr = 0.01
I0925 11:55:30.433351  2600 solver.cpp:218] Iteration 20400 (6.84841 iter/s, 14.6019s/100 iters), loss = 0.324526
I0925 11:55:30.433382  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.324526 (* 1 = 0.324526 loss)
I0925 11:55:30.433388  2600 sgd_solver.cpp:105] Iteration 20400, lr = 0.01
I0925 11:55:44.311271  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:55:44.894682  2600 solver.cpp:330] Iteration 20500, Testing net (#0)
I0925 11:55:48.321746  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:55:48.465085  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.812
I0925 11:55:48.465122  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.54285 (* 1 = 0.54285 loss)
I0925 11:55:48.609750  2600 solver.cpp:218] Iteration 20500 (5.50166 iter/s, 18.1763s/100 iters), loss = 0.302096
I0925 11:55:48.609781  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.302096 (* 1 = 0.302096 loss)
I0925 11:55:48.609788  2600 sgd_solver.cpp:105] Iteration 20500, lr = 0.01
I0925 11:56:03.208118  2600 solver.cpp:218] Iteration 20600 (6.85011 iter/s, 14.5983s/100 iters), loss = 0.306361
I0925 11:56:03.208149  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306361 (* 1 = 0.306361 loss)
I0925 11:56:03.208156  2600 sgd_solver.cpp:105] Iteration 20600, lr = 0.01
I0925 11:56:17.810366  2600 solver.cpp:218] Iteration 20700 (6.84829 iter/s, 14.6022s/100 iters), loss = 0.329525
I0925 11:56:17.810487  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329525 (* 1 = 0.329525 loss)
I0925 11:56:17.810494  2600 sgd_solver.cpp:105] Iteration 20700, lr = 0.01
I0925 11:56:32.406718  2600 solver.cpp:218] Iteration 20800 (6.8511 iter/s, 14.5962s/100 iters), loss = 0.362298
I0925 11:56:32.406749  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.362298 (* 1 = 0.362298 loss)
I0925 11:56:32.406754  2600 sgd_solver.cpp:105] Iteration 20800, lr = 0.01
I0925 11:56:47.008307  2600 solver.cpp:218] Iteration 20900 (6.8486 iter/s, 14.6015s/100 iters), loss = 0.266261
I0925 11:56:47.008348  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266261 (* 1 = 0.266261 loss)
I0925 11:56:47.008354  2600 sgd_solver.cpp:105] Iteration 20900, lr = 0.01
I0925 11:57:00.890357  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:57:01.475695  2600 solver.cpp:330] Iteration 21000, Testing net (#0)
I0925 11:57:04.902750  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:57:05.045650  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8186
I0925 11:57:05.045684  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.529583 (* 1 = 0.529583 loss)
I0925 11:57:05.190094  2600 solver.cpp:218] Iteration 21000 (5.50004 iter/s, 18.1817s/100 iters), loss = 0.243358
I0925 11:57:05.190124  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243358 (* 1 = 0.243358 loss)
I0925 11:57:05.190129  2600 sgd_solver.cpp:105] Iteration 21000, lr = 0.01
I0925 11:57:19.773496  2600 solver.cpp:218] Iteration 21100 (6.85714 iter/s, 14.5833s/100 iters), loss = 0.327353
I0925 11:57:19.773525  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.327353 (* 1 = 0.327353 loss)
I0925 11:57:19.773531  2600 sgd_solver.cpp:105] Iteration 21100, lr = 0.01
I0925 11:57:34.366407  2600 solver.cpp:218] Iteration 21200 (6.85267 iter/s, 14.5928s/100 iters), loss = 0.26776
I0925 11:57:34.366536  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267759 (* 1 = 0.267759 loss)
I0925 11:57:34.366545  2600 sgd_solver.cpp:105] Iteration 21200, lr = 0.01
I0925 11:57:48.955327  2600 solver.cpp:218] Iteration 21300 (6.85459 iter/s, 14.5888s/100 iters), loss = 0.311104
I0925 11:57:48.955358  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.311103 (* 1 = 0.311103 loss)
I0925 11:57:48.955364  2600 sgd_solver.cpp:105] Iteration 21300, lr = 0.01
I0925 11:58:03.543642  2600 solver.cpp:218] Iteration 21400 (6.85483 iter/s, 14.5882s/100 iters), loss = 0.263832
I0925 11:58:03.543670  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263832 (* 1 = 0.263832 loss)
I0925 11:58:03.543676  2600 sgd_solver.cpp:105] Iteration 21400, lr = 0.01
I0925 11:58:17.411787  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:58:17.994830  2600 solver.cpp:330] Iteration 21500, Testing net (#0)
I0925 11:58:21.423113  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:58:21.566221  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8254
I0925 11:58:21.566257  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.515323 (* 1 = 0.515323 loss)
I0925 11:58:21.711160  2600 solver.cpp:218] Iteration 21500 (5.50435 iter/s, 18.1674s/100 iters), loss = 0.272474
I0925 11:58:21.711189  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272474 (* 1 = 0.272474 loss)
I0925 11:58:21.711196  2600 sgd_solver.cpp:105] Iteration 21500, lr = 0.01
I0925 11:58:36.306417  2600 solver.cpp:218] Iteration 21600 (6.85157 iter/s, 14.5952s/100 iters), loss = 0.34165
I0925 11:58:36.306448  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.341649 (* 1 = 0.341649 loss)
I0925 11:58:36.306457  2600 sgd_solver.cpp:105] Iteration 21600, lr = 0.01
I0925 11:58:50.910765  2600 solver.cpp:218] Iteration 21700 (6.84731 iter/s, 14.6043s/100 iters), loss = 0.336464
I0925 11:58:50.910872  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.336464 (* 1 = 0.336464 loss)
I0925 11:58:50.910879  2600 sgd_solver.cpp:105] Iteration 21700, lr = 0.01
I0925 11:59:05.513566  2600 solver.cpp:218] Iteration 21800 (6.84807 iter/s, 14.6027s/100 iters), loss = 0.353978
I0925 11:59:05.513595  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353978 (* 1 = 0.353978 loss)
I0925 11:59:05.513602  2600 sgd_solver.cpp:105] Iteration 21800, lr = 0.01
I0925 11:59:20.115607  2600 solver.cpp:218] Iteration 21900 (6.84839 iter/s, 14.602s/100 iters), loss = 0.271494
I0925 11:59:20.115639  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.271494 (* 1 = 0.271494 loss)
I0925 11:59:20.115645  2600 sgd_solver.cpp:105] Iteration 21900, lr = 0.01
I0925 11:59:33.990784  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:59:34.575647  2600 solver.cpp:330] Iteration 22000, Testing net (#0)
I0925 11:59:38.001101  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 11:59:38.144011  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8313
I0925 11:59:38.144047  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.491226 (* 1 = 0.491226 loss)
I0925 11:59:38.289372  2600 solver.cpp:218] Iteration 22000 (5.50246 iter/s, 18.1737s/100 iters), loss = 0.327027
I0925 11:59:38.289402  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.327027 (* 1 = 0.327027 loss)
I0925 11:59:38.289409  2600 sgd_solver.cpp:105] Iteration 22000, lr = 0.01
I0925 11:59:52.892405  2600 solver.cpp:218] Iteration 22100 (6.84793 iter/s, 14.603s/100 iters), loss = 0.272913
I0925 11:59:52.892446  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272912 (* 1 = 0.272912 loss)
I0925 11:59:52.892452  2600 sgd_solver.cpp:105] Iteration 22100, lr = 0.01
I0925 12:00:07.499665  2600 solver.cpp:218] Iteration 22200 (6.84595 iter/s, 14.6072s/100 iters), loss = 0.236663
I0925 12:00:07.499778  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236663 (* 1 = 0.236663 loss)
I0925 12:00:07.499794  2600 sgd_solver.cpp:105] Iteration 22200, lr = 0.01
I0925 12:00:22.097738  2600 solver.cpp:218] Iteration 22300 (6.85029 iter/s, 14.5979s/100 iters), loss = 0.376858
I0925 12:00:22.097779  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376858 (* 1 = 0.376858 loss)
I0925 12:00:22.097785  2600 sgd_solver.cpp:105] Iteration 22300, lr = 0.01
I0925 12:00:36.707269  2600 solver.cpp:218] Iteration 22400 (6.84488 iter/s, 14.6095s/100 iters), loss = 0.261707
I0925 12:00:36.707311  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261707 (* 1 = 0.261707 loss)
I0925 12:00:36.707317  2600 sgd_solver.cpp:105] Iteration 22400, lr = 0.01
I0925 12:00:50.586688  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:00:51.170773  2600 solver.cpp:330] Iteration 22500, Testing net (#0)
I0925 12:00:54.598189  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:00:54.740562  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8345
I0925 12:00:54.740587  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.474453 (* 1 = 0.474453 loss)
I0925 12:00:54.885104  2600 solver.cpp:218] Iteration 22500 (5.50123 iter/s, 18.1777s/100 iters), loss = 0.270159
I0925 12:00:54.885136  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270159 (* 1 = 0.270159 loss)
I0925 12:00:54.885143  2600 sgd_solver.cpp:105] Iteration 22500, lr = 0.01
I0925 12:01:09.484887  2600 solver.cpp:218] Iteration 22600 (6.84945 iter/s, 14.5997s/100 iters), loss = 0.248988
I0925 12:01:09.484930  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248988 (* 1 = 0.248988 loss)
I0925 12:01:09.484936  2600 sgd_solver.cpp:105] Iteration 22600, lr = 0.01
I0925 12:01:24.080199  2600 solver.cpp:218] Iteration 22700 (6.85155 iter/s, 14.5952s/100 iters), loss = 0.329951
I0925 12:01:24.080340  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.32995 (* 1 = 0.32995 loss)
I0925 12:01:24.080348  2600 sgd_solver.cpp:105] Iteration 22700, lr = 0.01
I0925 12:01:38.687443  2600 solver.cpp:218] Iteration 22800 (6.846 iter/s, 14.6071s/100 iters), loss = 0.419838
I0925 12:01:38.687474  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419838 (* 1 = 0.419838 loss)
I0925 12:01:38.687479  2600 sgd_solver.cpp:105] Iteration 22800, lr = 0.01
I0925 12:01:53.294250  2600 solver.cpp:218] Iteration 22900 (6.84616 iter/s, 14.6067s/100 iters), loss = 0.30342
I0925 12:01:53.294311  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.30342 (* 1 = 0.30342 loss)
I0925 12:01:53.294317  2600 sgd_solver.cpp:105] Iteration 22900, lr = 0.01
I0925 12:02:07.174798  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:02:07.759205  2600 solver.cpp:330] Iteration 23000, Testing net (#0)
I0925 12:02:11.186614  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:02:11.329802  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7955
I0925 12:02:11.329839  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.600287 (* 1 = 0.600287 loss)
I0925 12:02:11.474609  2600 solver.cpp:218] Iteration 23000 (5.50047 iter/s, 18.1803s/100 iters), loss = 0.28762
I0925 12:02:11.474639  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28762 (* 1 = 0.28762 loss)
I0925 12:02:11.474647  2600 sgd_solver.cpp:105] Iteration 23000, lr = 0.01
I0925 12:02:26.071259  2600 solver.cpp:218] Iteration 23100 (6.85092 iter/s, 14.5966s/100 iters), loss = 0.267587
I0925 12:02:26.071300  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267587 (* 1 = 0.267587 loss)
I0925 12:02:26.071306  2600 sgd_solver.cpp:105] Iteration 23100, lr = 0.01
I0925 12:02:40.671026  2600 solver.cpp:218] Iteration 23200 (6.84946 iter/s, 14.5997s/100 iters), loss = 0.269813
I0925 12:02:40.671144  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269813 (* 1 = 0.269813 loss)
I0925 12:02:40.671159  2600 sgd_solver.cpp:105] Iteration 23200, lr = 0.01
I0925 12:02:55.270072  2600 solver.cpp:218] Iteration 23300 (6.84983 iter/s, 14.5989s/100 iters), loss = 0.35532
I0925 12:02:55.270103  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.35532 (* 1 = 0.35532 loss)
I0925 12:02:55.270110  2600 sgd_solver.cpp:105] Iteration 23300, lr = 0.01
I0925 12:03:09.868022  2600 solver.cpp:218] Iteration 23400 (6.85031 iter/s, 14.5979s/100 iters), loss = 0.245085
I0925 12:03:09.868052  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245085 (* 1 = 0.245085 loss)
I0925 12:03:09.868058  2600 sgd_solver.cpp:105] Iteration 23400, lr = 0.01
I0925 12:03:23.744060  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:03:24.327841  2600 solver.cpp:330] Iteration 23500, Testing net (#0)
I0925 12:03:27.753458  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:03:27.896085  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8235
I0925 12:03:27.896121  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.517317 (* 1 = 0.517317 loss)
I0925 12:03:28.041218  2600 solver.cpp:218] Iteration 23500 (5.50263 iter/s, 18.1731s/100 iters), loss = 0.343971
I0925 12:03:28.041246  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.343971 (* 1 = 0.343971 loss)
I0925 12:03:28.041254  2600 sgd_solver.cpp:105] Iteration 23500, lr = 0.01
I0925 12:03:42.635867  2600 solver.cpp:218] Iteration 23600 (6.85186 iter/s, 14.5946s/100 iters), loss = 0.313531
I0925 12:03:42.635908  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313531 (* 1 = 0.313531 loss)
I0925 12:03:42.635915  2600 sgd_solver.cpp:105] Iteration 23600, lr = 0.01
I0925 12:03:57.232520  2600 solver.cpp:218] Iteration 23700 (6.85092 iter/s, 14.5966s/100 iters), loss = 0.311666
I0925 12:03:57.232625  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.311666 (* 1 = 0.311666 loss)
I0925 12:03:57.232632  2600 sgd_solver.cpp:105] Iteration 23700, lr = 0.01
I0925 12:04:11.830651  2600 solver.cpp:218] Iteration 23800 (6.85026 iter/s, 14.598s/100 iters), loss = 0.338854
I0925 12:04:11.830679  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.338854 (* 1 = 0.338854 loss)
I0925 12:04:11.830685  2600 sgd_solver.cpp:105] Iteration 23800, lr = 0.01
I0925 12:04:26.425498  2600 solver.cpp:218] Iteration 23900 (6.85176 iter/s, 14.5948s/100 iters), loss = 0.28327
I0925 12:04:26.425529  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28327 (* 1 = 0.28327 loss)
I0925 12:04:26.425544  2600 sgd_solver.cpp:105] Iteration 23900, lr = 0.01
I0925 12:04:40.296116  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:04:40.880367  2600 solver.cpp:330] Iteration 24000, Testing net (#0)
I0925 12:04:44.307720  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:04:44.450757  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8281
I0925 12:04:44.450793  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.520466 (* 1 = 0.520466 loss)
I0925 12:04:44.595795  2600 solver.cpp:218] Iteration 24000 (5.50351 iter/s, 18.1702s/100 iters), loss = 0.312747
I0925 12:04:44.595825  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312747 (* 1 = 0.312747 loss)
I0925 12:04:44.595832  2600 sgd_solver.cpp:105] Iteration 24000, lr = 0.01
I0925 12:04:59.194347  2600 solver.cpp:218] Iteration 24100 (6.85003 iter/s, 14.5985s/100 iters), loss = 0.190979
I0925 12:04:59.194377  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190979 (* 1 = 0.190979 loss)
I0925 12:04:59.194383  2600 sgd_solver.cpp:105] Iteration 24100, lr = 0.01
I0925 12:05:13.793653  2600 solver.cpp:218] Iteration 24200 (6.84967 iter/s, 14.5992s/100 iters), loss = 0.309256
I0925 12:05:13.793758  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309256 (* 1 = 0.309256 loss)
I0925 12:05:13.793766  2600 sgd_solver.cpp:105] Iteration 24200, lr = 0.01
I0925 12:05:28.398643  2600 solver.cpp:218] Iteration 24300 (6.84704 iter/s, 14.6048s/100 iters), loss = 0.334683
I0925 12:05:28.398674  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334683 (* 1 = 0.334683 loss)
I0925 12:05:28.398679  2600 sgd_solver.cpp:105] Iteration 24300, lr = 0.01
I0925 12:05:42.998230  2600 solver.cpp:218] Iteration 24400 (6.84954 iter/s, 14.5995s/100 iters), loss = 0.200516
I0925 12:05:42.998270  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200516 (* 1 = 0.200516 loss)
I0925 12:05:42.998276  2600 sgd_solver.cpp:105] Iteration 24400, lr = 0.01
I0925 12:05:56.875823  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:05:57.457767  2600 solver.cpp:330] Iteration 24500, Testing net (#0)
I0925 12:06:00.886765  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:06:01.029855  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8213
I0925 12:06:01.029878  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535949 (* 1 = 0.535949 loss)
I0925 12:06:01.175390  2600 solver.cpp:218] Iteration 24500 (5.50144 iter/s, 18.1771s/100 iters), loss = 0.23772
I0925 12:06:01.175420  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23772 (* 1 = 0.23772 loss)
I0925 12:06:01.175426  2600 sgd_solver.cpp:105] Iteration 24500, lr = 0.01
I0925 12:06:15.765281  2600 solver.cpp:218] Iteration 24600 (6.85409 iter/s, 14.5898s/100 iters), loss = 0.280586
I0925 12:06:15.765321  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.280586 (* 1 = 0.280586 loss)
I0925 12:06:15.765327  2600 sgd_solver.cpp:105] Iteration 24600, lr = 0.01
I0925 12:06:30.361181  2600 solver.cpp:218] Iteration 24700 (6.85128 iter/s, 14.5958s/100 iters), loss = 0.270546
I0925 12:06:30.361290  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270546 (* 1 = 0.270546 loss)
I0925 12:06:30.361299  2600 sgd_solver.cpp:105] Iteration 24700, lr = 0.01
I0925 12:06:44.956342  2600 solver.cpp:218] Iteration 24800 (6.85165 iter/s, 14.595s/100 iters), loss = 0.342348
I0925 12:06:44.956372  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.342348 (* 1 = 0.342348 loss)
I0925 12:06:44.956377  2600 sgd_solver.cpp:105] Iteration 24800, lr = 0.01
I0925 12:06:59.558475  2600 solver.cpp:218] Iteration 24900 (6.84835 iter/s, 14.6021s/100 iters), loss = 0.264072
I0925 12:06:59.558516  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264072 (* 1 = 0.264072 loss)
I0925 12:06:59.558521  2600 sgd_solver.cpp:105] Iteration 24900, lr = 0.01
I0925 12:07:13.436347  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:07:14.020305  2600 solver.cpp:330] Iteration 25000, Testing net (#0)
I0925 12:07:17.448271  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:07:17.591071  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8371
I0925 12:07:17.591106  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.489458 (* 1 = 0.489458 loss)
I0925 12:07:17.736178  2600 solver.cpp:218] Iteration 25000 (5.50127 iter/s, 18.1776s/100 iters), loss = 0.272521
I0925 12:07:17.736210  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272521 (* 1 = 0.272521 loss)
I0925 12:07:17.736217  2600 sgd_solver.cpp:105] Iteration 25000, lr = 0.01
I0925 12:07:32.332698  2600 solver.cpp:218] Iteration 25100 (6.85098 iter/s, 14.5965s/100 iters), loss = 0.297258
I0925 12:07:32.332731  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297257 (* 1 = 0.297257 loss)
I0925 12:07:32.332749  2600 sgd_solver.cpp:105] Iteration 25100, lr = 0.01
I0925 12:07:46.940726  2600 solver.cpp:218] Iteration 25200 (6.84558 iter/s, 14.608s/100 iters), loss = 0.340785
I0925 12:07:46.940840  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340785 (* 1 = 0.340785 loss)
I0925 12:07:46.940851  2600 sgd_solver.cpp:105] Iteration 25200, lr = 0.01
I0925 12:08:01.543604  2600 solver.cpp:218] Iteration 25300 (6.84803 iter/s, 14.6027s/100 iters), loss = 0.261611
I0925 12:08:01.543634  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261611 (* 1 = 0.261611 loss)
I0925 12:08:01.543640  2600 sgd_solver.cpp:105] Iteration 25300, lr = 0.01
I0925 12:08:16.149169  2600 solver.cpp:218] Iteration 25400 (6.84674 iter/s, 14.6055s/100 iters), loss = 0.221758
I0925 12:08:16.149199  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221758 (* 1 = 0.221758 loss)
I0925 12:08:16.149205  2600 sgd_solver.cpp:105] Iteration 25400, lr = 0.01
I0925 12:08:30.022918  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:08:30.607364  2600 solver.cpp:330] Iteration 25500, Testing net (#0)
I0925 12:08:34.033143  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:08:34.176095  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8354
I0925 12:08:34.176131  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.481358 (* 1 = 0.481358 loss)
I0925 12:08:34.320875  2600 solver.cpp:218] Iteration 25500 (5.50308 iter/s, 18.1716s/100 iters), loss = 0.234214
I0925 12:08:34.320904  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234213 (* 1 = 0.234213 loss)
I0925 12:08:34.320910  2600 sgd_solver.cpp:105] Iteration 25500, lr = 0.01
I0925 12:08:48.919533  2600 solver.cpp:218] Iteration 25600 (6.84998 iter/s, 14.5986s/100 iters), loss = 0.276073
I0925 12:08:48.919564  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276073 (* 1 = 0.276073 loss)
I0925 12:08:48.919570  2600 sgd_solver.cpp:105] Iteration 25600, lr = 0.01
I0925 12:09:03.530637  2600 solver.cpp:218] Iteration 25700 (6.84414 iter/s, 14.611s/100 iters), loss = 0.303857
I0925 12:09:03.530740  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303857 (* 1 = 0.303857 loss)
I0925 12:09:03.530760  2600 sgd_solver.cpp:105] Iteration 25700, lr = 0.01
I0925 12:09:18.136962  2600 solver.cpp:218] Iteration 25800 (6.84641 iter/s, 14.6062s/100 iters), loss = 0.265708
I0925 12:09:18.136992  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265708 (* 1 = 0.265708 loss)
I0925 12:09:18.136998  2600 sgd_solver.cpp:105] Iteration 25800, lr = 0.01
I0925 12:09:32.742468  2600 solver.cpp:218] Iteration 25900 (6.84676 iter/s, 14.6054s/100 iters), loss = 0.234415
I0925 12:09:32.742499  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234415 (* 1 = 0.234415 loss)
I0925 12:09:32.742506  2600 sgd_solver.cpp:105] Iteration 25900, lr = 0.01
I0925 12:09:46.615095  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:09:47.198956  2600 solver.cpp:330] Iteration 26000, Testing net (#0)
I0925 12:09:50.626433  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:09:50.769274  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8205
I0925 12:09:50.769310  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.522595 (* 1 = 0.522595 loss)
I0925 12:09:50.914150  2600 solver.cpp:218] Iteration 26000 (5.50309 iter/s, 18.1716s/100 iters), loss = 0.264666
I0925 12:09:50.914180  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.264666 (* 1 = 0.264666 loss)
I0925 12:09:50.914187  2600 sgd_solver.cpp:105] Iteration 26000, lr = 0.01
I0925 12:10:05.501372  2600 solver.cpp:218] Iteration 26100 (6.85535 iter/s, 14.5872s/100 iters), loss = 0.256015
I0925 12:10:05.501402  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256015 (* 1 = 0.256015 loss)
I0925 12:10:05.501408  2600 sgd_solver.cpp:105] Iteration 26100, lr = 0.01
I0925 12:10:20.094094  2600 solver.cpp:218] Iteration 26200 (6.85276 iter/s, 14.5927s/100 iters), loss = 0.289127
I0925 12:10:20.094188  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289127 (* 1 = 0.289127 loss)
I0925 12:10:20.094195  2600 sgd_solver.cpp:105] Iteration 26200, lr = 0.01
I0925 12:10:34.683245  2600 solver.cpp:218] Iteration 26300 (6.85447 iter/s, 14.589s/100 iters), loss = 0.256105
I0925 12:10:34.683286  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256105 (* 1 = 0.256105 loss)
I0925 12:10:34.683292  2600 sgd_solver.cpp:105] Iteration 26300, lr = 0.01
I0925 12:10:49.278688  2600 solver.cpp:218] Iteration 26400 (6.85149 iter/s, 14.5954s/100 iters), loss = 0.196331
I0925 12:10:49.278719  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196331 (* 1 = 0.196331 loss)
I0925 12:10:49.278725  2600 sgd_solver.cpp:105] Iteration 26400, lr = 0.01
I0925 12:11:03.140622  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:11:03.726766  2600 solver.cpp:330] Iteration 26500, Testing net (#0)
I0925 12:11:07.153874  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:11:07.296497  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8314
I0925 12:11:07.296533  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.509985 (* 1 = 0.509985 loss)
I0925 12:11:07.441825  2600 solver.cpp:218] Iteration 26500 (5.50568 iter/s, 18.1631s/100 iters), loss = 0.263152
I0925 12:11:07.441855  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263152 (* 1 = 0.263152 loss)
I0925 12:11:07.441861  2600 sgd_solver.cpp:105] Iteration 26500, lr = 0.01
I0925 12:11:22.042930  2600 solver.cpp:218] Iteration 26600 (6.84883 iter/s, 14.601s/100 iters), loss = 0.269272
I0925 12:11:22.042973  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269272 (* 1 = 0.269272 loss)
I0925 12:11:22.042979  2600 sgd_solver.cpp:105] Iteration 26600, lr = 0.01
I0925 12:11:36.644265  2600 solver.cpp:218] Iteration 26700 (6.84873 iter/s, 14.6013s/100 iters), loss = 0.346215
I0925 12:11:36.644368  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.346215 (* 1 = 0.346215 loss)
I0925 12:11:36.644376  2600 sgd_solver.cpp:105] Iteration 26700, lr = 0.01
I0925 12:11:51.245847  2600 solver.cpp:218] Iteration 26800 (6.84864 iter/s, 14.6014s/100 iters), loss = 0.344948
I0925 12:11:51.245892  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.344948 (* 1 = 0.344948 loss)
I0925 12:11:51.245898  2600 sgd_solver.cpp:105] Iteration 26800, lr = 0.01
I0925 12:12:05.851348  2600 solver.cpp:218] Iteration 26900 (6.84677 iter/s, 14.6054s/100 iters), loss = 0.212423
I0925 12:12:05.851379  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212423 (* 1 = 0.212423 loss)
I0925 12:12:05.851384  2600 sgd_solver.cpp:105] Iteration 26900, lr = 0.01
I0925 12:12:19.727048  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:12:20.311604  2600 solver.cpp:330] Iteration 27000, Testing net (#0)
I0925 12:12:23.738636  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:12:23.881512  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.827
I0925 12:12:23.881547  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.52797 (* 1 = 0.52797 loss)
I0925 12:12:24.026222  2600 solver.cpp:218] Iteration 27000 (5.50212 iter/s, 18.1748s/100 iters), loss = 0.289026
I0925 12:12:24.026252  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289026 (* 1 = 0.289026 loss)
I0925 12:12:24.026259  2600 sgd_solver.cpp:105] Iteration 27000, lr = 0.01
I0925 12:12:38.626853  2600 solver.cpp:218] Iteration 27100 (6.84905 iter/s, 14.6006s/100 iters), loss = 0.329237
I0925 12:12:38.626894  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329236 (* 1 = 0.329236 loss)
I0925 12:12:38.626900  2600 sgd_solver.cpp:105] Iteration 27100, lr = 0.01
I0925 12:12:53.233685  2600 solver.cpp:218] Iteration 27200 (6.84615 iter/s, 14.6068s/100 iters), loss = 0.286858
I0925 12:12:53.233816  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286857 (* 1 = 0.286857 loss)
I0925 12:12:53.233824  2600 sgd_solver.cpp:105] Iteration 27200, lr = 0.01
I0925 12:13:07.841049  2600 solver.cpp:218] Iteration 27300 (6.84594 iter/s, 14.6072s/100 iters), loss = 0.239998
I0925 12:13:07.841079  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239998 (* 1 = 0.239998 loss)
I0925 12:13:07.841085  2600 sgd_solver.cpp:105] Iteration 27300, lr = 0.01
I0925 12:13:22.446982  2600 solver.cpp:218] Iteration 27400 (6.84656 iter/s, 14.6059s/100 iters), loss = 0.21841
I0925 12:13:22.447013  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218409 (* 1 = 0.218409 loss)
I0925 12:13:22.447031  2600 sgd_solver.cpp:105] Iteration 27400, lr = 0.01
I0925 12:13:36.326328  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:13:36.910245  2600 solver.cpp:330] Iteration 27500, Testing net (#0)
I0925 12:13:40.336800  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:13:40.479815  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8137
I0925 12:13:40.479849  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.578376 (* 1 = 0.578376 loss)
I0925 12:13:40.625039  2600 solver.cpp:218] Iteration 27500 (5.50116 iter/s, 18.178s/100 iters), loss = 0.28405
I0925 12:13:40.625067  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28405 (* 1 = 0.28405 loss)
I0925 12:13:40.625073  2600 sgd_solver.cpp:105] Iteration 27500, lr = 0.01
I0925 12:13:55.220051  2600 solver.cpp:218] Iteration 27600 (6.85169 iter/s, 14.5949s/100 iters), loss = 0.21819
I0925 12:13:55.220082  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218189 (* 1 = 0.218189 loss)
I0925 12:13:55.220088  2600 sgd_solver.cpp:105] Iteration 27600, lr = 0.01
I0925 12:14:09.817209  2600 solver.cpp:218] Iteration 27700 (6.85068 iter/s, 14.5971s/100 iters), loss = 0.328944
I0925 12:14:09.817306  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.328944 (* 1 = 0.328944 loss)
I0925 12:14:09.817324  2600 sgd_solver.cpp:105] Iteration 27700, lr = 0.01
I0925 12:14:24.422818  2600 solver.cpp:218] Iteration 27800 (6.84675 iter/s, 14.6055s/100 iters), loss = 0.293972
I0925 12:14:24.422858  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293972 (* 1 = 0.293972 loss)
I0925 12:14:24.422865  2600 sgd_solver.cpp:105] Iteration 27800, lr = 0.01
I0925 12:14:39.050714  2600 solver.cpp:218] Iteration 27900 (6.83629 iter/s, 14.6278s/100 iters), loss = 0.202869
I0925 12:14:39.050745  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202869 (* 1 = 0.202869 loss)
I0925 12:14:39.050751  2600 sgd_solver.cpp:105] Iteration 27900, lr = 0.01
I0925 12:14:52.995185  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:14:53.579601  2600 solver.cpp:330] Iteration 28000, Testing net (#0)
I0925 12:14:57.007599  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:14:57.150598  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8315
I0925 12:14:57.150632  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.487939 (* 1 = 0.487939 loss)
I0925 12:14:57.295454  2600 solver.cpp:218] Iteration 28000 (5.48106 iter/s, 18.2447s/100 iters), loss = 0.18674
I0925 12:14:57.295485  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186739 (* 1 = 0.186739 loss)
I0925 12:14:57.295493  2600 sgd_solver.cpp:105] Iteration 28000, lr = 0.01
I0925 12:15:11.901536  2600 solver.cpp:218] Iteration 28100 (6.84649 iter/s, 14.606s/100 iters), loss = 0.240605
I0925 12:15:11.901576  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240605 (* 1 = 0.240605 loss)
I0925 12:15:11.901583  2600 sgd_solver.cpp:105] Iteration 28100, lr = 0.01
I0925 12:15:26.503742  2600 solver.cpp:218] Iteration 28200 (6.84832 iter/s, 14.6021s/100 iters), loss = 0.353819
I0925 12:15:26.503871  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353818 (* 1 = 0.353818 loss)
I0925 12:15:26.503880  2600 sgd_solver.cpp:105] Iteration 28200, lr = 0.01
I0925 12:15:41.103119  2600 solver.cpp:218] Iteration 28300 (6.84968 iter/s, 14.5992s/100 iters), loss = 0.293401
I0925 12:15:41.103148  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293401 (* 1 = 0.293401 loss)
I0925 12:15:41.103154  2600 sgd_solver.cpp:105] Iteration 28300, lr = 0.01
I0925 12:15:55.703784  2600 solver.cpp:218] Iteration 28400 (6.84903 iter/s, 14.6006s/100 iters), loss = 0.305254
I0925 12:15:55.703814  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305254 (* 1 = 0.305254 loss)
I0925 12:15:55.703820  2600 sgd_solver.cpp:105] Iteration 28400, lr = 0.01
I0925 12:16:09.580030  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:16:10.162991  2600 solver.cpp:330] Iteration 28500, Testing net (#0)
I0925 12:16:13.589048  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:16:13.731725  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8199
I0925 12:16:13.731760  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.554829 (* 1 = 0.554829 loss)
I0925 12:16:13.875944  2600 solver.cpp:218] Iteration 28500 (5.50294 iter/s, 18.1721s/100 iters), loss = 0.293244
I0925 12:16:13.875973  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293244 (* 1 = 0.293244 loss)
I0925 12:16:13.875979  2600 sgd_solver.cpp:105] Iteration 28500, lr = 0.01
I0925 12:16:28.467624  2600 solver.cpp:218] Iteration 28600 (6.85325 iter/s, 14.5916s/100 iters), loss = 0.219012
I0925 12:16:28.467664  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219012 (* 1 = 0.219012 loss)
I0925 12:16:28.467670  2600 sgd_solver.cpp:105] Iteration 28600, lr = 0.01
I0925 12:16:43.066150  2600 solver.cpp:218] Iteration 28700 (6.85004 iter/s, 14.5985s/100 iters), loss = 0.286562
I0925 12:16:43.066247  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286562 (* 1 = 0.286562 loss)
I0925 12:16:43.066267  2600 sgd_solver.cpp:105] Iteration 28700, lr = 0.01
I0925 12:16:57.659196  2600 solver.cpp:218] Iteration 28800 (6.85264 iter/s, 14.5929s/100 iters), loss = 0.306021
I0925 12:16:57.659229  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306021 (* 1 = 0.306021 loss)
I0925 12:16:57.659238  2600 sgd_solver.cpp:105] Iteration 28800, lr = 0.01
I0925 12:17:12.251240  2600 solver.cpp:218] Iteration 28900 (6.85308 iter/s, 14.592s/100 iters), loss = 0.247448
I0925 12:17:12.251269  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247448 (* 1 = 0.247448 loss)
I0925 12:17:12.251276  2600 sgd_solver.cpp:105] Iteration 28900, lr = 0.01
I0925 12:17:26.122719  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:17:26.707116  2600 solver.cpp:330] Iteration 29000, Testing net (#0)
I0925 12:17:30.133613  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:17:30.276484  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.829
I0925 12:17:30.276512  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.51204 (* 1 = 0.51204 loss)
I0925 12:17:30.421602  2600 solver.cpp:218] Iteration 29000 (5.50349 iter/s, 18.1703s/100 iters), loss = 0.242018
I0925 12:17:30.421638  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242018 (* 1 = 0.242018 loss)
I0925 12:17:30.421644  2600 sgd_solver.cpp:105] Iteration 29000, lr = 0.01
I0925 12:17:45.017980  2600 solver.cpp:218] Iteration 29100 (6.85105 iter/s, 14.5963s/100 iters), loss = 0.296047
I0925 12:17:45.018009  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296046 (* 1 = 0.296046 loss)
I0925 12:17:45.018015  2600 sgd_solver.cpp:105] Iteration 29100, lr = 0.01
I0925 12:17:59.622745  2600 solver.cpp:218] Iteration 29200 (6.84711 iter/s, 14.6047s/100 iters), loss = 0.301929
I0925 12:17:59.622913  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301929 (* 1 = 0.301929 loss)
I0925 12:17:59.622941  2600 sgd_solver.cpp:105] Iteration 29200, lr = 0.01
I0925 12:18:14.223904  2600 solver.cpp:218] Iteration 29300 (6.84886 iter/s, 14.601s/100 iters), loss = 0.319353
I0925 12:18:14.223934  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319352 (* 1 = 0.319352 loss)
I0925 12:18:14.223942  2600 sgd_solver.cpp:105] Iteration 29300, lr = 0.01
I0925 12:18:28.821995  2600 solver.cpp:218] Iteration 29400 (6.85024 iter/s, 14.598s/100 iters), loss = 0.20494
I0925 12:18:28.822023  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204939 (* 1 = 0.204939 loss)
I0925 12:18:28.822039  2600 sgd_solver.cpp:105] Iteration 29400, lr = 0.01
I0925 12:18:42.696537  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:18:43.280300  2600 solver.cpp:330] Iteration 29500, Testing net (#0)
I0925 12:18:46.707502  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:18:46.850421  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8155
I0925 12:18:46.850457  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.569356 (* 1 = 0.569356 loss)
I0925 12:18:46.995170  2600 solver.cpp:218] Iteration 29500 (5.50264 iter/s, 18.1731s/100 iters), loss = 0.195173
I0925 12:18:46.995201  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195172 (* 1 = 0.195172 loss)
I0925 12:18:46.995208  2600 sgd_solver.cpp:105] Iteration 29500, lr = 0.01
I0925 12:19:01.594720  2600 solver.cpp:218] Iteration 29600 (6.84956 iter/s, 14.5995s/100 iters), loss = 0.281046
I0925 12:19:01.594760  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281045 (* 1 = 0.281045 loss)
I0925 12:19:01.594766  2600 sgd_solver.cpp:105] Iteration 29600, lr = 0.01
I0925 12:19:16.200937  2600 solver.cpp:218] Iteration 29700 (6.84644 iter/s, 14.6061s/100 iters), loss = 0.304053
I0925 12:19:16.201063  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.304053 (* 1 = 0.304053 loss)
I0925 12:19:16.201071  2600 sgd_solver.cpp:105] Iteration 29700, lr = 0.01
I0925 12:19:30.802625  2600 solver.cpp:218] Iteration 29800 (6.8486 iter/s, 14.6015s/100 iters), loss = 0.251562
I0925 12:19:30.802667  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.251561 (* 1 = 0.251561 loss)
I0925 12:19:30.802673  2600 sgd_solver.cpp:105] Iteration 29800, lr = 0.01
I0925 12:19:45.401034  2600 solver.cpp:218] Iteration 29900 (6.8501 iter/s, 14.5983s/100 iters), loss = 0.274998
I0925 12:19:45.401079  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.274997 (* 1 = 0.274997 loss)
I0925 12:19:45.401087  2600 sgd_solver.cpp:105] Iteration 29900, lr = 0.01
I0925 12:19:59.275812  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:19:59.860227  2600 solver.cpp:330] Iteration 30000, Testing net (#0)
I0925 12:20:03.289779  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:20:03.433197  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8029
I0925 12:20:03.433233  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.612174 (* 1 = 0.612174 loss)
I0925 12:20:03.578490  2600 solver.cpp:218] Iteration 30000 (5.50135 iter/s, 18.1774s/100 iters), loss = 0.236652
I0925 12:20:03.578522  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236651 (* 1 = 0.236651 loss)
I0925 12:20:03.578529  2600 sgd_solver.cpp:105] Iteration 30000, lr = 0.01
I0925 12:20:18.178813  2600 solver.cpp:218] Iteration 30100 (6.8492 iter/s, 14.6003s/100 iters), loss = 0.263204
I0925 12:20:18.178843  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263203 (* 1 = 0.263203 loss)
I0925 12:20:18.178850  2600 sgd_solver.cpp:105] Iteration 30100, lr = 0.01
I0925 12:20:32.784169  2600 solver.cpp:218] Iteration 30200 (6.84683 iter/s, 14.6053s/100 iters), loss = 0.293149
I0925 12:20:32.784292  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293149 (* 1 = 0.293149 loss)
I0925 12:20:32.784310  2600 sgd_solver.cpp:105] Iteration 30200, lr = 0.01
I0925 12:20:47.389432  2600 solver.cpp:218] Iteration 30300 (6.84692 iter/s, 14.6051s/100 iters), loss = 0.286752
I0925 12:20:47.389474  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286751 (* 1 = 0.286751 loss)
I0925 12:20:47.389480  2600 sgd_solver.cpp:105] Iteration 30300, lr = 0.01
I0925 12:21:02.000566  2600 solver.cpp:218] Iteration 30400 (6.84413 iter/s, 14.6111s/100 iters), loss = 0.214029
I0925 12:21:02.000607  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214029 (* 1 = 0.214029 loss)
I0925 12:21:02.000614  2600 sgd_solver.cpp:105] Iteration 30400, lr = 0.01
I0925 12:21:15.876145  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:21:16.460484  2600 solver.cpp:330] Iteration 30500, Testing net (#0)
I0925 12:21:19.889825  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:21:20.032786  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8271
I0925 12:21:20.032811  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.531364 (* 1 = 0.531364 loss)
I0925 12:21:20.177561  2600 solver.cpp:218] Iteration 30500 (5.50148 iter/s, 18.1769s/100 iters), loss = 0.244447
I0925 12:21:20.177590  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244447 (* 1 = 0.244447 loss)
I0925 12:21:20.177597  2600 sgd_solver.cpp:105] Iteration 30500, lr = 0.01
I0925 12:21:34.778692  2600 solver.cpp:218] Iteration 30600 (6.84882 iter/s, 14.6011s/100 iters), loss = 0.245151
I0925 12:21:34.778723  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24515 (* 1 = 0.24515 loss)
I0925 12:21:34.778739  2600 sgd_solver.cpp:105] Iteration 30600, lr = 0.01
I0925 12:21:49.375675  2600 solver.cpp:218] Iteration 30700 (6.85076 iter/s, 14.5969s/100 iters), loss = 0.313196
I0925 12:21:49.375826  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313196 (* 1 = 0.313196 loss)
I0925 12:21:49.375834  2600 sgd_solver.cpp:105] Iteration 30700, lr = 0.01
I0925 12:22:03.973815  2600 solver.cpp:218] Iteration 30800 (6.85028 iter/s, 14.598s/100 iters), loss = 0.26647
I0925 12:22:03.973846  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26647 (* 1 = 0.26647 loss)
I0925 12:22:03.973855  2600 sgd_solver.cpp:105] Iteration 30800, lr = 0.01
I0925 12:22:18.574364  2600 solver.cpp:218] Iteration 30900 (6.84909 iter/s, 14.6005s/100 iters), loss = 0.201566
I0925 12:22:18.574396  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201565 (* 1 = 0.201565 loss)
I0925 12:22:18.574403  2600 sgd_solver.cpp:105] Iteration 30900, lr = 0.01
I0925 12:22:32.451944  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:22:33.035110  2600 solver.cpp:330] Iteration 31000, Testing net (#0)
I0925 12:22:36.462154  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:22:36.605181  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8382
I0925 12:22:36.605207  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.471319 (* 1 = 0.471319 loss)
I0925 12:22:36.749784  2600 solver.cpp:218] Iteration 31000 (5.50196 iter/s, 18.1753s/100 iters), loss = 0.26057
I0925 12:22:36.749814  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260569 (* 1 = 0.260569 loss)
I0925 12:22:36.749821  2600 sgd_solver.cpp:105] Iteration 31000, lr = 0.01
I0925 12:22:51.335675  2600 solver.cpp:218] Iteration 31100 (6.85597 iter/s, 14.5858s/100 iters), loss = 0.224889
I0925 12:22:51.335706  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224889 (* 1 = 0.224889 loss)
I0925 12:22:51.335713  2600 sgd_solver.cpp:105] Iteration 31100, lr = 0.01
I0925 12:23:05.925092  2600 solver.cpp:218] Iteration 31200 (6.85432 iter/s, 14.5893s/100 iters), loss = 0.348484
I0925 12:23:05.925225  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348484 (* 1 = 0.348484 loss)
I0925 12:23:05.925232  2600 sgd_solver.cpp:105] Iteration 31200, lr = 0.01
I0925 12:23:20.515297  2600 solver.cpp:218] Iteration 31300 (6.85399 iter/s, 14.59s/100 iters), loss = 0.312924
I0925 12:23:20.515327  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312923 (* 1 = 0.312923 loss)
I0925 12:23:20.515333  2600 sgd_solver.cpp:105] Iteration 31300, lr = 0.01
I0925 12:23:35.100952  2600 solver.cpp:218] Iteration 31400 (6.85608 iter/s, 14.5856s/100 iters), loss = 0.214398
I0925 12:23:35.100982  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214397 (* 1 = 0.214397 loss)
I0925 12:23:35.100988  2600 sgd_solver.cpp:105] Iteration 31400, lr = 0.01
I0925 12:23:48.963232  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:23:49.547461  2600 solver.cpp:330] Iteration 31500, Testing net (#0)
I0925 12:23:52.974046  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:23:53.117210  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8189
I0925 12:23:53.117246  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.554609 (* 1 = 0.554609 loss)
I0925 12:23:53.262190  2600 solver.cpp:218] Iteration 31500 (5.50626 iter/s, 18.1612s/100 iters), loss = 0.18431
I0925 12:23:53.262220  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184309 (* 1 = 0.184309 loss)
I0925 12:23:53.262226  2600 sgd_solver.cpp:105] Iteration 31500, lr = 0.01
I0925 12:24:07.862133  2600 solver.cpp:218] Iteration 31600 (6.84937 iter/s, 14.5999s/100 iters), loss = 0.312464
I0925 12:24:07.862164  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312463 (* 1 = 0.312463 loss)
I0925 12:24:07.862170  2600 sgd_solver.cpp:105] Iteration 31600, lr = 0.01
I0925 12:24:22.462002  2600 solver.cpp:218] Iteration 31700 (6.84941 iter/s, 14.5998s/100 iters), loss = 0.286783
I0925 12:24:22.462098  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286782 (* 1 = 0.286782 loss)
I0925 12:24:22.462105  2600 sgd_solver.cpp:105] Iteration 31700, lr = 0.01
I0925 12:24:37.064646  2600 solver.cpp:218] Iteration 31800 (6.84814 iter/s, 14.6025s/100 iters), loss = 0.267224
I0925 12:24:37.064687  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267224 (* 1 = 0.267224 loss)
I0925 12:24:37.064692  2600 sgd_solver.cpp:105] Iteration 31800, lr = 0.01
I0925 12:24:51.669482  2600 solver.cpp:218] Iteration 31900 (6.84708 iter/s, 14.6048s/100 iters), loss = 0.245271
I0925 12:24:51.669514  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24527 (* 1 = 0.24527 loss)
I0925 12:24:51.669520  2600 sgd_solver.cpp:105] Iteration 31900, lr = 0.01
I0925 12:25:05.542871  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:25:06.125931  2600 solver.cpp:330] Iteration 32000, Testing net (#0)
I0925 12:25:09.553561  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:25:09.695904  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8198
I0925 12:25:09.695938  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.530022 (* 1 = 0.530022 loss)
I0925 12:25:09.841576  2600 solver.cpp:218] Iteration 32000 (5.50297 iter/s, 18.172s/100 iters), loss = 0.197071
I0925 12:25:09.841606  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197071 (* 1 = 0.197071 loss)
I0925 12:25:09.841612  2600 sgd_solver.cpp:105] Iteration 32000, lr = 0.01
I0925 12:25:24.441548  2600 solver.cpp:218] Iteration 32100 (6.84936 iter/s, 14.5999s/100 iters), loss = 0.26599
I0925 12:25:24.441582  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26599 (* 1 = 0.26599 loss)
I0925 12:25:24.441589  2600 sgd_solver.cpp:105] Iteration 32100, lr = 0.01
I0925 12:25:39.045290  2600 solver.cpp:218] Iteration 32200 (6.84759 iter/s, 14.6037s/100 iters), loss = 0.373018
I0925 12:25:39.045418  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.373017 (* 1 = 0.373017 loss)
I0925 12:25:39.045425  2600 sgd_solver.cpp:105] Iteration 32200, lr = 0.01
I0925 12:25:53.718303  2600 solver.cpp:218] Iteration 32300 (6.81531 iter/s, 14.6729s/100 iters), loss = 0.346473
I0925 12:25:53.718336  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.346473 (* 1 = 0.346473 loss)
I0925 12:25:53.718343  2600 sgd_solver.cpp:105] Iteration 32300, lr = 0.01
I0925 12:26:08.390913  2600 solver.cpp:218] Iteration 32400 (6.81545 iter/s, 14.6725s/100 iters), loss = 0.232521
I0925 12:26:08.390944  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232521 (* 1 = 0.232521 loss)
I0925 12:26:08.390949  2600 sgd_solver.cpp:105] Iteration 32400, lr = 0.01
I0925 12:26:22.384608  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:26:22.966930  2600 solver.cpp:330] Iteration 32500, Testing net (#0)
I0925 12:26:26.455020  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:26:26.597498  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8288
I0925 12:26:26.597534  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.52688 (* 1 = 0.52688 loss)
I0925 12:26:26.740898  2600 solver.cpp:218] Iteration 32500 (5.44962 iter/s, 18.3499s/100 iters), loss = 0.267229
I0925 12:26:26.740926  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267228 (* 1 = 0.267228 loss)
I0925 12:26:26.740931  2600 sgd_solver.cpp:105] Iteration 32500, lr = 0.01
I0925 12:26:41.572409  2600 solver.cpp:218] Iteration 32600 (6.74243 iter/s, 14.8314s/100 iters), loss = 0.201268
I0925 12:26:41.572438  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201267 (* 1 = 0.201267 loss)
I0925 12:26:41.572445  2600 sgd_solver.cpp:105] Iteration 32600, lr = 0.01
I0925 12:26:56.242278  2600 solver.cpp:218] Iteration 32700 (6.81672 iter/s, 14.6698s/100 iters), loss = 0.369468
I0925 12:26:56.242410  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.369467 (* 1 = 0.369467 loss)
I0925 12:26:56.242418  2600 sgd_solver.cpp:105] Iteration 32700, lr = 0.01
I0925 12:27:10.903204  2600 solver.cpp:218] Iteration 32800 (6.82093 iter/s, 14.6608s/100 iters), loss = 0.513429
I0925 12:27:10.903234  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.513428 (* 1 = 0.513428 loss)
I0925 12:27:10.903240  2600 sgd_solver.cpp:105] Iteration 32800, lr = 0.01
I0925 12:27:25.503882  2600 solver.cpp:218] Iteration 32900 (6.84903 iter/s, 14.6006s/100 iters), loss = 0.297749
I0925 12:27:25.503921  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297749 (* 1 = 0.297749 loss)
I0925 12:27:25.503927  2600 sgd_solver.cpp:105] Iteration 32900, lr = 0.01
I0925 12:27:39.377866  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:27:39.962260  2600 solver.cpp:330] Iteration 33000, Testing net (#0)
I0925 12:27:43.389647  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:27:43.532656  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8269
I0925 12:27:43.532691  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.512789 (* 1 = 0.512789 loss)
I0925 12:27:43.677698  2600 solver.cpp:218] Iteration 33000 (5.50245 iter/s, 18.1737s/100 iters), loss = 0.302704
I0925 12:27:43.677727  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.302703 (* 1 = 0.302703 loss)
I0925 12:27:43.677734  2600 sgd_solver.cpp:105] Iteration 33000, lr = 0.01
I0925 12:27:58.272441  2600 solver.cpp:218] Iteration 33100 (6.85181 iter/s, 14.5947s/100 iters), loss = 0.193884
I0925 12:27:58.272503  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193883 (* 1 = 0.193883 loss)
I0925 12:27:58.272511  2600 sgd_solver.cpp:105] Iteration 33100, lr = 0.01
I0925 12:28:12.880291  2600 solver.cpp:218] Iteration 33200 (6.84568 iter/s, 14.6078s/100 iters), loss = 0.39913
I0925 12:28:12.880477  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.39913 (* 1 = 0.39913 loss)
I0925 12:28:12.880503  2600 sgd_solver.cpp:105] Iteration 33200, lr = 0.01
I0925 12:28:27.478185  2600 solver.cpp:218] Iteration 33300 (6.8504 iter/s, 14.5977s/100 iters), loss = 0.300739
I0925 12:28:27.478217  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.300738 (* 1 = 0.300738 loss)
I0925 12:28:27.478224  2600 sgd_solver.cpp:105] Iteration 33300, lr = 0.01
I0925 12:28:42.077565  2600 solver.cpp:218] Iteration 33400 (6.84964 iter/s, 14.5993s/100 iters), loss = 0.232854
I0925 12:28:42.077596  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232853 (* 1 = 0.232853 loss)
I0925 12:28:42.077612  2600 sgd_solver.cpp:105] Iteration 33400, lr = 0.01
I0925 12:28:55.949774  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:28:56.531734  2600 solver.cpp:330] Iteration 33500, Testing net (#0)
I0925 12:28:59.960821  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:29:00.103982  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8409
I0925 12:29:00.104018  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.483834 (* 1 = 0.483834 loss)
I0925 12:29:00.249343  2600 solver.cpp:218] Iteration 33500 (5.50306 iter/s, 18.1717s/100 iters), loss = 0.276875
I0925 12:29:00.249373  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276874 (* 1 = 0.276874 loss)
I0925 12:29:00.249380  2600 sgd_solver.cpp:105] Iteration 33500, lr = 0.01
I0925 12:29:14.832732  2600 solver.cpp:218] Iteration 33600 (6.85715 iter/s, 14.5833s/100 iters), loss = 0.258316
I0925 12:29:14.832762  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258315 (* 1 = 0.258315 loss)
I0925 12:29:14.832768  2600 sgd_solver.cpp:105] Iteration 33600, lr = 0.01
I0925 12:29:29.423599  2600 solver.cpp:218] Iteration 33700 (6.85363 iter/s, 14.5908s/100 iters), loss = 0.321683
I0925 12:29:29.423709  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.321683 (* 1 = 0.321683 loss)
I0925 12:29:29.423717  2600 sgd_solver.cpp:105] Iteration 33700, lr = 0.01
I0925 12:29:44.014403  2600 solver.cpp:218] Iteration 33800 (6.85369 iter/s, 14.5907s/100 iters), loss = 0.323905
I0925 12:29:44.014444  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.323905 (* 1 = 0.323905 loss)
I0925 12:29:44.014451  2600 sgd_solver.cpp:105] Iteration 33800, lr = 0.01
I0925 12:29:58.608528  2600 solver.cpp:218] Iteration 33900 (6.85211 iter/s, 14.594s/100 iters), loss = 0.202503
I0925 12:29:58.608559  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202503 (* 1 = 0.202503 loss)
I0925 12:29:58.608566  2600 sgd_solver.cpp:105] Iteration 33900, lr = 0.01
I0925 12:30:12.479202  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:30:13.063556  2600 solver.cpp:330] Iteration 34000, Testing net (#0)
I0925 12:30:16.490613  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:30:16.633646  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8381
I0925 12:30:16.633682  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.490268 (* 1 = 0.490268 loss)
I0925 12:30:16.779268  2600 solver.cpp:218] Iteration 34000 (5.50338 iter/s, 18.1707s/100 iters), loss = 0.249722
I0925 12:30:16.779299  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249722 (* 1 = 0.249722 loss)
I0925 12:30:16.779305  2600 sgd_solver.cpp:105] Iteration 34000, lr = 0.01
I0925 12:30:31.375922  2600 solver.cpp:218] Iteration 34100 (6.85092 iter/s, 14.5966s/100 iters), loss = 0.249897
I0925 12:30:31.375963  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249897 (* 1 = 0.249897 loss)
I0925 12:30:31.375970  2600 sgd_solver.cpp:105] Iteration 34100, lr = 0.01
I0925 12:30:45.976946  2600 solver.cpp:218] Iteration 34200 (6.84887 iter/s, 14.6009s/100 iters), loss = 0.308143
I0925 12:30:45.977079  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.308143 (* 1 = 0.308143 loss)
I0925 12:30:45.977088  2600 sgd_solver.cpp:105] Iteration 34200, lr = 0.01
I0925 12:31:00.577011  2600 solver.cpp:218] Iteration 34300 (6.84936 iter/s, 14.5999s/100 iters), loss = 0.304329
I0925 12:31:00.577042  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.304328 (* 1 = 0.304328 loss)
I0925 12:31:00.577049  2600 sgd_solver.cpp:105] Iteration 34300, lr = 0.01
I0925 12:31:15.184103  2600 solver.cpp:218] Iteration 34400 (6.84602 iter/s, 14.607s/100 iters), loss = 0.171968
I0925 12:31:15.184144  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171968 (* 1 = 0.171968 loss)
I0925 12:31:15.184149  2600 sgd_solver.cpp:105] Iteration 34400, lr = 0.01
I0925 12:31:29.056910  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:31:29.640769  2600 solver.cpp:330] Iteration 34500, Testing net (#0)
I0925 12:31:33.067052  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:31:33.209743  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8331
I0925 12:31:33.209780  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.508318 (* 1 = 0.508318 loss)
I0925 12:31:33.354204  2600 solver.cpp:218] Iteration 34500 (5.50357 iter/s, 18.17s/100 iters), loss = 0.201773
I0925 12:31:33.354233  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201772 (* 1 = 0.201772 loss)
I0925 12:31:33.354238  2600 sgd_solver.cpp:105] Iteration 34500, lr = 0.01
I0925 12:31:47.953399  2600 solver.cpp:218] Iteration 34600 (6.84972 iter/s, 14.5991s/100 iters), loss = 0.261189
I0925 12:31:47.953440  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261189 (* 1 = 0.261189 loss)
I0925 12:31:47.953446  2600 sgd_solver.cpp:105] Iteration 34600, lr = 0.01
I0925 12:32:02.555800  2600 solver.cpp:218] Iteration 34700 (6.84822 iter/s, 14.6023s/100 iters), loss = 0.236851
I0925 12:32:02.555943  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236851 (* 1 = 0.236851 loss)
I0925 12:32:02.555953  2600 sgd_solver.cpp:105] Iteration 34700, lr = 0.01
I0925 12:32:17.156924  2600 solver.cpp:218] Iteration 34800 (6.84887 iter/s, 14.6009s/100 iters), loss = 0.303127
I0925 12:32:17.156965  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303127 (* 1 = 0.303127 loss)
I0925 12:32:17.156971  2600 sgd_solver.cpp:105] Iteration 34800, lr = 0.01
I0925 12:32:31.757001  2600 solver.cpp:218] Iteration 34900 (6.84932 iter/s, 14.6s/100 iters), loss = 0.155326
I0925 12:32:31.757033  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.155325 (* 1 = 0.155325 loss)
I0925 12:32:31.757040  2600 sgd_solver.cpp:105] Iteration 34900, lr = 0.01
I0925 12:32:45.638764  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:32:46.222826  2600 solver.cpp:330] Iteration 35000, Testing net (#0)
I0925 12:32:49.649322  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:32:49.792142  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8104
I0925 12:32:49.792178  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.606335 (* 1 = 0.606335 loss)
I0925 12:32:49.937268  2600 solver.cpp:218] Iteration 35000 (5.50049 iter/s, 18.1802s/100 iters), loss = 0.210351
I0925 12:32:49.937296  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210351 (* 1 = 0.210351 loss)
I0925 12:32:49.937304  2600 sgd_solver.cpp:105] Iteration 35000, lr = 0.01
I0925 12:33:04.545959  2600 solver.cpp:218] Iteration 35100 (6.84527 iter/s, 14.6086s/100 iters), loss = 0.266329
I0925 12:33:04.546000  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266329 (* 1 = 0.266329 loss)
I0925 12:33:04.546007  2600 sgd_solver.cpp:105] Iteration 35100, lr = 0.01
I0925 12:33:19.151082  2600 solver.cpp:218] Iteration 35200 (6.84695 iter/s, 14.605s/100 iters), loss = 0.269068
I0925 12:33:19.151214  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269068 (* 1 = 0.269068 loss)
I0925 12:33:19.151232  2600 sgd_solver.cpp:105] Iteration 35200, lr = 0.01
I0925 12:33:33.749842  2600 solver.cpp:218] Iteration 35300 (6.84997 iter/s, 14.5986s/100 iters), loss = 0.270617
I0925 12:33:33.749883  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270617 (* 1 = 0.270617 loss)
I0925 12:33:33.749891  2600 sgd_solver.cpp:105] Iteration 35300, lr = 0.01
I0925 12:33:48.348240  2600 solver.cpp:218] Iteration 35400 (6.8501 iter/s, 14.5983s/100 iters), loss = 0.164903
I0925 12:33:48.348273  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164902 (* 1 = 0.164902 loss)
I0925 12:33:48.348278  2600 sgd_solver.cpp:105] Iteration 35400, lr = 0.01
I0925 12:34:02.228719  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:34:02.812880  2600 solver.cpp:330] Iteration 35500, Testing net (#0)
I0925 12:34:06.239506  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:34:06.382601  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8221
I0925 12:34:06.382635  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.564136 (* 1 = 0.564136 loss)
I0925 12:34:06.528137  2600 solver.cpp:218] Iteration 35500 (5.5006 iter/s, 18.1798s/100 iters), loss = 0.176125
I0925 12:34:06.528172  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176124 (* 1 = 0.176124 loss)
I0925 12:34:06.528178  2600 sgd_solver.cpp:105] Iteration 35500, lr = 0.01
I0925 12:34:21.121091  2600 solver.cpp:218] Iteration 35600 (6.85266 iter/s, 14.5929s/100 iters), loss = 0.281976
I0925 12:34:21.121132  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281976 (* 1 = 0.281976 loss)
I0925 12:34:21.121139  2600 sgd_solver.cpp:105] Iteration 35600, lr = 0.01
I0925 12:34:35.723418  2600 solver.cpp:218] Iteration 35700 (6.84826 iter/s, 14.6023s/100 iters), loss = 0.176373
I0925 12:34:35.723564  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176372 (* 1 = 0.176372 loss)
I0925 12:34:35.723572  2600 sgd_solver.cpp:105] Iteration 35700, lr = 0.01
I0925 12:34:50.327003  2600 solver.cpp:218] Iteration 35800 (6.84772 iter/s, 14.6034s/100 iters), loss = 0.261909
I0925 12:34:50.327033  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261909 (* 1 = 0.261909 loss)
I0925 12:34:50.327039  2600 sgd_solver.cpp:105] Iteration 35800, lr = 0.01
I0925 12:35:04.927642  2600 solver.cpp:218] Iteration 35900 (6.84905 iter/s, 14.6006s/100 iters), loss = 0.201857
I0925 12:35:04.927683  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201857 (* 1 = 0.201857 loss)
I0925 12:35:04.927690  2600 sgd_solver.cpp:105] Iteration 35900, lr = 0.01
I0925 12:35:18.810109  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:35:19.394529  2600 solver.cpp:330] Iteration 36000, Testing net (#0)
I0925 12:35:22.823056  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:35:22.965947  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8351
I0925 12:35:22.965984  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.507982 (* 1 = 0.507982 loss)
I0925 12:35:23.110805  2600 solver.cpp:218] Iteration 36000 (5.49962 iter/s, 18.1831s/100 iters), loss = 0.192132
I0925 12:35:23.110832  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192132 (* 1 = 0.192132 loss)
I0925 12:35:23.110839  2600 sgd_solver.cpp:105] Iteration 36000, lr = 0.01
I0925 12:35:37.699580  2600 solver.cpp:218] Iteration 36100 (6.85461 iter/s, 14.5887s/100 iters), loss = 0.337901
I0925 12:35:37.699620  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.3379 (* 1 = 0.3379 loss)
I0925 12:35:37.699626  2600 sgd_solver.cpp:105] Iteration 36100, lr = 0.01
I0925 12:35:52.292812  2600 solver.cpp:218] Iteration 36200 (6.85253 iter/s, 14.5932s/100 iters), loss = 0.197309
I0925 12:35:52.292899  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197309 (* 1 = 0.197309 loss)
I0925 12:35:52.292915  2600 sgd_solver.cpp:105] Iteration 36200, lr = 0.01
I0925 12:36:06.885885  2600 solver.cpp:218] Iteration 36300 (6.85262 iter/s, 14.593s/100 iters), loss = 0.328848
I0925 12:36:06.885926  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.328847 (* 1 = 0.328847 loss)
I0925 12:36:06.885933  2600 sgd_solver.cpp:105] Iteration 36300, lr = 0.01
I0925 12:36:21.476941  2600 solver.cpp:218] Iteration 36400 (6.85355 iter/s, 14.591s/100 iters), loss = 0.148237
I0925 12:36:21.476984  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148236 (* 1 = 0.148236 loss)
I0925 12:36:21.476991  2600 sgd_solver.cpp:105] Iteration 36400, lr = 0.01
I0925 12:36:35.346738  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:36:35.930354  2600 solver.cpp:330] Iteration 36500, Testing net (#0)
I0925 12:36:39.357072  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:36:39.500139  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8248
I0925 12:36:39.500176  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.532739 (* 1 = 0.532739 loss)
I0925 12:36:39.644976  2600 solver.cpp:218] Iteration 36500 (5.5042 iter/s, 18.1679s/100 iters), loss = 0.283406
I0925 12:36:39.645010  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283406 (* 1 = 0.283406 loss)
I0925 12:36:39.645017  2600 sgd_solver.cpp:105] Iteration 36500, lr = 0.01
I0925 12:36:54.235268  2600 solver.cpp:218] Iteration 36600 (6.85391 iter/s, 14.5902s/100 iters), loss = 0.258292
I0925 12:36:54.235296  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258292 (* 1 = 0.258292 loss)
I0925 12:36:54.235302  2600 sgd_solver.cpp:105] Iteration 36600, lr = 0.01
I0925 12:37:08.834621  2600 solver.cpp:218] Iteration 36700 (6.84965 iter/s, 14.5993s/100 iters), loss = 0.259727
I0925 12:37:08.834734  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.259726 (* 1 = 0.259726 loss)
I0925 12:37:08.834753  2600 sgd_solver.cpp:105] Iteration 36700, lr = 0.01
I0925 12:37:23.434892  2600 solver.cpp:218] Iteration 36800 (6.84925 iter/s, 14.6001s/100 iters), loss = 0.226809
I0925 12:37:23.434933  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226808 (* 1 = 0.226808 loss)
I0925 12:37:23.434939  2600 sgd_solver.cpp:105] Iteration 36800, lr = 0.01
I0925 12:37:38.038476  2600 solver.cpp:218] Iteration 36900 (6.84767 iter/s, 14.6035s/100 iters), loss = 0.145133
I0925 12:37:38.038516  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145132 (* 1 = 0.145132 loss)
I0925 12:37:38.038522  2600 sgd_solver.cpp:105] Iteration 36900, lr = 0.01
I0925 12:37:51.916963  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:37:52.500519  2600 solver.cpp:330] Iteration 37000, Testing net (#0)
I0925 12:37:55.927861  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:37:56.070526  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8302
I0925 12:37:56.070562  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.519316 (* 1 = 0.519316 loss)
I0925 12:37:56.215843  2600 solver.cpp:218] Iteration 37000 (5.50137 iter/s, 18.1773s/100 iters), loss = 0.213792
I0925 12:37:56.215873  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213791 (* 1 = 0.213791 loss)
I0925 12:37:56.215878  2600 sgd_solver.cpp:105] Iteration 37000, lr = 0.01
I0925 12:38:10.820135  2600 solver.cpp:218] Iteration 37100 (6.84733 iter/s, 14.6042s/100 iters), loss = 0.192181
I0925 12:38:10.820166  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192181 (* 1 = 0.192181 loss)
I0925 12:38:10.820173  2600 sgd_solver.cpp:105] Iteration 37100, lr = 0.01
I0925 12:38:25.429319  2600 solver.cpp:218] Iteration 37200 (6.84504 iter/s, 14.6091s/100 iters), loss = 0.31897
I0925 12:38:25.429432  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.31897 (* 1 = 0.31897 loss)
I0925 12:38:25.429450  2600 sgd_solver.cpp:105] Iteration 37200, lr = 0.01
I0925 12:38:40.034699  2600 solver.cpp:218] Iteration 37300 (6.84686 iter/s, 14.6052s/100 iters), loss = 0.235144
I0925 12:38:40.034739  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235143 (* 1 = 0.235143 loss)
I0925 12:38:40.034745  2600 sgd_solver.cpp:105] Iteration 37300, lr = 0.01
I0925 12:38:54.636247  2600 solver.cpp:218] Iteration 37400 (6.84862 iter/s, 14.6015s/100 iters), loss = 0.186449
I0925 12:38:54.636279  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186448 (* 1 = 0.186448 loss)
I0925 12:38:54.636286  2600 sgd_solver.cpp:105] Iteration 37400, lr = 0.01
I0925 12:39:08.517688  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:39:09.101259  2600 solver.cpp:330] Iteration 37500, Testing net (#0)
I0925 12:39:12.527200  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:39:12.670362  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8433
I0925 12:39:12.670399  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.475908 (* 1 = 0.475908 loss)
I0925 12:39:12.815268  2600 solver.cpp:218] Iteration 37500 (5.50087 iter/s, 18.1789s/100 iters), loss = 0.169692
I0925 12:39:12.815300  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169691 (* 1 = 0.169691 loss)
I0925 12:39:12.815307  2600 sgd_solver.cpp:105] Iteration 37500, lr = 0.01
I0925 12:39:27.413836  2600 solver.cpp:218] Iteration 37600 (6.85002 iter/s, 14.5985s/100 iters), loss = 0.245782
I0925 12:39:27.413875  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245781 (* 1 = 0.245781 loss)
I0925 12:39:27.413882  2600 sgd_solver.cpp:105] Iteration 37600, lr = 0.01
I0925 12:39:42.016350  2600 solver.cpp:218] Iteration 37700 (6.84817 iter/s, 14.6024s/100 iters), loss = 0.237791
I0925 12:39:42.016474  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23779 (* 1 = 0.23779 loss)
I0925 12:39:42.016491  2600 sgd_solver.cpp:105] Iteration 37700, lr = 0.01
I0925 12:39:56.620817  2600 solver.cpp:218] Iteration 37800 (6.84729 iter/s, 14.6043s/100 iters), loss = 0.231164
I0925 12:39:56.620848  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231163 (* 1 = 0.231163 loss)
I0925 12:39:56.620854  2600 sgd_solver.cpp:105] Iteration 37800, lr = 0.01
I0925 12:40:11.233199  2600 solver.cpp:218] Iteration 37900 (6.84354 iter/s, 14.6123s/100 iters), loss = 0.208779
I0925 12:40:11.233230  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208778 (* 1 = 0.208778 loss)
I0925 12:40:11.233247  2600 sgd_solver.cpp:105] Iteration 37900, lr = 0.01
I0925 12:40:25.107290  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:40:25.693327  2600 solver.cpp:330] Iteration 38000, Testing net (#0)
I0925 12:40:29.118744  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:40:29.261579  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8479
I0925 12:40:29.261603  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.455383 (* 1 = 0.455383 loss)
I0925 12:40:29.406600  2600 solver.cpp:218] Iteration 38000 (5.50257 iter/s, 18.1733s/100 iters), loss = 0.230908
I0925 12:40:29.406628  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.230907 (* 1 = 0.230907 loss)
I0925 12:40:29.406635  2600 sgd_solver.cpp:105] Iteration 38000, lr = 0.01
I0925 12:40:44.005931  2600 solver.cpp:218] Iteration 38100 (6.84966 iter/s, 14.5993s/100 iters), loss = 0.165386
I0925 12:40:44.005961  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165386 (* 1 = 0.165386 loss)
I0925 12:40:44.005967  2600 sgd_solver.cpp:105] Iteration 38100, lr = 0.01
I0925 12:40:58.604713  2600 solver.cpp:218] Iteration 38200 (6.84992 iter/s, 14.5987s/100 iters), loss = 0.190961
I0925 12:40:58.604840  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19096 (* 1 = 0.19096 loss)
I0925 12:40:58.604849  2600 sgd_solver.cpp:105] Iteration 38200, lr = 0.01
I0925 12:41:13.207275  2600 solver.cpp:218] Iteration 38300 (6.84818 iter/s, 14.6024s/100 iters), loss = 0.247475
I0925 12:41:13.207316  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247475 (* 1 = 0.247475 loss)
I0925 12:41:13.207322  2600 sgd_solver.cpp:105] Iteration 38300, lr = 0.01
I0925 12:41:27.836338  2600 solver.cpp:218] Iteration 38400 (6.83574 iter/s, 14.629s/100 iters), loss = 0.194891
I0925 12:41:27.836369  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194891 (* 1 = 0.194891 loss)
I0925 12:41:27.836375  2600 sgd_solver.cpp:105] Iteration 38400, lr = 0.01
I0925 12:41:41.759696  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:41:42.344422  2600 solver.cpp:330] Iteration 38500, Testing net (#0)
I0925 12:41:45.770261  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:41:45.912394  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8477
I0925 12:41:45.912428  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.475218 (* 1 = 0.475218 loss)
I0925 12:41:46.056450  2600 solver.cpp:218] Iteration 38500 (5.48846 iter/s, 18.22s/100 iters), loss = 0.21844
I0925 12:41:46.056479  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218439 (* 1 = 0.218439 loss)
I0925 12:41:46.056486  2600 sgd_solver.cpp:105] Iteration 38500, lr = 0.01
I0925 12:42:00.650897  2600 solver.cpp:218] Iteration 38600 (6.85195 iter/s, 14.5944s/100 iters), loss = 0.160496
I0925 12:42:00.650928  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160496 (* 1 = 0.160496 loss)
I0925 12:42:00.650933  2600 sgd_solver.cpp:105] Iteration 38600, lr = 0.01
I0925 12:42:15.235224  2600 solver.cpp:218] Iteration 38700 (6.85671 iter/s, 14.5843s/100 iters), loss = 0.220958
I0925 12:42:15.235347  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220958 (* 1 = 0.220958 loss)
I0925 12:42:15.235354  2600 sgd_solver.cpp:105] Iteration 38700, lr = 0.01
I0925 12:42:29.825740  2600 solver.cpp:218] Iteration 38800 (6.85384 iter/s, 14.5904s/100 iters), loss = 0.209184
I0925 12:42:29.825770  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209183 (* 1 = 0.209183 loss)
I0925 12:42:29.825775  2600 sgd_solver.cpp:105] Iteration 38800, lr = 0.01
I0925 12:42:44.417891  2600 solver.cpp:218] Iteration 38900 (6.85303 iter/s, 14.5921s/100 iters), loss = 0.213564
I0925 12:42:44.417932  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213563 (* 1 = 0.213563 loss)
I0925 12:42:44.417937  2600 sgd_solver.cpp:105] Iteration 38900, lr = 0.01
I0925 12:42:58.287166  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:42:58.870527  2600 solver.cpp:330] Iteration 39000, Testing net (#0)
I0925 12:43:02.300412  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:43:02.443528  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8294
I0925 12:43:02.443564  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.529386 (* 1 = 0.529386 loss)
I0925 12:43:02.588513  2600 solver.cpp:218] Iteration 39000 (5.50341 iter/s, 18.1705s/100 iters), loss = 0.148947
I0925 12:43:02.588543  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148946 (* 1 = 0.148946 loss)
I0925 12:43:02.588551  2600 sgd_solver.cpp:105] Iteration 39000, lr = 0.01
I0925 12:43:17.184556  2600 solver.cpp:218] Iteration 39100 (6.8512 iter/s, 14.596s/100 iters), loss = 0.235127
I0925 12:43:17.184597  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235126 (* 1 = 0.235126 loss)
I0925 12:43:17.184603  2600 sgd_solver.cpp:105] Iteration 39100, lr = 0.01
I0925 12:43:31.785859  2600 solver.cpp:218] Iteration 39200 (6.84874 iter/s, 14.6012s/100 iters), loss = 0.24254
I0925 12:43:31.786003  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24254 (* 1 = 0.24254 loss)
I0925 12:43:31.786012  2600 sgd_solver.cpp:105] Iteration 39200, lr = 0.01
I0925 12:43:46.392626  2600 solver.cpp:218] Iteration 39300 (6.84622 iter/s, 14.6066s/100 iters), loss = 0.210544
I0925 12:43:46.392654  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210543 (* 1 = 0.210543 loss)
I0925 12:43:46.392660  2600 sgd_solver.cpp:105] Iteration 39300, lr = 0.01
I0925 12:44:00.991971  2600 solver.cpp:218] Iteration 39400 (6.84965 iter/s, 14.5993s/100 iters), loss = 0.213834
I0925 12:44:00.992002  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213834 (* 1 = 0.213834 loss)
I0925 12:44:00.992009  2600 sgd_solver.cpp:105] Iteration 39400, lr = 0.01
I0925 12:44:14.872239  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:44:15.455670  2600 solver.cpp:330] Iteration 39500, Testing net (#0)
I0925 12:44:18.882213  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:44:19.024297  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8321
I0925 12:44:19.024322  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.523702 (* 1 = 0.523702 loss)
I0925 12:44:19.169576  2600 solver.cpp:218] Iteration 39500 (5.5013 iter/s, 18.1775s/100 iters), loss = 0.177125
I0925 12:44:19.169610  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177124 (* 1 = 0.177124 loss)
I0925 12:44:19.169616  2600 sgd_solver.cpp:105] Iteration 39500, lr = 0.01
I0925 12:44:33.771018  2600 solver.cpp:218] Iteration 39600 (6.84867 iter/s, 14.6014s/100 iters), loss = 0.239624
I0925 12:44:33.771059  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239623 (* 1 = 0.239623 loss)
I0925 12:44:33.771066  2600 sgd_solver.cpp:105] Iteration 39600, lr = 0.01
I0925 12:44:48.370496  2600 solver.cpp:218] Iteration 39700 (6.8496 iter/s, 14.5994s/100 iters), loss = 0.240498
I0925 12:44:48.370618  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240498 (* 1 = 0.240498 loss)
I0925 12:44:48.370635  2600 sgd_solver.cpp:105] Iteration 39700, lr = 0.01
I0925 12:45:02.970363  2600 solver.cpp:218] Iteration 39800 (6.84945 iter/s, 14.5997s/100 iters), loss = 0.180036
I0925 12:45:02.970405  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180035 (* 1 = 0.180035 loss)
I0925 12:45:02.970412  2600 sgd_solver.cpp:105] Iteration 39800, lr = 0.01
I0925 12:45:17.569422  2600 solver.cpp:218] Iteration 39900 (6.84979 iter/s, 14.599s/100 iters), loss = 0.198605
I0925 12:45:17.569453  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198604 (* 1 = 0.198604 loss)
I0925 12:45:17.569458  2600 sgd_solver.cpp:105] Iteration 39900, lr = 0.01
I0925 12:45:31.452813  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:45:32.036548  2600 solver.cpp:330] Iteration 40000, Testing net (#0)
I0925 12:45:35.463179  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:45:35.605774  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8245
I0925 12:45:35.605811  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.550603 (* 1 = 0.550603 loss)
I0925 12:45:35.750002  2600 solver.cpp:218] Iteration 40000 (5.5004 iter/s, 18.1805s/100 iters), loss = 0.17486
I0925 12:45:35.750033  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.174859 (* 1 = 0.174859 loss)
I0925 12:45:35.750039  2600 sgd_solver.cpp:46] MultiStep Status: Iteration 40000, step = 2
I0925 12:45:35.750043  2600 sgd_solver.cpp:105] Iteration 40000, lr = 0.001
I0925 12:45:50.351379  2600 solver.cpp:218] Iteration 40100 (6.8487 iter/s, 14.6013s/100 iters), loss = 0.238306
I0925 12:45:50.351419  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238305 (* 1 = 0.238305 loss)
I0925 12:45:50.351425  2600 sgd_solver.cpp:105] Iteration 40100, lr = 0.001
I0925 12:46:04.962155  2600 solver.cpp:218] Iteration 40200 (6.8443 iter/s, 14.6107s/100 iters), loss = 0.131937
I0925 12:46:04.962255  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131936 (* 1 = 0.131936 loss)
I0925 12:46:04.962261  2600 sgd_solver.cpp:105] Iteration 40200, lr = 0.001
I0925 12:46:19.566972  2600 solver.cpp:218] Iteration 40300 (6.84712 iter/s, 14.6047s/100 iters), loss = 0.245666
I0925 12:46:19.567013  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245665 (* 1 = 0.245665 loss)
I0925 12:46:19.567018  2600 sgd_solver.cpp:105] Iteration 40300, lr = 0.001
I0925 12:46:34.175170  2600 solver.cpp:218] Iteration 40400 (6.84551 iter/s, 14.6081s/100 iters), loss = 0.150972
I0925 12:46:34.175202  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.150971 (* 1 = 0.150971 loss)
I0925 12:46:34.175218  2600 sgd_solver.cpp:105] Iteration 40400, lr = 0.001
I0925 12:46:48.053897  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:46:48.636260  2600 solver.cpp:330] Iteration 40500, Testing net (#0)
I0925 12:46:52.063948  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:46:52.206966  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8636
I0925 12:46:52.207001  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.419925 (* 1 = 0.419925 loss)
I0925 12:46:52.351413  2600 solver.cpp:218] Iteration 40500 (5.50171 iter/s, 18.1762s/100 iters), loss = 0.151868
I0925 12:46:52.351446  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151868 (* 1 = 0.151868 loss)
I0925 12:46:52.351454  2600 sgd_solver.cpp:105] Iteration 40500, lr = 0.001
I0925 12:47:06.950618  2600 solver.cpp:218] Iteration 40600 (6.84972 iter/s, 14.5991s/100 iters), loss = 0.1666
I0925 12:47:06.950659  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166599 (* 1 = 0.166599 loss)
I0925 12:47:06.950664  2600 sgd_solver.cpp:105] Iteration 40600, lr = 0.001
I0925 12:47:21.553560  2600 solver.cpp:218] Iteration 40700 (6.84797 iter/s, 14.6029s/100 iters), loss = 0.130687
I0925 12:47:21.553664  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130687 (* 1 = 0.130687 loss)
I0925 12:47:21.553673  2600 sgd_solver.cpp:105] Iteration 40700, lr = 0.001
I0925 12:47:36.149749  2600 solver.cpp:218] Iteration 40800 (6.85117 iter/s, 14.5961s/100 iters), loss = 0.170159
I0925 12:47:36.149780  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170158 (* 1 = 0.170158 loss)
I0925 12:47:36.149785  2600 sgd_solver.cpp:105] Iteration 40800, lr = 0.001
I0925 12:47:50.754149  2600 solver.cpp:218] Iteration 40900 (6.84728 iter/s, 14.6043s/100 iters), loss = 0.10542
I0925 12:47:50.754179  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105419 (* 1 = 0.105419 loss)
I0925 12:47:50.754185  2600 sgd_solver.cpp:105] Iteration 40900, lr = 0.001
I0925 12:48:04.633118  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:48:05.218089  2600 solver.cpp:330] Iteration 41000, Testing net (#0)
I0925 12:48:08.646914  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:48:08.789743  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8675
I0925 12:48:08.789769  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.409038 (* 1 = 0.409038 loss)
I0925 12:48:08.934295  2600 solver.cpp:218] Iteration 41000 (5.50053 iter/s, 18.1801s/100 iters), loss = 0.154548
I0925 12:48:08.934325  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154547 (* 1 = 0.154547 loss)
I0925 12:48:08.934332  2600 sgd_solver.cpp:105] Iteration 41000, lr = 0.001
I0925 12:48:23.518779  2600 solver.cpp:218] Iteration 41100 (6.85663 iter/s, 14.5844s/100 iters), loss = 0.155064
I0925 12:48:23.518810  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.155063 (* 1 = 0.155063 loss)
I0925 12:48:23.518816  2600 sgd_solver.cpp:105] Iteration 41100, lr = 0.001
I0925 12:48:38.109918  2600 solver.cpp:218] Iteration 41200 (6.85351 iter/s, 14.5911s/100 iters), loss = 0.160873
I0925 12:48:38.110031  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160872 (* 1 = 0.160872 loss)
I0925 12:48:38.110038  2600 sgd_solver.cpp:105] Iteration 41200, lr = 0.001
I0925 12:48:52.697453  2600 solver.cpp:218] Iteration 41300 (6.85524 iter/s, 14.5874s/100 iters), loss = 0.163057
I0925 12:48:52.697484  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163056 (* 1 = 0.163056 loss)
I0925 12:48:52.697489  2600 sgd_solver.cpp:105] Iteration 41300, lr = 0.001
I0925 12:49:07.288563  2600 solver.cpp:218] Iteration 41400 (6.85352 iter/s, 14.591s/100 iters), loss = 0.103378
I0925 12:49:07.288604  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103377 (* 1 = 0.103377 loss)
I0925 12:49:07.288611  2600 sgd_solver.cpp:105] Iteration 41400, lr = 0.001
I0925 12:49:21.152875  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:49:21.736737  2600 solver.cpp:330] Iteration 41500, Testing net (#0)
I0925 12:49:25.166123  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:49:25.309200  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8738
I0925 12:49:25.309223  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.393626 (* 1 = 0.393626 loss)
I0925 12:49:25.453919  2600 solver.cpp:218] Iteration 41500 (5.50501 iter/s, 18.1653s/100 iters), loss = 0.14833
I0925 12:49:25.453949  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14833 (* 1 = 0.14833 loss)
I0925 12:49:25.453956  2600 sgd_solver.cpp:105] Iteration 41500, lr = 0.001
I0925 12:49:40.049777  2600 solver.cpp:218] Iteration 41600 (6.85129 iter/s, 14.5958s/100 iters), loss = 0.181603
I0925 12:49:40.049816  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181603 (* 1 = 0.181603 loss)
I0925 12:49:40.049823  2600 sgd_solver.cpp:105] Iteration 41600, lr = 0.001
I0925 12:49:54.647943  2600 solver.cpp:218] Iteration 41700 (6.85021 iter/s, 14.5981s/100 iters), loss = 0.0955853
I0925 12:49:54.648062  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0955847 (* 1 = 0.0955847 loss)
I0925 12:49:54.648072  2600 sgd_solver.cpp:105] Iteration 41700, lr = 0.001
I0925 12:50:09.245064  2600 solver.cpp:218] Iteration 41800 (6.85074 iter/s, 14.597s/100 iters), loss = 0.172163
I0925 12:50:09.245093  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.172162 (* 1 = 0.172162 loss)
I0925 12:50:09.245100  2600 sgd_solver.cpp:105] Iteration 41800, lr = 0.001
I0925 12:50:23.843955  2600 solver.cpp:218] Iteration 41900 (6.84987 iter/s, 14.5988s/100 iters), loss = 0.0928059
I0925 12:50:23.843986  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0928052 (* 1 = 0.0928052 loss)
I0925 12:50:23.843991  2600 sgd_solver.cpp:105] Iteration 41900, lr = 0.001
I0925 12:50:37.714481  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:50:38.298491  2600 solver.cpp:330] Iteration 42000, Testing net (#0)
I0925 12:50:41.727452  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:50:41.870352  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8736
I0925 12:50:41.870378  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.396159 (* 1 = 0.396159 loss)
I0925 12:50:42.015831  2600 solver.cpp:218] Iteration 42000 (5.50303 iter/s, 18.1718s/100 iters), loss = 0.139229
I0925 12:50:42.015864  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139228 (* 1 = 0.139228 loss)
I0925 12:50:42.015874  2600 sgd_solver.cpp:105] Iteration 42000, lr = 0.001
I0925 12:50:56.613809  2600 solver.cpp:218] Iteration 42100 (6.8503 iter/s, 14.5979s/100 iters), loss = 0.133703
I0925 12:50:56.613842  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133702 (* 1 = 0.133702 loss)
I0925 12:50:56.613852  2600 sgd_solver.cpp:105] Iteration 42100, lr = 0.001
I0925 12:51:11.215713  2600 solver.cpp:218] Iteration 42200 (6.84845 iter/s, 14.6018s/100 iters), loss = 0.12674
I0925 12:51:11.215826  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126739 (* 1 = 0.126739 loss)
I0925 12:51:11.215836  2600 sgd_solver.cpp:105] Iteration 42200, lr = 0.001
I0925 12:51:25.819295  2600 solver.cpp:218] Iteration 42300 (6.8477 iter/s, 14.6034s/100 iters), loss = 0.138698
I0925 12:51:25.819337  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138697 (* 1 = 0.138697 loss)
I0925 12:51:25.819344  2600 sgd_solver.cpp:105] Iteration 42300, lr = 0.001
I0925 12:51:40.428215  2600 solver.cpp:218] Iteration 42400 (6.84517 iter/s, 14.6088s/100 iters), loss = 0.0643642
I0925 12:51:40.428244  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0643635 (* 1 = 0.0643635 loss)
I0925 12:51:40.428251  2600 sgd_solver.cpp:105] Iteration 42400, lr = 0.001
I0925 12:51:54.312018  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:51:54.897774  2600 solver.cpp:330] Iteration 42500, Testing net (#0)
I0925 12:51:58.324342  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:51:58.467440  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8698
I0925 12:51:58.467476  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.405495 (* 1 = 0.405495 loss)
I0925 12:51:58.612548  2600 solver.cpp:218] Iteration 42500 (5.49926 iter/s, 18.1843s/100 iters), loss = 0.117265
I0925 12:51:58.612577  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117265 (* 1 = 0.117265 loss)
I0925 12:51:58.612584  2600 sgd_solver.cpp:105] Iteration 42500, lr = 0.001
I0925 12:52:13.211287  2600 solver.cpp:218] Iteration 42600 (6.84994 iter/s, 14.5987s/100 iters), loss = 0.173101
I0925 12:52:13.211315  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1731 (* 1 = 0.1731 loss)
I0925 12:52:13.211323  2600 sgd_solver.cpp:105] Iteration 42600, lr = 0.001
I0925 12:52:27.812180  2600 solver.cpp:218] Iteration 42700 (6.84893 iter/s, 14.6008s/100 iters), loss = 0.110506
I0925 12:52:27.812297  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110505 (* 1 = 0.110505 loss)
I0925 12:52:27.812305  2600 sgd_solver.cpp:105] Iteration 42700, lr = 0.001
I0925 12:52:42.418860  2600 solver.cpp:218] Iteration 42800 (6.84625 iter/s, 14.6065s/100 iters), loss = 0.225851
I0925 12:52:42.418892  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.225851 (* 1 = 0.225851 loss)
I0925 12:52:42.418898  2600 sgd_solver.cpp:105] Iteration 42800, lr = 0.001
I0925 12:52:57.026789  2600 solver.cpp:218] Iteration 42900 (6.84563 iter/s, 14.6079s/100 iters), loss = 0.0989342
I0925 12:52:57.026819  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0989337 (* 1 = 0.0989337 loss)
I0925 12:52:57.026825  2600 sgd_solver.cpp:105] Iteration 42900, lr = 0.001
I0925 12:53:10.906123  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:53:11.490937  2600 solver.cpp:330] Iteration 43000, Testing net (#0)
I0925 12:53:14.919242  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:53:15.062403  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8718
I0925 12:53:15.062439  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.399882 (* 1 = 0.399882 loss)
I0925 12:53:15.206953  2600 solver.cpp:218] Iteration 43000 (5.50052 iter/s, 18.1801s/100 iters), loss = 0.0944036
I0925 12:53:15.206981  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0944031 (* 1 = 0.0944031 loss)
I0925 12:53:15.206989  2600 sgd_solver.cpp:105] Iteration 43000, lr = 0.001
I0925 12:53:29.830577  2600 solver.cpp:218] Iteration 43100 (6.83828 iter/s, 14.6236s/100 iters), loss = 0.22264
I0925 12:53:29.830607  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222639 (* 1 = 0.222639 loss)
I0925 12:53:29.830615  2600 sgd_solver.cpp:105] Iteration 43100, lr = 0.001
I0925 12:53:44.485561  2600 solver.cpp:218] Iteration 43200 (6.82365 iter/s, 14.6549s/100 iters), loss = 0.142482
I0925 12:53:44.485711  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.142481 (* 1 = 0.142481 loss)
I0925 12:53:44.485718  2600 sgd_solver.cpp:105] Iteration 43200, lr = 0.001
I0925 12:53:59.084164  2600 solver.cpp:218] Iteration 43300 (6.85006 iter/s, 14.5984s/100 iters), loss = 0.112847
I0925 12:53:59.084203  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.112846 (* 1 = 0.112846 loss)
I0925 12:53:59.084210  2600 sgd_solver.cpp:105] Iteration 43300, lr = 0.001
I0925 12:54:13.676718  2600 solver.cpp:218] Iteration 43400 (6.85285 iter/s, 14.5925s/100 iters), loss = 0.0859151
I0925 12:54:13.676749  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0859146 (* 1 = 0.0859146 loss)
I0925 12:54:13.676755  2600 sgd_solver.cpp:105] Iteration 43400, lr = 0.001
I0925 12:54:27.540416  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:54:28.124531  2600 solver.cpp:330] Iteration 43500, Testing net (#0)
I0925 12:54:31.552156  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:54:31.695219  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8679
I0925 12:54:31.695243  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.414306 (* 1 = 0.414306 loss)
I0925 12:54:31.839975  2600 solver.cpp:218] Iteration 43500 (5.50564 iter/s, 18.1632s/100 iters), loss = 0.159214
I0925 12:54:31.840008  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159213 (* 1 = 0.159213 loss)
I0925 12:54:31.840015  2600 sgd_solver.cpp:105] Iteration 43500, lr = 0.001
I0925 12:54:46.423015  2600 solver.cpp:218] Iteration 43600 (6.85731 iter/s, 14.583s/100 iters), loss = 0.12217
I0925 12:54:46.423044  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.122169 (* 1 = 0.122169 loss)
I0925 12:54:46.423050  2600 sgd_solver.cpp:105] Iteration 43600, lr = 0.001
I0925 12:55:01.015666  2600 solver.cpp:218] Iteration 43700 (6.8528 iter/s, 14.5926s/100 iters), loss = 0.126775
I0925 12:55:01.015806  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126774 (* 1 = 0.126774 loss)
I0925 12:55:01.015815  2600 sgd_solver.cpp:105] Iteration 43700, lr = 0.001
I0925 12:55:15.609645  2600 solver.cpp:218] Iteration 43800 (6.85222 iter/s, 14.5938s/100 iters), loss = 0.176725
I0925 12:55:15.609688  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176724 (* 1 = 0.176724 loss)
I0925 12:55:15.609695  2600 sgd_solver.cpp:105] Iteration 43800, lr = 0.001
I0925 12:55:30.195694  2600 solver.cpp:218] Iteration 43900 (6.8559 iter/s, 14.586s/100 iters), loss = 0.128659
I0925 12:55:30.195724  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.128659 (* 1 = 0.128659 loss)
I0925 12:55:30.195729  2600 sgd_solver.cpp:105] Iteration 43900, lr = 0.001
I0925 12:55:44.065690  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:55:44.649423  2600 solver.cpp:330] Iteration 44000, Testing net (#0)
I0925 12:55:48.075959  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:55:48.219236  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8722
I0925 12:55:48.219271  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.405682 (* 1 = 0.405682 loss)
I0925 12:55:48.364265  2600 solver.cpp:218] Iteration 44000 (5.50403 iter/s, 18.1685s/100 iters), loss = 0.0768146
I0925 12:55:48.364295  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.076814 (* 1 = 0.076814 loss)
I0925 12:55:48.364302  2600 sgd_solver.cpp:105] Iteration 44000, lr = 0.001
I0925 12:56:02.967414  2600 solver.cpp:218] Iteration 44100 (6.84787 iter/s, 14.6031s/100 iters), loss = 0.132151
I0925 12:56:02.967453  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132151 (* 1 = 0.132151 loss)
I0925 12:56:02.967459  2600 sgd_solver.cpp:105] Iteration 44100, lr = 0.001
I0925 12:56:17.570349  2600 solver.cpp:218] Iteration 44200 (6.84797 iter/s, 14.6029s/100 iters), loss = 0.0960509
I0925 12:56:17.570471  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0960504 (* 1 = 0.0960504 loss)
I0925 12:56:17.570478  2600 sgd_solver.cpp:105] Iteration 44200, lr = 0.001
I0925 12:56:32.178508  2600 solver.cpp:218] Iteration 44300 (6.84556 iter/s, 14.608s/100 iters), loss = 0.118872
I0925 12:56:32.178539  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118871 (* 1 = 0.118871 loss)
I0925 12:56:32.178544  2600 sgd_solver.cpp:105] Iteration 44300, lr = 0.001
I0925 12:56:46.785753  2600 solver.cpp:218] Iteration 44400 (6.84595 iter/s, 14.6072s/100 iters), loss = 0.095132
I0925 12:56:46.785785  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0951315 (* 1 = 0.0951315 loss)
I0925 12:56:46.785791  2600 sgd_solver.cpp:105] Iteration 44400, lr = 0.001
I0925 12:57:00.666534  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:57:01.251461  2600 solver.cpp:330] Iteration 44500, Testing net (#0)
I0925 12:57:04.679136  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:57:04.821606  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8708
I0925 12:57:04.821642  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.408043 (* 1 = 0.408043 loss)
I0925 12:57:04.966771  2600 solver.cpp:218] Iteration 44500 (5.50027 iter/s, 18.1809s/100 iters), loss = 0.0661093
I0925 12:57:04.966801  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0661087 (* 1 = 0.0661087 loss)
I0925 12:57:04.966809  2600 sgd_solver.cpp:105] Iteration 44500, lr = 0.001
I0925 12:57:19.565946  2600 solver.cpp:218] Iteration 44600 (6.84973 iter/s, 14.5991s/100 iters), loss = 0.162277
I0925 12:57:19.565979  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.162277 (* 1 = 0.162277 loss)
I0925 12:57:19.565984  2600 sgd_solver.cpp:105] Iteration 44600, lr = 0.001
I0925 12:57:34.165999  2600 solver.cpp:218] Iteration 44700 (6.84932 iter/s, 14.6s/100 iters), loss = 0.147538
I0925 12:57:34.166115  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.147537 (* 1 = 0.147537 loss)
I0925 12:57:34.166122  2600 sgd_solver.cpp:105] Iteration 44700, lr = 0.001
I0925 12:57:48.764757  2600 solver.cpp:218] Iteration 44800 (6.84996 iter/s, 14.5986s/100 iters), loss = 0.141725
I0925 12:57:48.764788  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141725 (* 1 = 0.141725 loss)
I0925 12:57:48.764796  2600 sgd_solver.cpp:105] Iteration 44800, lr = 0.001
I0925 12:58:03.363914  2600 solver.cpp:218] Iteration 44900 (6.84974 iter/s, 14.5991s/100 iters), loss = 0.0570556
I0925 12:58:03.363943  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0570551 (* 1 = 0.0570551 loss)
I0925 12:58:03.363950  2600 sgd_solver.cpp:105] Iteration 44900, lr = 0.001
I0925 12:58:17.241683  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:58:17.826063  2600 solver.cpp:330] Iteration 45000, Testing net (#0)
I0925 12:58:21.252382  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:58:21.394701  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8732
I0925 12:58:21.394737  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.408403 (* 1 = 0.408403 loss)
I0925 12:58:21.539327  2600 solver.cpp:218] Iteration 45000 (5.50196 iter/s, 18.1753s/100 iters), loss = 0.105369
I0925 12:58:21.539355  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105369 (* 1 = 0.105369 loss)
I0925 12:58:21.539363  2600 sgd_solver.cpp:105] Iteration 45000, lr = 0.001
I0925 12:58:36.136639  2600 solver.cpp:218] Iteration 45100 (6.85061 iter/s, 14.5972s/100 iters), loss = 0.157026
I0925 12:58:36.136670  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.157026 (* 1 = 0.157026 loss)
I0925 12:58:36.136677  2600 sgd_solver.cpp:105] Iteration 45100, lr = 0.001
I0925 12:58:50.739210  2600 solver.cpp:218] Iteration 45200 (6.84814 iter/s, 14.6025s/100 iters), loss = 0.147068
I0925 12:58:50.739326  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.147068 (* 1 = 0.147068 loss)
I0925 12:58:50.739333  2600 sgd_solver.cpp:105] Iteration 45200, lr = 0.001
I0925 12:59:05.352519  2600 solver.cpp:218] Iteration 45300 (6.84315 iter/s, 14.6132s/100 iters), loss = 0.148923
I0925 12:59:05.352561  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148922 (* 1 = 0.148922 loss)
I0925 12:59:05.352567  2600 sgd_solver.cpp:105] Iteration 45300, lr = 0.001
I0925 12:59:19.955492  2600 solver.cpp:218] Iteration 45400 (6.84796 iter/s, 14.6029s/100 iters), loss = 0.098208
I0925 12:59:19.955523  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0982075 (* 1 = 0.0982075 loss)
I0925 12:59:19.955528  2600 sgd_solver.cpp:105] Iteration 45400, lr = 0.001
I0925 12:59:33.834918  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:59:34.418314  2600 solver.cpp:330] Iteration 45500, Testing net (#0)
I0925 12:59:37.845304  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 12:59:37.988450  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8743
I0925 12:59:37.988487  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.409792 (* 1 = 0.409792 loss)
I0925 12:59:38.133364  2600 solver.cpp:218] Iteration 45500 (5.50122 iter/s, 18.1778s/100 iters), loss = 0.106214
I0925 12:59:38.133394  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106213 (* 1 = 0.106213 loss)
I0925 12:59:38.133400  2600 sgd_solver.cpp:105] Iteration 45500, lr = 0.001
I0925 12:59:52.738533  2600 solver.cpp:218] Iteration 45600 (6.84692 iter/s, 14.6051s/100 iters), loss = 0.179165
I0925 12:59:52.738574  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179165 (* 1 = 0.179165 loss)
I0925 12:59:52.738580  2600 sgd_solver.cpp:105] Iteration 45600, lr = 0.001
I0925 13:00:07.339179  2600 solver.cpp:218] Iteration 45700 (6.84905 iter/s, 14.6006s/100 iters), loss = 0.140591
I0925 13:00:07.339295  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140591 (* 1 = 0.140591 loss)
I0925 13:00:07.339303  2600 sgd_solver.cpp:105] Iteration 45700, lr = 0.001
I0925 13:00:21.939995  2600 solver.cpp:218] Iteration 45800 (6.849 iter/s, 14.6007s/100 iters), loss = 0.127728
I0925 13:00:21.940026  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127728 (* 1 = 0.127728 loss)
I0925 13:00:21.940032  2600 sgd_solver.cpp:105] Iteration 45800, lr = 0.001
I0925 13:00:36.539808  2600 solver.cpp:218] Iteration 45900 (6.84943 iter/s, 14.5997s/100 iters), loss = 0.118537
I0925 13:00:36.539839  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118537 (* 1 = 0.118537 loss)
I0925 13:00:36.539855  2600 sgd_solver.cpp:105] Iteration 45900, lr = 0.001
I0925 13:00:50.417027  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:00:51.001428  2600 solver.cpp:330] Iteration 46000, Testing net (#0)
I0925 13:00:54.428975  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:00:54.571854  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8689
I0925 13:00:54.571890  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.420417 (* 1 = 0.420417 loss)
I0925 13:00:54.717159  2600 solver.cpp:218] Iteration 46000 (5.50137 iter/s, 18.1773s/100 iters), loss = 0.0862255
I0925 13:00:54.717192  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0862249 (* 1 = 0.0862249 loss)
I0925 13:00:54.717200  2600 sgd_solver.cpp:105] Iteration 46000, lr = 0.001
I0925 13:01:09.340900  2600 solver.cpp:218] Iteration 46100 (6.83823 iter/s, 14.6237s/100 iters), loss = 0.118932
I0925 13:01:09.340930  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118932 (* 1 = 0.118932 loss)
I0925 13:01:09.340936  2600 sgd_solver.cpp:105] Iteration 46100, lr = 0.001
I0925 13:01:24.127743  2600 solver.cpp:218] Iteration 46200 (6.7628 iter/s, 14.7868s/100 iters), loss = 0.121254
I0925 13:01:24.127841  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121253 (* 1 = 0.121253 loss)
I0925 13:01:24.127858  2600 sgd_solver.cpp:105] Iteration 46200, lr = 0.001
I0925 13:01:38.853421  2600 solver.cpp:218] Iteration 46300 (6.79092 iter/s, 14.7255s/100 iters), loss = 0.151255
I0925 13:01:38.853461  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151255 (* 1 = 0.151255 loss)
I0925 13:01:38.853468  2600 sgd_solver.cpp:105] Iteration 46300, lr = 0.001
I0925 13:01:53.515162  2600 solver.cpp:218] Iteration 46400 (6.82051 iter/s, 14.6617s/100 iters), loss = 0.0896208
I0925 13:01:53.515193  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0896203 (* 1 = 0.0896203 loss)
I0925 13:01:53.515200  2600 sgd_solver.cpp:105] Iteration 46400, lr = 0.001
I0925 13:02:07.380262  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:02:07.963781  2600 solver.cpp:330] Iteration 46500, Testing net (#0)
I0925 13:02:11.391708  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:02:11.534951  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8704
I0925 13:02:11.534987  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.419928 (* 1 = 0.419928 loss)
I0925 13:02:11.679188  2600 solver.cpp:218] Iteration 46500 (5.50541 iter/s, 18.164s/100 iters), loss = 0.10426
I0925 13:02:11.679219  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.10426 (* 1 = 0.10426 loss)
I0925 13:02:11.679225  2600 sgd_solver.cpp:105] Iteration 46500, lr = 0.001
I0925 13:02:26.278285  2600 solver.cpp:218] Iteration 46600 (6.84977 iter/s, 14.599s/100 iters), loss = 0.0787978
I0925 13:02:26.278316  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0787972 (* 1 = 0.0787972 loss)
I0925 13:02:26.278322  2600 sgd_solver.cpp:105] Iteration 46600, lr = 0.001
I0925 13:02:40.881844  2600 solver.cpp:218] Iteration 46700 (6.84768 iter/s, 14.6035s/100 iters), loss = 0.135956
I0925 13:02:40.882019  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135956 (* 1 = 0.135956 loss)
I0925 13:02:40.882030  2600 sgd_solver.cpp:105] Iteration 46700, lr = 0.001
I0925 13:02:55.485677  2600 solver.cpp:218] Iteration 46800 (6.84761 iter/s, 14.6036s/100 iters), loss = 0.08766
I0925 13:02:55.485718  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0876594 (* 1 = 0.0876594 loss)
I0925 13:02:55.485724  2600 sgd_solver.cpp:105] Iteration 46800, lr = 0.001
I0925 13:03:10.082049  2600 solver.cpp:218] Iteration 46900 (6.85105 iter/s, 14.5963s/100 iters), loss = 0.117367
I0925 13:03:10.082079  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117366 (* 1 = 0.117366 loss)
I0925 13:03:10.082085  2600 sgd_solver.cpp:105] Iteration 46900, lr = 0.001
I0925 13:03:23.957772  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:03:24.542011  2600 solver.cpp:330] Iteration 47000, Testing net (#0)
I0925 13:03:27.969560  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:03:28.112391  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8731
I0925 13:03:28.112428  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.422462 (* 1 = 0.422462 loss)
I0925 13:03:28.257812  2600 solver.cpp:218] Iteration 47000 (5.50185 iter/s, 18.1757s/100 iters), loss = 0.135392
I0925 13:03:28.257841  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135391 (* 1 = 0.135391 loss)
I0925 13:03:28.257849  2600 sgd_solver.cpp:105] Iteration 47000, lr = 0.001
I0925 13:03:42.856144  2600 solver.cpp:218] Iteration 47100 (6.85013 iter/s, 14.5983s/100 iters), loss = 0.180712
I0925 13:03:42.856174  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180712 (* 1 = 0.180712 loss)
I0925 13:03:42.856180  2600 sgd_solver.cpp:105] Iteration 47100, lr = 0.001
I0925 13:03:57.459604  2600 solver.cpp:218] Iteration 47200 (6.84772 iter/s, 14.6034s/100 iters), loss = 0.117602
I0925 13:03:57.459748  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117601 (* 1 = 0.117601 loss)
I0925 13:03:57.459756  2600 sgd_solver.cpp:105] Iteration 47200, lr = 0.001
I0925 13:04:12.067888  2600 solver.cpp:218] Iteration 47300 (6.84551 iter/s, 14.6081s/100 iters), loss = 0.101728
I0925 13:04:12.067917  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.101727 (* 1 = 0.101727 loss)
I0925 13:04:12.067924  2600 sgd_solver.cpp:105] Iteration 47300, lr = 0.001
I0925 13:04:26.671711  2600 solver.cpp:218] Iteration 47400 (6.84755 iter/s, 14.6038s/100 iters), loss = 0.0623125
I0925 13:04:26.671753  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0623119 (* 1 = 0.0623119 loss)
I0925 13:04:26.671759  2600 sgd_solver.cpp:105] Iteration 47400, lr = 0.001
I0925 13:04:40.554816  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:04:41.139462  2600 solver.cpp:330] Iteration 47500, Testing net (#0)
I0925 13:04:44.567102  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:04:44.710310  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8707
I0925 13:04:44.710345  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.426471 (* 1 = 0.426471 loss)
I0925 13:04:44.855618  2600 solver.cpp:218] Iteration 47500 (5.49939 iter/s, 18.1838s/100 iters), loss = 0.0943497
I0925 13:04:44.855648  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0943491 (* 1 = 0.0943491 loss)
I0925 13:04:44.855654  2600 sgd_solver.cpp:105] Iteration 47500, lr = 0.001
I0925 13:04:59.452730  2600 solver.cpp:218] Iteration 47600 (6.8507 iter/s, 14.597s/100 iters), loss = 0.134611
I0925 13:04:59.452769  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134611 (* 1 = 0.134611 loss)
I0925 13:04:59.452776  2600 sgd_solver.cpp:105] Iteration 47600, lr = 0.001
I0925 13:05:14.057014  2600 solver.cpp:218] Iteration 47700 (6.84734 iter/s, 14.6042s/100 iters), loss = 0.106442
I0925 13:05:14.057165  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106442 (* 1 = 0.106442 loss)
I0925 13:05:14.057173  2600 sgd_solver.cpp:105] Iteration 47700, lr = 0.001
I0925 13:05:28.658041  2600 solver.cpp:218] Iteration 47800 (6.84892 iter/s, 14.6009s/100 iters), loss = 0.0910244
I0925 13:05:28.658082  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0910238 (* 1 = 0.0910238 loss)
I0925 13:05:28.658088  2600 sgd_solver.cpp:105] Iteration 47800, lr = 0.001
I0925 13:05:43.258183  2600 solver.cpp:218] Iteration 47900 (6.84928 iter/s, 14.6001s/100 iters), loss = 0.0644688
I0925 13:05:43.258214  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0644682 (* 1 = 0.0644682 loss)
I0925 13:05:43.258221  2600 sgd_solver.cpp:105] Iteration 47900, lr = 0.001
I0925 13:05:57.135211  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:05:57.719815  2600 solver.cpp:330] Iteration 48000, Testing net (#0)
I0925 13:06:01.147627  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:06:01.291105  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8719
I0925 13:06:01.291139  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.423975 (* 1 = 0.423975 loss)
I0925 13:06:01.436062  2600 solver.cpp:218] Iteration 48000 (5.50121 iter/s, 18.1778s/100 iters), loss = 0.0757206
I0925 13:06:01.436089  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.07572 (* 1 = 0.07572 loss)
I0925 13:06:01.436096  2600 sgd_solver.cpp:105] Iteration 48000, lr = 0.001
I0925 13:06:16.031311  2600 solver.cpp:218] Iteration 48100 (6.85157 iter/s, 14.5952s/100 iters), loss = 0.146922
I0925 13:06:16.031352  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.146922 (* 1 = 0.146922 loss)
I0925 13:06:16.031358  2600 sgd_solver.cpp:105] Iteration 48100, lr = 0.001
I0925 13:06:30.633299  2600 solver.cpp:218] Iteration 48200 (6.84842 iter/s, 14.6019s/100 iters), loss = 0.105896
I0925 13:06:30.633429  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105896 (* 1 = 0.105896 loss)
I0925 13:06:30.633448  2600 sgd_solver.cpp:105] Iteration 48200, lr = 0.001
I0925 13:06:45.229753  2600 solver.cpp:218] Iteration 48300 (6.85106 iter/s, 14.5963s/100 iters), loss = 0.109994
I0925 13:06:45.229794  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.109994 (* 1 = 0.109994 loss)
I0925 13:06:45.229800  2600 sgd_solver.cpp:105] Iteration 48300, lr = 0.001
I0925 13:06:59.831349  2600 solver.cpp:218] Iteration 48400 (6.8486 iter/s, 14.6015s/100 iters), loss = 0.0708982
I0925 13:06:59.831380  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0708976 (* 1 = 0.0708976 loss)
I0925 13:06:59.831387  2600 sgd_solver.cpp:105] Iteration 48400, lr = 0.001
I0925 13:07:13.710420  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:07:14.294083  2600 solver.cpp:330] Iteration 48500, Testing net (#0)
I0925 13:07:17.720685  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:07:17.863401  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8737
I0925 13:07:17.863437  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.4228 (* 1 = 0.4228 loss)
I0925 13:07:18.008555  2600 solver.cpp:218] Iteration 48500 (5.50142 iter/s, 18.1771s/100 iters), loss = 0.0580425
I0925 13:07:18.008585  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0580418 (* 1 = 0.0580418 loss)
I0925 13:07:18.008592  2600 sgd_solver.cpp:105] Iteration 48500, lr = 0.001
I0925 13:07:32.599396  2600 solver.cpp:218] Iteration 48600 (6.85365 iter/s, 14.5908s/100 iters), loss = 0.118062
I0925 13:07:32.599426  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118061 (* 1 = 0.118061 loss)
I0925 13:07:32.599432  2600 sgd_solver.cpp:105] Iteration 48600, lr = 0.001
I0925 13:07:47.197574  2600 solver.cpp:218] Iteration 48700 (6.8502 iter/s, 14.5981s/100 iters), loss = 0.124116
I0925 13:07:47.197702  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124116 (* 1 = 0.124116 loss)
I0925 13:07:47.197710  2600 sgd_solver.cpp:105] Iteration 48700, lr = 0.001
I0925 13:08:01.792618  2600 solver.cpp:218] Iteration 48800 (6.85172 iter/s, 14.5949s/100 iters), loss = 0.133244
I0925 13:08:01.792659  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133243 (* 1 = 0.133243 loss)
I0925 13:08:01.792665  2600 sgd_solver.cpp:105] Iteration 48800, lr = 0.001
I0925 13:08:16.386868  2600 solver.cpp:218] Iteration 48900 (6.85205 iter/s, 14.5942s/100 iters), loss = 0.0617332
I0925 13:08:16.386898  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0617326 (* 1 = 0.0617326 loss)
I0925 13:08:16.386904  2600 sgd_solver.cpp:105] Iteration 48900, lr = 0.001
I0925 13:08:30.256641  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:08:30.840135  2600 solver.cpp:330] Iteration 49000, Testing net (#0)
I0925 13:08:34.267544  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:08:34.410876  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8734
I0925 13:08:34.410912  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.426092 (* 1 = 0.426092 loss)
I0925 13:08:34.555831  2600 solver.cpp:218] Iteration 49000 (5.50391 iter/s, 18.1689s/100 iters), loss = 0.0935821
I0925 13:08:34.555861  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0935815 (* 1 = 0.0935815 loss)
I0925 13:08:34.555868  2600 sgd_solver.cpp:105] Iteration 49000, lr = 0.001
I0925 13:08:49.155871  2600 solver.cpp:218] Iteration 49100 (6.84933 iter/s, 14.6s/100 iters), loss = 0.110864
I0925 13:08:49.155901  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110863 (* 1 = 0.110863 loss)
I0925 13:08:49.155908  2600 sgd_solver.cpp:105] Iteration 49100, lr = 0.001
I0925 13:09:03.763316  2600 solver.cpp:218] Iteration 49200 (6.84586 iter/s, 14.6074s/100 iters), loss = 0.0775499
I0925 13:09:03.763459  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0775493 (* 1 = 0.0775493 loss)
I0925 13:09:03.763468  2600 sgd_solver.cpp:105] Iteration 49200, lr = 0.001
I0925 13:09:18.362604  2600 solver.cpp:218] Iteration 49300 (6.84973 iter/s, 14.5991s/100 iters), loss = 0.105744
I0925 13:09:18.362645  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105743 (* 1 = 0.105743 loss)
I0925 13:09:18.362651  2600 sgd_solver.cpp:105] Iteration 49300, lr = 0.001
I0925 13:09:32.969035  2600 solver.cpp:218] Iteration 49400 (6.84634 iter/s, 14.6064s/100 iters), loss = 0.0877157
I0925 13:09:32.969064  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0877151 (* 1 = 0.0877151 loss)
I0925 13:09:32.969070  2600 sgd_solver.cpp:105] Iteration 49400, lr = 0.001
I0925 13:09:46.845196  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:09:47.428828  2600 solver.cpp:330] Iteration 49500, Testing net (#0)
I0925 13:09:50.857601  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:09:51.000052  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.872
I0925 13:09:51.000088  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.427913 (* 1 = 0.427913 loss)
I0925 13:09:51.145226  2600 solver.cpp:218] Iteration 49500 (5.50172 iter/s, 18.1761s/100 iters), loss = 0.0497688
I0925 13:09:51.145256  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0497682 (* 1 = 0.0497682 loss)
I0925 13:09:51.145263  2600 sgd_solver.cpp:105] Iteration 49500, lr = 0.001
I0925 13:10:05.743468  2600 solver.cpp:218] Iteration 49600 (6.85017 iter/s, 14.5982s/100 iters), loss = 0.165885
I0925 13:10:05.743510  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165885 (* 1 = 0.165885 loss)
I0925 13:10:05.743515  2600 sgd_solver.cpp:105] Iteration 49600, lr = 0.001
I0925 13:10:20.344475  2600 solver.cpp:218] Iteration 49700 (6.84888 iter/s, 14.6009s/100 iters), loss = 0.134875
I0925 13:10:20.344614  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134875 (* 1 = 0.134875 loss)
I0925 13:10:20.344621  2600 sgd_solver.cpp:105] Iteration 49700, lr = 0.001
I0925 13:10:34.941586  2600 solver.cpp:218] Iteration 49800 (6.85075 iter/s, 14.5969s/100 iters), loss = 0.113564
I0925 13:10:34.941618  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113563 (* 1 = 0.113563 loss)
I0925 13:10:34.941624  2600 sgd_solver.cpp:105] Iteration 49800, lr = 0.001
I0925 13:10:49.539418  2600 solver.cpp:218] Iteration 49900 (6.85037 iter/s, 14.5978s/100 iters), loss = 0.044689
I0925 13:10:49.539458  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446885 (* 1 = 0.0446885 loss)
I0925 13:10:49.539464  2600 sgd_solver.cpp:105] Iteration 49900, lr = 0.001
I0925 13:11:03.418751  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:11:04.003458  2600 solver.cpp:330] Iteration 50000, Testing net (#0)
I0925 13:11:07.429807  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:11:07.573143  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8732
I0925 13:11:07.573179  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.425231 (* 1 = 0.425231 loss)
I0925 13:11:07.718021  2600 solver.cpp:218] Iteration 50000 (5.501 iter/s, 18.1785s/100 iters), loss = 0.0747838
I0925 13:11:07.718050  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0747832 (* 1 = 0.0747832 loss)
I0925 13:11:07.718057  2600 sgd_solver.cpp:105] Iteration 50000, lr = 0.001
I0925 13:11:22.321743  2600 solver.cpp:218] Iteration 50100 (6.8476 iter/s, 14.6037s/100 iters), loss = 0.20763
I0925 13:11:22.321774  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20763 (* 1 = 0.20763 loss)
I0925 13:11:22.321780  2600 sgd_solver.cpp:105] Iteration 50100, lr = 0.001
I0925 13:11:36.915195  2600 solver.cpp:218] Iteration 50200 (6.85242 iter/s, 14.5934s/100 iters), loss = 0.0852257
I0925 13:11:36.915323  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0852251 (* 1 = 0.0852251 loss)
I0925 13:11:36.915340  2600 sgd_solver.cpp:105] Iteration 50200, lr = 0.001
I0925 13:11:51.518483  2600 solver.cpp:218] Iteration 50300 (6.84785 iter/s, 14.6031s/100 iters), loss = 0.111951
I0925 13:11:51.518513  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.11195 (* 1 = 0.11195 loss)
I0925 13:11:51.518519  2600 sgd_solver.cpp:105] Iteration 50300, lr = 0.001
I0925 13:12:06.121325  2600 solver.cpp:218] Iteration 50400 (6.84801 iter/s, 14.6028s/100 iters), loss = 0.0758974
I0925 13:12:06.121364  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0758968 (* 1 = 0.0758968 loss)
I0925 13:12:06.121371  2600 sgd_solver.cpp:105] Iteration 50400, lr = 0.001
I0925 13:12:19.995681  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:12:20.580201  2600 solver.cpp:330] Iteration 50500, Testing net (#0)
I0925 13:12:24.008463  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:12:24.151463  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8722
I0925 13:12:24.151497  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.437459 (* 1 = 0.437459 loss)
I0925 13:12:24.296502  2600 solver.cpp:218] Iteration 50500 (5.50204 iter/s, 18.1751s/100 iters), loss = 0.0740404
I0925 13:12:24.296532  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0740398 (* 1 = 0.0740398 loss)
I0925 13:12:24.296540  2600 sgd_solver.cpp:105] Iteration 50500, lr = 0.001
I0925 13:12:38.892132  2600 solver.cpp:218] Iteration 50600 (6.8514 iter/s, 14.5956s/100 iters), loss = 0.077577
I0925 13:12:38.892163  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0775764 (* 1 = 0.0775764 loss)
I0925 13:12:38.892179  2600 sgd_solver.cpp:105] Iteration 50600, lr = 0.001
I0925 13:12:53.490756  2600 solver.cpp:218] Iteration 50700 (6.84999 iter/s, 14.5986s/100 iters), loss = 0.0927889
I0925 13:12:53.490880  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0927884 (* 1 = 0.0927884 loss)
I0925 13:12:53.490898  2600 sgd_solver.cpp:105] Iteration 50700, lr = 0.001
I0925 13:13:08.159304  2600 solver.cpp:218] Iteration 50800 (6.81738 iter/s, 14.6684s/100 iters), loss = 0.126188
I0925 13:13:08.159335  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126187 (* 1 = 0.126187 loss)
I0925 13:13:08.159353  2600 sgd_solver.cpp:105] Iteration 50800, lr = 0.001
I0925 13:13:22.949192  2600 solver.cpp:218] Iteration 50900 (6.76141 iter/s, 14.7898s/100 iters), loss = 0.0710667
I0925 13:13:22.949240  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0710661 (* 1 = 0.0710661 loss)
I0925 13:13:22.949259  2600 sgd_solver.cpp:105] Iteration 50900, lr = 0.001
I0925 13:13:36.921600  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:13:37.505883  2600 solver.cpp:330] Iteration 51000, Testing net (#0)
I0925 13:13:40.973839  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:13:41.124111  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8723
I0925 13:13:41.124150  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.438127 (* 1 = 0.438127 loss)
I0925 13:13:41.274080  2600 solver.cpp:218] Iteration 51000 (5.45709 iter/s, 18.3248s/100 iters), loss = 0.0536719
I0925 13:13:41.274119  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0536713 (* 1 = 0.0536713 loss)
I0925 13:13:41.274127  2600 sgd_solver.cpp:105] Iteration 51000, lr = 0.001
I0925 13:13:55.929263  2600 solver.cpp:218] Iteration 51100 (6.82384 iter/s, 14.6545s/100 iters), loss = 0.107359
I0925 13:13:55.929306  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107358 (* 1 = 0.107358 loss)
I0925 13:13:55.929313  2600 sgd_solver.cpp:105] Iteration 51100, lr = 0.001
I0925 13:14:10.745888  2600 solver.cpp:218] Iteration 51200 (6.74921 iter/s, 14.8165s/100 iters), loss = 0.1325
I0925 13:14:10.746031  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132499 (* 1 = 0.132499 loss)
I0925 13:14:10.746039  2600 sgd_solver.cpp:105] Iteration 51200, lr = 0.001
I0925 13:14:25.755064  2600 solver.cpp:218] Iteration 51300 (6.66267 iter/s, 15.009s/100 iters), loss = 0.133506
I0925 13:14:25.755120  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133506 (* 1 = 0.133506 loss)
I0925 13:14:25.755139  2600 sgd_solver.cpp:105] Iteration 51300, lr = 0.001
I0925 13:14:40.698261  2600 solver.cpp:218] Iteration 51400 (6.69205 iter/s, 14.9431s/100 iters), loss = 0.0694589
I0925 13:14:40.698295  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0694583 (* 1 = 0.0694583 loss)
I0925 13:14:40.698302  2600 sgd_solver.cpp:105] Iteration 51400, lr = 0.001
I0925 13:14:54.750169  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:14:55.340549  2600 solver.cpp:330] Iteration 51500, Testing net (#0)
I0925 13:14:58.802464  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:14:58.946708  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8697
I0925 13:14:58.946732  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.443631 (* 1 = 0.443631 loss)
I0925 13:14:59.092473  2600 solver.cpp:218] Iteration 51500 (5.43652 iter/s, 18.3941s/100 iters), loss = 0.103985
I0925 13:14:59.092506  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103985 (* 1 = 0.103985 loss)
I0925 13:14:59.092514  2600 sgd_solver.cpp:105] Iteration 51500, lr = 0.001
I0925 13:15:13.824651  2600 solver.cpp:218] Iteration 51600 (6.78789 iter/s, 14.7321s/100 iters), loss = 0.0736605
I0925 13:15:13.824682  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0736599 (* 1 = 0.0736599 loss)
I0925 13:15:13.824689  2600 sgd_solver.cpp:105] Iteration 51600, lr = 0.001
I0925 13:15:28.558070  2600 solver.cpp:218] Iteration 51700 (6.78732 iter/s, 14.7334s/100 iters), loss = 0.13667
I0925 13:15:28.558209  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.136669 (* 1 = 0.136669 loss)
I0925 13:15:28.558218  2600 sgd_solver.cpp:105] Iteration 51700, lr = 0.001
I0925 13:15:43.303279  2600 solver.cpp:218] Iteration 51800 (6.78194 iter/s, 14.745s/100 iters), loss = 0.115401
I0925 13:15:43.303311  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.115401 (* 1 = 0.115401 loss)
I0925 13:15:43.303318  2600 sgd_solver.cpp:105] Iteration 51800, lr = 0.001
I0925 13:15:58.280393  2600 solver.cpp:218] Iteration 51900 (6.67689 iter/s, 14.977s/100 iters), loss = 0.0774699
I0925 13:15:58.280450  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0774693 (* 1 = 0.0774693 loss)
I0925 13:15:58.280462  2600 sgd_solver.cpp:105] Iteration 51900, lr = 0.001
I0925 13:16:12.513427  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:16:13.144810  2600 solver.cpp:330] Iteration 52000, Testing net (#0)
I0925 13:16:16.675369  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:16:16.818800  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8737
I0925 13:16:16.818826  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.437804 (* 1 = 0.437804 loss)
I0925 13:16:16.963925  2600 solver.cpp:218] Iteration 52000 (5.35235 iter/s, 18.6834s/100 iters), loss = 0.119703
I0925 13:16:16.963960  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.119702 (* 1 = 0.119702 loss)
I0925 13:16:16.963968  2600 sgd_solver.cpp:105] Iteration 52000, lr = 0.001
I0925 13:16:31.799687  2600 solver.cpp:218] Iteration 52100 (6.7405 iter/s, 14.8357s/100 iters), loss = 0.0836034
I0925 13:16:31.799721  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0836028 (* 1 = 0.0836028 loss)
I0925 13:16:31.799727  2600 sgd_solver.cpp:105] Iteration 52100, lr = 0.001
I0925 13:16:46.668084  2600 solver.cpp:218] Iteration 52200 (6.72571 iter/s, 14.8683s/100 iters), loss = 0.0893975
I0925 13:16:46.668236  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.089397 (* 1 = 0.089397 loss)
I0925 13:16:46.668247  2600 sgd_solver.cpp:105] Iteration 52200, lr = 0.001
I0925 13:17:01.403440  2600 solver.cpp:218] Iteration 52300 (6.78653 iter/s, 14.7351s/100 iters), loss = 0.0689682
I0925 13:17:01.403472  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0689677 (* 1 = 0.0689677 loss)
I0925 13:17:01.403479  2600 sgd_solver.cpp:105] Iteration 52300, lr = 0.001
I0925 13:17:16.138528  2600 solver.cpp:218] Iteration 52400 (6.78655 iter/s, 14.735s/100 iters), loss = 0.0513486
I0925 13:17:16.138557  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0513481 (* 1 = 0.0513481 loss)
I0925 13:17:16.138564  2600 sgd_solver.cpp:105] Iteration 52400, lr = 0.001
I0925 13:17:30.025238  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:17:30.609097  2600 solver.cpp:330] Iteration 52500, Testing net (#0)
I0925 13:17:34.032364  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:17:34.175182  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.872
I0925 13:17:34.175217  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.447123 (* 1 = 0.447123 loss)
I0925 13:17:34.319020  2600 solver.cpp:218] Iteration 52500 (5.50042 iter/s, 18.1804s/100 iters), loss = 0.076981
I0925 13:17:34.319046  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0769805 (* 1 = 0.0769805 loss)
I0925 13:17:34.319052  2600 sgd_solver.cpp:105] Iteration 52500, lr = 0.001
I0925 13:17:48.907937  2600 solver.cpp:218] Iteration 52600 (6.85455 iter/s, 14.5889s/100 iters), loss = 0.150862
I0925 13:17:48.907976  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.150862 (* 1 = 0.150862 loss)
I0925 13:17:48.907982  2600 sgd_solver.cpp:105] Iteration 52600, lr = 0.001
I0925 13:18:03.506485  2600 solver.cpp:218] Iteration 52700 (6.85003 iter/s, 14.5985s/100 iters), loss = 0.141773
I0925 13:18:03.506651  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141773 (* 1 = 0.141773 loss)
I0925 13:18:03.506671  2600 sgd_solver.cpp:105] Iteration 52700, lr = 0.001
I0925 13:18:18.093267  2600 solver.cpp:218] Iteration 52800 (6.85562 iter/s, 14.5866s/100 iters), loss = 0.118242
I0925 13:18:18.093308  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118241 (* 1 = 0.118241 loss)
I0925 13:18:18.093314  2600 sgd_solver.cpp:105] Iteration 52800, lr = 0.001
I0925 13:18:32.683984  2600 solver.cpp:218] Iteration 52900 (6.85371 iter/s, 14.5906s/100 iters), loss = 0.0607181
I0925 13:18:32.684026  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0607176 (* 1 = 0.0607176 loss)
I0925 13:18:32.684032  2600 sgd_solver.cpp:105] Iteration 52900, lr = 0.001
I0925 13:18:46.548837  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:18:47.133394  2600 solver.cpp:330] Iteration 53000, Testing net (#0)
I0925 13:18:50.556725  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:18:50.699527  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8679
I0925 13:18:50.699563  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.45415 (* 1 = 0.45415 loss)
I0925 13:18:50.843123  2600 solver.cpp:218] Iteration 53000 (5.5069 iter/s, 18.1591s/100 iters), loss = 0.0653044
I0925 13:18:50.843150  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0653038 (* 1 = 0.0653038 loss)
I0925 13:18:50.843158  2600 sgd_solver.cpp:105] Iteration 53000, lr = 0.001
I0925 13:19:05.433657  2600 solver.cpp:218] Iteration 53100 (6.85379 iter/s, 14.5905s/100 iters), loss = 0.0950645
I0925 13:19:05.433687  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.095064 (* 1 = 0.095064 loss)
I0925 13:19:05.433693  2600 sgd_solver.cpp:105] Iteration 53100, lr = 0.001
I0925 13:19:20.018606  2600 solver.cpp:218] Iteration 53200 (6.85641 iter/s, 14.5849s/100 iters), loss = 0.0848106
I0925 13:19:20.018721  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.08481 (* 1 = 0.08481 loss)
I0925 13:19:20.018729  2600 sgd_solver.cpp:105] Iteration 53200, lr = 0.001
I0925 13:19:34.607048  2600 solver.cpp:218] Iteration 53300 (6.85481 iter/s, 14.5883s/100 iters), loss = 0.119934
I0925 13:19:34.607089  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.119933 (* 1 = 0.119933 loss)
I0925 13:19:34.607095  2600 sgd_solver.cpp:105] Iteration 53300, lr = 0.001
I0925 13:19:49.195752  2600 solver.cpp:218] Iteration 53400 (6.85465 iter/s, 14.5886s/100 iters), loss = 0.060616
I0925 13:19:49.195794  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0606154 (* 1 = 0.0606154 loss)
I0925 13:19:49.195801  2600 sgd_solver.cpp:105] Iteration 53400, lr = 0.001
I0925 13:20:03.067529  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:20:03.651641  2600 solver.cpp:330] Iteration 53500, Testing net (#0)
I0925 13:20:07.075645  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:20:07.218485  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8688
I0925 13:20:07.218521  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.445855 (* 1 = 0.445855 loss)
I0925 13:20:07.363090  2600 solver.cpp:218] Iteration 53500 (5.50441 iter/s, 18.1673s/100 iters), loss = 0.0515142
I0925 13:20:07.363121  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0515136 (* 1 = 0.0515136 loss)
I0925 13:20:07.363128  2600 sgd_solver.cpp:105] Iteration 53500, lr = 0.001
I0925 13:20:21.936650  2600 solver.cpp:218] Iteration 53600 (6.86177 iter/s, 14.5735s/100 iters), loss = 0.128587
I0925 13:20:21.936681  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.128586 (* 1 = 0.128586 loss)
I0925 13:20:21.936686  2600 sgd_solver.cpp:105] Iteration 53600, lr = 0.001
I0925 13:20:36.511777  2600 solver.cpp:218] Iteration 53700 (6.86103 iter/s, 14.5751s/100 iters), loss = 0.075287
I0925 13:20:36.511879  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0752865 (* 1 = 0.0752865 loss)
I0925 13:20:36.511896  2600 sgd_solver.cpp:105] Iteration 53700, lr = 0.001
I0925 13:20:51.094317  2600 solver.cpp:218] Iteration 53800 (6.85758 iter/s, 14.5824s/100 iters), loss = 0.0965164
I0925 13:20:51.094359  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0965158 (* 1 = 0.0965158 loss)
I0925 13:20:51.094367  2600 sgd_solver.cpp:105] Iteration 53800, lr = 0.001
I0925 13:21:05.674441  2600 solver.cpp:218] Iteration 53900 (6.85869 iter/s, 14.58s/100 iters), loss = 0.0751031
I0925 13:21:05.674473  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0751025 (* 1 = 0.0751025 loss)
I0925 13:21:05.674479  2600 sgd_solver.cpp:105] Iteration 53900, lr = 0.001
I0925 13:21:19.528619  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:21:20.111847  2600 solver.cpp:330] Iteration 54000, Testing net (#0)
I0925 13:21:23.535696  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:21:23.678141  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8733
I0925 13:21:23.678176  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.443432 (* 1 = 0.443432 loss)
I0925 13:21:23.821735  2600 solver.cpp:218] Iteration 54000 (5.51049 iter/s, 18.1472s/100 iters), loss = 0.0618136
I0925 13:21:23.821764  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.061813 (* 1 = 0.061813 loss)
I0925 13:21:23.821771  2600 sgd_solver.cpp:105] Iteration 54000, lr = 0.001
I0925 13:21:38.406226  2600 solver.cpp:218] Iteration 54100 (6.85663 iter/s, 14.5844s/100 iters), loss = 0.0520402
I0925 13:21:38.406256  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0520396 (* 1 = 0.0520396 loss)
I0925 13:21:38.406263  2600 sgd_solver.cpp:105] Iteration 54100, lr = 0.001
I0925 13:21:52.995061  2600 solver.cpp:218] Iteration 54200 (6.85459 iter/s, 14.5888s/100 iters), loss = 0.172785
I0925 13:21:52.995167  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.172784 (* 1 = 0.172784 loss)
I0925 13:21:52.995173  2600 sgd_solver.cpp:105] Iteration 54200, lr = 0.001
I0925 13:22:07.579535  2600 solver.cpp:218] Iteration 54300 (6.85667 iter/s, 14.5843s/100 iters), loss = 0.10571
I0925 13:22:07.579566  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105709 (* 1 = 0.105709 loss)
I0925 13:22:07.579572  2600 sgd_solver.cpp:105] Iteration 54300, lr = 0.001
I0925 13:22:22.169826  2600 solver.cpp:218] Iteration 54400 (6.8539 iter/s, 14.5902s/100 iters), loss = 0.103196
I0925 13:22:22.169857  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103195 (* 1 = 0.103195 loss)
I0925 13:22:22.169862  2600 sgd_solver.cpp:105] Iteration 54400, lr = 0.001
I0925 13:22:36.027660  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:22:36.613113  2600 solver.cpp:330] Iteration 54500, Testing net (#0)
I0925 13:22:40.035545  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:22:40.178844  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.872
I0925 13:22:40.178880  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.449334 (* 1 = 0.449334 loss)
I0925 13:22:40.322940  2600 solver.cpp:218] Iteration 54500 (5.50872 iter/s, 18.153s/100 iters), loss = 0.0555618
I0925 13:22:40.322971  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0555612 (* 1 = 0.0555612 loss)
I0925 13:22:40.322978  2600 sgd_solver.cpp:105] Iteration 54500, lr = 0.001
I0925 13:22:54.908967  2600 solver.cpp:218] Iteration 54600 (6.85591 iter/s, 14.586s/100 iters), loss = 0.0986395
I0925 13:22:54.909008  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0986389 (* 1 = 0.0986389 loss)
I0925 13:22:54.909014  2600 sgd_solver.cpp:105] Iteration 54600, lr = 0.001
I0925 13:23:09.497468  2600 solver.cpp:218] Iteration 54700 (6.85475 iter/s, 14.5884s/100 iters), loss = 0.0955611
I0925 13:23:09.497613  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0955605 (* 1 = 0.0955605 loss)
I0925 13:23:09.497632  2600 sgd_solver.cpp:105] Iteration 54700, lr = 0.001
I0925 13:23:24.090847  2600 solver.cpp:218] Iteration 54800 (6.85251 iter/s, 14.5932s/100 iters), loss = 0.063425
I0925 13:23:24.090888  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0634244 (* 1 = 0.0634244 loss)
I0925 13:23:24.090894  2600 sgd_solver.cpp:105] Iteration 54800, lr = 0.001
I0925 13:23:38.682325  2600 solver.cpp:218] Iteration 54900 (6.85335 iter/s, 14.5914s/100 iters), loss = 0.0850586
I0925 13:23:38.682365  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.085058 (* 1 = 0.085058 loss)
I0925 13:23:38.682373  2600 sgd_solver.cpp:105] Iteration 54900, lr = 0.001
I0925 13:23:52.550688  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:23:53.134407  2600 solver.cpp:330] Iteration 55000, Testing net (#0)
I0925 13:23:56.558464  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:23:56.701284  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8705
I0925 13:23:56.701320  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.457879 (* 1 = 0.457879 loss)
I0925 13:23:56.845713  2600 solver.cpp:218] Iteration 55000 (5.50561 iter/s, 18.1633s/100 iters), loss = 0.0659724
I0925 13:23:56.845742  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0659718 (* 1 = 0.0659718 loss)
I0925 13:23:56.845749  2600 sgd_solver.cpp:105] Iteration 55000, lr = 0.001
I0925 13:24:11.432325  2600 solver.cpp:218] Iteration 55100 (6.85563 iter/s, 14.5865s/100 iters), loss = 0.0836149
I0925 13:24:11.432366  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0836143 (* 1 = 0.0836143 loss)
I0925 13:24:11.432373  2600 sgd_solver.cpp:105] Iteration 55100, lr = 0.001
I0925 13:24:26.022200  2600 solver.cpp:218] Iteration 55200 (6.8541 iter/s, 14.5898s/100 iters), loss = 0.0805974
I0925 13:24:26.022341  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0805968 (* 1 = 0.0805968 loss)
I0925 13:24:26.022349  2600 sgd_solver.cpp:105] Iteration 55200, lr = 0.001
I0925 13:24:40.617744  2600 solver.cpp:218] Iteration 55300 (6.85149 iter/s, 14.5954s/100 iters), loss = 0.130562
I0925 13:24:40.617782  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130561 (* 1 = 0.130561 loss)
I0925 13:24:40.617789  2600 sgd_solver.cpp:105] Iteration 55300, lr = 0.001
I0925 13:24:55.213214  2600 solver.cpp:218] Iteration 55400 (6.85148 iter/s, 14.5954s/100 iters), loss = 0.093392
I0925 13:24:55.213245  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0933914 (* 1 = 0.0933914 loss)
I0925 13:24:55.213251  2600 sgd_solver.cpp:105] Iteration 55400, lr = 0.001
I0925 13:25:09.085119  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:25:09.670073  2600 solver.cpp:330] Iteration 55500, Testing net (#0)
I0925 13:25:13.093943  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:25:13.236915  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8711
I0925 13:25:13.236951  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.451743 (* 1 = 0.451743 loss)
I0925 13:25:13.380745  2600 solver.cpp:218] Iteration 55500 (5.50435 iter/s, 18.1675s/100 iters), loss = 0.06119
I0925 13:25:13.380775  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0611894 (* 1 = 0.0611894 loss)
I0925 13:25:13.380782  2600 sgd_solver.cpp:105] Iteration 55500, lr = 0.001
I0925 13:25:27.970677  2600 solver.cpp:218] Iteration 55600 (6.85407 iter/s, 14.5899s/100 iters), loss = 0.0961716
I0925 13:25:27.970707  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.096171 (* 1 = 0.096171 loss)
I0925 13:25:27.970713  2600 sgd_solver.cpp:105] Iteration 55600, lr = 0.001
I0925 13:25:42.557901  2600 solver.cpp:218] Iteration 55700 (6.85535 iter/s, 14.5872s/100 iters), loss = 0.0463179
I0925 13:25:42.558084  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0463173 (* 1 = 0.0463173 loss)
I0925 13:25:42.558094  2600 sgd_solver.cpp:105] Iteration 55700, lr = 0.001
I0925 13:25:57.147392  2600 solver.cpp:218] Iteration 55800 (6.85434 iter/s, 14.5893s/100 iters), loss = 0.0954726
I0925 13:25:57.147423  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.095472 (* 1 = 0.095472 loss)
I0925 13:25:57.147430  2600 sgd_solver.cpp:105] Iteration 55800, lr = 0.001
I0925 13:26:11.730300  2600 solver.cpp:218] Iteration 55900 (6.85737 iter/s, 14.5828s/100 iters), loss = 0.0448965
I0925 13:26:11.730332  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0448959 (* 1 = 0.0448959 loss)
I0925 13:26:11.730350  2600 sgd_solver.cpp:105] Iteration 55900, lr = 0.001
I0925 13:26:25.592732  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:26:26.176247  2600 solver.cpp:330] Iteration 56000, Testing net (#0)
I0925 13:26:29.601166  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:26:29.743906  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8719
I0925 13:26:29.743932  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.455851 (* 1 = 0.455851 loss)
I0925 13:26:29.888619  2600 solver.cpp:218] Iteration 56000 (5.50714 iter/s, 18.1582s/100 iters), loss = 0.0750125
I0925 13:26:29.888659  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0750119 (* 1 = 0.0750119 loss)
I0925 13:26:29.888666  2600 sgd_solver.cpp:105] Iteration 56000, lr = 0.001
I0925 13:26:44.464859  2600 solver.cpp:218] Iteration 56100 (6.86052 iter/s, 14.5762s/100 iters), loss = 0.0962126
I0925 13:26:44.464901  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.096212 (* 1 = 0.096212 loss)
I0925 13:26:44.464908  2600 sgd_solver.cpp:105] Iteration 56100, lr = 0.001
I0925 13:26:59.044590  2600 solver.cpp:218] Iteration 56200 (6.85887 iter/s, 14.5797s/100 iters), loss = 0.100255
I0925 13:26:59.044703  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100255 (* 1 = 0.100255 loss)
I0925 13:26:59.044721  2600 sgd_solver.cpp:105] Iteration 56200, lr = 0.001
I0925 13:27:13.627950  2600 solver.cpp:218] Iteration 56300 (6.8572 iter/s, 14.5832s/100 iters), loss = 0.111101
I0925 13:27:13.627991  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111101 (* 1 = 0.111101 loss)
I0925 13:27:13.627997  2600 sgd_solver.cpp:105] Iteration 56300, lr = 0.001
I0925 13:27:28.206228  2600 solver.cpp:218] Iteration 56400 (6.85956 iter/s, 14.5782s/100 iters), loss = 0.103887
I0925 13:27:28.206259  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103887 (* 1 = 0.103887 loss)
I0925 13:27:28.206264  2600 sgd_solver.cpp:105] Iteration 56400, lr = 0.001
I0925 13:27:42.062507  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:27:42.646632  2600 solver.cpp:330] Iteration 56500, Testing net (#0)
I0925 13:27:46.071454  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:27:46.215090  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8687
I0925 13:27:46.215126  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.468108 (* 1 = 0.468108 loss)
I0925 13:27:46.359928  2600 solver.cpp:218] Iteration 56500 (5.50854 iter/s, 18.1536s/100 iters), loss = 0.0798612
I0925 13:27:46.359954  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0798605 (* 1 = 0.0798605 loss)
I0925 13:27:46.359961  2600 sgd_solver.cpp:105] Iteration 56500, lr = 0.001
I0925 13:28:00.947361  2600 solver.cpp:218] Iteration 56600 (6.85525 iter/s, 14.5874s/100 iters), loss = 0.151267
I0925 13:28:00.947401  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151266 (* 1 = 0.151266 loss)
I0925 13:28:00.947407  2600 sgd_solver.cpp:105] Iteration 56600, lr = 0.001
I0925 13:28:15.535178  2600 solver.cpp:218] Iteration 56700 (6.85507 iter/s, 14.5877s/100 iters), loss = 0.129206
I0925 13:28:15.535297  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.129205 (* 1 = 0.129205 loss)
I0925 13:28:15.535315  2600 sgd_solver.cpp:105] Iteration 56700, lr = 0.001
I0925 13:28:30.129469  2600 solver.cpp:218] Iteration 56800 (6.85207 iter/s, 14.5941s/100 iters), loss = 0.0600273
I0925 13:28:30.129509  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0600266 (* 1 = 0.0600266 loss)
I0925 13:28:30.129516  2600 sgd_solver.cpp:105] Iteration 56800, lr = 0.001
I0925 13:28:44.719492  2600 solver.cpp:218] Iteration 56900 (6.85404 iter/s, 14.5899s/100 iters), loss = 0.0430855
I0925 13:28:44.719534  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0430848 (* 1 = 0.0430848 loss)
I0925 13:28:44.719540  2600 sgd_solver.cpp:105] Iteration 56900, lr = 0.001
I0925 13:28:58.584348  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:28:59.170255  2600 solver.cpp:330] Iteration 57000, Testing net (#0)
I0925 13:29:02.595862  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:29:02.738878  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.869
I0925 13:29:02.738914  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.461572 (* 1 = 0.461572 loss)
I0925 13:29:02.883543  2600 solver.cpp:218] Iteration 57000 (5.50541 iter/s, 18.164s/100 iters), loss = 0.0680159
I0925 13:29:02.883576  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0680152 (* 1 = 0.0680152 loss)
I0925 13:29:02.883584  2600 sgd_solver.cpp:105] Iteration 57000, lr = 0.001
I0925 13:29:17.469447  2600 solver.cpp:218] Iteration 57100 (6.85597 iter/s, 14.5858s/100 iters), loss = 0.126291
I0925 13:29:17.469478  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.12629 (* 1 = 0.12629 loss)
I0925 13:29:17.469485  2600 sgd_solver.cpp:105] Iteration 57100, lr = 0.001
I0925 13:29:32.059237  2600 solver.cpp:218] Iteration 57200 (6.85414 iter/s, 14.5897s/100 iters), loss = 0.0646485
I0925 13:29:32.059379  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0646478 (* 1 = 0.0646478 loss)
I0925 13:29:32.059387  2600 sgd_solver.cpp:105] Iteration 57200, lr = 0.001
I0925 13:29:46.647477  2600 solver.cpp:218] Iteration 57300 (6.85492 iter/s, 14.5881s/100 iters), loss = 0.103253
I0925 13:29:46.647518  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103253 (* 1 = 0.103253 loss)
I0925 13:29:46.647526  2600 sgd_solver.cpp:105] Iteration 57300, lr = 0.001
I0925 13:30:01.235720  2600 solver.cpp:218] Iteration 57400 (6.85487 iter/s, 14.5882s/100 iters), loss = 0.0664821
I0925 13:30:01.235750  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0664815 (* 1 = 0.0664815 loss)
I0925 13:30:01.235756  2600 sgd_solver.cpp:105] Iteration 57400, lr = 0.001
I0925 13:30:15.097090  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:30:15.681504  2600 solver.cpp:330] Iteration 57500, Testing net (#0)
I0925 13:30:19.104542  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:30:19.247251  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8705
I0925 13:30:19.247277  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.457658 (* 1 = 0.457658 loss)
I0925 13:30:19.391916  2600 solver.cpp:218] Iteration 57500 (5.50778 iter/s, 18.1561s/100 iters), loss = 0.0827085
I0925 13:30:19.391942  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0827078 (* 1 = 0.0827078 loss)
I0925 13:30:19.391949  2600 sgd_solver.cpp:105] Iteration 57500, lr = 0.001
I0925 13:30:33.982363  2600 solver.cpp:218] Iteration 57600 (6.85383 iter/s, 14.5904s/100 iters), loss = 0.0842672
I0925 13:30:33.982394  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0842666 (* 1 = 0.0842666 loss)
I0925 13:30:33.982400  2600 sgd_solver.cpp:105] Iteration 57600, lr = 0.001
I0925 13:30:48.569444  2600 solver.cpp:218] Iteration 57700 (6.85541 iter/s, 14.587s/100 iters), loss = 0.053368
I0925 13:30:48.569577  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0533674 (* 1 = 0.0533674 loss)
I0925 13:30:48.569586  2600 sgd_solver.cpp:105] Iteration 57700, lr = 0.001
I0925 13:31:03.160459  2600 solver.cpp:218] Iteration 57800 (6.85361 iter/s, 14.5908s/100 iters), loss = 0.11691
I0925 13:31:03.160501  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.116909 (* 1 = 0.116909 loss)
I0925 13:31:03.160508  2600 sgd_solver.cpp:105] Iteration 57800, lr = 0.001
I0925 13:31:17.755631  2600 solver.cpp:218] Iteration 57900 (6.85162 iter/s, 14.5951s/100 iters), loss = 0.0595113
I0925 13:31:17.755661  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0595107 (* 1 = 0.0595107 loss)
I0925 13:31:17.755667  2600 sgd_solver.cpp:105] Iteration 57900, lr = 0.001
I0925 13:31:31.622622  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:31:32.206728  2600 solver.cpp:330] Iteration 58000, Testing net (#0)
I0925 13:31:35.632537  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:31:35.775547  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8704
I0925 13:31:35.775583  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.461033 (* 1 = 0.461033 loss)
I0925 13:31:35.920326  2600 solver.cpp:218] Iteration 58000 (5.50521 iter/s, 18.1646s/100 iters), loss = 0.078073
I0925 13:31:35.920356  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0780724 (* 1 = 0.0780724 loss)
I0925 13:31:35.920363  2600 sgd_solver.cpp:105] Iteration 58000, lr = 0.001
I0925 13:31:50.501312  2600 solver.cpp:218] Iteration 58100 (6.85828 iter/s, 14.5809s/100 iters), loss = 0.0721754
I0925 13:31:50.501340  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0721748 (* 1 = 0.0721748 loss)
I0925 13:31:50.501346  2600 sgd_solver.cpp:105] Iteration 58100, lr = 0.001
I0925 13:32:05.089296  2600 solver.cpp:218] Iteration 58200 (6.85499 iter/s, 14.5879s/100 iters), loss = 0.114074
I0925 13:32:05.089422  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.114073 (* 1 = 0.114073 loss)
I0925 13:32:05.089431  2600 sgd_solver.cpp:105] Iteration 58200, lr = 0.001
I0925 13:32:19.676827  2600 solver.cpp:218] Iteration 58300 (6.85525 iter/s, 14.5874s/100 iters), loss = 0.0984369
I0925 13:32:19.676858  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0984363 (* 1 = 0.0984363 loss)
I0925 13:32:19.676864  2600 sgd_solver.cpp:105] Iteration 58300, lr = 0.001
I0925 13:32:34.269199  2600 solver.cpp:218] Iteration 58400 (6.85293 iter/s, 14.5923s/100 iters), loss = 0.058842
I0925 13:32:34.269230  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0588414 (* 1 = 0.0588414 loss)
I0925 13:32:34.269237  2600 sgd_solver.cpp:105] Iteration 58400, lr = 0.001
I0925 13:32:48.132100  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:32:48.717080  2600 solver.cpp:330] Iteration 58500, Testing net (#0)
I0925 13:32:52.142263  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:32:52.285009  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8715
I0925 13:32:52.285034  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.462579 (* 1 = 0.462579 loss)
I0925 13:32:52.429287  2600 solver.cpp:218] Iteration 58500 (5.5066 iter/s, 18.16s/100 iters), loss = 0.0840428
I0925 13:32:52.429316  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0840422 (* 1 = 0.0840422 loss)
I0925 13:32:52.429322  2600 sgd_solver.cpp:105] Iteration 58500, lr = 0.001
I0925 13:33:07.007930  2600 solver.cpp:218] Iteration 58600 (6.85938 iter/s, 14.5786s/100 iters), loss = 0.0783514
I0925 13:33:07.007963  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0783508 (* 1 = 0.0783508 loss)
I0925 13:33:07.007969  2600 sgd_solver.cpp:105] Iteration 58600, lr = 0.001
I0925 13:33:21.587265  2600 solver.cpp:218] Iteration 58700 (6.85906 iter/s, 14.5793s/100 iters), loss = 0.105813
I0925 13:33:21.587384  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105812 (* 1 = 0.105812 loss)
I0925 13:33:21.587393  2600 sgd_solver.cpp:105] Iteration 58700, lr = 0.001
I0925 13:33:36.172968  2600 solver.cpp:218] Iteration 58800 (6.8561 iter/s, 14.5855s/100 iters), loss = 0.130171
I0925 13:33:36.172998  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130171 (* 1 = 0.130171 loss)
I0925 13:33:36.173005  2600 sgd_solver.cpp:105] Iteration 58800, lr = 0.001
I0925 13:33:50.751657  2600 solver.cpp:218] Iteration 58900 (6.85936 iter/s, 14.5786s/100 iters), loss = 0.0421924
I0925 13:33:50.751688  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0421918 (* 1 = 0.0421918 loss)
I0925 13:33:50.751694  2600 sgd_solver.cpp:105] Iteration 58900, lr = 0.001
I0925 13:34:04.603790  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:34:05.187427  2600 solver.cpp:330] Iteration 59000, Testing net (#0)
I0925 13:34:08.609657  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:34:08.752221  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8707
I0925 13:34:08.752245  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.470833 (* 1 = 0.470833 loss)
I0925 13:34:08.896271  2600 solver.cpp:218] Iteration 59000 (5.5113 iter/s, 18.1445s/100 iters), loss = 0.0550647
I0925 13:34:08.896329  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.055064 (* 1 = 0.055064 loss)
I0925 13:34:08.896337  2600 sgd_solver.cpp:105] Iteration 59000, lr = 0.001
I0925 13:34:23.483656  2600 solver.cpp:218] Iteration 59100 (6.85528 iter/s, 14.5873s/100 iters), loss = 0.0860248
I0925 13:34:23.483697  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0860242 (* 1 = 0.0860242 loss)
I0925 13:34:23.483703  2600 sgd_solver.cpp:105] Iteration 59100, lr = 0.001
I0925 13:34:38.075844  2600 solver.cpp:218] Iteration 59200 (6.85302 iter/s, 14.5921s/100 iters), loss = 0.11479
I0925 13:34:38.075953  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.114789 (* 1 = 0.114789 loss)
I0925 13:34:38.075960  2600 sgd_solver.cpp:105] Iteration 59200, lr = 0.001
I0925 13:34:52.666152  2600 solver.cpp:218] Iteration 59300 (6.85393 iter/s, 14.5902s/100 iters), loss = 0.0379464
I0925 13:34:52.666193  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0379458 (* 1 = 0.0379458 loss)
I0925 13:34:52.666198  2600 sgd_solver.cpp:105] Iteration 59300, lr = 0.001
I0925 13:35:07.255439  2600 solver.cpp:218] Iteration 59400 (6.85438 iter/s, 14.5892s/100 iters), loss = 0.0452961
I0925 13:35:07.255471  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0452954 (* 1 = 0.0452954 loss)
I0925 13:35:07.255478  2600 sgd_solver.cpp:105] Iteration 59400, lr = 0.001
I0925 13:35:21.119546  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:35:21.702466  2600 solver.cpp:330] Iteration 59500, Testing net (#0)
I0925 13:35:25.124734  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:35:25.267684  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.87
I0925 13:35:25.267719  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.468677 (* 1 = 0.468677 loss)
I0925 13:35:25.412940  2600 solver.cpp:218] Iteration 59500 (5.50739 iter/s, 18.1574s/100 iters), loss = 0.0363394
I0925 13:35:25.412973  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0363388 (* 1 = 0.0363388 loss)
I0925 13:35:25.412979  2600 sgd_solver.cpp:105] Iteration 59500, lr = 0.001
I0925 13:35:40.002945  2600 solver.cpp:218] Iteration 59600 (6.85404 iter/s, 14.5899s/100 iters), loss = 0.0905776
I0925 13:35:40.002985  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0905769 (* 1 = 0.0905769 loss)
I0925 13:35:40.002992  2600 sgd_solver.cpp:105] Iteration 59600, lr = 0.001
I0925 13:35:54.598969  2600 solver.cpp:218] Iteration 59700 (6.85122 iter/s, 14.5959s/100 iters), loss = 0.1267
I0925 13:35:54.599109  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126699 (* 1 = 0.126699 loss)
I0925 13:35:54.599128  2600 sgd_solver.cpp:105] Iteration 59700, lr = 0.001
I0925 13:36:09.194878  2600 solver.cpp:218] Iteration 59800 (6.85131 iter/s, 14.5957s/100 iters), loss = 0.119077
I0925 13:36:09.194921  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.119076 (* 1 = 0.119076 loss)
I0925 13:36:09.194927  2600 sgd_solver.cpp:105] Iteration 59800, lr = 0.001
I0925 13:36:23.788724  2600 solver.cpp:218] Iteration 59900 (6.85224 iter/s, 14.5938s/100 iters), loss = 0.0383993
I0925 13:36:23.788765  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0383986 (* 1 = 0.0383986 loss)
I0925 13:36:23.788772  2600 sgd_solver.cpp:105] Iteration 59900, lr = 0.001
I0925 13:36:37.656697  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:36:38.241286  2600 solver.cpp:330] Iteration 60000, Testing net (#0)
I0925 13:36:41.664008  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:36:41.806859  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8707
I0925 13:36:41.806884  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.464232 (* 1 = 0.464232 loss)
I0925 13:36:41.951529  2600 solver.cpp:218] Iteration 60000 (5.50578 iter/s, 18.1627s/100 iters), loss = 0.0964945
I0925 13:36:41.951560  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0964938 (* 1 = 0.0964938 loss)
I0925 13:36:41.951566  2600 sgd_solver.cpp:105] Iteration 60000, lr = 0.001
I0925 13:36:56.538765  2600 solver.cpp:218] Iteration 60100 (6.85534 iter/s, 14.5872s/100 iters), loss = 0.0699132
I0925 13:36:56.538795  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0699125 (* 1 = 0.0699125 loss)
I0925 13:36:56.538801  2600 sgd_solver.cpp:105] Iteration 60100, lr = 0.001
I0925 13:37:11.129546  2600 solver.cpp:218] Iteration 60200 (6.85367 iter/s, 14.5907s/100 iters), loss = 0.0852877
I0925 13:37:11.129650  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.085287 (* 1 = 0.085287 loss)
I0925 13:37:11.129658  2600 sgd_solver.cpp:105] Iteration 60200, lr = 0.001
I0925 13:37:25.721328  2600 solver.cpp:218] Iteration 60300 (6.85324 iter/s, 14.5916s/100 iters), loss = 0.19417
I0925 13:37:25.721369  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194169 (* 1 = 0.194169 loss)
I0925 13:37:25.721375  2600 sgd_solver.cpp:105] Iteration 60300, lr = 0.001
I0925 13:37:40.311743  2600 solver.cpp:218] Iteration 60400 (6.85385 iter/s, 14.5903s/100 iters), loss = 0.0700765
I0925 13:37:40.311772  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0700758 (* 1 = 0.0700758 loss)
I0925 13:37:40.311779  2600 sgd_solver.cpp:105] Iteration 60400, lr = 0.001
I0925 13:37:54.178210  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:37:54.761945  2600 solver.cpp:330] Iteration 60500, Testing net (#0)
I0925 13:37:58.184222  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:37:58.326959  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8689
I0925 13:37:58.326983  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.486477 (* 1 = 0.486477 loss)
I0925 13:37:58.470759  2600 solver.cpp:218] Iteration 60500 (5.50693 iter/s, 18.1589s/100 iters), loss = 0.109826
I0925 13:37:58.470788  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.109825 (* 1 = 0.109825 loss)
I0925 13:37:58.470794  2600 sgd_solver.cpp:105] Iteration 60500, lr = 0.001
I0925 13:38:13.055351  2600 solver.cpp:218] Iteration 60600 (6.85658 iter/s, 14.5845s/100 iters), loss = 0.116706
I0925 13:38:13.055392  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.116706 (* 1 = 0.116706 loss)
I0925 13:38:13.055397  2600 sgd_solver.cpp:105] Iteration 60600, lr = 0.001
I0925 13:38:27.645318  2600 solver.cpp:218] Iteration 60700 (6.85406 iter/s, 14.5899s/100 iters), loss = 0.0701634
I0925 13:38:27.645462  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0701626 (* 1 = 0.0701626 loss)
I0925 13:38:27.645470  2600 sgd_solver.cpp:105] Iteration 60700, lr = 0.001
I0925 13:38:42.236192  2600 solver.cpp:218] Iteration 60800 (6.85368 iter/s, 14.5907s/100 iters), loss = 0.107838
I0925 13:38:42.236233  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107837 (* 1 = 0.107837 loss)
I0925 13:38:42.236239  2600 sgd_solver.cpp:105] Iteration 60800, lr = 0.001
I0925 13:38:56.825784  2600 solver.cpp:218] Iteration 60900 (6.85424 iter/s, 14.5895s/100 iters), loss = 0.132823
I0925 13:38:56.825824  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132822 (* 1 = 0.132822 loss)
I0925 13:38:56.825830  2600 sgd_solver.cpp:105] Iteration 60900, lr = 0.001
I0925 13:39:10.684866  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:39:11.268422  2600 solver.cpp:330] Iteration 61000, Testing net (#0)
I0925 13:39:14.690516  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:39:14.833169  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8723
I0925 13:39:14.833206  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.464176 (* 1 = 0.464176 loss)
I0925 13:39:14.977434  2600 solver.cpp:218] Iteration 61000 (5.50917 iter/s, 18.1516s/100 iters), loss = 0.0762235
I0925 13:39:14.977463  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0762228 (* 1 = 0.0762228 loss)
I0925 13:39:14.977470  2600 sgd_solver.cpp:105] Iteration 61000, lr = 0.001
I0925 13:39:29.560961  2600 solver.cpp:218] Iteration 61100 (6.85708 iter/s, 14.5835s/100 iters), loss = 0.125795
I0925 13:39:29.560989  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125795 (* 1 = 0.125795 loss)
I0925 13:39:29.560995  2600 sgd_solver.cpp:105] Iteration 61100, lr = 0.001
I0925 13:39:44.146188  2600 solver.cpp:218] Iteration 61200 (6.85628 iter/s, 14.5852s/100 iters), loss = 0.0841429
I0925 13:39:44.146296  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0841421 (* 1 = 0.0841421 loss)
I0925 13:39:44.146304  2600 sgd_solver.cpp:105] Iteration 61200, lr = 0.001
I0925 13:39:58.723644  2600 solver.cpp:218] Iteration 61300 (6.85997 iter/s, 14.5773s/100 iters), loss = 0.0723417
I0925 13:39:58.723675  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.072341 (* 1 = 0.072341 loss)
I0925 13:39:58.723681  2600 sgd_solver.cpp:105] Iteration 61300, lr = 0.001
I0925 13:40:13.304992  2600 solver.cpp:218] Iteration 61400 (6.85811 iter/s, 14.5813s/100 iters), loss = 0.111736
I0925 13:40:13.305034  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111735 (* 1 = 0.111735 loss)
I0925 13:40:13.305042  2600 sgd_solver.cpp:105] Iteration 61400, lr = 0.001
I0925 13:40:27.162694  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:40:27.746636  2600 solver.cpp:330] Iteration 61500, Testing net (#0)
I0925 13:40:31.170172  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:40:31.313191  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8727
I0925 13:40:31.313228  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.465126 (* 1 = 0.465126 loss)
I0925 13:40:31.457896  2600 solver.cpp:218] Iteration 61500 (5.50879 iter/s, 18.1528s/100 iters), loss = 0.0746384
I0925 13:40:31.457926  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0746377 (* 1 = 0.0746377 loss)
I0925 13:40:31.457932  2600 sgd_solver.cpp:105] Iteration 61500, lr = 0.001
I0925 13:40:46.045892  2600 solver.cpp:218] Iteration 61600 (6.85498 iter/s, 14.5879s/100 iters), loss = 0.0713675
I0925 13:40:46.045933  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0713668 (* 1 = 0.0713668 loss)
I0925 13:40:46.045938  2600 sgd_solver.cpp:105] Iteration 61600, lr = 0.001
I0925 13:41:00.638464  2600 solver.cpp:218] Iteration 61700 (6.85284 iter/s, 14.5925s/100 iters), loss = 0.107995
I0925 13:41:00.638612  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107994 (* 1 = 0.107994 loss)
I0925 13:41:00.638622  2600 sgd_solver.cpp:105] Iteration 61700, lr = 0.001
I0925 13:41:15.228449  2600 solver.cpp:218] Iteration 61800 (6.8541 iter/s, 14.5898s/100 iters), loss = 0.129307
I0925 13:41:15.228492  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.129306 (* 1 = 0.129306 loss)
I0925 13:41:15.228499  2600 sgd_solver.cpp:105] Iteration 61800, lr = 0.001
I0925 13:41:29.820333  2600 solver.cpp:218] Iteration 61900 (6.85316 iter/s, 14.5918s/100 iters), loss = 0.0770487
I0925 13:41:29.820372  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.077048 (* 1 = 0.077048 loss)
I0925 13:41:29.820379  2600 sgd_solver.cpp:105] Iteration 61900, lr = 0.001
I0925 13:41:43.683959  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:41:44.268393  2600 solver.cpp:330] Iteration 62000, Testing net (#0)
I0925 13:41:47.692103  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:41:47.834928  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8738
I0925 13:41:47.834964  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.46841 (* 1 = 0.46841 loss)
I0925 13:41:47.979408  2600 solver.cpp:218] Iteration 62000 (5.50691 iter/s, 18.159s/100 iters), loss = 0.0984582
I0925 13:41:47.979439  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0984575 (* 1 = 0.0984575 loss)
I0925 13:41:47.979445  2600 sgd_solver.cpp:105] Iteration 62000, lr = 0.001
I0925 13:42:02.570418  2600 solver.cpp:218] Iteration 62100 (6.85357 iter/s, 14.5909s/100 iters), loss = 0.139299
I0925 13:42:02.570451  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139298 (* 1 = 0.139298 loss)
I0925 13:42:02.570456  2600 sgd_solver.cpp:105] Iteration 62100, lr = 0.001
I0925 13:42:17.151590  2600 solver.cpp:218] Iteration 62200 (6.85819 iter/s, 14.5811s/100 iters), loss = 0.0743208
I0925 13:42:17.151696  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0743201 (* 1 = 0.0743201 loss)
I0925 13:42:17.151705  2600 sgd_solver.cpp:105] Iteration 62200, lr = 0.001
I0925 13:42:31.746799  2600 solver.cpp:218] Iteration 62300 (6.85162 iter/s, 14.5951s/100 iters), loss = 0.0851692
I0925 13:42:31.746830  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0851685 (* 1 = 0.0851685 loss)
I0925 13:42:31.746847  2600 sgd_solver.cpp:105] Iteration 62300, lr = 0.001
I0925 13:42:46.334036  2600 solver.cpp:218] Iteration 62400 (6.85534 iter/s, 14.5872s/100 iters), loss = 0.0781404
I0925 13:42:46.334066  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0781397 (* 1 = 0.0781397 loss)
I0925 13:42:46.334084  2600 sgd_solver.cpp:105] Iteration 62400, lr = 0.001
I0925 13:43:00.200232  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:43:00.784832  2600 solver.cpp:330] Iteration 62500, Testing net (#0)
I0925 13:43:04.208662  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:43:04.350872  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8724
I0925 13:43:04.350906  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.470638 (* 1 = 0.470638 loss)
I0925 13:43:04.495301  2600 solver.cpp:218] Iteration 62500 (5.50625 iter/s, 18.1612s/100 iters), loss = 0.124424
I0925 13:43:04.495332  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124423 (* 1 = 0.124423 loss)
I0925 13:43:04.495338  2600 sgd_solver.cpp:105] Iteration 62500, lr = 0.001
I0925 13:43:19.094827  2600 solver.cpp:218] Iteration 62600 (6.84957 iter/s, 14.5995s/100 iters), loss = 0.104292
I0925 13:43:19.094869  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.104292 (* 1 = 0.104292 loss)
I0925 13:43:19.094876  2600 sgd_solver.cpp:105] Iteration 62600, lr = 0.001
I0925 13:43:33.686149  2600 solver.cpp:218] Iteration 62700 (6.85343 iter/s, 14.5912s/100 iters), loss = 0.0769453
I0925 13:43:33.686264  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0769446 (* 1 = 0.0769446 loss)
I0925 13:43:33.686282  2600 sgd_solver.cpp:105] Iteration 62700, lr = 0.001
I0925 13:43:48.282598  2600 solver.cpp:218] Iteration 62800 (6.85105 iter/s, 14.5963s/100 iters), loss = 0.115067
I0925 13:43:48.282639  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.115066 (* 1 = 0.115066 loss)
I0925 13:43:48.282646  2600 sgd_solver.cpp:105] Iteration 62800, lr = 0.001
I0925 13:44:02.877465  2600 solver.cpp:218] Iteration 62900 (6.85176 iter/s, 14.5948s/100 iters), loss = 0.0850494
I0925 13:44:02.877506  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0850487 (* 1 = 0.0850487 loss)
I0925 13:44:02.877511  2600 sgd_solver.cpp:105] Iteration 62900, lr = 0.001
I0925 13:44:16.745935  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:44:17.331483  2600 solver.cpp:330] Iteration 63000, Testing net (#0)
I0925 13:44:20.754914  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:44:20.897815  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.872
I0925 13:44:20.897851  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.469591 (* 1 = 0.469591 loss)
I0925 13:44:21.042142  2600 solver.cpp:218] Iteration 63000 (5.50522 iter/s, 18.1646s/100 iters), loss = 0.0531022
I0925 13:44:21.042171  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0531015 (* 1 = 0.0531015 loss)
I0925 13:44:21.042178  2600 sgd_solver.cpp:105] Iteration 63000, lr = 0.001
I0925 13:44:35.632122  2600 solver.cpp:218] Iteration 63100 (6.85405 iter/s, 14.5899s/100 iters), loss = 0.0903448
I0925 13:44:35.632164  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.090344 (* 1 = 0.090344 loss)
I0925 13:44:35.632169  2600 sgd_solver.cpp:105] Iteration 63100, lr = 0.001
I0925 13:44:50.224215  2600 solver.cpp:218] Iteration 63200 (6.85306 iter/s, 14.592s/100 iters), loss = 0.111422
I0925 13:44:50.224321  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111421 (* 1 = 0.111421 loss)
I0925 13:44:50.224339  2600 sgd_solver.cpp:105] Iteration 63200, lr = 0.001
I0925 13:45:04.821673  2600 solver.cpp:218] Iteration 63300 (6.85057 iter/s, 14.5973s/100 iters), loss = 0.0649716
I0925 13:45:04.821713  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0649708 (* 1 = 0.0649708 loss)
I0925 13:45:04.821719  2600 sgd_solver.cpp:105] Iteration 63300, lr = 0.001
I0925 13:45:19.409255  2600 solver.cpp:218] Iteration 63400 (6.85518 iter/s, 14.5875s/100 iters), loss = 0.0993805
I0925 13:45:19.409284  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0993797 (* 1 = 0.0993797 loss)
I0925 13:45:19.409291  2600 sgd_solver.cpp:105] Iteration 63400, lr = 0.001
I0925 13:45:33.280273  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:45:33.865238  2600 solver.cpp:330] Iteration 63500, Testing net (#0)
I0925 13:45:37.287358  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:45:37.430541  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8712
I0925 13:45:37.430577  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.47994 (* 1 = 0.47994 loss)
I0925 13:45:37.574323  2600 solver.cpp:218] Iteration 63500 (5.50509 iter/s, 18.165s/100 iters), loss = 0.0901529
I0925 13:45:37.574352  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0901521 (* 1 = 0.0901521 loss)
I0925 13:45:37.574358  2600 sgd_solver.cpp:105] Iteration 63500, lr = 0.001
I0925 13:45:52.157027  2600 solver.cpp:218] Iteration 63600 (6.85747 iter/s, 14.5826s/100 iters), loss = 0.0465363
I0925 13:45:52.157066  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0465355 (* 1 = 0.0465355 loss)
I0925 13:45:52.157073  2600 sgd_solver.cpp:105] Iteration 63600, lr = 0.001
I0925 13:46:06.737181  2600 solver.cpp:218] Iteration 63700 (6.85867 iter/s, 14.5801s/100 iters), loss = 0.069878
I0925 13:46:06.737293  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0698772 (* 1 = 0.0698772 loss)
I0925 13:46:06.737311  2600 sgd_solver.cpp:105] Iteration 63700, lr = 0.001
I0925 13:46:21.321715  2600 solver.cpp:218] Iteration 63800 (6.85665 iter/s, 14.5844s/100 iters), loss = 0.0651749
I0925 13:46:21.321748  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0651741 (* 1 = 0.0651741 loss)
I0925 13:46:21.321754  2600 sgd_solver.cpp:105] Iteration 63800, lr = 0.001
I0925 13:46:35.912273  2600 solver.cpp:218] Iteration 63900 (6.85378 iter/s, 14.5905s/100 iters), loss = 0.0380373
I0925 13:46:35.912315  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380365 (* 1 = 0.0380365 loss)
I0925 13:46:35.912322  2600 sgd_solver.cpp:105] Iteration 63900, lr = 0.001
I0925 13:46:49.767287  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:46:50.350566  2600 solver.cpp:330] Iteration 64000, Testing net (#0)
I0925 13:46:53.773691  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:46:53.916607  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8709
I0925 13:46:53.916630  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.47784 (* 1 = 0.47784 loss)
I0925 13:46:54.060626  2600 solver.cpp:218] Iteration 64000 (5.51017 iter/s, 18.1483s/100 iters), loss = 0.024617
I0925 13:46:54.060655  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0246162 (* 1 = 0.0246162 loss)
I0925 13:46:54.060662  2600 sgd_solver.cpp:105] Iteration 64000, lr = 0.001
I0925 13:47:08.647070  2600 solver.cpp:218] Iteration 64100 (6.85571 iter/s, 14.5864s/100 iters), loss = 0.0602214
I0925 13:47:08.647100  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0602206 (* 1 = 0.0602206 loss)
I0925 13:47:08.647106  2600 sgd_solver.cpp:105] Iteration 64100, lr = 0.001
I0925 13:47:23.232571  2600 solver.cpp:218] Iteration 64200 (6.85616 iter/s, 14.5854s/100 iters), loss = 0.0963431
I0925 13:47:23.232702  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0963423 (* 1 = 0.0963423 loss)
I0925 13:47:23.232722  2600 sgd_solver.cpp:105] Iteration 64200, lr = 0.001
I0925 13:47:37.822191  2600 solver.cpp:218] Iteration 64300 (6.85427 iter/s, 14.5895s/100 iters), loss = 0.0762002
I0925 13:47:37.822232  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0761994 (* 1 = 0.0761994 loss)
I0925 13:47:37.822238  2600 sgd_solver.cpp:105] Iteration 64300, lr = 0.001
I0925 13:47:52.404006  2600 solver.cpp:218] Iteration 64400 (6.85789 iter/s, 14.5817s/100 iters), loss = 0.0429762
I0925 13:47:52.404036  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0429754 (* 1 = 0.0429754 loss)
I0925 13:47:52.404042  2600 sgd_solver.cpp:105] Iteration 64400, lr = 0.001
I0925 13:48:06.269420  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:48:06.853809  2600 solver.cpp:330] Iteration 64500, Testing net (#0)
I0925 13:48:10.275552  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:48:10.418627  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8712
I0925 13:48:10.418661  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.479507 (* 1 = 0.479507 loss)
I0925 13:48:10.563940  2600 solver.cpp:218] Iteration 64500 (5.50665 iter/s, 18.1599s/100 iters), loss = 0.0481452
I0925 13:48:10.563971  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0481445 (* 1 = 0.0481445 loss)
I0925 13:48:10.563978  2600 sgd_solver.cpp:105] Iteration 64500, lr = 0.001
I0925 13:48:25.155282  2600 solver.cpp:218] Iteration 64600 (6.85341 iter/s, 14.5913s/100 iters), loss = 0.0353412
I0925 13:48:25.155323  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0353404 (* 1 = 0.0353404 loss)
I0925 13:48:25.155328  2600 sgd_solver.cpp:105] Iteration 64600, lr = 0.001
I0925 13:48:39.742208  2600 solver.cpp:218] Iteration 64700 (6.85549 iter/s, 14.5868s/100 iters), loss = 0.105097
I0925 13:48:39.742316  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105096 (* 1 = 0.105096 loss)
I0925 13:48:39.742324  2600 sgd_solver.cpp:105] Iteration 64700, lr = 0.001
I0925 13:48:54.330545  2600 solver.cpp:218] Iteration 64800 (6.85486 iter/s, 14.5882s/100 iters), loss = 0.0917999
I0925 13:48:54.330575  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0917991 (* 1 = 0.0917991 loss)
I0925 13:48:54.330584  2600 sgd_solver.cpp:105] Iteration 64800, lr = 0.001
I0925 13:49:08.914260  2600 solver.cpp:218] Iteration 64900 (6.857 iter/s, 14.5836s/100 iters), loss = 0.106537
I0925 13:49:08.914291  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106536 (* 1 = 0.106536 loss)
I0925 13:49:08.914297  2600 sgd_solver.cpp:105] Iteration 64900, lr = 0.001
I0925 13:49:22.775285  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:49:23.360189  2600 solver.cpp:330] Iteration 65000, Testing net (#0)
I0925 13:49:26.782205  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:49:26.925348  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8667
I0925 13:49:26.925384  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.485158 (* 1 = 0.485158 loss)
I0925 13:49:27.069488  2600 solver.cpp:218] Iteration 65000 (5.50808 iter/s, 18.1552s/100 iters), loss = 0.124964
I0925 13:49:27.069514  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124963 (* 1 = 0.124963 loss)
I0925 13:49:27.069521  2600 sgd_solver.cpp:105] Iteration 65000, lr = 0.001
I0925 13:49:41.658195  2600 solver.cpp:218] Iteration 65100 (6.85465 iter/s, 14.5886s/100 iters), loss = 0.0691894
I0925 13:49:41.658236  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0691885 (* 1 = 0.0691885 loss)
I0925 13:49:41.658242  2600 sgd_solver.cpp:105] Iteration 65100, lr = 0.001
I0925 13:49:56.248863  2600 solver.cpp:218] Iteration 65200 (6.85373 iter/s, 14.5906s/100 iters), loss = 0.0938295
I0925 13:49:56.248977  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0938286 (* 1 = 0.0938286 loss)
I0925 13:49:56.248996  2600 sgd_solver.cpp:105] Iteration 65200, lr = 0.001
I0925 13:50:10.841378  2600 solver.cpp:218] Iteration 65300 (6.8529 iter/s, 14.5924s/100 iters), loss = 0.0638964
I0925 13:50:10.841419  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0638956 (* 1 = 0.0638956 loss)
I0925 13:50:10.841426  2600 sgd_solver.cpp:105] Iteration 65300, lr = 0.001
I0925 13:50:25.431152  2600 solver.cpp:218] Iteration 65400 (6.85415 iter/s, 14.5897s/100 iters), loss = 0.0690155
I0925 13:50:25.431182  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0690146 (* 1 = 0.0690146 loss)
I0925 13:50:25.431190  2600 sgd_solver.cpp:105] Iteration 65400, lr = 0.001
I0925 13:50:39.291950  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:50:39.875490  2600 solver.cpp:330] Iteration 65500, Testing net (#0)
I0925 13:50:43.299962  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:50:43.442678  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8709
I0925 13:50:43.442714  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.490134 (* 1 = 0.490134 loss)
I0925 13:50:43.586920  2600 solver.cpp:218] Iteration 65500 (5.50792 iter/s, 18.1557s/100 iters), loss = 0.118372
I0925 13:50:43.586948  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118371 (* 1 = 0.118371 loss)
I0925 13:50:43.586956  2600 sgd_solver.cpp:105] Iteration 65500, lr = 0.001
I0925 13:50:58.174443  2600 solver.cpp:218] Iteration 65600 (6.8552 iter/s, 14.5875s/100 iters), loss = 0.141954
I0925 13:50:58.174482  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141953 (* 1 = 0.141953 loss)
I0925 13:50:58.174489  2600 sgd_solver.cpp:105] Iteration 65600, lr = 0.001
I0925 13:51:12.758857  2600 solver.cpp:218] Iteration 65700 (6.85667 iter/s, 14.5843s/100 iters), loss = 0.0632921
I0925 13:51:12.758945  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0632913 (* 1 = 0.0632913 loss)
I0925 13:51:12.758963  2600 sgd_solver.cpp:105] Iteration 65700, lr = 0.001
I0925 13:51:27.346629  2600 solver.cpp:218] Iteration 65800 (6.85511 iter/s, 14.5877s/100 iters), loss = 0.0884622
I0925 13:51:27.346660  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0884613 (* 1 = 0.0884613 loss)
I0925 13:51:27.346666  2600 sgd_solver.cpp:105] Iteration 65800, lr = 0.001
I0925 13:51:41.930455  2600 solver.cpp:218] Iteration 65900 (6.85694 iter/s, 14.5838s/100 iters), loss = 0.0742734
I0925 13:51:41.930485  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0742726 (* 1 = 0.0742726 loss)
I0925 13:51:41.930492  2600 sgd_solver.cpp:105] Iteration 65900, lr = 0.001
I0925 13:51:55.791142  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:51:56.375869  2600 solver.cpp:330] Iteration 66000, Testing net (#0)
I0925 13:51:59.799648  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:51:59.942703  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.873
I0925 13:51:59.942729  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.489017 (* 1 = 0.489017 loss)
I0925 13:52:00.086606  2600 solver.cpp:218] Iteration 66000 (5.5078 iter/s, 18.1561s/100 iters), loss = 0.0898613
I0925 13:52:00.086637  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0898604 (* 1 = 0.0898604 loss)
I0925 13:52:00.086643  2600 sgd_solver.cpp:105] Iteration 66000, lr = 0.001
I0925 13:52:14.663931  2600 solver.cpp:218] Iteration 66100 (6.86 iter/s, 14.5773s/100 iters), loss = 0.0944396
I0925 13:52:14.663962  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0944387 (* 1 = 0.0944387 loss)
I0925 13:52:14.663969  2600 sgd_solver.cpp:105] Iteration 66100, lr = 0.001
I0925 13:52:29.245421  2600 solver.cpp:218] Iteration 66200 (6.85804 iter/s, 14.5814s/100 iters), loss = 0.0712851
I0925 13:52:29.245525  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0712843 (* 1 = 0.0712843 loss)
I0925 13:52:29.245532  2600 sgd_solver.cpp:105] Iteration 66200, lr = 0.001
I0925 13:52:43.825762  2600 solver.cpp:218] Iteration 66300 (6.85862 iter/s, 14.5802s/100 iters), loss = 0.0886969
I0925 13:52:43.825793  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0886961 (* 1 = 0.0886961 loss)
I0925 13:52:43.825799  2600 sgd_solver.cpp:105] Iteration 66300, lr = 0.001
I0925 13:52:58.407662  2600 solver.cpp:218] Iteration 66400 (6.85785 iter/s, 14.5818s/100 iters), loss = 0.0410537
I0925 13:52:58.407692  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0410529 (* 1 = 0.0410529 loss)
I0925 13:52:58.407698  2600 sgd_solver.cpp:105] Iteration 66400, lr = 0.001
I0925 13:53:12.258553  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:53:12.841495  2600 solver.cpp:330] Iteration 66500, Testing net (#0)
I0925 13:53:16.265741  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:53:16.408792  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.872
I0925 13:53:16.408828  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.485356 (* 1 = 0.485356 loss)
I0925 13:53:16.553030  2600 solver.cpp:218] Iteration 66500 (5.51107 iter/s, 18.1453s/100 iters), loss = 0.0825898
I0925 13:53:16.553058  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.082589 (* 1 = 0.082589 loss)
I0925 13:53:16.553066  2600 sgd_solver.cpp:105] Iteration 66500, lr = 0.001
I0925 13:53:31.142489  2600 solver.cpp:218] Iteration 66600 (6.8543 iter/s, 14.5894s/100 iters), loss = 0.125932
I0925 13:53:31.142530  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125931 (* 1 = 0.125931 loss)
I0925 13:53:31.142536  2600 sgd_solver.cpp:105] Iteration 66600, lr = 0.001
I0925 13:53:45.728808  2600 solver.cpp:218] Iteration 66700 (6.85578 iter/s, 14.5862s/100 iters), loss = 0.0622123
I0925 13:53:45.728932  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0622115 (* 1 = 0.0622115 loss)
I0925 13:53:45.728940  2600 sgd_solver.cpp:105] Iteration 66700, lr = 0.001
I0925 13:54:00.318111  2600 solver.cpp:218] Iteration 66800 (6.85441 iter/s, 14.5891s/100 iters), loss = 0.137698
I0925 13:54:00.318142  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.137697 (* 1 = 0.137697 loss)
I0925 13:54:00.318148  2600 sgd_solver.cpp:105] Iteration 66800, lr = 0.001
I0925 13:54:14.906971  2600 solver.cpp:218] Iteration 66900 (6.85458 iter/s, 14.5888s/100 iters), loss = 0.0672374
I0925 13:54:14.907001  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0672365 (* 1 = 0.0672365 loss)
I0925 13:54:14.907006  2600 sgd_solver.cpp:105] Iteration 66900, lr = 0.001
I0925 13:54:28.769660  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:54:29.353252  2600 solver.cpp:330] Iteration 67000, Testing net (#0)
I0925 13:54:32.775614  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:54:32.918575  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8679
I0925 13:54:32.918612  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.49738 (* 1 = 0.49738 loss)
I0925 13:54:33.063060  2600 solver.cpp:218] Iteration 67000 (5.50782 iter/s, 18.156s/100 iters), loss = 0.0633512
I0925 13:54:33.063086  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0633503 (* 1 = 0.0633503 loss)
I0925 13:54:33.063094  2600 sgd_solver.cpp:105] Iteration 67000, lr = 0.001
I0925 13:54:47.646620  2600 solver.cpp:218] Iteration 67100 (6.85707 iter/s, 14.5835s/100 iters), loss = 0.0998416
I0925 13:54:47.646661  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0998406 (* 1 = 0.0998406 loss)
I0925 13:54:47.646667  2600 sgd_solver.cpp:105] Iteration 67100, lr = 0.001
I0925 13:55:02.228298  2600 solver.cpp:218] Iteration 67200 (6.85796 iter/s, 14.5816s/100 iters), loss = 0.0687017
I0925 13:55:02.228443  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0687008 (* 1 = 0.0687008 loss)
I0925 13:55:02.228451  2600 sgd_solver.cpp:105] Iteration 67200, lr = 0.001
I0925 13:55:16.818428  2600 solver.cpp:218] Iteration 67300 (6.85403 iter/s, 14.5899s/100 iters), loss = 0.0604064
I0925 13:55:16.818457  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0604054 (* 1 = 0.0604054 loss)
I0925 13:55:16.818464  2600 sgd_solver.cpp:105] Iteration 67300, lr = 0.001
I0925 13:55:31.404772  2600 solver.cpp:218] Iteration 67400 (6.85576 iter/s, 14.5863s/100 iters), loss = 0.0563553
I0925 13:55:31.404801  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0563544 (* 1 = 0.0563544 loss)
I0925 13:55:31.404808  2600 sgd_solver.cpp:105] Iteration 67400, lr = 0.001
I0925 13:55:45.263401  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:55:45.846654  2600 solver.cpp:330] Iteration 67500, Testing net (#0)
I0925 13:55:49.271322  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:55:49.414002  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8721
I0925 13:55:49.414027  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.491792 (* 1 = 0.491792 loss)
I0925 13:55:49.558116  2600 solver.cpp:218] Iteration 67500 (5.50865 iter/s, 18.1533s/100 iters), loss = 0.0582546
I0925 13:55:49.558145  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0582536 (* 1 = 0.0582536 loss)
I0925 13:55:49.558151  2600 sgd_solver.cpp:105] Iteration 67500, lr = 0.001
I0925 13:56:04.150688  2600 solver.cpp:218] Iteration 67600 (6.85283 iter/s, 14.5925s/100 iters), loss = 0.0275548
I0925 13:56:04.150719  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275539 (* 1 = 0.0275539 loss)
I0925 13:56:04.150725  2600 sgd_solver.cpp:105] Iteration 67600, lr = 0.001
I0925 13:56:18.736981  2600 solver.cpp:218] Iteration 67700 (6.85578 iter/s, 14.5862s/100 iters), loss = 0.0749108
I0925 13:56:18.737084  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0749098 (* 1 = 0.0749098 loss)
I0925 13:56:18.737102  2600 sgd_solver.cpp:105] Iteration 67700, lr = 0.001
I0925 13:56:33.328719  2600 solver.cpp:218] Iteration 67800 (6.85326 iter/s, 14.5916s/100 iters), loss = 0.0726742
I0925 13:56:33.328752  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0726732 (* 1 = 0.0726732 loss)
I0925 13:56:33.328768  2600 sgd_solver.cpp:105] Iteration 67800, lr = 0.001
I0925 13:56:47.915473  2600 solver.cpp:218] Iteration 67900 (6.85557 iter/s, 14.5867s/100 iters), loss = 0.0587469
I0925 13:56:47.915504  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0587459 (* 1 = 0.0587459 loss)
I0925 13:56:47.915511  2600 sgd_solver.cpp:105] Iteration 67900, lr = 0.001
I0925 13:57:01.780414  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:57:02.363034  2600 solver.cpp:330] Iteration 68000, Testing net (#0)
I0925 13:57:05.785873  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:57:05.928683  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8724
I0925 13:57:05.928719  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.498755 (* 1 = 0.498755 loss)
I0925 13:57:06.072808  2600 solver.cpp:218] Iteration 68000 (5.50744 iter/s, 18.1573s/100 iters), loss = 0.0861949
I0925 13:57:06.072834  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0861939 (* 1 = 0.0861939 loss)
I0925 13:57:06.072842  2600 sgd_solver.cpp:105] Iteration 68000, lr = 0.001
I0925 13:57:20.666683  2600 solver.cpp:218] Iteration 68100 (6.85222 iter/s, 14.5938s/100 iters), loss = 0.0852418
I0925 13:57:20.666725  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0852408 (* 1 = 0.0852408 loss)
I0925 13:57:20.666733  2600 sgd_solver.cpp:105] Iteration 68100, lr = 0.001
I0925 13:57:35.254284  2600 solver.cpp:218] Iteration 68200 (6.85517 iter/s, 14.5875s/100 iters), loss = 0.107462
I0925 13:57:35.254392  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107461 (* 1 = 0.107461 loss)
I0925 13:57:35.254408  2600 sgd_solver.cpp:105] Iteration 68200, lr = 0.001
I0925 13:57:49.844146  2600 solver.cpp:218] Iteration 68300 (6.85414 iter/s, 14.5897s/100 iters), loss = 0.0630048
I0925 13:57:49.844187  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0630038 (* 1 = 0.0630038 loss)
I0925 13:57:49.844193  2600 sgd_solver.cpp:105] Iteration 68300, lr = 0.001
I0925 13:58:04.432133  2600 solver.cpp:218] Iteration 68400 (6.85499 iter/s, 14.5879s/100 iters), loss = 0.0584642
I0925 13:58:04.432163  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0584632 (* 1 = 0.0584632 loss)
I0925 13:58:04.432169  2600 sgd_solver.cpp:105] Iteration 68400, lr = 0.001
I0925 13:58:18.295397  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:58:18.878623  2600 solver.cpp:330] Iteration 68500, Testing net (#0)
I0925 13:58:22.302284  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:58:22.444953  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8733
I0925 13:58:22.444979  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.495087 (* 1 = 0.495087 loss)
I0925 13:58:22.588716  2600 solver.cpp:218] Iteration 68500 (5.50767 iter/s, 18.1565s/100 iters), loss = 0.0333109
I0925 13:58:22.588747  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.03331 (* 1 = 0.03331 loss)
I0925 13:58:22.588754  2600 sgd_solver.cpp:105] Iteration 68500, lr = 0.001
I0925 13:58:37.174607  2600 solver.cpp:218] Iteration 68600 (6.85597 iter/s, 14.5858s/100 iters), loss = 0.0608662
I0925 13:58:37.174648  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0608653 (* 1 = 0.0608653 loss)
I0925 13:58:37.174654  2600 sgd_solver.cpp:105] Iteration 68600, lr = 0.001
I0925 13:58:51.752594  2600 solver.cpp:218] Iteration 68700 (6.85969 iter/s, 14.5779s/100 iters), loss = 0.0871954
I0925 13:58:51.752703  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0871945 (* 1 = 0.0871945 loss)
I0925 13:58:51.752722  2600 sgd_solver.cpp:105] Iteration 68700, lr = 0.001
I0925 13:59:06.329751  2600 solver.cpp:218] Iteration 68800 (6.86012 iter/s, 14.577s/100 iters), loss = 0.0628288
I0925 13:59:06.329792  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0628278 (* 1 = 0.0628278 loss)
I0925 13:59:06.329798  2600 sgd_solver.cpp:105] Iteration 68800, lr = 0.001
I0925 13:59:20.904904  2600 solver.cpp:218] Iteration 68900 (6.86103 iter/s, 14.5751s/100 iters), loss = 0.0410815
I0925 13:59:20.904937  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0410806 (* 1 = 0.0410806 loss)
I0925 13:59:20.904942  2600 sgd_solver.cpp:105] Iteration 68900, lr = 0.001
I0925 13:59:34.761940  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:59:35.346369  2600 solver.cpp:330] Iteration 69000, Testing net (#0)
I0925 13:59:38.770090  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 13:59:38.913205  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8707
I0925 13:59:38.913240  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.504048 (* 1 = 0.504048 loss)
I0925 13:59:39.057247  2600 solver.cpp:218] Iteration 69000 (5.50895 iter/s, 18.1523s/100 iters), loss = 0.101241
I0925 13:59:39.057274  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.10124 (* 1 = 0.10124 loss)
I0925 13:59:39.057281  2600 sgd_solver.cpp:105] Iteration 69000, lr = 0.001
I0925 13:59:53.646407  2600 solver.cpp:218] Iteration 69100 (6.85443 iter/s, 14.5891s/100 iters), loss = 0.105396
I0925 13:59:53.646450  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105395 (* 1 = 0.105395 loss)
I0925 13:59:53.646456  2600 sgd_solver.cpp:105] Iteration 69100, lr = 0.001
I0925 14:00:08.236109  2600 solver.cpp:218] Iteration 69200 (6.85419 iter/s, 14.5896s/100 iters), loss = 0.112152
I0925 14:00:08.236227  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.112151 (* 1 = 0.112151 loss)
I0925 14:00:08.236234  2600 sgd_solver.cpp:105] Iteration 69200, lr = 0.001
I0925 14:00:22.820801  2600 solver.cpp:218] Iteration 69300 (6.85657 iter/s, 14.5845s/100 iters), loss = 0.0667676
I0925 14:00:22.820832  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0667667 (* 1 = 0.0667667 loss)
I0925 14:00:22.820837  2600 sgd_solver.cpp:105] Iteration 69300, lr = 0.001
I0925 14:00:37.414731  2600 solver.cpp:218] Iteration 69400 (6.8522 iter/s, 14.5939s/100 iters), loss = 0.0851117
I0925 14:00:37.414770  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0851108 (* 1 = 0.0851108 loss)
I0925 14:00:37.414777  2600 sgd_solver.cpp:105] Iteration 69400, lr = 0.001
I0925 14:00:51.279002  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:00:51.863055  2600 solver.cpp:330] Iteration 69500, Testing net (#0)
I0925 14:00:55.286095  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:00:55.428799  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8722
I0925 14:00:55.428835  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.497838 (* 1 = 0.497838 loss)
I0925 14:00:55.573043  2600 solver.cpp:218] Iteration 69500 (5.50715 iter/s, 18.1582s/100 iters), loss = 0.0522679
I0925 14:00:55.573071  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.052267 (* 1 = 0.052267 loss)
I0925 14:00:55.573078  2600 sgd_solver.cpp:105] Iteration 69500, lr = 0.001
I0925 14:01:10.166162  2600 solver.cpp:218] Iteration 69600 (6.85258 iter/s, 14.5931s/100 iters), loss = 0.0598256
I0925 14:01:10.166193  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0598247 (* 1 = 0.0598247 loss)
I0925 14:01:10.166198  2600 sgd_solver.cpp:105] Iteration 69600, lr = 0.001
I0925 14:01:24.761412  2600 solver.cpp:218] Iteration 69700 (6.85158 iter/s, 14.5952s/100 iters), loss = 0.0552752
I0925 14:01:24.761554  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0552742 (* 1 = 0.0552742 loss)
I0925 14:01:24.761564  2600 sgd_solver.cpp:105] Iteration 69700, lr = 0.001
I0925 14:01:39.351974  2600 solver.cpp:218] Iteration 69800 (6.85383 iter/s, 14.5904s/100 iters), loss = 0.0659418
I0925 14:01:39.352005  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0659409 (* 1 = 0.0659409 loss)
I0925 14:01:39.352011  2600 sgd_solver.cpp:105] Iteration 69800, lr = 0.001
I0925 14:01:53.943127  2600 solver.cpp:218] Iteration 69900 (6.8535 iter/s, 14.5911s/100 iters), loss = 0.0739126
I0925 14:01:53.943169  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0739116 (* 1 = 0.0739116 loss)
I0925 14:01:53.943176  2600 sgd_solver.cpp:105] Iteration 69900, lr = 0.001
I0925 14:02:07.814280  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:02:08.399430  2600 solver.cpp:330] Iteration 70000, Testing net (#0)
I0925 14:02:11.823541  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:02:11.966348  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8686
I0925 14:02:11.966384  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.508792 (* 1 = 0.508792 loss)
I0925 14:02:12.110667  2600 solver.cpp:218] Iteration 70000 (5.50435 iter/s, 18.1675s/100 iters), loss = 0.0655242
I0925 14:02:12.110698  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0655232 (* 1 = 0.0655232 loss)
I0925 14:02:12.110705  2600 sgd_solver.cpp:105] Iteration 70000, lr = 0.001
I0925 14:02:26.702173  2600 solver.cpp:218] Iteration 70100 (6.85333 iter/s, 14.5914s/100 iters), loss = 0.078896
I0925 14:02:26.702203  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.078895 (* 1 = 0.078895 loss)
I0925 14:02:26.702210  2600 sgd_solver.cpp:105] Iteration 70100, lr = 0.001
I0925 14:02:41.295413  2600 solver.cpp:218] Iteration 70200 (6.85252 iter/s, 14.5932s/100 iters), loss = 0.0706914
I0925 14:02:41.295531  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0706904 (* 1 = 0.0706904 loss)
I0925 14:02:41.295547  2600 sgd_solver.cpp:105] Iteration 70200, lr = 0.001
I0925 14:02:55.889514  2600 solver.cpp:218] Iteration 70300 (6.85215 iter/s, 14.594s/100 iters), loss = 0.051829
I0925 14:02:55.889555  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.051828 (* 1 = 0.051828 loss)
I0925 14:02:55.889562  2600 sgd_solver.cpp:105] Iteration 70300, lr = 0.001
I0925 14:03:10.478147  2600 solver.cpp:218] Iteration 70400 (6.85469 iter/s, 14.5886s/100 iters), loss = 0.0484625
I0925 14:03:10.478176  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0484615 (* 1 = 0.0484615 loss)
I0925 14:03:10.478183  2600 sgd_solver.cpp:105] Iteration 70400, lr = 0.001
I0925 14:03:24.343889  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:03:24.926724  2600 solver.cpp:330] Iteration 70500, Testing net (#0)
I0925 14:03:28.350421  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:03:28.493041  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8743
I0925 14:03:28.493075  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.503614 (* 1 = 0.503614 loss)
I0925 14:03:28.637053  2600 solver.cpp:218] Iteration 70500 (5.50696 iter/s, 18.1588s/100 iters), loss = 0.0505113
I0925 14:03:28.637082  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0505104 (* 1 = 0.0505104 loss)
I0925 14:03:28.637089  2600 sgd_solver.cpp:105] Iteration 70500, lr = 0.001
I0925 14:03:43.225531  2600 solver.cpp:218] Iteration 70600 (6.85475 iter/s, 14.5884s/100 iters), loss = 0.0393324
I0925 14:03:43.225560  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0393315 (* 1 = 0.0393315 loss)
I0925 14:03:43.225566  2600 sgd_solver.cpp:105] Iteration 70600, lr = 0.001
I0925 14:03:57.806706  2600 solver.cpp:218] Iteration 70700 (6.85819 iter/s, 14.5811s/100 iters), loss = 0.0694304
I0925 14:03:57.806818  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0694294 (* 1 = 0.0694294 loss)
I0925 14:03:57.806838  2600 sgd_solver.cpp:105] Iteration 70700, lr = 0.001
I0925 14:04:12.387619  2600 solver.cpp:218] Iteration 70800 (6.85835 iter/s, 14.5808s/100 iters), loss = 0.0578719
I0925 14:04:12.387650  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.057871 (* 1 = 0.057871 loss)
I0925 14:04:12.387655  2600 sgd_solver.cpp:105] Iteration 70800, lr = 0.001
I0925 14:04:26.969069  2600 solver.cpp:218] Iteration 70900 (6.85806 iter/s, 14.5814s/100 iters), loss = 0.0495017
I0925 14:04:26.969110  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0495008 (* 1 = 0.0495008 loss)
I0925 14:04:26.969115  2600 sgd_solver.cpp:105] Iteration 70900, lr = 0.001
I0925 14:04:40.833823  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:04:41.417920  2600 solver.cpp:330] Iteration 71000, Testing net (#0)
I0925 14:04:44.841941  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:04:44.984699  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8695
I0925 14:04:44.984735  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.517732 (* 1 = 0.517732 loss)
I0925 14:04:45.128837  2600 solver.cpp:218] Iteration 71000 (5.5067 iter/s, 18.1597s/100 iters), loss = 0.0597022
I0925 14:04:45.128865  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0597013 (* 1 = 0.0597013 loss)
I0925 14:04:45.128872  2600 sgd_solver.cpp:105] Iteration 71000, lr = 0.001
I0925 14:04:59.708801  2600 solver.cpp:218] Iteration 71100 (6.85876 iter/s, 14.5799s/100 iters), loss = 0.0928284
I0925 14:04:59.708832  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0928274 (* 1 = 0.0928274 loss)
I0925 14:04:59.708837  2600 sgd_solver.cpp:105] Iteration 71100, lr = 0.001
I0925 14:05:14.288712  2600 solver.cpp:218] Iteration 71200 (6.85879 iter/s, 14.5798s/100 iters), loss = 0.0894825
I0925 14:05:14.288851  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0894816 (* 1 = 0.0894816 loss)
I0925 14:05:14.288858  2600 sgd_solver.cpp:105] Iteration 71200, lr = 0.001
I0925 14:05:28.862143  2600 solver.cpp:218] Iteration 71300 (6.86188 iter/s, 14.5733s/100 iters), loss = 0.0681527
I0925 14:05:28.862172  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0681517 (* 1 = 0.0681517 loss)
I0925 14:05:28.862179  2600 sgd_solver.cpp:105] Iteration 71300, lr = 0.001
I0925 14:05:43.440109  2600 solver.cpp:218] Iteration 71400 (6.8597 iter/s, 14.5779s/100 iters), loss = 0.0263059
I0925 14:05:43.440140  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0263049 (* 1 = 0.0263049 loss)
I0925 14:05:43.440145  2600 sgd_solver.cpp:105] Iteration 71400, lr = 0.001
I0925 14:05:57.297224  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:05:57.882164  2600 solver.cpp:330] Iteration 71500, Testing net (#0)
I0925 14:06:01.303761  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:06:01.446712  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8702
I0925 14:06:01.446746  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.510456 (* 1 = 0.510456 loss)
I0925 14:06:01.591068  2600 solver.cpp:218] Iteration 71500 (5.50937 iter/s, 18.1509s/100 iters), loss = 0.0246621
I0925 14:06:01.591094  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0246611 (* 1 = 0.0246611 loss)
I0925 14:06:01.591101  2600 sgd_solver.cpp:105] Iteration 71500, lr = 0.001
I0925 14:06:16.183146  2600 solver.cpp:218] Iteration 71600 (6.85306 iter/s, 14.592s/100 iters), loss = 0.045156
I0925 14:06:16.183187  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.045155 (* 1 = 0.045155 loss)
I0925 14:06:16.183193  2600 sgd_solver.cpp:105] Iteration 71600, lr = 0.001
I0925 14:06:30.771786  2600 solver.cpp:218] Iteration 71700 (6.85469 iter/s, 14.5886s/100 iters), loss = 0.081767
I0925 14:06:30.771916  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.081766 (* 1 = 0.081766 loss)
I0925 14:06:30.771925  2600 sgd_solver.cpp:105] Iteration 71700, lr = 0.001
I0925 14:06:45.360427  2600 solver.cpp:218] Iteration 71800 (6.85473 iter/s, 14.5885s/100 iters), loss = 0.0677948
I0925 14:06:45.360468  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0677938 (* 1 = 0.0677938 loss)
I0925 14:06:45.360474  2600 sgd_solver.cpp:105] Iteration 71800, lr = 0.001
I0925 14:06:59.947975  2600 solver.cpp:218] Iteration 71900 (6.8552 iter/s, 14.5875s/100 iters), loss = 0.0446694
I0925 14:06:59.948015  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446685 (* 1 = 0.0446685 loss)
I0925 14:06:59.948022  2600 sgd_solver.cpp:105] Iteration 71900, lr = 0.001
I0925 14:07:13.806140  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:07:14.389977  2600 solver.cpp:330] Iteration 72000, Testing net (#0)
I0925 14:07:17.811514  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:07:17.954422  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8741
I0925 14:07:17.954457  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.503347 (* 1 = 0.503347 loss)
I0925 14:07:18.099120  2600 solver.cpp:218] Iteration 72000 (5.50932 iter/s, 18.1511s/100 iters), loss = 0.039066
I0925 14:07:18.099148  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.039065 (* 1 = 0.039065 loss)
I0925 14:07:18.099155  2600 sgd_solver.cpp:105] Iteration 72000, lr = 0.001
I0925 14:07:32.681650  2600 solver.cpp:218] Iteration 72100 (6.85755 iter/s, 14.5825s/100 iters), loss = 0.0863927
I0925 14:07:32.681680  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0863918 (* 1 = 0.0863918 loss)
I0925 14:07:32.681687  2600 sgd_solver.cpp:105] Iteration 72100, lr = 0.001
I0925 14:07:47.273811  2600 solver.cpp:218] Iteration 72200 (6.85303 iter/s, 14.5921s/100 iters), loss = 0.0531673
I0925 14:07:47.273950  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0531663 (* 1 = 0.0531663 loss)
I0925 14:07:47.273958  2600 sgd_solver.cpp:105] Iteration 72200, lr = 0.001
I0925 14:08:01.857628  2600 solver.cpp:218] Iteration 72300 (6.857 iter/s, 14.5836s/100 iters), loss = 0.0767519
I0925 14:08:01.857668  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.076751 (* 1 = 0.076751 loss)
I0925 14:08:01.857676  2600 sgd_solver.cpp:105] Iteration 72300, lr = 0.001
I0925 14:08:16.444430  2600 solver.cpp:218] Iteration 72400 (6.85555 iter/s, 14.5867s/100 iters), loss = 0.0710988
I0925 14:08:16.444460  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0710978 (* 1 = 0.0710978 loss)
I0925 14:08:16.444466  2600 sgd_solver.cpp:105] Iteration 72400, lr = 0.001
I0925 14:08:30.307266  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:08:30.891367  2600 solver.cpp:330] Iteration 72500, Testing net (#0)
I0925 14:08:34.314512  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:08:34.457974  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8692
I0925 14:08:34.458010  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.520696 (* 1 = 0.520696 loss)
I0925 14:08:34.602749  2600 solver.cpp:218] Iteration 72500 (5.50714 iter/s, 18.1582s/100 iters), loss = 0.0515998
I0925 14:08:34.602778  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0515988 (* 1 = 0.0515988 loss)
I0925 14:08:34.602785  2600 sgd_solver.cpp:105] Iteration 72500, lr = 0.001
I0925 14:08:49.192404  2600 solver.cpp:218] Iteration 72600 (6.8542 iter/s, 14.5896s/100 iters), loss = 0.086158
I0925 14:08:49.192435  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0861571 (* 1 = 0.0861571 loss)
I0925 14:08:49.192441  2600 sgd_solver.cpp:105] Iteration 72600, lr = 0.001
I0925 14:09:03.786384  2600 solver.cpp:218] Iteration 72700 (6.85217 iter/s, 14.5939s/100 iters), loss = 0.0637582
I0925 14:09:03.786499  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0637572 (* 1 = 0.0637572 loss)
I0925 14:09:03.786517  2600 sgd_solver.cpp:105] Iteration 72700, lr = 0.001
I0925 14:09:18.379292  2600 solver.cpp:218] Iteration 72800 (6.85271 iter/s, 14.5928s/100 iters), loss = 0.0336535
I0925 14:09:18.379323  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0336526 (* 1 = 0.0336526 loss)
I0925 14:09:18.379328  2600 sgd_solver.cpp:105] Iteration 72800, lr = 0.001
I0925 14:09:32.970566  2600 solver.cpp:218] Iteration 72900 (6.85344 iter/s, 14.5912s/100 iters), loss = 0.0244306
I0925 14:09:32.970607  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0244296 (* 1 = 0.0244296 loss)
I0925 14:09:32.970615  2600 sgd_solver.cpp:105] Iteration 72900, lr = 0.001
I0925 14:09:46.837281  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:09:47.420882  2600 solver.cpp:330] Iteration 73000, Testing net (#0)
I0925 14:09:50.843539  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:09:50.986003  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8733
I0925 14:09:50.986038  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.513606 (* 1 = 0.513606 loss)
I0925 14:09:51.129787  2600 solver.cpp:218] Iteration 73000 (5.50687 iter/s, 18.1591s/100 iters), loss = 0.0493388
I0925 14:09:51.129815  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0493379 (* 1 = 0.0493379 loss)
I0925 14:09:51.129822  2600 sgd_solver.cpp:105] Iteration 73000, lr = 0.001
I0925 14:10:05.719465  2600 solver.cpp:218] Iteration 73100 (6.85419 iter/s, 14.5896s/100 iters), loss = 0.0477916
I0925 14:10:05.719494  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0477906 (* 1 = 0.0477906 loss)
I0925 14:10:05.719501  2600 sgd_solver.cpp:105] Iteration 73100, lr = 0.001
I0925 14:10:20.306584  2600 solver.cpp:218] Iteration 73200 (6.85539 iter/s, 14.5871s/100 iters), loss = 0.060774
I0925 14:10:20.306732  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.060773 (* 1 = 0.060773 loss)
I0925 14:10:20.306754  2600 sgd_solver.cpp:105] Iteration 73200, lr = 0.001
I0925 14:10:34.897801  2600 solver.cpp:218] Iteration 73300 (6.85352 iter/s, 14.591s/100 iters), loss = 0.0464189
I0925 14:10:34.897831  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0464179 (* 1 = 0.0464179 loss)
I0925 14:10:34.897837  2600 sgd_solver.cpp:105] Iteration 73300, lr = 0.001
I0925 14:10:49.489033  2600 solver.cpp:218] Iteration 73400 (6.85346 iter/s, 14.5912s/100 iters), loss = 0.0380165
I0925 14:10:49.489063  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380155 (* 1 = 0.0380155 loss)
I0925 14:10:49.489069  2600 sgd_solver.cpp:105] Iteration 73400, lr = 0.001
I0925 14:11:03.358279  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:11:03.941612  2600 solver.cpp:330] Iteration 73500, Testing net (#0)
I0925 14:11:07.365629  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:11:07.508472  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.872
I0925 14:11:07.508503  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.517819 (* 1 = 0.517819 loss)
I0925 14:11:07.652709  2600 solver.cpp:218] Iteration 73500 (5.50552 iter/s, 18.1636s/100 iters), loss = 0.033467
I0925 14:11:07.652740  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0334661 (* 1 = 0.0334661 loss)
I0925 14:11:07.652750  2600 sgd_solver.cpp:105] Iteration 73500, lr = 0.001
I0925 14:11:22.231354  2600 solver.cpp:218] Iteration 73600 (6.85938 iter/s, 14.5786s/100 iters), loss = 0.0916291
I0925 14:11:22.231387  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0916281 (* 1 = 0.0916281 loss)
I0925 14:11:22.231397  2600 sgd_solver.cpp:105] Iteration 73600, lr = 0.001
I0925 14:11:36.808078  2600 solver.cpp:218] Iteration 73700 (6.86029 iter/s, 14.5767s/100 iters), loss = 0.0323879
I0925 14:11:36.808224  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0323871 (* 1 = 0.0323871 loss)
I0925 14:11:36.808248  2600 sgd_solver.cpp:105] Iteration 73700, lr = 0.001
I0925 14:11:51.391939  2600 solver.cpp:218] Iteration 73800 (6.85698 iter/s, 14.5837s/100 iters), loss = 0.0536879
I0925 14:11:51.391973  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.053687 (* 1 = 0.053687 loss)
I0925 14:11:51.391991  2600 sgd_solver.cpp:105] Iteration 73800, lr = 0.001
I0925 14:12:05.971200  2600 solver.cpp:218] Iteration 73900 (6.85909 iter/s, 14.5792s/100 iters), loss = 0.0317592
I0925 14:12:05.971235  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0317583 (* 1 = 0.0317583 loss)
I0925 14:12:05.971243  2600 sgd_solver.cpp:105] Iteration 73900, lr = 0.001
I0925 14:12:19.824739  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:12:20.408468  2600 solver.cpp:330] Iteration 74000, Testing net (#0)
I0925 14:12:23.832407  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:12:23.975699  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8697
I0925 14:12:23.975726  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.520582 (* 1 = 0.520582 loss)
I0925 14:12:24.119839  2600 solver.cpp:218] Iteration 74000 (5.51008 iter/s, 18.1486s/100 iters), loss = 0.0455611
I0925 14:12:24.119868  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0455602 (* 1 = 0.0455602 loss)
I0925 14:12:24.119879  2600 sgd_solver.cpp:105] Iteration 74000, lr = 0.001
I0925 14:12:38.709300  2600 solver.cpp:218] Iteration 74100 (6.85429 iter/s, 14.5894s/100 iters), loss = 0.0905997
I0925 14:12:38.709332  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0905987 (* 1 = 0.0905987 loss)
I0925 14:12:38.709342  2600 sgd_solver.cpp:105] Iteration 74100, lr = 0.001
I0925 14:12:53.296154  2600 solver.cpp:218] Iteration 74200 (6.85552 iter/s, 14.5868s/100 iters), loss = 0.0663802
I0925 14:12:53.296334  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0663792 (* 1 = 0.0663792 loss)
I0925 14:12:53.296345  2600 sgd_solver.cpp:105] Iteration 74200, lr = 0.001
I0925 14:13:07.889770  2600 solver.cpp:218] Iteration 74300 (6.85241 iter/s, 14.5934s/100 iters), loss = 0.108628
I0925 14:13:07.889801  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108627 (* 1 = 0.108627 loss)
I0925 14:13:07.889809  2600 sgd_solver.cpp:105] Iteration 74300, lr = 0.001
I0925 14:13:22.484395  2600 solver.cpp:218] Iteration 74400 (6.85187 iter/s, 14.5946s/100 iters), loss = 0.044672
I0925 14:13:22.484428  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.044671 (* 1 = 0.044671 loss)
I0925 14:13:22.484447  2600 sgd_solver.cpp:105] Iteration 74400, lr = 0.001
I0925 14:13:36.352463  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:13:36.938073  2600 solver.cpp:330] Iteration 74500, Testing net (#0)
I0925 14:13:40.361055  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:13:40.504222  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8706
I0925 14:13:40.504248  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.519522 (* 1 = 0.519522 loss)
I0925 14:13:40.648908  2600 solver.cpp:218] Iteration 74500 (5.50526 iter/s, 18.1644s/100 iters), loss = 0.0612303
I0925 14:13:40.648939  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0612293 (* 1 = 0.0612293 loss)
I0925 14:13:40.648949  2600 sgd_solver.cpp:105] Iteration 74500, lr = 0.001
I0925 14:13:55.239578  2600 solver.cpp:218] Iteration 74600 (6.85373 iter/s, 14.5906s/100 iters), loss = 0.135096
I0925 14:13:55.239611  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135095 (* 1 = 0.135095 loss)
I0925 14:13:55.239630  2600 sgd_solver.cpp:105] Iteration 74600, lr = 0.001
I0925 14:14:09.832589  2600 solver.cpp:218] Iteration 74700 (6.85263 iter/s, 14.5929s/100 iters), loss = 0.0538162
I0925 14:14:09.832693  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0538153 (* 1 = 0.0538153 loss)
I0925 14:14:09.832705  2600 sgd_solver.cpp:105] Iteration 74700, lr = 0.001
I0925 14:14:24.428527  2600 solver.cpp:218] Iteration 74800 (6.85129 iter/s, 14.5958s/100 iters), loss = 0.0557931
I0925 14:14:24.428561  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0557922 (* 1 = 0.0557922 loss)
I0925 14:14:24.428580  2600 sgd_solver.cpp:105] Iteration 74800, lr = 0.001
I0925 14:14:39.018854  2600 solver.cpp:218] Iteration 74900 (6.85389 iter/s, 14.5903s/100 iters), loss = 0.0329615
I0925 14:14:39.018888  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0329606 (* 1 = 0.0329606 loss)
I0925 14:14:39.018906  2600 sgd_solver.cpp:105] Iteration 74900, lr = 0.001
I0925 14:14:52.883329  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:14:53.466764  2600 solver.cpp:330] Iteration 75000, Testing net (#0)
I0925 14:14:56.889642  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:14:57.032588  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.872
I0925 14:14:57.032615  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.520257 (* 1 = 0.520257 loss)
I0925 14:14:57.177126  2600 solver.cpp:218] Iteration 75000 (5.50715 iter/s, 18.1582s/100 iters), loss = 0.0657081
I0925 14:14:57.177156  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0657071 (* 1 = 0.0657071 loss)
I0925 14:14:57.177167  2600 sgd_solver.cpp:105] Iteration 75000, lr = 0.001
I0925 14:15:11.765574  2600 solver.cpp:218] Iteration 75100 (6.85477 iter/s, 14.5884s/100 iters), loss = 0.136617
I0925 14:15:11.765607  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.136616 (* 1 = 0.136616 loss)
I0925 14:15:11.765616  2600 sgd_solver.cpp:105] Iteration 75100, lr = 0.001
I0925 14:15:26.358599  2600 solver.cpp:218] Iteration 75200 (6.85262 iter/s, 14.593s/100 iters), loss = 0.0866583
I0925 14:15:26.358738  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0866574 (* 1 = 0.0866574 loss)
I0925 14:15:26.358749  2600 sgd_solver.cpp:105] Iteration 75200, lr = 0.001
I0925 14:15:40.949496  2600 solver.cpp:218] Iteration 75300 (6.85367 iter/s, 14.5907s/100 iters), loss = 0.108284
I0925 14:15:40.949528  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108283 (* 1 = 0.108283 loss)
I0925 14:15:40.949537  2600 sgd_solver.cpp:105] Iteration 75300, lr = 0.001
I0925 14:15:55.540258  2600 solver.cpp:218] Iteration 75400 (6.85368 iter/s, 14.5907s/100 iters), loss = 0.0283036
I0925 14:15:55.540292  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0283027 (* 1 = 0.0283027 loss)
I0925 14:15:55.540311  2600 sgd_solver.cpp:105] Iteration 75400, lr = 0.001
I0925 14:16:09.400357  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:16:09.984694  2600 solver.cpp:330] Iteration 75500, Testing net (#0)
I0925 14:16:13.407346  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:16:13.550285  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8768
I0925 14:16:13.550312  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.513186 (* 1 = 0.513186 loss)
I0925 14:16:13.694154  2600 solver.cpp:218] Iteration 75500 (5.50848 iter/s, 18.1538s/100 iters), loss = 0.0732505
I0925 14:16:13.694185  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0732496 (* 1 = 0.0732496 loss)
I0925 14:16:13.694195  2600 sgd_solver.cpp:105] Iteration 75500, lr = 0.001
I0925 14:16:28.290421  2600 solver.cpp:218] Iteration 75600 (6.8511 iter/s, 14.5962s/100 iters), loss = 0.0912591
I0925 14:16:28.290453  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0912582 (* 1 = 0.0912582 loss)
I0925 14:16:28.290462  2600 sgd_solver.cpp:105] Iteration 75600, lr = 0.001
I0925 14:16:42.875933  2600 solver.cpp:218] Iteration 75700 (6.85615 iter/s, 14.5854s/100 iters), loss = 0.060232
I0925 14:16:42.876052  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0602311 (* 1 = 0.0602311 loss)
I0925 14:16:42.876075  2600 sgd_solver.cpp:105] Iteration 75700, lr = 0.001
I0925 14:16:57.464148  2600 solver.cpp:218] Iteration 75800 (6.85492 iter/s, 14.5881s/100 iters), loss = 0.0829304
I0925 14:16:57.464181  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0829296 (* 1 = 0.0829296 loss)
I0925 14:16:57.464190  2600 sgd_solver.cpp:105] Iteration 75800, lr = 0.001
I0925 14:17:12.047963  2600 solver.cpp:218] Iteration 75900 (6.85695 iter/s, 14.5837s/100 iters), loss = 0.0372686
I0925 14:17:12.047996  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0372677 (* 1 = 0.0372677 loss)
I0925 14:17:12.048004  2600 sgd_solver.cpp:105] Iteration 75900, lr = 0.001
I0925 14:17:25.907141  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:17:26.490315  2600 solver.cpp:330] Iteration 76000, Testing net (#0)
I0925 14:17:29.914237  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:17:30.057090  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.874
I0925 14:17:30.057117  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.528342 (* 1 = 0.528342 loss)
I0925 14:17:30.201079  2600 solver.cpp:218] Iteration 76000 (5.50872 iter/s, 18.153s/100 iters), loss = 0.033908
I0925 14:17:30.201114  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0339071 (* 1 = 0.0339071 loss)
I0925 14:17:30.201124  2600 sgd_solver.cpp:105] Iteration 76000, lr = 0.001
I0925 14:17:44.781664  2600 solver.cpp:218] Iteration 76100 (6.85847 iter/s, 14.5805s/100 iters), loss = 0.0572677
I0925 14:17:44.781699  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0572669 (* 1 = 0.0572669 loss)
I0925 14:17:44.781718  2600 sgd_solver.cpp:105] Iteration 76100, lr = 0.001
I0925 14:17:59.357630  2600 solver.cpp:218] Iteration 76200 (6.86064 iter/s, 14.5759s/100 iters), loss = 0.0713519
I0925 14:17:59.357798  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0713511 (* 1 = 0.0713511 loss)
I0925 14:17:59.357810  2600 sgd_solver.cpp:105] Iteration 76200, lr = 0.001
I0925 14:18:13.934442  2600 solver.cpp:218] Iteration 76300 (6.8603 iter/s, 14.5766s/100 iters), loss = 0.0536774
I0925 14:18:13.934476  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0536766 (* 1 = 0.0536766 loss)
I0925 14:18:13.934485  2600 sgd_solver.cpp:105] Iteration 76300, lr = 0.001
I0925 14:18:28.509088  2600 solver.cpp:218] Iteration 76400 (6.86126 iter/s, 14.5746s/100 iters), loss = 0.0345313
I0925 14:18:28.509124  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0345304 (* 1 = 0.0345304 loss)
I0925 14:18:28.509133  2600 sgd_solver.cpp:105] Iteration 76400, lr = 0.001
I0925 14:18:42.365816  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:18:42.948839  2600 solver.cpp:330] Iteration 76500, Testing net (#0)
I0925 14:18:46.373152  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:18:46.515765  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8728
I0925 14:18:46.515794  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.521137 (* 1 = 0.521137 loss)
I0925 14:18:46.660439  2600 solver.cpp:218] Iteration 76500 (5.50926 iter/s, 18.1513s/100 iters), loss = 0.0724976
I0925 14:18:46.660470  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0724967 (* 1 = 0.0724967 loss)
I0925 14:18:46.660480  2600 sgd_solver.cpp:105] Iteration 76500, lr = 0.001
I0925 14:19:01.256283  2600 solver.cpp:218] Iteration 76600 (6.8513 iter/s, 14.5958s/100 iters), loss = 0.0637162
I0925 14:19:01.256316  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0637153 (* 1 = 0.0637153 loss)
I0925 14:19:01.256325  2600 sgd_solver.cpp:105] Iteration 76600, lr = 0.001
I0925 14:19:15.846371  2600 solver.cpp:218] Iteration 76700 (6.854 iter/s, 14.59s/100 iters), loss = 0.0561968
I0925 14:19:15.846473  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0561959 (* 1 = 0.0561959 loss)
I0925 14:19:15.846483  2600 sgd_solver.cpp:105] Iteration 76700, lr = 0.001
I0925 14:19:30.433045  2600 solver.cpp:218] Iteration 76800 (6.85564 iter/s, 14.5865s/100 iters), loss = 0.125474
I0925 14:19:30.433079  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125473 (* 1 = 0.125473 loss)
I0925 14:19:30.433089  2600 sgd_solver.cpp:105] Iteration 76800, lr = 0.001
I0925 14:19:45.019486  2600 solver.cpp:218] Iteration 76900 (6.85571 iter/s, 14.5864s/100 iters), loss = 0.0755542
I0925 14:19:45.019520  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0755533 (* 1 = 0.0755533 loss)
I0925 14:19:45.019528  2600 sgd_solver.cpp:105] Iteration 76900, lr = 0.001
I0925 14:19:58.885778  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:19:59.469900  2600 solver.cpp:330] Iteration 77000, Testing net (#0)
I0925 14:20:02.896759  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:20:03.039433  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8705
I0925 14:20:03.039460  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.529213 (* 1 = 0.529213 loss)
I0925 14:20:03.184167  2600 solver.cpp:218] Iteration 77000 (5.50521 iter/s, 18.1646s/100 iters), loss = 0.0388336
I0925 14:20:03.184204  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0388327 (* 1 = 0.0388327 loss)
I0925 14:20:03.184214  2600 sgd_solver.cpp:105] Iteration 77000, lr = 0.001
I0925 14:20:17.775820  2600 solver.cpp:218] Iteration 77100 (6.85327 iter/s, 14.5916s/100 iters), loss = 0.0812325
I0925 14:20:17.775853  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0812316 (* 1 = 0.0812316 loss)
I0925 14:20:17.775862  2600 sgd_solver.cpp:105] Iteration 77100, lr = 0.001
I0925 14:20:32.364795  2600 solver.cpp:218] Iteration 77200 (6.85452 iter/s, 14.5889s/100 iters), loss = 0.0507498
I0925 14:20:32.364917  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0507489 (* 1 = 0.0507489 loss)
I0925 14:20:32.364943  2600 sgd_solver.cpp:105] Iteration 77200, lr = 0.001
I0925 14:20:46.952791  2600 solver.cpp:218] Iteration 77300 (6.85502 iter/s, 14.5878s/100 iters), loss = 0.0370822
I0925 14:20:46.952826  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0370813 (* 1 = 0.0370813 loss)
I0925 14:20:46.952844  2600 sgd_solver.cpp:105] Iteration 77300, lr = 0.001
I0925 14:21:01.544000  2600 solver.cpp:218] Iteration 77400 (6.85347 iter/s, 14.5911s/100 iters), loss = 0.0368015
I0925 14:21:01.544033  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0368006 (* 1 = 0.0368006 loss)
I0925 14:21:01.544041  2600 sgd_solver.cpp:105] Iteration 77400, lr = 0.001
I0925 14:21:15.405261  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:21:15.989449  2600 solver.cpp:330] Iteration 77500, Testing net (#0)
I0925 14:21:19.412451  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:21:19.555657  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8723
I0925 14:21:19.555685  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.529475 (* 1 = 0.529475 loss)
I0925 14:21:19.700183  2600 solver.cpp:218] Iteration 77500 (5.50779 iter/s, 18.1561s/100 iters), loss = 0.0416633
I0925 14:21:19.700215  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0416624 (* 1 = 0.0416624 loss)
I0925 14:21:19.700225  2600 sgd_solver.cpp:105] Iteration 77500, lr = 0.001
I0925 14:21:34.295071  2600 solver.cpp:218] Iteration 77600 (6.85175 iter/s, 14.5948s/100 iters), loss = 0.0824165
I0925 14:21:34.295104  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0824157 (* 1 = 0.0824157 loss)
I0925 14:21:34.295114  2600 sgd_solver.cpp:105] Iteration 77600, lr = 0.001
I0925 14:21:48.884337  2600 solver.cpp:218] Iteration 77700 (6.85439 iter/s, 14.5892s/100 iters), loss = 0.0280622
I0925 14:21:48.884477  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0280613 (* 1 = 0.0280613 loss)
I0925 14:21:48.884510  2600 sgd_solver.cpp:105] Iteration 77700, lr = 0.001
I0925 14:22:03.481081  2600 solver.cpp:218] Iteration 77800 (6.85092 iter/s, 14.5966s/100 iters), loss = 0.056422
I0925 14:22:03.481113  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0564211 (* 1 = 0.0564211 loss)
I0925 14:22:03.481132  2600 sgd_solver.cpp:105] Iteration 77800, lr = 0.001
I0925 14:22:18.077402  2600 solver.cpp:218] Iteration 77900 (6.85107 iter/s, 14.5963s/100 iters), loss = 0.0948855
I0925 14:22:18.077435  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0948846 (* 1 = 0.0948846 loss)
I0925 14:22:18.077455  2600 sgd_solver.cpp:105] Iteration 77900, lr = 0.001
I0925 14:22:31.942399  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:22:32.525988  2600 solver.cpp:330] Iteration 78000, Testing net (#0)
I0925 14:22:35.949287  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:22:36.091989  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8738
I0925 14:22:36.092016  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.528663 (* 1 = 0.528663 loss)
I0925 14:22:36.236423  2600 solver.cpp:218] Iteration 78000 (5.50693 iter/s, 18.1589s/100 iters), loss = 0.0649037
I0925 14:22:36.236459  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0649029 (* 1 = 0.0649029 loss)
I0925 14:22:36.236469  2600 sgd_solver.cpp:105] Iteration 78000, lr = 0.001
I0925 14:22:50.825512  2600 solver.cpp:218] Iteration 78100 (6.85447 iter/s, 14.589s/100 iters), loss = 0.036793
I0925 14:22:50.825546  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0367921 (* 1 = 0.0367921 loss)
I0925 14:22:50.825556  2600 sgd_solver.cpp:105] Iteration 78100, lr = 0.001
I0925 14:23:05.417934  2600 solver.cpp:218] Iteration 78200 (6.8529 iter/s, 14.5924s/100 iters), loss = 0.0558049
I0925 14:23:05.418072  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.055804 (* 1 = 0.055804 loss)
I0925 14:23:05.418093  2600 sgd_solver.cpp:105] Iteration 78200, lr = 0.001
I0925 14:23:20.012516  2600 solver.cpp:218] Iteration 78300 (6.85194 iter/s, 14.5944s/100 iters), loss = 0.0550487
I0925 14:23:20.012550  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0550478 (* 1 = 0.0550478 loss)
I0925 14:23:20.012559  2600 sgd_solver.cpp:105] Iteration 78300, lr = 0.001
I0925 14:23:34.605408  2600 solver.cpp:218] Iteration 78400 (6.85268 iter/s, 14.5928s/100 iters), loss = 0.034895
I0925 14:23:34.605443  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0348941 (* 1 = 0.0348941 loss)
I0925 14:23:34.605461  2600 sgd_solver.cpp:105] Iteration 78400, lr = 0.001
I0925 14:23:48.469889  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:23:49.053750  2600 solver.cpp:330] Iteration 78500, Testing net (#0)
I0925 14:23:52.477926  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:23:52.620633  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8732
I0925 14:23:52.620661  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.5176 (* 1 = 0.5176 loss)
I0925 14:23:52.764489  2600 solver.cpp:218] Iteration 78500 (5.50691 iter/s, 18.159s/100 iters), loss = 0.042582
I0925 14:23:52.764521  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0425812 (* 1 = 0.0425812 loss)
I0925 14:23:52.764533  2600 sgd_solver.cpp:105] Iteration 78500, lr = 0.001
I0925 14:24:07.344408  2600 solver.cpp:218] Iteration 78600 (6.85878 iter/s, 14.5799s/100 iters), loss = 0.069117
I0925 14:24:07.344440  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0691162 (* 1 = 0.0691162 loss)
I0925 14:24:07.344449  2600 sgd_solver.cpp:105] Iteration 78600, lr = 0.001
I0925 14:24:21.926023  2600 solver.cpp:218] Iteration 78700 (6.85798 iter/s, 14.5815s/100 iters), loss = 0.112079
I0925 14:24:21.926179  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.112079 (* 1 = 0.112079 loss)
I0925 14:24:21.926218  2600 sgd_solver.cpp:105] Iteration 78700, lr = 0.001
I0925 14:24:36.503115  2600 solver.cpp:218] Iteration 78800 (6.86016 iter/s, 14.5769s/100 iters), loss = 0.0999596
I0925 14:24:36.503149  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0999588 (* 1 = 0.0999588 loss)
I0925 14:24:36.503167  2600 sgd_solver.cpp:105] Iteration 78800, lr = 0.001
I0925 14:24:51.083446  2600 solver.cpp:218] Iteration 78900 (6.85859 iter/s, 14.5803s/100 iters), loss = 0.0224059
I0925 14:24:51.083478  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0224051 (* 1 = 0.0224051 loss)
I0925 14:24:51.083500  2600 sgd_solver.cpp:105] Iteration 78900, lr = 0.001
I0925 14:25:04.937623  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:25:05.520813  2600 solver.cpp:330] Iteration 79000, Testing net (#0)
I0925 14:25:08.943699  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:25:09.086889  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8713
I0925 14:25:09.086917  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.542279 (* 1 = 0.542279 loss)
I0925 14:25:09.230347  2600 solver.cpp:218] Iteration 79000 (5.5106 iter/s, 18.1468s/100 iters), loss = 0.063249
I0925 14:25:09.230378  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0632482 (* 1 = 0.0632482 loss)
I0925 14:25:09.230388  2600 sgd_solver.cpp:105] Iteration 79000, lr = 0.001
I0925 14:25:23.824450  2600 solver.cpp:218] Iteration 79100 (6.85211 iter/s, 14.594s/100 iters), loss = 0.0313069
I0925 14:25:23.824483  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.031306 (* 1 = 0.031306 loss)
I0925 14:25:23.824493  2600 sgd_solver.cpp:105] Iteration 79100, lr = 0.001
I0925 14:25:38.413360  2600 solver.cpp:218] Iteration 79200 (6.85455 iter/s, 14.5888s/100 iters), loss = 0.0495877
I0925 14:25:38.413487  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0495868 (* 1 = 0.0495868 loss)
I0925 14:25:38.413499  2600 sgd_solver.cpp:105] Iteration 79200, lr = 0.001
I0925 14:25:53.003810  2600 solver.cpp:218] Iteration 79300 (6.85387 iter/s, 14.5903s/100 iters), loss = 0.0524082
I0925 14:25:53.003844  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0524074 (* 1 = 0.0524074 loss)
I0925 14:25:53.003862  2600 sgd_solver.cpp:105] Iteration 79300, lr = 0.001
I0925 14:26:07.596678  2600 solver.cpp:218] Iteration 79400 (6.85269 iter/s, 14.5928s/100 iters), loss = 0.036973
I0925 14:26:07.596711  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0369721 (* 1 = 0.0369721 loss)
I0925 14:26:07.596720  2600 sgd_solver.cpp:105] Iteration 79400, lr = 0.001
I0925 14:26:21.464192  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:26:22.048544  2600 solver.cpp:330] Iteration 79500, Testing net (#0)
I0925 14:26:25.472005  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:26:25.614622  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8725
I0925 14:26:25.614650  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.538212 (* 1 = 0.538212 loss)
I0925 14:26:25.759162  2600 solver.cpp:218] Iteration 79500 (5.50588 iter/s, 18.1624s/100 iters), loss = 0.0448839
I0925 14:26:25.759193  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.044883 (* 1 = 0.044883 loss)
I0925 14:26:25.759203  2600 sgd_solver.cpp:105] Iteration 79500, lr = 0.001
I0925 14:26:40.356565  2600 solver.cpp:218] Iteration 79600 (6.85056 iter/s, 14.5973s/100 iters), loss = 0.0375519
I0925 14:26:40.356596  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0375511 (* 1 = 0.0375511 loss)
I0925 14:26:40.356606  2600 sgd_solver.cpp:105] Iteration 79600, lr = 0.001
I0925 14:26:54.948788  2600 solver.cpp:218] Iteration 79700 (6.853 iter/s, 14.5922s/100 iters), loss = 0.085053
I0925 14:26:54.948900  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0850522 (* 1 = 0.0850522 loss)
I0925 14:26:54.948923  2600 sgd_solver.cpp:105] Iteration 79700, lr = 0.001
I0925 14:27:09.546916  2600 solver.cpp:218] Iteration 79800 (6.85026 iter/s, 14.598s/100 iters), loss = 0.126736
I0925 14:27:09.546950  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126735 (* 1 = 0.126735 loss)
I0925 14:27:09.546958  2600 sgd_solver.cpp:105] Iteration 79800, lr = 0.001
I0925 14:27:24.142678  2600 solver.cpp:218] Iteration 79900 (6.85134 iter/s, 14.5957s/100 iters), loss = 0.0231917
I0925 14:27:24.142711  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0231908 (* 1 = 0.0231908 loss)
I0925 14:27:24.142720  2600 sgd_solver.cpp:105] Iteration 79900, lr = 0.001
I0925 14:27:38.006574  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:27:38.591876  2600 solver.cpp:330] Iteration 80000, Testing net (#0)
I0925 14:27:42.016162  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:27:42.158944  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8735
I0925 14:27:42.158972  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.534678 (* 1 = 0.534678 loss)
I0925 14:27:42.303565  2600 solver.cpp:218] Iteration 80000 (5.50636 iter/s, 18.1608s/100 iters), loss = 0.0380318
I0925 14:27:42.303596  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.038031 (* 1 = 0.038031 loss)
I0925 14:27:42.303604  2600 sgd_solver.cpp:46] MultiStep Status: Iteration 80000, step = 3
I0925 14:27:42.303622  2600 sgd_solver.cpp:105] Iteration 80000, lr = 0.0001
I0925 14:27:56.899363  2600 solver.cpp:218] Iteration 80100 (6.85132 iter/s, 14.5957s/100 iters), loss = 0.0729385
I0925 14:27:56.899396  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0729377 (* 1 = 0.0729377 loss)
I0925 14:27:56.899415  2600 sgd_solver.cpp:105] Iteration 80100, lr = 0.0001
I0925 14:28:11.491981  2600 solver.cpp:218] Iteration 80200 (6.85281 iter/s, 14.5925s/100 iters), loss = 0.0324372
I0925 14:28:11.492115  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0324364 (* 1 = 0.0324364 loss)
I0925 14:28:11.492125  2600 sgd_solver.cpp:105] Iteration 80200, lr = 0.0001
I0925 14:28:26.086179  2600 solver.cpp:218] Iteration 80300 (6.85211 iter/s, 14.594s/100 iters), loss = 0.0539258
I0925 14:28:26.086212  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.053925 (* 1 = 0.053925 loss)
I0925 14:28:26.086231  2600 sgd_solver.cpp:105] Iteration 80300, lr = 0.0001
I0925 14:28:40.675580  2600 solver.cpp:218] Iteration 80400 (6.85432 iter/s, 14.5893s/100 iters), loss = 0.0468555
I0925 14:28:40.675612  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0468547 (* 1 = 0.0468547 loss)
I0925 14:28:40.675631  2600 sgd_solver.cpp:105] Iteration 80400, lr = 0.0001
I0925 14:28:54.542695  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:28:55.126324  2600 solver.cpp:330] Iteration 80500, Testing net (#0)
I0925 14:28:58.550817  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:28:58.693665  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8766
I0925 14:28:58.693692  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.526207 (* 1 = 0.526207 loss)
I0925 14:28:58.837599  2600 solver.cpp:218] Iteration 80500 (5.50602 iter/s, 18.1619s/100 iters), loss = 0.0765799
I0925 14:28:58.837630  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0765791 (* 1 = 0.0765791 loss)
I0925 14:28:58.837641  2600 sgd_solver.cpp:105] Iteration 80500, lr = 0.0001
I0925 14:29:13.429747  2600 solver.cpp:218] Iteration 80600 (6.85303 iter/s, 14.5921s/100 iters), loss = 0.0541337
I0925 14:29:13.429781  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0541329 (* 1 = 0.0541329 loss)
I0925 14:29:13.429800  2600 sgd_solver.cpp:105] Iteration 80600, lr = 0.0001
I0925 14:29:28.017621  2600 solver.cpp:218] Iteration 80700 (6.85504 iter/s, 14.5878s/100 iters), loss = 0.0453934
I0925 14:29:28.017768  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0453926 (* 1 = 0.0453926 loss)
I0925 14:29:28.017791  2600 sgd_solver.cpp:105] Iteration 80700, lr = 0.0001
I0925 14:29:42.610090  2600 solver.cpp:218] Iteration 80800 (6.85293 iter/s, 14.5923s/100 iters), loss = 0.0298422
I0925 14:29:42.610124  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0298414 (* 1 = 0.0298414 loss)
I0925 14:29:42.610133  2600 sgd_solver.cpp:105] Iteration 80800, lr = 0.0001
I0925 14:29:57.195672  2600 solver.cpp:218] Iteration 80900 (6.85612 iter/s, 14.5855s/100 iters), loss = 0.0499914
I0925 14:29:57.195704  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0499906 (* 1 = 0.0499906 loss)
I0925 14:29:57.195722  2600 sgd_solver.cpp:105] Iteration 80900, lr = 0.0001
I0925 14:30:11.054533  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:30:11.638964  2600 solver.cpp:330] Iteration 81000, Testing net (#0)
I0925 14:30:15.064041  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:30:15.207397  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8767
I0925 14:30:15.207425  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.524894 (* 1 = 0.524894 loss)
I0925 14:30:15.351382  2600 solver.cpp:218] Iteration 81000 (5.50793 iter/s, 18.1556s/100 iters), loss = 0.0981695
I0925 14:30:15.351410  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0981687 (* 1 = 0.0981687 loss)
I0925 14:30:15.351423  2600 sgd_solver.cpp:105] Iteration 81000, lr = 0.0001
I0925 14:30:29.940454  2600 solver.cpp:218] Iteration 81100 (6.85448 iter/s, 14.589s/100 iters), loss = 0.0549147
I0925 14:30:29.940487  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0549139 (* 1 = 0.0549139 loss)
I0925 14:30:29.940510  2600 sgd_solver.cpp:105] Iteration 81100, lr = 0.0001
I0925 14:30:44.525408  2600 solver.cpp:218] Iteration 81200 (6.85641 iter/s, 14.5849s/100 iters), loss = 0.0720321
I0925 14:30:44.525574  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0720313 (* 1 = 0.0720313 loss)
I0925 14:30:44.525586  2600 sgd_solver.cpp:105] Iteration 81200, lr = 0.0001
I0925 14:30:59.109570  2600 solver.cpp:218] Iteration 81300 (6.85685 iter/s, 14.584s/100 iters), loss = 0.0694143
I0925 14:30:59.109603  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0694135 (* 1 = 0.0694135 loss)
I0925 14:30:59.109622  2600 sgd_solver.cpp:105] Iteration 81300, lr = 0.0001
I0925 14:31:13.740772  2600 solver.cpp:218] Iteration 81400 (6.83474 iter/s, 14.6311s/100 iters), loss = 0.0478672
I0925 14:31:13.740805  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0478664 (* 1 = 0.0478664 loss)
I0925 14:31:13.740824  2600 sgd_solver.cpp:105] Iteration 81400, lr = 0.0001
I0925 14:31:27.604539  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:31:28.188534  2600 solver.cpp:330] Iteration 81500, Testing net (#0)
I0925 14:31:31.614488  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:31:31.757067  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8778
I0925 14:31:31.757103  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.525512 (* 1 = 0.525512 loss)
I0925 14:31:31.902607  2600 solver.cpp:218] Iteration 81500 (5.50607 iter/s, 18.1618s/100 iters), loss = 0.0355903
I0925 14:31:31.902639  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0355894 (* 1 = 0.0355894 loss)
I0925 14:31:31.902647  2600 sgd_solver.cpp:105] Iteration 81500, lr = 0.0001
I0925 14:31:46.552693  2600 solver.cpp:218] Iteration 81600 (6.82593 iter/s, 14.65s/100 iters), loss = 0.0429792
I0925 14:31:46.552734  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0429784 (* 1 = 0.0429784 loss)
I0925 14:31:46.552741  2600 sgd_solver.cpp:105] Iteration 81600, lr = 0.0001
I0925 14:32:01.222854  2600 solver.cpp:218] Iteration 81700 (6.81659 iter/s, 14.6701s/100 iters), loss = 0.0330331
I0925 14:32:01.222972  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0330323 (* 1 = 0.0330323 loss)
I0925 14:32:01.222981  2600 sgd_solver.cpp:105] Iteration 81700, lr = 0.0001
I0925 14:32:15.888557  2600 solver.cpp:218] Iteration 81800 (6.8187 iter/s, 14.6655s/100 iters), loss = 0.0292615
I0925 14:32:15.888587  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0292607 (* 1 = 0.0292607 loss)
I0925 14:32:15.888593  2600 sgd_solver.cpp:105] Iteration 81800, lr = 0.0001
I0925 14:32:30.488137  2600 solver.cpp:218] Iteration 81900 (6.84954 iter/s, 14.5995s/100 iters), loss = 0.0656062
I0925 14:32:30.488168  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0656054 (* 1 = 0.0656054 loss)
I0925 14:32:30.488176  2600 sgd_solver.cpp:105] Iteration 81900, lr = 0.0001
I0925 14:32:44.364358  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:32:44.948084  2600 solver.cpp:330] Iteration 82000, Testing net (#0)
I0925 14:32:48.375416  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:32:48.518172  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8756
I0925 14:32:48.518208  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.52731 (* 1 = 0.52731 loss)
I0925 14:32:48.663502  2600 solver.cpp:218] Iteration 82000 (5.50198 iter/s, 18.1753s/100 iters), loss = 0.0399048
I0925 14:32:48.663532  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.039904 (* 1 = 0.039904 loss)
I0925 14:32:48.663540  2600 sgd_solver.cpp:105] Iteration 82000, lr = 0.0001
I0925 14:33:03.257565  2600 solver.cpp:218] Iteration 82100 (6.85213 iter/s, 14.594s/100 iters), loss = 0.139783
I0925 14:33:03.257594  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139782 (* 1 = 0.139782 loss)
I0925 14:33:03.257601  2600 sgd_solver.cpp:105] Iteration 82100, lr = 0.0001
I0925 14:33:17.855691  2600 solver.cpp:218] Iteration 82200 (6.85023 iter/s, 14.5981s/100 iters), loss = 0.0576264
I0925 14:33:17.855826  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0576256 (* 1 = 0.0576256 loss)
I0925 14:33:17.855834  2600 sgd_solver.cpp:105] Iteration 82200, lr = 0.0001
I0925 14:33:32.456959  2600 solver.cpp:218] Iteration 82300 (6.8488 iter/s, 14.6011s/100 iters), loss = 0.0399365
I0925 14:33:32.456989  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0399357 (* 1 = 0.0399357 loss)
I0925 14:33:32.456995  2600 sgd_solver.cpp:105] Iteration 82300, lr = 0.0001
I0925 14:33:47.055991  2600 solver.cpp:218] Iteration 82400 (6.8498 iter/s, 14.599s/100 iters), loss = 0.0373278
I0925 14:33:47.056022  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0373269 (* 1 = 0.0373269 loss)
I0925 14:33:47.056031  2600 sgd_solver.cpp:105] Iteration 82400, lr = 0.0001
I0925 14:34:00.927600  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:34:01.512833  2600 solver.cpp:330] Iteration 82500, Testing net (#0)
I0925 14:34:04.939401  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:34:05.082559  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8775
I0925 14:34:05.082594  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.526138 (* 1 = 0.526138 loss)
I0925 14:34:05.227385  2600 solver.cpp:218] Iteration 82500 (5.50318 iter/s, 18.1713s/100 iters), loss = 0.0611602
I0925 14:34:05.227414  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0611593 (* 1 = 0.0611593 loss)
I0925 14:34:05.227422  2600 sgd_solver.cpp:105] Iteration 82500, lr = 0.0001
I0925 14:34:19.818642  2600 solver.cpp:218] Iteration 82600 (6.85345 iter/s, 14.5912s/100 iters), loss = 0.0262147
I0925 14:34:19.818684  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0262139 (* 1 = 0.0262139 loss)
I0925 14:34:19.818691  2600 sgd_solver.cpp:105] Iteration 82600, lr = 0.0001
I0925 14:34:34.416662  2600 solver.cpp:218] Iteration 82700 (6.85028 iter/s, 14.5979s/100 iters), loss = 0.0463521
I0925 14:34:34.416750  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0463513 (* 1 = 0.0463513 loss)
I0925 14:34:34.416766  2600 sgd_solver.cpp:105] Iteration 82700, lr = 0.0001
I0925 14:34:49.015837  2600 solver.cpp:218] Iteration 82800 (6.84976 iter/s, 14.5991s/100 iters), loss = 0.0525243
I0925 14:34:49.015877  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0525234 (* 1 = 0.0525234 loss)
I0925 14:34:49.015883  2600 sgd_solver.cpp:105] Iteration 82800, lr = 0.0001
I0925 14:35:03.616540  2600 solver.cpp:218] Iteration 82900 (6.84902 iter/s, 14.6006s/100 iters), loss = 0.0418286
I0925 14:35:03.616571  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0418278 (* 1 = 0.0418278 loss)
I0925 14:35:03.616577  2600 sgd_solver.cpp:105] Iteration 82900, lr = 0.0001
I0925 14:35:17.490351  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:35:18.074966  2600 solver.cpp:330] Iteration 83000, Testing net (#0)
I0925 14:35:21.502542  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:35:21.645068  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8781
I0925 14:35:21.645104  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.527476 (* 1 = 0.527476 loss)
I0925 14:35:21.790117  2600 solver.cpp:218] Iteration 83000 (5.50252 iter/s, 18.1735s/100 iters), loss = 0.0399342
I0925 14:35:21.790149  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0399334 (* 1 = 0.0399334 loss)
I0925 14:35:21.790158  2600 sgd_solver.cpp:105] Iteration 83000, lr = 0.0001
I0925 14:35:36.381356  2600 solver.cpp:218] Iteration 83100 (6.85346 iter/s, 14.5912s/100 iters), loss = 0.0233106
I0925 14:35:36.381386  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0233098 (* 1 = 0.0233098 loss)
I0925 14:35:36.381393  2600 sgd_solver.cpp:105] Iteration 83100, lr = 0.0001
I0925 14:35:50.983258  2600 solver.cpp:218] Iteration 83200 (6.84846 iter/s, 14.6018s/100 iters), loss = 0.0893334
I0925 14:35:50.983409  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0893326 (* 1 = 0.0893326 loss)
I0925 14:35:50.983418  2600 sgd_solver.cpp:105] Iteration 83200, lr = 0.0001
I0925 14:36:05.583735  2600 solver.cpp:218] Iteration 83300 (6.84918 iter/s, 14.6003s/100 iters), loss = 0.042115
I0925 14:36:05.583766  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0421142 (* 1 = 0.0421142 loss)
I0925 14:36:05.583772  2600 sgd_solver.cpp:105] Iteration 83300, lr = 0.0001
I0925 14:36:20.183280  2600 solver.cpp:218] Iteration 83400 (6.84956 iter/s, 14.5995s/100 iters), loss = 0.0398513
I0925 14:36:20.183310  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0398505 (* 1 = 0.0398505 loss)
I0925 14:36:20.183316  2600 sgd_solver.cpp:105] Iteration 83400, lr = 0.0001
I0925 14:36:34.058917  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:36:34.642655  2600 solver.cpp:330] Iteration 83500, Testing net (#0)
I0925 14:36:38.069725  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:36:38.213418  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8781
I0925 14:36:38.213451  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.527528 (* 1 = 0.527528 loss)
I0925 14:36:38.358007  2600 solver.cpp:218] Iteration 83500 (5.50217 iter/s, 18.1747s/100 iters), loss = 0.0584039
I0925 14:36:38.358036  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0584031 (* 1 = 0.0584031 loss)
I0925 14:36:38.358043  2600 sgd_solver.cpp:105] Iteration 83500, lr = 0.0001
I0925 14:36:52.936324  2600 solver.cpp:218] Iteration 83600 (6.85953 iter/s, 14.5783s/100 iters), loss = 0.0541278
I0925 14:36:52.936362  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.054127 (* 1 = 0.054127 loss)
I0925 14:36:52.936369  2600 sgd_solver.cpp:105] Iteration 83600, lr = 0.0001
I0925 14:37:07.525924  2600 solver.cpp:218] Iteration 83700 (6.85423 iter/s, 14.5895s/100 iters), loss = 0.0317423
I0925 14:37:07.526072  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0317415 (* 1 = 0.0317415 loss)
I0925 14:37:07.526080  2600 sgd_solver.cpp:105] Iteration 83700, lr = 0.0001
I0925 14:37:22.115926  2600 solver.cpp:218] Iteration 83800 (6.85409 iter/s, 14.5898s/100 iters), loss = 0.0443392
I0925 14:37:22.115968  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0443384 (* 1 = 0.0443384 loss)
I0925 14:37:22.115974  2600 sgd_solver.cpp:105] Iteration 83800, lr = 0.0001
I0925 14:37:36.714184  2600 solver.cpp:218] Iteration 83900 (6.85017 iter/s, 14.5982s/100 iters), loss = 0.0489975
I0925 14:37:36.714224  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0489967 (* 1 = 0.0489967 loss)
I0925 14:37:36.714231  2600 sgd_solver.cpp:105] Iteration 83900, lr = 0.0001
I0925 14:37:50.590903  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:37:51.175204  2600 solver.cpp:330] Iteration 84000, Testing net (#0)
I0925 14:37:54.601774  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:37:54.744855  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8793
I0925 14:37:54.744881  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.527387 (* 1 = 0.527387 loss)
I0925 14:37:54.889999  2600 solver.cpp:218] Iteration 84000 (5.50184 iter/s, 18.1757s/100 iters), loss = 0.0617659
I0925 14:37:54.890030  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0617651 (* 1 = 0.0617651 loss)
I0925 14:37:54.890038  2600 sgd_solver.cpp:105] Iteration 84000, lr = 0.0001
I0925 14:38:09.483136  2600 solver.cpp:218] Iteration 84100 (6.85257 iter/s, 14.5931s/100 iters), loss = 0.0692817
I0925 14:38:09.483167  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0692809 (* 1 = 0.0692809 loss)
I0925 14:38:09.483173  2600 sgd_solver.cpp:105] Iteration 84100, lr = 0.0001
I0925 14:38:24.080468  2600 solver.cpp:218] Iteration 84200 (6.8506 iter/s, 14.5973s/100 iters), loss = 0.0709151
I0925 14:38:24.080580  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0709142 (* 1 = 0.0709142 loss)
I0925 14:38:24.080596  2600 sgd_solver.cpp:105] Iteration 84200, lr = 0.0001
I0925 14:38:38.682011  2600 solver.cpp:218] Iteration 84300 (6.84866 iter/s, 14.6014s/100 iters), loss = 0.0430257
I0925 14:38:38.682042  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0430249 (* 1 = 0.0430249 loss)
I0925 14:38:38.682049  2600 sgd_solver.cpp:105] Iteration 84300, lr = 0.0001
I0925 14:38:53.281522  2600 solver.cpp:218] Iteration 84400 (6.84958 iter/s, 14.5994s/100 iters), loss = 0.0186752
I0925 14:38:53.281550  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0186744 (* 1 = 0.0186744 loss)
I0925 14:38:53.281558  2600 sgd_solver.cpp:105] Iteration 84400, lr = 0.0001
I0925 14:39:07.155059  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:39:07.739159  2600 solver.cpp:330] Iteration 84500, Testing net (#0)
I0925 14:39:11.164463  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:39:11.307355  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8786
I0925 14:39:11.307390  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.525248 (* 1 = 0.525248 loss)
I0925 14:39:11.452069  2600 solver.cpp:218] Iteration 84500 (5.50343 iter/s, 18.1705s/100 iters), loss = 0.0365458
I0925 14:39:11.452113  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0365449 (* 1 = 0.0365449 loss)
I0925 14:39:11.452121  2600 sgd_solver.cpp:105] Iteration 84500, lr = 0.0001
I0925 14:39:26.045713  2600 solver.cpp:218] Iteration 84600 (6.85234 iter/s, 14.5936s/100 iters), loss = 0.0427321
I0925 14:39:26.045754  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0427313 (* 1 = 0.0427313 loss)
I0925 14:39:26.045760  2600 sgd_solver.cpp:105] Iteration 84600, lr = 0.0001
I0925 14:39:40.644932  2600 solver.cpp:218] Iteration 84700 (6.84972 iter/s, 14.5991s/100 iters), loss = 0.0466409
I0925 14:39:40.645040  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.04664 (* 1 = 0.04664 loss)
I0925 14:39:40.645059  2600 sgd_solver.cpp:105] Iteration 84700, lr = 0.0001
I0925 14:39:55.245651  2600 solver.cpp:218] Iteration 84800 (6.84905 iter/s, 14.6006s/100 iters), loss = 0.0949623
I0925 14:39:55.245692  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0949614 (* 1 = 0.0949614 loss)
I0925 14:39:55.245698  2600 sgd_solver.cpp:105] Iteration 84800, lr = 0.0001
I0925 14:40:09.844305  2600 solver.cpp:218] Iteration 84900 (6.84998 iter/s, 14.5986s/100 iters), loss = 0.0552295
I0925 14:40:09.844334  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0552286 (* 1 = 0.0552286 loss)
I0925 14:40:09.844341  2600 sgd_solver.cpp:105] Iteration 84900, lr = 0.0001
I0925 14:40:23.718286  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:40:24.302173  2600 solver.cpp:330] Iteration 85000, Testing net (#0)
I0925 14:40:27.729197  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:40:27.872366  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8779
I0925 14:40:27.872401  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.528535 (* 1 = 0.528535 loss)
I0925 14:40:28.017269  2600 solver.cpp:218] Iteration 85000 (5.5027 iter/s, 18.1729s/100 iters), loss = 0.0309304
I0925 14:40:28.017298  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0309295 (* 1 = 0.0309295 loss)
I0925 14:40:28.017307  2600 sgd_solver.cpp:105] Iteration 85000, lr = 0.0001
I0925 14:40:42.617275  2600 solver.cpp:218] Iteration 85100 (6.84934 iter/s, 14.5999s/100 iters), loss = 0.0387308
I0925 14:40:42.617306  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0387299 (* 1 = 0.0387299 loss)
I0925 14:40:42.617312  2600 sgd_solver.cpp:105] Iteration 85100, lr = 0.0001
I0925 14:40:57.217941  2600 solver.cpp:218] Iteration 85200 (6.84903 iter/s, 14.6006s/100 iters), loss = 0.050936
I0925 14:40:57.218044  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0509351 (* 1 = 0.0509351 loss)
I0925 14:40:57.218051  2600 sgd_solver.cpp:105] Iteration 85200, lr = 0.0001
I0925 14:41:11.824822  2600 solver.cpp:218] Iteration 85300 (6.84615 iter/s, 14.6067s/100 iters), loss = 0.0695299
I0925 14:41:11.824853  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.069529 (* 1 = 0.069529 loss)
I0925 14:41:11.824859  2600 sgd_solver.cpp:105] Iteration 85300, lr = 0.0001
I0925 14:41:26.429570  2600 solver.cpp:218] Iteration 85400 (6.84712 iter/s, 14.6047s/100 iters), loss = 0.0794805
I0925 14:41:26.429611  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0794796 (* 1 = 0.0794796 loss)
I0925 14:41:26.429620  2600 sgd_solver.cpp:105] Iteration 85400, lr = 0.0001
I0925 14:41:40.308490  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:41:40.892814  2600 solver.cpp:330] Iteration 85500, Testing net (#0)
I0925 14:41:44.319015  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:41:44.462157  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8784
I0925 14:41:44.462183  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.527952 (* 1 = 0.527952 loss)
I0925 14:41:44.606567  2600 solver.cpp:218] Iteration 85500 (5.50148 iter/s, 18.1769s/100 iters), loss = 0.0300931
I0925 14:41:44.606598  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0300922 (* 1 = 0.0300922 loss)
I0925 14:41:44.606606  2600 sgd_solver.cpp:105] Iteration 85500, lr = 0.0001
I0925 14:41:59.199151  2600 solver.cpp:218] Iteration 85600 (6.85283 iter/s, 14.5925s/100 iters), loss = 0.0509111
I0925 14:41:59.199180  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0509102 (* 1 = 0.0509102 loss)
I0925 14:41:59.199187  2600 sgd_solver.cpp:105] Iteration 85600, lr = 0.0001
I0925 14:42:13.790329  2600 solver.cpp:218] Iteration 85700 (6.85349 iter/s, 14.5911s/100 iters), loss = 0.0491697
I0925 14:42:13.790443  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0491688 (* 1 = 0.0491688 loss)
I0925 14:42:13.790451  2600 sgd_solver.cpp:105] Iteration 85700, lr = 0.0001
I0925 14:42:28.385818  2600 solver.cpp:218] Iteration 85800 (6.8515 iter/s, 14.5954s/100 iters), loss = 0.0622651
I0925 14:42:28.385859  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0622643 (* 1 = 0.0622643 loss)
I0925 14:42:28.385866  2600 sgd_solver.cpp:105] Iteration 85800, lr = 0.0001
I0925 14:42:42.981956  2600 solver.cpp:218] Iteration 85900 (6.85116 iter/s, 14.5961s/100 iters), loss = 0.0378659
I0925 14:42:42.981997  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0378651 (* 1 = 0.0378651 loss)
I0925 14:42:42.982004  2600 sgd_solver.cpp:105] Iteration 85900, lr = 0.0001
I0925 14:42:56.854544  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:42:57.437947  2600 solver.cpp:330] Iteration 86000, Testing net (#0)
I0925 14:43:00.865353  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:43:01.008493  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8773
I0925 14:43:01.008522  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.528067 (* 1 = 0.528067 loss)
I0925 14:43:01.153568  2600 solver.cpp:218] Iteration 86000 (5.50312 iter/s, 18.1715s/100 iters), loss = 0.0490983
I0925 14:43:01.153599  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0490975 (* 1 = 0.0490975 loss)
I0925 14:43:01.153605  2600 sgd_solver.cpp:105] Iteration 86000, lr = 0.0001
I0925 14:43:15.757926  2600 solver.cpp:218] Iteration 86100 (6.8473 iter/s, 14.6043s/100 iters), loss = 0.0476052
I0925 14:43:15.757967  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0476044 (* 1 = 0.0476044 loss)
I0925 14:43:15.757974  2600 sgd_solver.cpp:105] Iteration 86100, lr = 0.0001
I0925 14:43:30.366547  2600 solver.cpp:218] Iteration 86200 (6.84531 iter/s, 14.6085s/100 iters), loss = 0.0747106
I0925 14:43:30.366679  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0747098 (* 1 = 0.0747098 loss)
I0925 14:43:30.366688  2600 sgd_solver.cpp:105] Iteration 86200, lr = 0.0001
I0925 14:43:44.962535  2600 solver.cpp:218] Iteration 86300 (6.85128 iter/s, 14.5958s/100 iters), loss = 0.0648512
I0925 14:43:44.962576  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0648504 (* 1 = 0.0648504 loss)
I0925 14:43:44.962584  2600 sgd_solver.cpp:105] Iteration 86300, lr = 0.0001
I0925 14:43:59.564750  2600 solver.cpp:218] Iteration 86400 (6.84831 iter/s, 14.6021s/100 iters), loss = 0.0525242
I0925 14:43:59.564791  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0525233 (* 1 = 0.0525233 loss)
I0925 14:43:59.564798  2600 sgd_solver.cpp:105] Iteration 86400, lr = 0.0001
I0925 14:44:13.440924  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:44:14.025604  2600 solver.cpp:330] Iteration 86500, Testing net (#0)
I0925 14:44:17.452275  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:44:17.595237  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8777
I0925 14:44:17.595264  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.528563 (* 1 = 0.528563 loss)
I0925 14:44:17.740298  2600 solver.cpp:218] Iteration 86500 (5.50192 iter/s, 18.1755s/100 iters), loss = 0.0298664
I0925 14:44:17.740329  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0298656 (* 1 = 0.0298656 loss)
I0925 14:44:17.740336  2600 sgd_solver.cpp:105] Iteration 86500, lr = 0.0001
I0925 14:44:32.334825  2600 solver.cpp:218] Iteration 86600 (6.85192 iter/s, 14.5945s/100 iters), loss = 0.0342547
I0925 14:44:32.334856  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0342539 (* 1 = 0.0342539 loss)
I0925 14:44:32.334863  2600 sgd_solver.cpp:105] Iteration 86600, lr = 0.0001
I0925 14:44:46.934734  2600 solver.cpp:218] Iteration 86700 (6.84939 iter/s, 14.5998s/100 iters), loss = 0.0448518
I0925 14:44:46.934882  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.044851 (* 1 = 0.044851 loss)
I0925 14:44:46.934891  2600 sgd_solver.cpp:105] Iteration 86700, lr = 0.0001
I0925 14:45:01.536675  2600 solver.cpp:218] Iteration 86800 (6.84849 iter/s, 14.6018s/100 iters), loss = 0.0145286
I0925 14:45:01.536706  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0145278 (* 1 = 0.0145278 loss)
I0925 14:45:01.536712  2600 sgd_solver.cpp:105] Iteration 86800, lr = 0.0001
I0925 14:45:16.137428  2600 solver.cpp:218] Iteration 86900 (6.84899 iter/s, 14.6007s/100 iters), loss = 0.0601056
I0925 14:45:16.137459  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0601047 (* 1 = 0.0601047 loss)
I0925 14:45:16.137465  2600 sgd_solver.cpp:105] Iteration 86900, lr = 0.0001
I0925 14:45:30.014621  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:45:30.598057  2600 solver.cpp:330] Iteration 87000, Testing net (#0)
I0925 14:45:34.023833  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:45:34.166811  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8771
I0925 14:45:34.166847  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.528907 (* 1 = 0.528907 loss)
I0925 14:45:34.311799  2600 solver.cpp:218] Iteration 87000 (5.50228 iter/s, 18.1743s/100 iters), loss = 0.0751864
I0925 14:45:34.311826  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0751856 (* 1 = 0.0751856 loss)
I0925 14:45:34.311833  2600 sgd_solver.cpp:105] Iteration 87000, lr = 0.0001
I0925 14:45:48.909772  2600 solver.cpp:218] Iteration 87100 (6.8503 iter/s, 14.5979s/100 iters), loss = 0.0332912
I0925 14:45:48.909802  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0332903 (* 1 = 0.0332903 loss)
I0925 14:45:48.909809  2600 sgd_solver.cpp:105] Iteration 87100, lr = 0.0001
I0925 14:46:03.514438  2600 solver.cpp:218] Iteration 87200 (6.84716 iter/s, 14.6046s/100 iters), loss = 0.0872296
I0925 14:46:03.514569  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0872288 (* 1 = 0.0872288 loss)
I0925 14:46:03.514587  2600 sgd_solver.cpp:105] Iteration 87200, lr = 0.0001
I0925 14:46:18.114116  2600 solver.cpp:218] Iteration 87300 (6.84954 iter/s, 14.5995s/100 iters), loss = 0.0471111
I0925 14:46:18.114147  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0471103 (* 1 = 0.0471103 loss)
I0925 14:46:18.114154  2600 sgd_solver.cpp:105] Iteration 87300, lr = 0.0001
I0925 14:46:32.716245  2600 solver.cpp:218] Iteration 87400 (6.84835 iter/s, 14.6021s/100 iters), loss = 0.0577848
I0925 14:46:32.716276  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.057784 (* 1 = 0.057784 loss)
I0925 14:46:32.716284  2600 sgd_solver.cpp:105] Iteration 87400, lr = 0.0001
I0925 14:46:46.589401  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:46:47.173132  2600 solver.cpp:330] Iteration 87500, Testing net (#0)
I0925 14:46:50.598855  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:46:50.741488  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8773
I0925 14:46:50.741511  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.531064 (* 1 = 0.531064 loss)
I0925 14:46:50.885747  2600 solver.cpp:218] Iteration 87500 (5.50375 iter/s, 18.1694s/100 iters), loss = 0.073161
I0925 14:46:50.885777  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0731602 (* 1 = 0.0731602 loss)
I0925 14:46:50.885785  2600 sgd_solver.cpp:105] Iteration 87500, lr = 0.0001
I0925 14:47:05.490424  2600 solver.cpp:218] Iteration 87600 (6.84715 iter/s, 14.6046s/100 iters), loss = 0.0424185
I0925 14:47:05.490455  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0424177 (* 1 = 0.0424177 loss)
I0925 14:47:05.490461  2600 sgd_solver.cpp:105] Iteration 87600, lr = 0.0001
I0925 14:47:20.096245  2600 solver.cpp:218] Iteration 87700 (6.84662 iter/s, 14.6058s/100 iters), loss = 0.049086
I0925 14:47:20.096354  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0490853 (* 1 = 0.0490853 loss)
I0925 14:47:20.096369  2600 sgd_solver.cpp:105] Iteration 87700, lr = 0.0001
I0925 14:47:34.700865  2600 solver.cpp:218] Iteration 87800 (6.84722 iter/s, 14.6045s/100 iters), loss = 0.0747781
I0925 14:47:34.700906  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0747773 (* 1 = 0.0747773 loss)
I0925 14:47:34.700912  2600 sgd_solver.cpp:105] Iteration 87800, lr = 0.0001
I0925 14:47:49.306041  2600 solver.cpp:218] Iteration 87900 (6.84692 iter/s, 14.6051s/100 iters), loss = 0.0343529
I0925 14:47:49.306071  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0343521 (* 1 = 0.0343521 loss)
I0925 14:47:49.306077  2600 sgd_solver.cpp:105] Iteration 87900, lr = 0.0001
I0925 14:48:03.182859  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:48:03.766917  2600 solver.cpp:330] Iteration 88000, Testing net (#0)
I0925 14:48:07.194178  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:48:07.337085  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8775
I0925 14:48:07.337121  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.530299 (* 1 = 0.530299 loss)
I0925 14:48:07.482172  2600 solver.cpp:218] Iteration 88000 (5.50174 iter/s, 18.1761s/100 iters), loss = 0.0319137
I0925 14:48:07.482205  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0319129 (* 1 = 0.0319129 loss)
I0925 14:48:07.482213  2600 sgd_solver.cpp:105] Iteration 88000, lr = 0.0001
I0925 14:48:22.071396  2600 solver.cpp:218] Iteration 88100 (6.85441 iter/s, 14.5892s/100 iters), loss = 0.0814821
I0925 14:48:22.071439  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0814813 (* 1 = 0.0814813 loss)
I0925 14:48:22.071445  2600 sgd_solver.cpp:105] Iteration 88100, lr = 0.0001
I0925 14:48:36.665954  2600 solver.cpp:218] Iteration 88200 (6.85191 iter/s, 14.5945s/100 iters), loss = 0.0430074
I0925 14:48:36.666117  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0430066 (* 1 = 0.0430066 loss)
I0925 14:48:36.666126  2600 sgd_solver.cpp:105] Iteration 88200, lr = 0.0001
I0925 14:48:51.260551  2600 solver.cpp:218] Iteration 88300 (6.85194 iter/s, 14.5944s/100 iters), loss = 0.0511528
I0925 14:48:51.260581  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0511521 (* 1 = 0.0511521 loss)
I0925 14:48:51.260588  2600 sgd_solver.cpp:105] Iteration 88300, lr = 0.0001
I0925 14:49:05.860668  2600 solver.cpp:218] Iteration 88400 (6.84929 iter/s, 14.6001s/100 iters), loss = 0.0328615
I0925 14:49:05.860697  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0328607 (* 1 = 0.0328607 loss)
I0925 14:49:05.860703  2600 sgd_solver.cpp:105] Iteration 88400, lr = 0.0001
I0925 14:49:19.732803  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:49:20.317137  2600 solver.cpp:330] Iteration 88500, Testing net (#0)
I0925 14:49:23.743453  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:49:23.886034  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8779
I0925 14:49:23.886070  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.531092 (* 1 = 0.531092 loss)
I0925 14:49:24.031245  2600 solver.cpp:218] Iteration 88500 (5.50342 iter/s, 18.1705s/100 iters), loss = 0.0531533
I0925 14:49:24.031277  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0531525 (* 1 = 0.0531525 loss)
I0925 14:49:24.031285  2600 sgd_solver.cpp:105] Iteration 88500, lr = 0.0001
I0925 14:49:38.619509  2600 solver.cpp:218] Iteration 88600 (6.85486 iter/s, 14.5882s/100 iters), loss = 0.0708325
I0925 14:49:38.619540  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0708318 (* 1 = 0.0708318 loss)
I0925 14:49:38.619547  2600 sgd_solver.cpp:105] Iteration 88600, lr = 0.0001
I0925 14:49:53.211443  2600 solver.cpp:218] Iteration 88700 (6.85313 iter/s, 14.5919s/100 iters), loss = 0.0996858
I0925 14:49:53.211554  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0996851 (* 1 = 0.0996851 loss)
I0925 14:49:53.211571  2600 sgd_solver.cpp:105] Iteration 88700, lr = 0.0001
I0925 14:50:07.808867  2600 solver.cpp:218] Iteration 88800 (6.85059 iter/s, 14.5973s/100 iters), loss = 0.0552808
I0925 14:50:07.808898  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0552801 (* 1 = 0.0552801 loss)
I0925 14:50:07.808904  2600 sgd_solver.cpp:105] Iteration 88800, lr = 0.0001
I0925 14:50:22.409471  2600 solver.cpp:218] Iteration 88900 (6.84906 iter/s, 14.6005s/100 iters), loss = 0.0187033
I0925 14:50:22.409512  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0187026 (* 1 = 0.0187026 loss)
I0925 14:50:22.409519  2600 sgd_solver.cpp:105] Iteration 88900, lr = 0.0001
I0925 14:50:36.283490  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:50:36.867166  2600 solver.cpp:330] Iteration 89000, Testing net (#0)
I0925 14:50:40.292336  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:50:40.435503  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8773
I0925 14:50:40.435539  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.53072 (* 1 = 0.53072 loss)
I0925 14:50:40.580451  2600 solver.cpp:218] Iteration 89000 (5.50331 iter/s, 18.1709s/100 iters), loss = 0.0295754
I0925 14:50:40.580480  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0295747 (* 1 = 0.0295747 loss)
I0925 14:50:40.580487  2600 sgd_solver.cpp:105] Iteration 89000, lr = 0.0001
I0925 14:50:55.169796  2600 solver.cpp:218] Iteration 89100 (6.85435 iter/s, 14.5893s/100 iters), loss = 0.0927051
I0925 14:50:55.169827  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0927044 (* 1 = 0.0927044 loss)
I0925 14:50:55.169833  2600 sgd_solver.cpp:105] Iteration 89100, lr = 0.0001
I0925 14:51:09.764575  2600 solver.cpp:218] Iteration 89200 (6.8518 iter/s, 14.5947s/100 iters), loss = 0.0229529
I0925 14:51:09.764710  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0229522 (* 1 = 0.0229522 loss)
I0925 14:51:09.764729  2600 sgd_solver.cpp:105] Iteration 89200, lr = 0.0001
I0925 14:51:24.362526  2600 solver.cpp:218] Iteration 89300 (6.85035 iter/s, 14.5978s/100 iters), loss = 0.125433
I0925 14:51:24.362567  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125433 (* 1 = 0.125433 loss)
I0925 14:51:24.362573  2600 sgd_solver.cpp:105] Iteration 89300, lr = 0.0001
I0925 14:51:38.960870  2600 solver.cpp:218] Iteration 89400 (6.85013 iter/s, 14.5983s/100 iters), loss = 0.0838597
I0925 14:51:38.960911  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.083859 (* 1 = 0.083859 loss)
I0925 14:51:38.960917  2600 sgd_solver.cpp:105] Iteration 89400, lr = 0.0001
I0925 14:51:52.833142  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:51:53.416900  2600 solver.cpp:330] Iteration 89500, Testing net (#0)
I0925 14:51:56.842067  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:51:56.985252  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8768
I0925 14:51:56.985288  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.533355 (* 1 = 0.533355 loss)
I0925 14:51:57.129976  2600 solver.cpp:218] Iteration 89500 (5.50387 iter/s, 18.169s/100 iters), loss = 0.0196012
I0925 14:51:57.130005  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0196005 (* 1 = 0.0196005 loss)
I0925 14:51:57.130012  2600 sgd_solver.cpp:105] Iteration 89500, lr = 0.0001
I0925 14:52:11.734426  2600 solver.cpp:218] Iteration 89600 (6.84726 iter/s, 14.6044s/100 iters), loss = 0.0635838
I0925 14:52:11.734455  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0635831 (* 1 = 0.0635831 loss)
I0925 14:52:11.734462  2600 sgd_solver.cpp:105] Iteration 89600, lr = 0.0001
I0925 14:52:26.343284  2600 solver.cpp:218] Iteration 89700 (6.84519 iter/s, 14.6088s/100 iters), loss = 0.0870427
I0925 14:52:26.343389  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.087042 (* 1 = 0.087042 loss)
I0925 14:52:26.343407  2600 sgd_solver.cpp:105] Iteration 89700, lr = 0.0001
I0925 14:52:40.948114  2600 solver.cpp:218] Iteration 89800 (6.84712 iter/s, 14.6047s/100 iters), loss = 0.0671034
I0925 14:52:40.948155  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0671027 (* 1 = 0.0671027 loss)
I0925 14:52:40.948161  2600 sgd_solver.cpp:105] Iteration 89800, lr = 0.0001
I0925 14:52:55.551203  2600 solver.cpp:218] Iteration 89900 (6.8479 iter/s, 14.603s/100 iters), loss = 0.0327724
I0925 14:52:55.551234  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0327717 (* 1 = 0.0327717 loss)
I0925 14:52:55.551240  2600 sgd_solver.cpp:105] Iteration 89900, lr = 0.0001
I0925 14:53:09.428426  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:53:10.012454  2600 solver.cpp:330] Iteration 90000, Testing net (#0)
I0925 14:53:13.438091  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:53:13.581326  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8776
I0925 14:53:13.581362  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.533161 (* 1 = 0.533161 loss)
I0925 14:53:13.726241  2600 solver.cpp:218] Iteration 90000 (5.50207 iter/s, 18.175s/100 iters), loss = 0.0745393
I0925 14:53:13.726270  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0745386 (* 1 = 0.0745386 loss)
I0925 14:53:13.726277  2600 sgd_solver.cpp:105] Iteration 90000, lr = 0.0001
I0925 14:53:28.327661  2600 solver.cpp:218] Iteration 90100 (6.84868 iter/s, 14.6014s/100 iters), loss = 0.0934393
I0925 14:53:28.327692  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0934386 (* 1 = 0.0934386 loss)
I0925 14:53:28.327697  2600 sgd_solver.cpp:105] Iteration 90100, lr = 0.0001
I0925 14:53:42.930601  2600 solver.cpp:218] Iteration 90200 (6.84797 iter/s, 14.6029s/100 iters), loss = 0.0433435
I0925 14:53:42.930768  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0433428 (* 1 = 0.0433428 loss)
I0925 14:53:42.930778  2600 sgd_solver.cpp:105] Iteration 90200, lr = 0.0001
I0925 14:53:57.540868  2600 solver.cpp:218] Iteration 90300 (6.8446 iter/s, 14.6101s/100 iters), loss = 0.0644714
I0925 14:53:57.540899  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0644707 (* 1 = 0.0644707 loss)
I0925 14:53:57.540905  2600 sgd_solver.cpp:105] Iteration 90300, lr = 0.0001
I0925 14:54:12.153743  2600 solver.cpp:218] Iteration 90400 (6.84331 iter/s, 14.6128s/100 iters), loss = 0.022967
I0925 14:54:12.153784  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0229663 (* 1 = 0.0229663 loss)
I0925 14:54:12.153790  2600 sgd_solver.cpp:105] Iteration 90400, lr = 0.0001
I0925 14:54:26.030084  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:54:26.614043  2600 solver.cpp:330] Iteration 90500, Testing net (#0)
I0925 14:54:30.040534  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:54:30.183326  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8772
I0925 14:54:30.183362  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535147 (* 1 = 0.535147 loss)
I0925 14:54:30.328251  2600 solver.cpp:218] Iteration 90500 (5.50224 iter/s, 18.1744s/100 iters), loss = 0.0210732
I0925 14:54:30.328281  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0210726 (* 1 = 0.0210726 loss)
I0925 14:54:30.328289  2600 sgd_solver.cpp:105] Iteration 90500, lr = 0.0001
I0925 14:54:44.915681  2600 solver.cpp:218] Iteration 90600 (6.85525 iter/s, 14.5874s/100 iters), loss = 0.0181276
I0925 14:54:44.915710  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.018127 (* 1 = 0.018127 loss)
I0925 14:54:44.915716  2600 sgd_solver.cpp:105] Iteration 90600, lr = 0.0001
I0925 14:54:59.504386  2600 solver.cpp:218] Iteration 90700 (6.85465 iter/s, 14.5886s/100 iters), loss = 0.0616068
I0925 14:54:59.504487  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0616062 (* 1 = 0.0616062 loss)
I0925 14:54:59.504506  2600 sgd_solver.cpp:105] Iteration 90700, lr = 0.0001
I0925 14:55:14.098548  2600 solver.cpp:218] Iteration 90800 (6.85212 iter/s, 14.594s/100 iters), loss = 0.0256995
I0925 14:55:14.098577  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0256988 (* 1 = 0.0256988 loss)
I0925 14:55:14.098582  2600 sgd_solver.cpp:105] Iteration 90800, lr = 0.0001
I0925 14:55:28.687893  2600 solver.cpp:218] Iteration 90900 (6.85435 iter/s, 14.5893s/100 iters), loss = 0.0297127
I0925 14:55:28.687934  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0297121 (* 1 = 0.0297121 loss)
I0925 14:55:28.687942  2600 sgd_solver.cpp:105] Iteration 90900, lr = 0.0001
I0925 14:55:42.557329  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:55:43.141839  2600 solver.cpp:330] Iteration 91000, Testing net (#0)
I0925 14:55:46.568636  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:55:46.710860  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8773
I0925 14:55:46.710896  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.536175 (* 1 = 0.536175 loss)
I0925 14:55:46.855432  2600 solver.cpp:218] Iteration 91000 (5.50435 iter/s, 18.1675s/100 iters), loss = 0.0413234
I0925 14:55:46.855463  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0413228 (* 1 = 0.0413228 loss)
I0925 14:55:46.855471  2600 sgd_solver.cpp:105] Iteration 91000, lr = 0.0001
I0925 14:56:01.454524  2600 solver.cpp:218] Iteration 91100 (6.84977 iter/s, 14.599s/100 iters), loss = 0.0351232
I0925 14:56:01.454565  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0351226 (* 1 = 0.0351226 loss)
I0925 14:56:01.454571  2600 sgd_solver.cpp:105] Iteration 91100, lr = 0.0001
I0925 14:56:16.058851  2600 solver.cpp:218] Iteration 91200 (6.84732 iter/s, 14.6042s/100 iters), loss = 0.0462746
I0925 14:56:16.058980  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.046274 (* 1 = 0.046274 loss)
I0925 14:56:16.058997  2600 sgd_solver.cpp:105] Iteration 91200, lr = 0.0001
I0925 14:56:30.664752  2600 solver.cpp:218] Iteration 91300 (6.84662 iter/s, 14.6057s/100 iters), loss = 0.0427731
I0925 14:56:30.664783  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0427725 (* 1 = 0.0427725 loss)
I0925 14:56:30.664788  2600 sgd_solver.cpp:105] Iteration 91300, lr = 0.0001
I0925 14:56:45.270018  2600 solver.cpp:218] Iteration 91400 (6.84688 iter/s, 14.6052s/100 iters), loss = 0.0583259
I0925 14:56:45.270061  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0583253 (* 1 = 0.0583253 loss)
I0925 14:56:45.270066  2600 sgd_solver.cpp:105] Iteration 91400, lr = 0.0001
I0925 14:56:59.156883  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:56:59.740859  2600 solver.cpp:330] Iteration 91500, Testing net (#0)
I0925 14:57:03.168169  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:57:03.311389  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.876
I0925 14:57:03.311425  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535778 (* 1 = 0.535778 loss)
I0925 14:57:03.456409  2600 solver.cpp:218] Iteration 91500 (5.49864 iter/s, 18.1863s/100 iters), loss = 0.0294214
I0925 14:57:03.456439  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0294208 (* 1 = 0.0294208 loss)
I0925 14:57:03.456446  2600 sgd_solver.cpp:105] Iteration 91500, lr = 0.0001
I0925 14:57:18.046268  2600 solver.cpp:218] Iteration 91600 (6.85411 iter/s, 14.5898s/100 iters), loss = 0.0213515
I0925 14:57:18.046298  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0213509 (* 1 = 0.0213509 loss)
I0925 14:57:18.046305  2600 sgd_solver.cpp:105] Iteration 91600, lr = 0.0001
I0925 14:57:32.631086  2600 solver.cpp:218] Iteration 91700 (6.85648 iter/s, 14.5848s/100 iters), loss = 0.0453452
I0925 14:57:32.631227  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0453446 (* 1 = 0.0453446 loss)
I0925 14:57:32.631237  2600 sgd_solver.cpp:105] Iteration 91700, lr = 0.0001
I0925 14:57:47.214015  2600 solver.cpp:218] Iteration 91800 (6.85742 iter/s, 14.5828s/100 iters), loss = 0.0424828
I0925 14:57:47.214056  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0424822 (* 1 = 0.0424822 loss)
I0925 14:57:47.214062  2600 sgd_solver.cpp:105] Iteration 91800, lr = 0.0001
I0925 14:58:01.802700  2600 solver.cpp:218] Iteration 91900 (6.85466 iter/s, 14.5886s/100 iters), loss = 0.0381838
I0925 14:58:01.802731  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0381832 (* 1 = 0.0381832 loss)
I0925 14:58:01.802737  2600 sgd_solver.cpp:105] Iteration 91900, lr = 0.0001
I0925 14:58:15.668592  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:58:16.252236  2600 solver.cpp:330] Iteration 92000, Testing net (#0)
I0925 14:58:19.678372  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:58:19.820741  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8767
I0925 14:58:19.820766  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.536214 (* 1 = 0.536214 loss)
I0925 14:58:19.966289  2600 solver.cpp:218] Iteration 92000 (5.50554 iter/s, 18.1635s/100 iters), loss = 0.0260308
I0925 14:58:19.966321  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0260302 (* 1 = 0.0260302 loss)
I0925 14:58:19.966329  2600 sgd_solver.cpp:105] Iteration 92000, lr = 0.0001
I0925 14:58:34.572079  2600 solver.cpp:218] Iteration 92100 (6.84663 iter/s, 14.6057s/100 iters), loss = 0.0959641
I0925 14:58:34.572110  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0959634 (* 1 = 0.0959634 loss)
I0925 14:58:34.572116  2600 sgd_solver.cpp:105] Iteration 92100, lr = 0.0001
I0925 14:58:49.176357  2600 solver.cpp:218] Iteration 92200 (6.84734 iter/s, 14.6042s/100 iters), loss = 0.0764584
I0925 14:58:49.176463  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0764577 (* 1 = 0.0764577 loss)
I0925 14:58:49.176481  2600 sgd_solver.cpp:105] Iteration 92200, lr = 0.0001
I0925 14:59:03.784766  2600 solver.cpp:218] Iteration 92300 (6.84544 iter/s, 14.6083s/100 iters), loss = 0.0549373
I0925 14:59:03.784808  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0549366 (* 1 = 0.0549366 loss)
I0925 14:59:03.784814  2600 sgd_solver.cpp:105] Iteration 92300, lr = 0.0001
I0925 14:59:18.393556  2600 solver.cpp:218] Iteration 92400 (6.84523 iter/s, 14.6087s/100 iters), loss = 0.0582099
I0925 14:59:18.393596  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0582092 (* 1 = 0.0582092 loss)
I0925 14:59:18.393602  2600 sgd_solver.cpp:105] Iteration 92400, lr = 0.0001
I0925 14:59:32.277101  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:59:32.861243  2600 solver.cpp:330] Iteration 92500, Testing net (#0)
I0925 14:59:36.289281  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 14:59:36.432559  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8776
I0925 14:59:36.432592  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.536621 (* 1 = 0.536621 loss)
I0925 14:59:36.577962  2600 solver.cpp:218] Iteration 92500 (5.49924 iter/s, 18.1843s/100 iters), loss = 0.0647102
I0925 14:59:36.577992  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0647095 (* 1 = 0.0647095 loss)
I0925 14:59:36.578001  2600 sgd_solver.cpp:105] Iteration 92500, lr = 0.0001
I0925 14:59:51.183993  2600 solver.cpp:218] Iteration 92600 (6.84652 iter/s, 14.606s/100 iters), loss = 0.0467991
I0925 14:59:51.184033  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0467984 (* 1 = 0.0467984 loss)
I0925 14:59:51.184041  2600 sgd_solver.cpp:105] Iteration 92600, lr = 0.0001
I0925 15:00:05.797451  2600 solver.cpp:218] Iteration 92700 (6.84304 iter/s, 14.6134s/100 iters), loss = 0.0371585
I0925 15:00:05.797572  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0371578 (* 1 = 0.0371578 loss)
I0925 15:00:05.797580  2600 sgd_solver.cpp:105] Iteration 92700, lr = 0.0001
I0925 15:00:20.396482  2600 solver.cpp:218] Iteration 92800 (6.84984 iter/s, 14.5989s/100 iters), loss = 0.0285353
I0925 15:00:20.396515  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0285346 (* 1 = 0.0285346 loss)
I0925 15:00:20.396531  2600 sgd_solver.cpp:105] Iteration 92800, lr = 0.0001
I0925 15:00:35.002439  2600 solver.cpp:218] Iteration 92900 (6.84656 iter/s, 14.6059s/100 iters), loss = 0.0171122
I0925 15:00:35.002470  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171115 (* 1 = 0.0171115 loss)
I0925 15:00:35.002487  2600 sgd_solver.cpp:105] Iteration 92900, lr = 0.0001
I0925 15:00:48.878053  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:00:49.462031  2600 solver.cpp:330] Iteration 93000, Testing net (#0)
I0925 15:00:52.889389  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:00:53.032192  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8771
I0925 15:00:53.032228  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.53737 (* 1 = 0.53737 loss)
I0925 15:00:53.177469  2600 solver.cpp:218] Iteration 93000 (5.50208 iter/s, 18.175s/100 iters), loss = 0.10316
I0925 15:00:53.177496  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103159 (* 1 = 0.103159 loss)
I0925 15:00:53.177505  2600 sgd_solver.cpp:105] Iteration 93000, lr = 0.0001
I0925 15:01:07.762543  2600 solver.cpp:218] Iteration 93100 (6.85636 iter/s, 14.585s/100 iters), loss = 0.0767478
I0925 15:01:07.762584  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0767471 (* 1 = 0.0767471 loss)
I0925 15:01:07.762591  2600 sgd_solver.cpp:105] Iteration 93100, lr = 0.0001
I0925 15:01:22.358134  2600 solver.cpp:218] Iteration 93200 (6.85142 iter/s, 14.5955s/100 iters), loss = 0.0587462
I0925 15:01:22.358247  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0587455 (* 1 = 0.0587455 loss)
I0925 15:01:22.358253  2600 sgd_solver.cpp:105] Iteration 93200, lr = 0.0001
I0925 15:01:36.948774  2600 solver.cpp:218] Iteration 93300 (6.85378 iter/s, 14.5905s/100 iters), loss = 0.0513187
I0925 15:01:36.948806  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.051318 (* 1 = 0.051318 loss)
I0925 15:01:36.948812  2600 sgd_solver.cpp:105] Iteration 93300, lr = 0.0001
I0925 15:01:51.548260  2600 solver.cpp:218] Iteration 93400 (6.84959 iter/s, 14.5994s/100 iters), loss = 0.0160491
I0925 15:01:51.548291  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0160484 (* 1 = 0.0160484 loss)
I0925 15:01:51.548298  2600 sgd_solver.cpp:105] Iteration 93400, lr = 0.0001
I0925 15:02:05.416671  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:02:06.000825  2600 solver.cpp:330] Iteration 93500, Testing net (#0)
I0925 15:02:09.428344  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:02:09.571497  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8772
I0925 15:02:09.571533  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535798 (* 1 = 0.535798 loss)
I0925 15:02:09.716780  2600 solver.cpp:218] Iteration 93500 (5.50405 iter/s, 18.1684s/100 iters), loss = 0.0500191
I0925 15:02:09.716809  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0500183 (* 1 = 0.0500183 loss)
I0925 15:02:09.716816  2600 sgd_solver.cpp:105] Iteration 93500, lr = 0.0001
I0925 15:02:24.318789  2600 solver.cpp:218] Iteration 93600 (6.8484 iter/s, 14.6019s/100 iters), loss = 0.0677548
I0925 15:02:24.318830  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.067754 (* 1 = 0.067754 loss)
I0925 15:02:24.318835  2600 sgd_solver.cpp:105] Iteration 93600, lr = 0.0001
I0925 15:02:38.929261  2600 solver.cpp:218] Iteration 93700 (6.84444 iter/s, 14.6104s/100 iters), loss = 0.0538723
I0925 15:02:38.929369  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0538715 (* 1 = 0.0538715 loss)
I0925 15:02:38.929378  2600 sgd_solver.cpp:105] Iteration 93700, lr = 0.0001
I0925 15:02:53.538795  2600 solver.cpp:218] Iteration 93800 (6.84491 iter/s, 14.6094s/100 iters), loss = 0.0700905
I0925 15:02:53.538825  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0700897 (* 1 = 0.0700897 loss)
I0925 15:02:53.538831  2600 sgd_solver.cpp:105] Iteration 93800, lr = 0.0001
I0925 15:03:08.147922  2600 solver.cpp:218] Iteration 93900 (6.84507 iter/s, 14.6091s/100 iters), loss = 0.0312066
I0925 15:03:08.147953  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0312058 (* 1 = 0.0312058 loss)
I0925 15:03:08.147958  2600 sgd_solver.cpp:105] Iteration 93900, lr = 0.0001
I0925 15:03:22.024140  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:03:22.607223  2600 solver.cpp:330] Iteration 94000, Testing net (#0)
I0925 15:03:26.036310  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:03:26.179116  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.878
I0925 15:03:26.179152  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.532509 (* 1 = 0.532509 loss)
I0925 15:03:26.324393  2600 solver.cpp:218] Iteration 94000 (5.50164 iter/s, 18.1764s/100 iters), loss = 0.0480154
I0925 15:03:26.324421  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0480146 (* 1 = 0.0480146 loss)
I0925 15:03:26.324429  2600 sgd_solver.cpp:105] Iteration 94000, lr = 0.0001
I0925 15:03:40.914299  2600 solver.cpp:218] Iteration 94100 (6.85409 iter/s, 14.5898s/100 iters), loss = 0.0399916
I0925 15:03:40.914340  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0399907 (* 1 = 0.0399907 loss)
I0925 15:03:40.914346  2600 sgd_solver.cpp:105] Iteration 94100, lr = 0.0001
I0925 15:03:55.505491  2600 solver.cpp:218] Iteration 94200 (6.85349 iter/s, 14.5911s/100 iters), loss = 0.0831216
I0925 15:03:55.505604  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0831208 (* 1 = 0.0831208 loss)
I0925 15:03:55.505611  2600 sgd_solver.cpp:105] Iteration 94200, lr = 0.0001
I0925 15:04:10.103375  2600 solver.cpp:218] Iteration 94300 (6.85038 iter/s, 14.5977s/100 iters), loss = 0.0293435
I0925 15:04:10.103404  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0293426 (* 1 = 0.0293426 loss)
I0925 15:04:10.103411  2600 sgd_solver.cpp:105] Iteration 94300, lr = 0.0001
I0925 15:04:24.694471  2600 solver.cpp:218] Iteration 94400 (6.85353 iter/s, 14.591s/100 iters), loss = 0.0269032
I0925 15:04:24.694511  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0269024 (* 1 = 0.0269024 loss)
I0925 15:04:24.694519  2600 sgd_solver.cpp:105] Iteration 94400, lr = 0.0001
I0925 15:04:38.559677  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:04:39.143715  2600 solver.cpp:330] Iteration 94500, Testing net (#0)
I0925 15:04:42.569913  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:04:42.712693  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8782
I0925 15:04:42.712729  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.53429 (* 1 = 0.53429 loss)
I0925 15:04:42.857509  2600 solver.cpp:218] Iteration 94500 (5.50571 iter/s, 18.163s/100 iters), loss = 0.0342651
I0925 15:04:42.857538  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0342643 (* 1 = 0.0342643 loss)
I0925 15:04:42.857545  2600 sgd_solver.cpp:105] Iteration 94500, lr = 0.0001
I0925 15:04:57.464897  2600 solver.cpp:218] Iteration 94600 (6.84588 iter/s, 14.6073s/100 iters), loss = 0.0524821
I0925 15:04:57.464927  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0524813 (* 1 = 0.0524813 loss)
I0925 15:04:57.464934  2600 sgd_solver.cpp:105] Iteration 94600, lr = 0.0001
I0925 15:05:12.078846  2600 solver.cpp:218] Iteration 94700 (6.84281 iter/s, 14.6139s/100 iters), loss = 0.0734279
I0925 15:05:12.078958  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0734271 (* 1 = 0.0734271 loss)
I0925 15:05:12.078965  2600 sgd_solver.cpp:105] Iteration 94700, lr = 0.0001
I0925 15:05:26.689333  2600 solver.cpp:218] Iteration 94800 (6.84446 iter/s, 14.6103s/100 iters), loss = 0.0212013
I0925 15:05:26.689363  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212005 (* 1 = 0.0212005 loss)
I0925 15:05:26.689369  2600 sgd_solver.cpp:105] Iteration 94800, lr = 0.0001
I0925 15:05:41.299788  2600 solver.cpp:218] Iteration 94900 (6.84445 iter/s, 14.6104s/100 iters), loss = 0.0315175
I0925 15:05:41.299819  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0315167 (* 1 = 0.0315167 loss)
I0925 15:05:41.299836  2600 sgd_solver.cpp:105] Iteration 94900, lr = 0.0001
I0925 15:05:55.184128  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:05:55.768527  2600 solver.cpp:330] Iteration 95000, Testing net (#0)
I0925 15:05:59.195996  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:05:59.338532  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8775
I0925 15:05:59.338569  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.536596 (* 1 = 0.536596 loss)
I0925 15:05:59.483793  2600 solver.cpp:218] Iteration 95000 (5.49936 iter/s, 18.1839s/100 iters), loss = 0.0238403
I0925 15:05:59.483822  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0238395 (* 1 = 0.0238395 loss)
I0925 15:05:59.483830  2600 sgd_solver.cpp:105] Iteration 95000, lr = 0.0001
I0925 15:06:14.087354  2600 solver.cpp:218] Iteration 95100 (6.84768 iter/s, 14.6035s/100 iters), loss = 0.0342074
I0925 15:06:14.087395  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0342066 (* 1 = 0.0342066 loss)
I0925 15:06:14.087402  2600 sgd_solver.cpp:105] Iteration 95100, lr = 0.0001
I0925 15:06:28.690989  2600 solver.cpp:218] Iteration 95200 (6.84765 iter/s, 14.6036s/100 iters), loss = 0.0667292
I0925 15:06:28.691104  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0667284 (* 1 = 0.0667284 loss)
I0925 15:06:28.691123  2600 sgd_solver.cpp:105] Iteration 95200, lr = 0.0001
I0925 15:06:43.297026  2600 solver.cpp:218] Iteration 95300 (6.84655 iter/s, 14.6059s/100 iters), loss = 0.0282634
I0925 15:06:43.297065  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0282626 (* 1 = 0.0282626 loss)
I0925 15:06:43.297072  2600 sgd_solver.cpp:105] Iteration 95300, lr = 0.0001
I0925 15:06:57.900290  2600 solver.cpp:218] Iteration 95400 (6.84782 iter/s, 14.6032s/100 iters), loss = 0.0346086
I0925 15:06:57.900321  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0346079 (* 1 = 0.0346079 loss)
I0925 15:06:57.900327  2600 sgd_solver.cpp:105] Iteration 95400, lr = 0.0001
I0925 15:07:11.778067  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:07:12.363972  2600 solver.cpp:330] Iteration 95500, Testing net (#0)
I0925 15:07:15.789829  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:07:15.932690  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8782
I0925 15:07:15.932716  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535953 (* 1 = 0.535953 loss)
I0925 15:07:16.077694  2600 solver.cpp:218] Iteration 95500 (5.50136 iter/s, 18.1773s/100 iters), loss = 0.0233477
I0925 15:07:16.077736  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0233469 (* 1 = 0.0233469 loss)
I0925 15:07:16.077744  2600 sgd_solver.cpp:105] Iteration 95500, lr = 0.0001
I0925 15:07:30.667618  2600 solver.cpp:218] Iteration 95600 (6.85408 iter/s, 14.5898s/100 iters), loss = 0.0758334
I0925 15:07:30.667649  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0758327 (* 1 = 0.0758327 loss)
I0925 15:07:30.667655  2600 sgd_solver.cpp:105] Iteration 95600, lr = 0.0001
I0925 15:07:45.261193  2600 solver.cpp:218] Iteration 95700 (6.85236 iter/s, 14.5935s/100 iters), loss = 0.0288289
I0925 15:07:45.261325  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0288281 (* 1 = 0.0288281 loss)
I0925 15:07:45.261333  2600 sgd_solver.cpp:105] Iteration 95700, lr = 0.0001
I0925 15:07:59.850896  2600 solver.cpp:218] Iteration 95800 (6.85422 iter/s, 14.5895s/100 iters), loss = 0.0718496
I0925 15:07:59.850926  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0718488 (* 1 = 0.0718488 loss)
I0925 15:07:59.850934  2600 sgd_solver.cpp:105] Iteration 95800, lr = 0.0001
I0925 15:08:14.446336  2600 solver.cpp:218] Iteration 95900 (6.85149 iter/s, 14.5954s/100 iters), loss = 0.0807227
I0925 15:08:14.446367  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.080722 (* 1 = 0.080722 loss)
I0925 15:08:14.446374  2600 sgd_solver.cpp:105] Iteration 95900, lr = 0.0001
I0925 15:08:28.316233  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:08:28.899793  2600 solver.cpp:330] Iteration 96000, Testing net (#0)
I0925 15:08:32.326308  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:08:32.469228  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8773
I0925 15:08:32.469264  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.538161 (* 1 = 0.538161 loss)
I0925 15:08:32.614176  2600 solver.cpp:218] Iteration 96000 (5.50426 iter/s, 18.1678s/100 iters), loss = 0.0372641
I0925 15:08:32.614205  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0372634 (* 1 = 0.0372634 loss)
I0925 15:08:32.614213  2600 sgd_solver.cpp:105] Iteration 96000, lr = 0.0001
I0925 15:08:47.218849  2600 solver.cpp:218] Iteration 96100 (6.84715 iter/s, 14.6046s/100 iters), loss = 0.0698405
I0925 15:08:47.218878  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0698397 (* 1 = 0.0698397 loss)
I0925 15:08:47.218884  2600 sgd_solver.cpp:105] Iteration 96100, lr = 0.0001
I0925 15:09:01.829749  2600 solver.cpp:218] Iteration 96200 (6.84424 iter/s, 14.6108s/100 iters), loss = 0.0545777
I0925 15:09:01.829896  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0545769 (* 1 = 0.0545769 loss)
I0925 15:09:01.829905  2600 sgd_solver.cpp:105] Iteration 96200, lr = 0.0001
I0925 15:09:16.438777  2600 solver.cpp:218] Iteration 96300 (6.84517 iter/s, 14.6088s/100 iters), loss = 0.0501839
I0925 15:09:16.438808  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.050183 (* 1 = 0.050183 loss)
I0925 15:09:16.438824  2600 sgd_solver.cpp:105] Iteration 96300, lr = 0.0001
I0925 15:09:31.052471  2600 solver.cpp:218] Iteration 96400 (6.84293 iter/s, 14.6136s/100 iters), loss = 0.0245523
I0925 15:09:31.052503  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0245515 (* 1 = 0.0245515 loss)
I0925 15:09:31.052510  2600 sgd_solver.cpp:105] Iteration 96400, lr = 0.0001
I0925 15:09:44.935837  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:09:45.519887  2600 solver.cpp:330] Iteration 96500, Testing net (#0)
I0925 15:09:48.947000  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:09:49.089596  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8772
I0925 15:09:49.089632  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.537129 (* 1 = 0.537129 loss)
I0925 15:09:49.234465  2600 solver.cpp:218] Iteration 96500 (5.49997 iter/s, 18.1819s/100 iters), loss = 0.0282737
I0925 15:09:49.234495  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0282729 (* 1 = 0.0282729 loss)
I0925 15:09:49.234503  2600 sgd_solver.cpp:105] Iteration 96500, lr = 0.0001
I0925 15:10:03.825937  2600 solver.cpp:218] Iteration 96600 (6.85335 iter/s, 14.5914s/100 iters), loss = 0.0378032
I0925 15:10:03.825968  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0378024 (* 1 = 0.0378024 loss)
I0925 15:10:03.825974  2600 sgd_solver.cpp:105] Iteration 96600, lr = 0.0001
I0925 15:10:18.417191  2600 solver.cpp:218] Iteration 96700 (6.85345 iter/s, 14.5912s/100 iters), loss = 0.0367197
I0925 15:10:18.417299  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0367189 (* 1 = 0.0367189 loss)
I0925 15:10:18.417306  2600 sgd_solver.cpp:105] Iteration 96700, lr = 0.0001
I0925 15:10:33.106458  2600 solver.cpp:218] Iteration 96800 (6.80776 iter/s, 14.6891s/100 iters), loss = 0.0680874
I0925 15:10:33.106487  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0680865 (* 1 = 0.0680865 loss)
I0925 15:10:33.106493  2600 sgd_solver.cpp:105] Iteration 96800, lr = 0.0001
I0925 15:10:47.738668  2600 solver.cpp:218] Iteration 96900 (6.83427 iter/s, 14.6321s/100 iters), loss = 0.0257971
I0925 15:10:47.738698  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0257962 (* 1 = 0.0257962 loss)
I0925 15:10:47.738705  2600 sgd_solver.cpp:105] Iteration 96900, lr = 0.0001
I0925 15:11:01.603427  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:11:02.187345  2600 solver.cpp:330] Iteration 97000, Testing net (#0)
I0925 15:11:05.614162  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:11:05.757287  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8767
I0925 15:11:05.757323  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.539657 (* 1 = 0.539657 loss)
I0925 15:11:05.902096  2600 solver.cpp:218] Iteration 97000 (5.50559 iter/s, 18.1634s/100 iters), loss = 0.0495951
I0925 15:11:05.902127  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0495943 (* 1 = 0.0495943 loss)
I0925 15:11:05.902134  2600 sgd_solver.cpp:105] Iteration 97000, lr = 0.0001
I0925 15:11:20.510671  2600 solver.cpp:218] Iteration 97100 (6.84533 iter/s, 14.6085s/100 iters), loss = 0.070754
I0925 15:11:20.510711  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0707531 (* 1 = 0.0707531 loss)
I0925 15:11:20.510718  2600 sgd_solver.cpp:105] Iteration 97100, lr = 0.0001
I0925 15:11:35.112355  2600 solver.cpp:218] Iteration 97200 (6.84856 iter/s, 14.6016s/100 iters), loss = 0.0923132
I0925 15:11:35.112468  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0923123 (* 1 = 0.0923123 loss)
I0925 15:11:35.112475  2600 sgd_solver.cpp:105] Iteration 97200, lr = 0.0001
I0925 15:11:49.720260  2600 solver.cpp:218] Iteration 97300 (6.84568 iter/s, 14.6078s/100 iters), loss = 0.0225034
I0925 15:11:49.720290  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0225025 (* 1 = 0.0225025 loss)
I0925 15:11:49.720297  2600 sgd_solver.cpp:105] Iteration 97300, lr = 0.0001
I0925 15:12:04.329718  2600 solver.cpp:218] Iteration 97400 (6.84491 iter/s, 14.6094s/100 iters), loss = 0.0617525
I0925 15:12:04.329748  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0617517 (* 1 = 0.0617517 loss)
I0925 15:12:04.329754  2600 sgd_solver.cpp:105] Iteration 97400, lr = 0.0001
I0925 15:12:18.214283  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:12:18.799209  2600 solver.cpp:330] Iteration 97500, Testing net (#0)
I0925 15:12:22.226449  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:12:22.368937  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8767
I0925 15:12:22.368973  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.541275 (* 1 = 0.541275 loss)
I0925 15:12:22.513813  2600 solver.cpp:218] Iteration 97500 (5.49933 iter/s, 18.184s/100 iters), loss = 0.0345094
I0925 15:12:22.513844  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0345085 (* 1 = 0.0345085 loss)
I0925 15:12:22.513851  2600 sgd_solver.cpp:105] Iteration 97500, lr = 0.0001
I0925 15:12:37.120600  2600 solver.cpp:218] Iteration 97600 (6.84616 iter/s, 14.6067s/100 iters), loss = 0.0216092
I0925 15:12:37.120641  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216084 (* 1 = 0.0216084 loss)
I0925 15:12:37.120647  2600 sgd_solver.cpp:105] Iteration 97600, lr = 0.0001
I0925 15:12:51.732770  2600 solver.cpp:218] Iteration 97700 (6.84365 iter/s, 14.6121s/100 iters), loss = 0.0275601
I0925 15:12:51.732913  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275592 (* 1 = 0.0275592 loss)
I0925 15:12:51.732923  2600 sgd_solver.cpp:105] Iteration 97700, lr = 0.0001
I0925 15:13:06.345064  2600 solver.cpp:218] Iteration 97800 (6.84364 iter/s, 14.6121s/100 iters), loss = 0.0516861
I0925 15:13:06.345105  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0516852 (* 1 = 0.0516852 loss)
I0925 15:13:06.345113  2600 sgd_solver.cpp:105] Iteration 97800, lr = 0.0001
I0925 15:13:20.957702  2600 solver.cpp:218] Iteration 97900 (6.84343 iter/s, 14.6126s/100 iters), loss = 0.0749326
I0925 15:13:20.957744  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0749317 (* 1 = 0.0749317 loss)
I0925 15:13:20.957751  2600 sgd_solver.cpp:105] Iteration 97900, lr = 0.0001
I0925 15:13:34.842025  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:13:35.426252  2600 solver.cpp:330] Iteration 98000, Testing net (#0)
I0925 15:13:38.855348  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:13:38.997851  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8772
I0925 15:13:38.997877  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.53778 (* 1 = 0.53778 loss)
I0925 15:13:39.142707  2600 solver.cpp:218] Iteration 98000 (5.49906 iter/s, 18.1849s/100 iters), loss = 0.0819459
I0925 15:13:39.142736  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.081945 (* 1 = 0.081945 loss)
I0925 15:13:39.142745  2600 sgd_solver.cpp:105] Iteration 98000, lr = 0.0001
I0925 15:13:53.736354  2600 solver.cpp:218] Iteration 98100 (6.85233 iter/s, 14.5936s/100 iters), loss = 0.040092
I0925 15:13:53.736395  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0400912 (* 1 = 0.0400912 loss)
I0925 15:13:53.736402  2600 sgd_solver.cpp:105] Iteration 98100, lr = 0.0001
I0925 15:14:08.333921  2600 solver.cpp:218] Iteration 98200 (6.85049 iter/s, 14.5975s/100 iters), loss = 0.0469308
I0925 15:14:08.334031  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.04693 (* 1 = 0.04693 loss)
I0925 15:14:08.334049  2600 sgd_solver.cpp:105] Iteration 98200, lr = 0.0001
I0925 15:14:22.930692  2600 solver.cpp:218] Iteration 98300 (6.8509 iter/s, 14.5966s/100 iters), loss = 0.0639578
I0925 15:14:22.930733  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0639569 (* 1 = 0.0639569 loss)
I0925 15:14:22.930740  2600 sgd_solver.cpp:105] Iteration 98300, lr = 0.0001
I0925 15:14:37.526666  2600 solver.cpp:218] Iteration 98400 (6.85124 iter/s, 14.5959s/100 iters), loss = 0.0157986
I0925 15:14:37.526696  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0157977 (* 1 = 0.0157977 loss)
I0925 15:14:37.526703  2600 sgd_solver.cpp:105] Iteration 98400, lr = 0.0001
I0925 15:14:51.401084  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:14:51.985141  2600 solver.cpp:330] Iteration 98500, Testing net (#0)
I0925 15:14:55.412919  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:14:55.555232  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8768
I0925 15:14:55.555268  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.538345 (* 1 = 0.538345 loss)
I0925 15:14:55.700153  2600 solver.cpp:218] Iteration 98500 (5.50254 iter/s, 18.1734s/100 iters), loss = 0.0425842
I0925 15:14:55.700183  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0425833 (* 1 = 0.0425833 loss)
I0925 15:14:55.700191  2600 sgd_solver.cpp:105] Iteration 98500, lr = 0.0001
I0925 15:15:10.301409  2600 solver.cpp:218] Iteration 98600 (6.84876 iter/s, 14.6012s/100 iters), loss = 0.0169569
I0925 15:15:10.301439  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0169561 (* 1 = 0.0169561 loss)
I0925 15:15:10.301445  2600 sgd_solver.cpp:105] Iteration 98600, lr = 0.0001
I0925 15:15:24.906913  2600 solver.cpp:218] Iteration 98700 (6.84677 iter/s, 14.6054s/100 iters), loss = 0.0556293
I0925 15:15:24.907058  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0556284 (* 1 = 0.0556284 loss)
I0925 15:15:24.907071  2600 sgd_solver.cpp:105] Iteration 98700, lr = 0.0001
I0925 15:15:39.520228  2600 solver.cpp:218] Iteration 98800 (6.84315 iter/s, 14.6131s/100 iters), loss = 0.0272406
I0925 15:15:39.520262  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0272397 (* 1 = 0.0272397 loss)
I0925 15:15:39.520270  2600 sgd_solver.cpp:105] Iteration 98800, lr = 0.0001
I0925 15:15:54.132302  2600 solver.cpp:218] Iteration 98900 (6.84369 iter/s, 14.612s/100 iters), loss = 0.0174986
I0925 15:15:54.132333  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0174977 (* 1 = 0.0174977 loss)
I0925 15:15:54.132340  2600 sgd_solver.cpp:105] Iteration 98900, lr = 0.0001
I0925 15:16:08.014221  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:16:08.599179  2600 solver.cpp:330] Iteration 99000, Testing net (#0)
I0925 15:16:12.026335  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:16:12.169643  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8775
I0925 15:16:12.169669  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.538702 (* 1 = 0.538702 loss)
I0925 15:16:12.314617  2600 solver.cpp:218] Iteration 99000 (5.49987 iter/s, 18.1822s/100 iters), loss = 0.0706911
I0925 15:16:12.314647  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0706902 (* 1 = 0.0706902 loss)
I0925 15:16:12.314654  2600 sgd_solver.cpp:105] Iteration 99000, lr = 0.0001
I0925 15:16:26.903810  2600 solver.cpp:218] Iteration 99100 (6.85442 iter/s, 14.5891s/100 iters), loss = 0.0207066
I0925 15:16:26.903851  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0207058 (* 1 = 0.0207058 loss)
I0925 15:16:26.903858  2600 sgd_solver.cpp:105] Iteration 99100, lr = 0.0001
I0925 15:16:41.682910  2600 solver.cpp:218] Iteration 99200 (6.76635 iter/s, 14.779s/100 iters), loss = 0.0324348
I0925 15:16:41.682989  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0324339 (* 1 = 0.0324339 loss)
I0925 15:16:41.682999  2600 sgd_solver.cpp:105] Iteration 99200, lr = 0.0001
I0925 15:16:56.481210  2600 solver.cpp:218] Iteration 99300 (6.75759 iter/s, 14.7982s/100 iters), loss = 0.0486318
I0925 15:16:56.481242  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0486309 (* 1 = 0.0486309 loss)
I0925 15:16:56.481250  2600 sgd_solver.cpp:105] Iteration 99300, lr = 0.0001
I0925 15:17:11.064043  2600 solver.cpp:218] Iteration 99400 (6.85741 iter/s, 14.5828s/100 iters), loss = 0.0293272
I0925 15:17:11.064072  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0293264 (* 1 = 0.0293264 loss)
I0925 15:17:11.064079  2600 sgd_solver.cpp:105] Iteration 99400, lr = 0.0001
I0925 15:17:25.016405  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:17:25.620970  2600 solver.cpp:330] Iteration 99500, Testing net (#0)
I0925 15:17:29.193789  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:17:29.336957  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8775
I0925 15:17:29.336993  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.539642 (* 1 = 0.539642 loss)
I0925 15:17:29.480926  2600 solver.cpp:218] Iteration 99500 (5.42982 iter/s, 18.4168s/100 iters), loss = 0.0423573
I0925 15:17:29.480955  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0423565 (* 1 = 0.0423565 loss)
I0925 15:17:29.480962  2600 sgd_solver.cpp:105] Iteration 99500, lr = 0.0001
I0925 15:17:44.193528  2600 solver.cpp:218] Iteration 99600 (6.79693 iter/s, 14.7125s/100 iters), loss = 0.017664
I0925 15:17:44.193559  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0176631 (* 1 = 0.0176631 loss)
I0925 15:17:44.193567  2600 sgd_solver.cpp:105] Iteration 99600, lr = 0.0001
I0925 15:17:58.877171  2600 solver.cpp:218] Iteration 99700 (6.81033 iter/s, 14.6836s/100 iters), loss = 0.0503853
I0925 15:17:58.877316  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0503845 (* 1 = 0.0503845 loss)
I0925 15:17:58.877326  2600 sgd_solver.cpp:105] Iteration 99700, lr = 0.0001
I0925 15:18:13.682824  2600 solver.cpp:218] Iteration 99800 (6.75427 iter/s, 14.8055s/100 iters), loss = 0.0479817
I0925 15:18:13.682868  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0479809 (* 1 = 0.0479809 loss)
I0925 15:18:13.682885  2600 sgd_solver.cpp:105] Iteration 99800, lr = 0.0001
I0925 15:18:28.292740  2600 solver.cpp:218] Iteration 99900 (6.8447 iter/s, 14.6098s/100 iters), loss = 0.0403534
I0925 15:18:28.292783  2600 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0403526 (* 1 = 0.0403526 loss)
I0925 15:18:28.292790  2600 sgd_solver.cpp:105] Iteration 99900, lr = 0.0001
I0925 15:18:42.152478  2608 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:18:42.735894  2600 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res56_penlu_alpha2_eta1_gauss_iter_100000.caffemodel
I0925 15:18:42.765076  2600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res56_penlu_alpha2_eta1_gauss_iter_100000.solverstate
I0925 15:18:42.805744  2600 solver.cpp:310] Iteration 100000, loss = 0.0474965
I0925 15:18:42.805771  2600 solver.cpp:330] Iteration 100000, Testing net (#0)
I0925 15:18:46.337509  2609 data_layer.cpp:73] Restarting data prefetching from start.
I0925 15:18:46.486405  2600 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8764
I0925 15:18:46.486451  2600 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.540608 (* 1 = 0.540608 loss)
I0925 15:18:46.486456  2600 solver.cpp:315] Optimization Done.
I0925 15:18:46.486459  2600 caffe.cpp:259] Optimization Done.
