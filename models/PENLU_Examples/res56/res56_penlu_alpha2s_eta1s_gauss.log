I0926 08:53:07.769775  4406 caffe.cpp:218] Using GPUs 0
I0926 08:53:07.795125  4406 caffe.cpp:223] GPU 0: GeForce GTX 1080
I0926 08:53:08.026420  4406 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 100000
snapshot_prefix: "xn/PENLU/snapshot/resnet/res56_penlu_alpha2s_eta1s_gauss"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 900
stepvalue: 80000
I0926 08:53:08.026571  4406 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0926 08:53:08.030480  4406 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0926 08:53:08.030493  4406 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0926 08:53:08.030777  4406 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I0926 08:53:08.030905  4406 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I0926 08:53:08.032083  4406 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 28
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution17"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution19"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution20"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu22"
  type: "PENLU"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std
I0926 08:53:08.033043  4406 layer_factory.hpp:77] Creating layer Data1
I0926 08:53:08.033118  4406 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_train_lmdb
I0926 08:53:08.033138  4406 net.cpp:84] Creating Layer Data1
I0926 08:53:08.033143  4406 net.cpp:380] Data1 -> Data1
I0926 08:53:08.033161  4406 net.cpp:380] Data1 -> Data2
I0926 08:53:08.033170  4406 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I0926 08:53:08.034535  4406 data_layer.cpp:45] output data size: 100,3,28,28
I0926 08:53:08.036844  4406 net.cpp:122] Setting up Data1
I0926 08:53:08.036856  4406 net.cpp:129] Top shape: 100 3 28 28 (235200)
I0926 08:53:08.036860  4406 net.cpp:129] Top shape: 100 (100)
I0926 08:53:08.036862  4406 net.cpp:137] Memory required for data: 941200
I0926 08:53:08.036869  4406 layer_factory.hpp:77] Creating layer Convolution1
I0926 08:53:08.036888  4406 net.cpp:84] Creating Layer Convolution1
I0926 08:53:08.036892  4406 net.cpp:406] Convolution1 <- Data1
I0926 08:53:08.036901  4406 net.cpp:380] Convolution1 -> Convolution1
I0926 08:53:08.181820  4406 net.cpp:122] Setting up Convolution1
I0926 08:53:08.181846  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.181849  4406 net.cpp:137] Memory required for data: 5958800
I0926 08:53:08.181865  4406 layer_factory.hpp:77] Creating layer BatchNorm1
I0926 08:53:08.181885  4406 net.cpp:84] Creating Layer BatchNorm1
I0926 08:53:08.181890  4406 net.cpp:406] BatchNorm1 <- Convolution1
I0926 08:53:08.181916  4406 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0926 08:53:08.182080  4406 net.cpp:122] Setting up BatchNorm1
I0926 08:53:08.182085  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.182087  4406 net.cpp:137] Memory required for data: 10976400
I0926 08:53:08.182095  4406 layer_factory.hpp:77] Creating layer Scale1
I0926 08:53:08.182113  4406 net.cpp:84] Creating Layer Scale1
I0926 08:53:08.182116  4406 net.cpp:406] Scale1 <- Convolution1
I0926 08:53:08.182121  4406 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0926 08:53:08.182179  4406 layer_factory.hpp:77] Creating layer Scale1
I0926 08:53:08.182317  4406 net.cpp:122] Setting up Scale1
I0926 08:53:08.182322  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.182324  4406 net.cpp:137] Memory required for data: 15994000
I0926 08:53:08.182329  4406 layer_factory.hpp:77] Creating layer penlu1
I0926 08:53:08.182338  4406 net.cpp:84] Creating Layer penlu1
I0926 08:53:08.182341  4406 net.cpp:406] penlu1 <- Convolution1
I0926 08:53:08.182354  4406 net.cpp:367] penlu1 -> Convolution1 (in-place)
I0926 08:53:08.182970  4406 net.cpp:122] Setting up penlu1
I0926 08:53:08.182979  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.182981  4406 net.cpp:137] Memory required for data: 21011600
I0926 08:53:08.182988  4406 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I0926 08:53:08.182996  4406 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I0926 08:53:08.183008  4406 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I0926 08:53:08.183012  4406 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I0926 08:53:08.183028  4406 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I0926 08:53:08.183074  4406 net.cpp:122] Setting up Convolution1_penlu1_0_split
I0926 08:53:08.183079  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.183091  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.183094  4406 net.cpp:137] Memory required for data: 31046800
I0926 08:53:08.183095  4406 layer_factory.hpp:77] Creating layer Convolution2
I0926 08:53:08.183113  4406 net.cpp:84] Creating Layer Convolution2
I0926 08:53:08.183116  4406 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I0926 08:53:08.183130  4406 net.cpp:380] Convolution2 -> Convolution2
I0926 08:53:08.183991  4406 net.cpp:122] Setting up Convolution2
I0926 08:53:08.184001  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.184005  4406 net.cpp:137] Memory required for data: 36064400
I0926 08:53:08.184008  4406 layer_factory.hpp:77] Creating layer BatchNorm2
I0926 08:53:08.184013  4406 net.cpp:84] Creating Layer BatchNorm2
I0926 08:53:08.184015  4406 net.cpp:406] BatchNorm2 <- Convolution2
I0926 08:53:08.184029  4406 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0926 08:53:08.184165  4406 net.cpp:122] Setting up BatchNorm2
I0926 08:53:08.184170  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.184173  4406 net.cpp:137] Memory required for data: 41082000
I0926 08:53:08.184178  4406 layer_factory.hpp:77] Creating layer Scale2
I0926 08:53:08.184183  4406 net.cpp:84] Creating Layer Scale2
I0926 08:53:08.184185  4406 net.cpp:406] Scale2 <- Convolution2
I0926 08:53:08.184188  4406 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0926 08:53:08.184247  4406 layer_factory.hpp:77] Creating layer Scale2
I0926 08:53:08.184357  4406 net.cpp:122] Setting up Scale2
I0926 08:53:08.184362  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.184365  4406 net.cpp:137] Memory required for data: 46099600
I0926 08:53:08.184370  4406 layer_factory.hpp:77] Creating layer penlu2
I0926 08:53:08.184376  4406 net.cpp:84] Creating Layer penlu2
I0926 08:53:08.184378  4406 net.cpp:406] penlu2 <- Convolution2
I0926 08:53:08.184391  4406 net.cpp:367] penlu2 -> Convolution2 (in-place)
I0926 08:53:08.184514  4406 net.cpp:122] Setting up penlu2
I0926 08:53:08.184520  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.184540  4406 net.cpp:137] Memory required for data: 51117200
I0926 08:53:08.184553  4406 layer_factory.hpp:77] Creating layer Convolution3
I0926 08:53:08.184571  4406 net.cpp:84] Creating Layer Convolution3
I0926 08:53:08.184574  4406 net.cpp:406] Convolution3 <- Convolution2
I0926 08:53:08.184578  4406 net.cpp:380] Convolution3 -> Convolution3
I0926 08:53:08.185422  4406 net.cpp:122] Setting up Convolution3
I0926 08:53:08.185432  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.185436  4406 net.cpp:137] Memory required for data: 56134800
I0926 08:53:08.185441  4406 layer_factory.hpp:77] Creating layer BatchNorm3
I0926 08:53:08.185446  4406 net.cpp:84] Creating Layer BatchNorm3
I0926 08:53:08.185448  4406 net.cpp:406] BatchNorm3 <- Convolution3
I0926 08:53:08.185462  4406 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0926 08:53:08.185595  4406 net.cpp:122] Setting up BatchNorm3
I0926 08:53:08.185600  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.185602  4406 net.cpp:137] Memory required for data: 61152400
I0926 08:53:08.185607  4406 layer_factory.hpp:77] Creating layer Scale3
I0926 08:53:08.185612  4406 net.cpp:84] Creating Layer Scale3
I0926 08:53:08.185616  4406 net.cpp:406] Scale3 <- Convolution3
I0926 08:53:08.185618  4406 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0926 08:53:08.185662  4406 layer_factory.hpp:77] Creating layer Scale3
I0926 08:53:08.185753  4406 net.cpp:122] Setting up Scale3
I0926 08:53:08.185760  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.185761  4406 net.cpp:137] Memory required for data: 66170000
I0926 08:53:08.185765  4406 layer_factory.hpp:77] Creating layer Eltwise1
I0926 08:53:08.185770  4406 net.cpp:84] Creating Layer Eltwise1
I0926 08:53:08.185771  4406 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I0926 08:53:08.185786  4406 net.cpp:406] Eltwise1 <- Convolution3
I0926 08:53:08.185791  4406 net.cpp:380] Eltwise1 -> Eltwise1
I0926 08:53:08.185817  4406 net.cpp:122] Setting up Eltwise1
I0926 08:53:08.185832  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.185834  4406 net.cpp:137] Memory required for data: 71187600
I0926 08:53:08.185837  4406 layer_factory.hpp:77] Creating layer penlu3
I0926 08:53:08.185842  4406 net.cpp:84] Creating Layer penlu3
I0926 08:53:08.185847  4406 net.cpp:406] penlu3 <- Eltwise1
I0926 08:53:08.185849  4406 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I0926 08:53:08.185956  4406 net.cpp:122] Setting up penlu3
I0926 08:53:08.185961  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.185964  4406 net.cpp:137] Memory required for data: 76205200
I0926 08:53:08.185978  4406 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I0926 08:53:08.185982  4406 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I0926 08:53:08.185986  4406 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I0926 08:53:08.185989  4406 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I0926 08:53:08.185999  4406 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I0926 08:53:08.186024  4406 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I0926 08:53:08.186030  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.186036  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.186041  4406 net.cpp:137] Memory required for data: 86240400
I0926 08:53:08.186046  4406 layer_factory.hpp:77] Creating layer Convolution4
I0926 08:53:08.186058  4406 net.cpp:84] Creating Layer Convolution4
I0926 08:53:08.186062  4406 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I0926 08:53:08.186069  4406 net.cpp:380] Convolution4 -> Convolution4
I0926 08:53:08.186945  4406 net.cpp:122] Setting up Convolution4
I0926 08:53:08.186956  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.186962  4406 net.cpp:137] Memory required for data: 91258000
I0926 08:53:08.186969  4406 layer_factory.hpp:77] Creating layer BatchNorm4
I0926 08:53:08.186977  4406 net.cpp:84] Creating Layer BatchNorm4
I0926 08:53:08.186988  4406 net.cpp:406] BatchNorm4 <- Convolution4
I0926 08:53:08.186996  4406 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0926 08:53:08.187122  4406 net.cpp:122] Setting up BatchNorm4
I0926 08:53:08.187129  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.187132  4406 net.cpp:137] Memory required for data: 96275600
I0926 08:53:08.187144  4406 layer_factory.hpp:77] Creating layer Scale4
I0926 08:53:08.187151  4406 net.cpp:84] Creating Layer Scale4
I0926 08:53:08.187155  4406 net.cpp:406] Scale4 <- Convolution4
I0926 08:53:08.187160  4406 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0926 08:53:08.187191  4406 layer_factory.hpp:77] Creating layer Scale4
I0926 08:53:08.187268  4406 net.cpp:122] Setting up Scale4
I0926 08:53:08.187274  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.187278  4406 net.cpp:137] Memory required for data: 101293200
I0926 08:53:08.187284  4406 layer_factory.hpp:77] Creating layer penlu4
I0926 08:53:08.187291  4406 net.cpp:84] Creating Layer penlu4
I0926 08:53:08.187295  4406 net.cpp:406] penlu4 <- Convolution4
I0926 08:53:08.187301  4406 net.cpp:367] penlu4 -> Convolution4 (in-place)
I0926 08:53:08.187402  4406 net.cpp:122] Setting up penlu4
I0926 08:53:08.187408  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.187413  4406 net.cpp:137] Memory required for data: 106310800
I0926 08:53:08.187419  4406 layer_factory.hpp:77] Creating layer Convolution5
I0926 08:53:08.187428  4406 net.cpp:84] Creating Layer Convolution5
I0926 08:53:08.187433  4406 net.cpp:406] Convolution5 <- Convolution4
I0926 08:53:08.187438  4406 net.cpp:380] Convolution5 -> Convolution5
I0926 08:53:08.188316  4406 net.cpp:122] Setting up Convolution5
I0926 08:53:08.188328  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.188333  4406 net.cpp:137] Memory required for data: 111328400
I0926 08:53:08.188340  4406 layer_factory.hpp:77] Creating layer BatchNorm5
I0926 08:53:08.188349  4406 net.cpp:84] Creating Layer BatchNorm5
I0926 08:53:08.188352  4406 net.cpp:406] BatchNorm5 <- Convolution5
I0926 08:53:08.188359  4406 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0926 08:53:08.188488  4406 net.cpp:122] Setting up BatchNorm5
I0926 08:53:08.188498  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.188501  4406 net.cpp:137] Memory required for data: 116346000
I0926 08:53:08.188506  4406 layer_factory.hpp:77] Creating layer Scale5
I0926 08:53:08.188511  4406 net.cpp:84] Creating Layer Scale5
I0926 08:53:08.188515  4406 net.cpp:406] Scale5 <- Convolution5
I0926 08:53:08.188521  4406 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0926 08:53:08.188552  4406 layer_factory.hpp:77] Creating layer Scale5
I0926 08:53:08.188629  4406 net.cpp:122] Setting up Scale5
I0926 08:53:08.188637  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.188642  4406 net.cpp:137] Memory required for data: 121363600
I0926 08:53:08.188647  4406 layer_factory.hpp:77] Creating layer Eltwise2
I0926 08:53:08.188653  4406 net.cpp:84] Creating Layer Eltwise2
I0926 08:53:08.188658  4406 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I0926 08:53:08.188663  4406 net.cpp:406] Eltwise2 <- Convolution5
I0926 08:53:08.188668  4406 net.cpp:380] Eltwise2 -> Eltwise2
I0926 08:53:08.188688  4406 net.cpp:122] Setting up Eltwise2
I0926 08:53:08.188693  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.188697  4406 net.cpp:137] Memory required for data: 126381200
I0926 08:53:08.188700  4406 layer_factory.hpp:77] Creating layer penlu5
I0926 08:53:08.188709  4406 net.cpp:84] Creating Layer penlu5
I0926 08:53:08.188711  4406 net.cpp:406] penlu5 <- Eltwise2
I0926 08:53:08.188717  4406 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I0926 08:53:08.188825  4406 net.cpp:122] Setting up penlu5
I0926 08:53:08.188832  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.188836  4406 net.cpp:137] Memory required for data: 131398800
I0926 08:53:08.188843  4406 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I0926 08:53:08.188849  4406 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I0926 08:53:08.188859  4406 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I0926 08:53:08.188866  4406 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I0926 08:53:08.188874  4406 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I0926 08:53:08.188899  4406 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I0926 08:53:08.188905  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.188910  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.188915  4406 net.cpp:137] Memory required for data: 141434000
I0926 08:53:08.188918  4406 layer_factory.hpp:77] Creating layer Convolution6
I0926 08:53:08.188927  4406 net.cpp:84] Creating Layer Convolution6
I0926 08:53:08.188931  4406 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I0926 08:53:08.188937  4406 net.cpp:380] Convolution6 -> Convolution6
I0926 08:53:08.189815  4406 net.cpp:122] Setting up Convolution6
I0926 08:53:08.189826  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.189831  4406 net.cpp:137] Memory required for data: 146451600
I0926 08:53:08.189839  4406 layer_factory.hpp:77] Creating layer BatchNorm6
I0926 08:53:08.189847  4406 net.cpp:84] Creating Layer BatchNorm6
I0926 08:53:08.189851  4406 net.cpp:406] BatchNorm6 <- Convolution6
I0926 08:53:08.189857  4406 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0926 08:53:08.189988  4406 net.cpp:122] Setting up BatchNorm6
I0926 08:53:08.189995  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.189999  4406 net.cpp:137] Memory required for data: 151469200
I0926 08:53:08.190006  4406 layer_factory.hpp:77] Creating layer Scale6
I0926 08:53:08.190012  4406 net.cpp:84] Creating Layer Scale6
I0926 08:53:08.190016  4406 net.cpp:406] Scale6 <- Convolution6
I0926 08:53:08.190022  4406 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0926 08:53:08.190052  4406 layer_factory.hpp:77] Creating layer Scale6
I0926 08:53:08.190130  4406 net.cpp:122] Setting up Scale6
I0926 08:53:08.190137  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.190141  4406 net.cpp:137] Memory required for data: 156486800
I0926 08:53:08.190147  4406 layer_factory.hpp:77] Creating layer penlu6
I0926 08:53:08.190155  4406 net.cpp:84] Creating Layer penlu6
I0926 08:53:08.190158  4406 net.cpp:406] penlu6 <- Convolution6
I0926 08:53:08.190165  4406 net.cpp:367] penlu6 -> Convolution6 (in-place)
I0926 08:53:08.190273  4406 net.cpp:122] Setting up penlu6
I0926 08:53:08.190279  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.190282  4406 net.cpp:137] Memory required for data: 161504400
I0926 08:53:08.190289  4406 layer_factory.hpp:77] Creating layer Convolution7
I0926 08:53:08.190299  4406 net.cpp:84] Creating Layer Convolution7
I0926 08:53:08.190301  4406 net.cpp:406] Convolution7 <- Convolution6
I0926 08:53:08.190307  4406 net.cpp:380] Convolution7 -> Convolution7
I0926 08:53:08.190861  4406 net.cpp:122] Setting up Convolution7
I0926 08:53:08.190871  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.190876  4406 net.cpp:137] Memory required for data: 166522000
I0926 08:53:08.190884  4406 layer_factory.hpp:77] Creating layer BatchNorm7
I0926 08:53:08.190891  4406 net.cpp:84] Creating Layer BatchNorm7
I0926 08:53:08.190896  4406 net.cpp:406] BatchNorm7 <- Convolution7
I0926 08:53:08.190901  4406 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0926 08:53:08.191031  4406 net.cpp:122] Setting up BatchNorm7
I0926 08:53:08.191037  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.191041  4406 net.cpp:137] Memory required for data: 171539600
I0926 08:53:08.191053  4406 layer_factory.hpp:77] Creating layer Scale7
I0926 08:53:08.191062  4406 net.cpp:84] Creating Layer Scale7
I0926 08:53:08.191066  4406 net.cpp:406] Scale7 <- Convolution7
I0926 08:53:08.191072  4406 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0926 08:53:08.191102  4406 layer_factory.hpp:77] Creating layer Scale7
I0926 08:53:08.191179  4406 net.cpp:122] Setting up Scale7
I0926 08:53:08.191192  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.191197  4406 net.cpp:137] Memory required for data: 176557200
I0926 08:53:08.191205  4406 layer_factory.hpp:77] Creating layer Eltwise3
I0926 08:53:08.191210  4406 net.cpp:84] Creating Layer Eltwise3
I0926 08:53:08.191215  4406 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I0926 08:53:08.191220  4406 net.cpp:406] Eltwise3 <- Convolution7
I0926 08:53:08.191226  4406 net.cpp:380] Eltwise3 -> Eltwise3
I0926 08:53:08.191246  4406 net.cpp:122] Setting up Eltwise3
I0926 08:53:08.191251  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.191254  4406 net.cpp:137] Memory required for data: 181574800
I0926 08:53:08.191258  4406 layer_factory.hpp:77] Creating layer penlu7
I0926 08:53:08.191267  4406 net.cpp:84] Creating Layer penlu7
I0926 08:53:08.191270  4406 net.cpp:406] penlu7 <- Eltwise3
I0926 08:53:08.191275  4406 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I0926 08:53:08.191382  4406 net.cpp:122] Setting up penlu7
I0926 08:53:08.191388  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.191392  4406 net.cpp:137] Memory required for data: 186592400
I0926 08:53:08.191399  4406 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I0926 08:53:08.191404  4406 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I0926 08:53:08.191408  4406 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I0926 08:53:08.191413  4406 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I0926 08:53:08.191419  4406 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I0926 08:53:08.191444  4406 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I0926 08:53:08.191450  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.191455  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.191459  4406 net.cpp:137] Memory required for data: 196627600
I0926 08:53:08.191463  4406 layer_factory.hpp:77] Creating layer Convolution8
I0926 08:53:08.191473  4406 net.cpp:84] Creating Layer Convolution8
I0926 08:53:08.191476  4406 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I0926 08:53:08.191483  4406 net.cpp:380] Convolution8 -> Convolution8
I0926 08:53:08.192374  4406 net.cpp:122] Setting up Convolution8
I0926 08:53:08.192386  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.192401  4406 net.cpp:137] Memory required for data: 201645200
I0926 08:53:08.192409  4406 layer_factory.hpp:77] Creating layer BatchNorm8
I0926 08:53:08.192417  4406 net.cpp:84] Creating Layer BatchNorm8
I0926 08:53:08.192421  4406 net.cpp:406] BatchNorm8 <- Convolution8
I0926 08:53:08.192438  4406 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0926 08:53:08.192587  4406 net.cpp:122] Setting up BatchNorm8
I0926 08:53:08.192595  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.192600  4406 net.cpp:137] Memory required for data: 206662800
I0926 08:53:08.192606  4406 layer_factory.hpp:77] Creating layer Scale8
I0926 08:53:08.192613  4406 net.cpp:84] Creating Layer Scale8
I0926 08:53:08.192617  4406 net.cpp:406] Scale8 <- Convolution8
I0926 08:53:08.192623  4406 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0926 08:53:08.192654  4406 layer_factory.hpp:77] Creating layer Scale8
I0926 08:53:08.192736  4406 net.cpp:122] Setting up Scale8
I0926 08:53:08.192742  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.192747  4406 net.cpp:137] Memory required for data: 211680400
I0926 08:53:08.192754  4406 layer_factory.hpp:77] Creating layer penlu8
I0926 08:53:08.192761  4406 net.cpp:84] Creating Layer penlu8
I0926 08:53:08.192764  4406 net.cpp:406] penlu8 <- Convolution8
I0926 08:53:08.192770  4406 net.cpp:367] penlu8 -> Convolution8 (in-place)
I0926 08:53:08.192878  4406 net.cpp:122] Setting up penlu8
I0926 08:53:08.192884  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.192888  4406 net.cpp:137] Memory required for data: 216698000
I0926 08:53:08.192895  4406 layer_factory.hpp:77] Creating layer Convolution9
I0926 08:53:08.192904  4406 net.cpp:84] Creating Layer Convolution9
I0926 08:53:08.192914  4406 net.cpp:406] Convolution9 <- Convolution8
I0926 08:53:08.192924  4406 net.cpp:380] Convolution9 -> Convolution9
I0926 08:53:08.193910  4406 net.cpp:122] Setting up Convolution9
I0926 08:53:08.193930  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.193933  4406 net.cpp:137] Memory required for data: 221715600
I0926 08:53:08.193948  4406 layer_factory.hpp:77] Creating layer BatchNorm9
I0926 08:53:08.193964  4406 net.cpp:84] Creating Layer BatchNorm9
I0926 08:53:08.193966  4406 net.cpp:406] BatchNorm9 <- Convolution9
I0926 08:53:08.193971  4406 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0926 08:53:08.194103  4406 net.cpp:122] Setting up BatchNorm9
I0926 08:53:08.194108  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.194111  4406 net.cpp:137] Memory required for data: 226733200
I0926 08:53:08.194116  4406 layer_factory.hpp:77] Creating layer Scale9
I0926 08:53:08.194120  4406 net.cpp:84] Creating Layer Scale9
I0926 08:53:08.194124  4406 net.cpp:406] Scale9 <- Convolution9
I0926 08:53:08.194128  4406 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0926 08:53:08.194154  4406 layer_factory.hpp:77] Creating layer Scale9
I0926 08:53:08.194233  4406 net.cpp:122] Setting up Scale9
I0926 08:53:08.194238  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.194242  4406 net.cpp:137] Memory required for data: 231750800
I0926 08:53:08.194245  4406 layer_factory.hpp:77] Creating layer Eltwise4
I0926 08:53:08.194250  4406 net.cpp:84] Creating Layer Eltwise4
I0926 08:53:08.194253  4406 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I0926 08:53:08.194257  4406 net.cpp:406] Eltwise4 <- Convolution9
I0926 08:53:08.194262  4406 net.cpp:380] Eltwise4 -> Eltwise4
I0926 08:53:08.194278  4406 net.cpp:122] Setting up Eltwise4
I0926 08:53:08.194283  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.194285  4406 net.cpp:137] Memory required for data: 236768400
I0926 08:53:08.194288  4406 layer_factory.hpp:77] Creating layer penlu9
I0926 08:53:08.194293  4406 net.cpp:84] Creating Layer penlu9
I0926 08:53:08.194295  4406 net.cpp:406] penlu9 <- Eltwise4
I0926 08:53:08.194299  4406 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I0926 08:53:08.194406  4406 net.cpp:122] Setting up penlu9
I0926 08:53:08.194412  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.194414  4406 net.cpp:137] Memory required for data: 241786000
I0926 08:53:08.194419  4406 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I0926 08:53:08.194423  4406 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I0926 08:53:08.194427  4406 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I0926 08:53:08.194430  4406 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I0926 08:53:08.194435  4406 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I0926 08:53:08.194458  4406 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I0926 08:53:08.194461  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.194465  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.194468  4406 net.cpp:137] Memory required for data: 251821200
I0926 08:53:08.194469  4406 layer_factory.hpp:77] Creating layer Convolution10
I0926 08:53:08.194478  4406 net.cpp:84] Creating Layer Convolution10
I0926 08:53:08.194480  4406 net.cpp:406] Convolution10 <- Eltwise4_penlu9_0_split_0
I0926 08:53:08.194484  4406 net.cpp:380] Convolution10 -> Convolution10
I0926 08:53:08.195420  4406 net.cpp:122] Setting up Convolution10
I0926 08:53:08.195430  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.195443  4406 net.cpp:137] Memory required for data: 256838800
I0926 08:53:08.195448  4406 layer_factory.hpp:77] Creating layer BatchNorm10
I0926 08:53:08.195454  4406 net.cpp:84] Creating Layer BatchNorm10
I0926 08:53:08.195457  4406 net.cpp:406] BatchNorm10 <- Convolution10
I0926 08:53:08.195462  4406 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0926 08:53:08.195598  4406 net.cpp:122] Setting up BatchNorm10
I0926 08:53:08.195611  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.195614  4406 net.cpp:137] Memory required for data: 261856400
I0926 08:53:08.195619  4406 layer_factory.hpp:77] Creating layer Scale10
I0926 08:53:08.195624  4406 net.cpp:84] Creating Layer Scale10
I0926 08:53:08.195628  4406 net.cpp:406] Scale10 <- Convolution10
I0926 08:53:08.195632  4406 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0926 08:53:08.195660  4406 layer_factory.hpp:77] Creating layer Scale10
I0926 08:53:08.195740  4406 net.cpp:122] Setting up Scale10
I0926 08:53:08.195745  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.195749  4406 net.cpp:137] Memory required for data: 266874000
I0926 08:53:08.195753  4406 layer_factory.hpp:77] Creating layer penlu10
I0926 08:53:08.195760  4406 net.cpp:84] Creating Layer penlu10
I0926 08:53:08.195762  4406 net.cpp:406] penlu10 <- Convolution10
I0926 08:53:08.195766  4406 net.cpp:367] penlu10 -> Convolution10 (in-place)
I0926 08:53:08.195875  4406 net.cpp:122] Setting up penlu10
I0926 08:53:08.195881  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.195884  4406 net.cpp:137] Memory required for data: 271891600
I0926 08:53:08.195889  4406 layer_factory.hpp:77] Creating layer Convolution11
I0926 08:53:08.195895  4406 net.cpp:84] Creating Layer Convolution11
I0926 08:53:08.195899  4406 net.cpp:406] Convolution11 <- Convolution10
I0926 08:53:08.195904  4406 net.cpp:380] Convolution11 -> Convolution11
I0926 08:53:08.196825  4406 net.cpp:122] Setting up Convolution11
I0926 08:53:08.196837  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.196841  4406 net.cpp:137] Memory required for data: 276909200
I0926 08:53:08.196846  4406 layer_factory.hpp:77] Creating layer BatchNorm11
I0926 08:53:08.196851  4406 net.cpp:84] Creating Layer BatchNorm11
I0926 08:53:08.196854  4406 net.cpp:406] BatchNorm11 <- Convolution11
I0926 08:53:08.196859  4406 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0926 08:53:08.196995  4406 net.cpp:122] Setting up BatchNorm11
I0926 08:53:08.197000  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.197003  4406 net.cpp:137] Memory required for data: 281926800
I0926 08:53:08.197008  4406 layer_factory.hpp:77] Creating layer Scale11
I0926 08:53:08.197013  4406 net.cpp:84] Creating Layer Scale11
I0926 08:53:08.197016  4406 net.cpp:406] Scale11 <- Convolution11
I0926 08:53:08.197021  4406 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0926 08:53:08.197047  4406 layer_factory.hpp:77] Creating layer Scale11
I0926 08:53:08.197129  4406 net.cpp:122] Setting up Scale11
I0926 08:53:08.197134  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.197137  4406 net.cpp:137] Memory required for data: 286944400
I0926 08:53:08.197141  4406 layer_factory.hpp:77] Creating layer Eltwise5
I0926 08:53:08.197146  4406 net.cpp:84] Creating Layer Eltwise5
I0926 08:53:08.197149  4406 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I0926 08:53:08.197152  4406 net.cpp:406] Eltwise5 <- Convolution11
I0926 08:53:08.197157  4406 net.cpp:380] Eltwise5 -> Eltwise5
I0926 08:53:08.197173  4406 net.cpp:122] Setting up Eltwise5
I0926 08:53:08.197180  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.197182  4406 net.cpp:137] Memory required for data: 291962000
I0926 08:53:08.197185  4406 layer_factory.hpp:77] Creating layer penlu11
I0926 08:53:08.197190  4406 net.cpp:84] Creating Layer penlu11
I0926 08:53:08.197192  4406 net.cpp:406] penlu11 <- Eltwise5
I0926 08:53:08.197196  4406 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I0926 08:53:08.197309  4406 net.cpp:122] Setting up penlu11
I0926 08:53:08.197315  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.197319  4406 net.cpp:137] Memory required for data: 296979600
I0926 08:53:08.197324  4406 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I0926 08:53:08.197327  4406 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I0926 08:53:08.197331  4406 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I0926 08:53:08.197342  4406 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I0926 08:53:08.197348  4406 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I0926 08:53:08.197373  4406 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I0926 08:53:08.197378  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.197382  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.197384  4406 net.cpp:137] Memory required for data: 307014800
I0926 08:53:08.197387  4406 layer_factory.hpp:77] Creating layer Convolution12
I0926 08:53:08.197393  4406 net.cpp:84] Creating Layer Convolution12
I0926 08:53:08.197397  4406 net.cpp:406] Convolution12 <- Eltwise5_penlu11_0_split_0
I0926 08:53:08.197402  4406 net.cpp:380] Convolution12 -> Convolution12
I0926 08:53:08.198312  4406 net.cpp:122] Setting up Convolution12
I0926 08:53:08.198323  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.198326  4406 net.cpp:137] Memory required for data: 312032400
I0926 08:53:08.198331  4406 layer_factory.hpp:77] Creating layer BatchNorm12
I0926 08:53:08.198338  4406 net.cpp:84] Creating Layer BatchNorm12
I0926 08:53:08.198340  4406 net.cpp:406] BatchNorm12 <- Convolution12
I0926 08:53:08.198344  4406 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0926 08:53:08.198482  4406 net.cpp:122] Setting up BatchNorm12
I0926 08:53:08.198487  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.198490  4406 net.cpp:137] Memory required for data: 317050000
I0926 08:53:08.198495  4406 layer_factory.hpp:77] Creating layer Scale12
I0926 08:53:08.198500  4406 net.cpp:84] Creating Layer Scale12
I0926 08:53:08.198503  4406 net.cpp:406] Scale12 <- Convolution12
I0926 08:53:08.198508  4406 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0926 08:53:08.198535  4406 layer_factory.hpp:77] Creating layer Scale12
I0926 08:53:08.198616  4406 net.cpp:122] Setting up Scale12
I0926 08:53:08.198621  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.198624  4406 net.cpp:137] Memory required for data: 322067600
I0926 08:53:08.198628  4406 layer_factory.hpp:77] Creating layer penlu12
I0926 08:53:08.198633  4406 net.cpp:84] Creating Layer penlu12
I0926 08:53:08.198637  4406 net.cpp:406] penlu12 <- Convolution12
I0926 08:53:08.198642  4406 net.cpp:367] penlu12 -> Convolution12 (in-place)
I0926 08:53:08.198753  4406 net.cpp:122] Setting up penlu12
I0926 08:53:08.198760  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.198761  4406 net.cpp:137] Memory required for data: 327085200
I0926 08:53:08.198766  4406 layer_factory.hpp:77] Creating layer Convolution13
I0926 08:53:08.198774  4406 net.cpp:84] Creating Layer Convolution13
I0926 08:53:08.198777  4406 net.cpp:406] Convolution13 <- Convolution12
I0926 08:53:08.198782  4406 net.cpp:380] Convolution13 -> Convolution13
I0926 08:53:08.199692  4406 net.cpp:122] Setting up Convolution13
I0926 08:53:08.199702  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.199705  4406 net.cpp:137] Memory required for data: 332102800
I0926 08:53:08.199710  4406 layer_factory.hpp:77] Creating layer BatchNorm13
I0926 08:53:08.199717  4406 net.cpp:84] Creating Layer BatchNorm13
I0926 08:53:08.199720  4406 net.cpp:406] BatchNorm13 <- Convolution13
I0926 08:53:08.199724  4406 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0926 08:53:08.199861  4406 net.cpp:122] Setting up BatchNorm13
I0926 08:53:08.199867  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.199869  4406 net.cpp:137] Memory required for data: 337120400
I0926 08:53:08.199874  4406 layer_factory.hpp:77] Creating layer Scale13
I0926 08:53:08.199879  4406 net.cpp:84] Creating Layer Scale13
I0926 08:53:08.199882  4406 net.cpp:406] Scale13 <- Convolution13
I0926 08:53:08.199887  4406 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0926 08:53:08.199914  4406 layer_factory.hpp:77] Creating layer Scale13
I0926 08:53:08.199996  4406 net.cpp:122] Setting up Scale13
I0926 08:53:08.200002  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.200012  4406 net.cpp:137] Memory required for data: 342138000
I0926 08:53:08.200016  4406 layer_factory.hpp:77] Creating layer Eltwise6
I0926 08:53:08.200022  4406 net.cpp:84] Creating Layer Eltwise6
I0926 08:53:08.200026  4406 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I0926 08:53:08.200028  4406 net.cpp:406] Eltwise6 <- Convolution13
I0926 08:53:08.200032  4406 net.cpp:380] Eltwise6 -> Eltwise6
I0926 08:53:08.200053  4406 net.cpp:122] Setting up Eltwise6
I0926 08:53:08.200059  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.200062  4406 net.cpp:137] Memory required for data: 347155600
I0926 08:53:08.200064  4406 layer_factory.hpp:77] Creating layer penlu13
I0926 08:53:08.200073  4406 net.cpp:84] Creating Layer penlu13
I0926 08:53:08.200076  4406 net.cpp:406] penlu13 <- Eltwise6
I0926 08:53:08.200080  4406 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I0926 08:53:08.200196  4406 net.cpp:122] Setting up penlu13
I0926 08:53:08.200201  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.200204  4406 net.cpp:137] Memory required for data: 352173200
I0926 08:53:08.200218  4406 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I0926 08:53:08.200223  4406 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I0926 08:53:08.200227  4406 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I0926 08:53:08.200232  4406 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I0926 08:53:08.200237  4406 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I0926 08:53:08.200263  4406 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I0926 08:53:08.200268  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.200273  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.200274  4406 net.cpp:137] Memory required for data: 362208400
I0926 08:53:08.200276  4406 layer_factory.hpp:77] Creating layer Convolution14
I0926 08:53:08.200284  4406 net.cpp:84] Creating Layer Convolution14
I0926 08:53:08.200286  4406 net.cpp:406] Convolution14 <- Eltwise6_penlu13_0_split_0
I0926 08:53:08.200290  4406 net.cpp:380] Convolution14 -> Convolution14
I0926 08:53:08.201205  4406 net.cpp:122] Setting up Convolution14
I0926 08:53:08.201215  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.201218  4406 net.cpp:137] Memory required for data: 367226000
I0926 08:53:08.201223  4406 layer_factory.hpp:77] Creating layer BatchNorm14
I0926 08:53:08.201227  4406 net.cpp:84] Creating Layer BatchNorm14
I0926 08:53:08.201231  4406 net.cpp:406] BatchNorm14 <- Convolution14
I0926 08:53:08.201236  4406 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0926 08:53:08.201373  4406 net.cpp:122] Setting up BatchNorm14
I0926 08:53:08.201378  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.201380  4406 net.cpp:137] Memory required for data: 372243600
I0926 08:53:08.201385  4406 layer_factory.hpp:77] Creating layer Scale14
I0926 08:53:08.201390  4406 net.cpp:84] Creating Layer Scale14
I0926 08:53:08.201392  4406 net.cpp:406] Scale14 <- Convolution14
I0926 08:53:08.201395  4406 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0926 08:53:08.201421  4406 layer_factory.hpp:77] Creating layer Scale14
I0926 08:53:08.201503  4406 net.cpp:122] Setting up Scale14
I0926 08:53:08.201506  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.201508  4406 net.cpp:137] Memory required for data: 377261200
I0926 08:53:08.201512  4406 layer_factory.hpp:77] Creating layer penlu14
I0926 08:53:08.201517  4406 net.cpp:84] Creating Layer penlu14
I0926 08:53:08.201519  4406 net.cpp:406] penlu14 <- Convolution14
I0926 08:53:08.201524  4406 net.cpp:367] penlu14 -> Convolution14 (in-place)
I0926 08:53:08.201638  4406 net.cpp:122] Setting up penlu14
I0926 08:53:08.201642  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.201645  4406 net.cpp:137] Memory required for data: 382278800
I0926 08:53:08.201649  4406 layer_factory.hpp:77] Creating layer Convolution15
I0926 08:53:08.201656  4406 net.cpp:84] Creating Layer Convolution15
I0926 08:53:08.201666  4406 net.cpp:406] Convolution15 <- Convolution14
I0926 08:53:08.201671  4406 net.cpp:380] Convolution15 -> Convolution15
I0926 08:53:08.202586  4406 net.cpp:122] Setting up Convolution15
I0926 08:53:08.202595  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.202597  4406 net.cpp:137] Memory required for data: 387296400
I0926 08:53:08.202602  4406 layer_factory.hpp:77] Creating layer BatchNorm15
I0926 08:53:08.202607  4406 net.cpp:84] Creating Layer BatchNorm15
I0926 08:53:08.202610  4406 net.cpp:406] BatchNorm15 <- Convolution15
I0926 08:53:08.202613  4406 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0926 08:53:08.202747  4406 net.cpp:122] Setting up BatchNorm15
I0926 08:53:08.202751  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.202754  4406 net.cpp:137] Memory required for data: 392314000
I0926 08:53:08.202759  4406 layer_factory.hpp:77] Creating layer Scale15
I0926 08:53:08.202764  4406 net.cpp:84] Creating Layer Scale15
I0926 08:53:08.202765  4406 net.cpp:406] Scale15 <- Convolution15
I0926 08:53:08.202769  4406 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0926 08:53:08.202795  4406 layer_factory.hpp:77] Creating layer Scale15
I0926 08:53:08.202872  4406 net.cpp:122] Setting up Scale15
I0926 08:53:08.202877  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.202878  4406 net.cpp:137] Memory required for data: 397331600
I0926 08:53:08.202883  4406 layer_factory.hpp:77] Creating layer Eltwise7
I0926 08:53:08.202886  4406 net.cpp:84] Creating Layer Eltwise7
I0926 08:53:08.202889  4406 net.cpp:406] Eltwise7 <- Eltwise6_penlu13_0_split_1
I0926 08:53:08.202893  4406 net.cpp:406] Eltwise7 <- Convolution15
I0926 08:53:08.202895  4406 net.cpp:380] Eltwise7 -> Eltwise7
I0926 08:53:08.202910  4406 net.cpp:122] Setting up Eltwise7
I0926 08:53:08.202914  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.202916  4406 net.cpp:137] Memory required for data: 402349200
I0926 08:53:08.202919  4406 layer_factory.hpp:77] Creating layer penlu15
I0926 08:53:08.202924  4406 net.cpp:84] Creating Layer penlu15
I0926 08:53:08.202926  4406 net.cpp:406] penlu15 <- Eltwise7
I0926 08:53:08.202930  4406 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I0926 08:53:08.203038  4406 net.cpp:122] Setting up penlu15
I0926 08:53:08.203043  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.203045  4406 net.cpp:137] Memory required for data: 407366800
I0926 08:53:08.203049  4406 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I0926 08:53:08.203053  4406 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I0926 08:53:08.203057  4406 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I0926 08:53:08.203059  4406 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I0926 08:53:08.203063  4406 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I0926 08:53:08.203084  4406 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I0926 08:53:08.203088  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.203091  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.203094  4406 net.cpp:137] Memory required for data: 417402000
I0926 08:53:08.203095  4406 layer_factory.hpp:77] Creating layer Convolution16
I0926 08:53:08.203101  4406 net.cpp:84] Creating Layer Convolution16
I0926 08:53:08.203104  4406 net.cpp:406] Convolution16 <- Eltwise7_penlu15_0_split_0
I0926 08:53:08.203109  4406 net.cpp:380] Convolution16 -> Convolution16
I0926 08:53:08.203992  4406 net.cpp:122] Setting up Convolution16
I0926 08:53:08.204000  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.204004  4406 net.cpp:137] Memory required for data: 422419600
I0926 08:53:08.204007  4406 layer_factory.hpp:77] Creating layer BatchNorm16
I0926 08:53:08.204012  4406 net.cpp:84] Creating Layer BatchNorm16
I0926 08:53:08.204015  4406 net.cpp:406] BatchNorm16 <- Convolution16
I0926 08:53:08.204020  4406 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0926 08:53:08.204154  4406 net.cpp:122] Setting up BatchNorm16
I0926 08:53:08.204164  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.204167  4406 net.cpp:137] Memory required for data: 427437200
I0926 08:53:08.204172  4406 layer_factory.hpp:77] Creating layer Scale16
I0926 08:53:08.204176  4406 net.cpp:84] Creating Layer Scale16
I0926 08:53:08.204179  4406 net.cpp:406] Scale16 <- Convolution16
I0926 08:53:08.204182  4406 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0926 08:53:08.204210  4406 layer_factory.hpp:77] Creating layer Scale16
I0926 08:53:08.204289  4406 net.cpp:122] Setting up Scale16
I0926 08:53:08.204295  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.204298  4406 net.cpp:137] Memory required for data: 432454800
I0926 08:53:08.204301  4406 layer_factory.hpp:77] Creating layer penlu16
I0926 08:53:08.204305  4406 net.cpp:84] Creating Layer penlu16
I0926 08:53:08.204308  4406 net.cpp:406] penlu16 <- Convolution16
I0926 08:53:08.204313  4406 net.cpp:367] penlu16 -> Convolution16 (in-place)
I0926 08:53:08.204422  4406 net.cpp:122] Setting up penlu16
I0926 08:53:08.204427  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.204429  4406 net.cpp:137] Memory required for data: 437472400
I0926 08:53:08.204433  4406 layer_factory.hpp:77] Creating layer Convolution17
I0926 08:53:08.204440  4406 net.cpp:84] Creating Layer Convolution17
I0926 08:53:08.204442  4406 net.cpp:406] Convolution17 <- Convolution16
I0926 08:53:08.204447  4406 net.cpp:380] Convolution17 -> Convolution17
I0926 08:53:08.205039  4406 net.cpp:122] Setting up Convolution17
I0926 08:53:08.205049  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.205050  4406 net.cpp:137] Memory required for data: 442490000
I0926 08:53:08.205055  4406 layer_factory.hpp:77] Creating layer BatchNorm17
I0926 08:53:08.205060  4406 net.cpp:84] Creating Layer BatchNorm17
I0926 08:53:08.205062  4406 net.cpp:406] BatchNorm17 <- Convolution17
I0926 08:53:08.205066  4406 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0926 08:53:08.205199  4406 net.cpp:122] Setting up BatchNorm17
I0926 08:53:08.205204  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.205205  4406 net.cpp:137] Memory required for data: 447507600
I0926 08:53:08.205209  4406 layer_factory.hpp:77] Creating layer Scale17
I0926 08:53:08.205214  4406 net.cpp:84] Creating Layer Scale17
I0926 08:53:08.205216  4406 net.cpp:406] Scale17 <- Convolution17
I0926 08:53:08.205220  4406 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0926 08:53:08.205245  4406 layer_factory.hpp:77] Creating layer Scale17
I0926 08:53:08.205323  4406 net.cpp:122] Setting up Scale17
I0926 08:53:08.205327  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.205330  4406 net.cpp:137] Memory required for data: 452525200
I0926 08:53:08.205334  4406 layer_factory.hpp:77] Creating layer Eltwise8
I0926 08:53:08.205338  4406 net.cpp:84] Creating Layer Eltwise8
I0926 08:53:08.205340  4406 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I0926 08:53:08.205343  4406 net.cpp:406] Eltwise8 <- Convolution17
I0926 08:53:08.205348  4406 net.cpp:380] Eltwise8 -> Eltwise8
I0926 08:53:08.205363  4406 net.cpp:122] Setting up Eltwise8
I0926 08:53:08.205366  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.205368  4406 net.cpp:137] Memory required for data: 457542800
I0926 08:53:08.205370  4406 layer_factory.hpp:77] Creating layer penlu17
I0926 08:53:08.205375  4406 net.cpp:84] Creating Layer penlu17
I0926 08:53:08.205377  4406 net.cpp:406] penlu17 <- Eltwise8
I0926 08:53:08.205381  4406 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I0926 08:53:08.205490  4406 net.cpp:122] Setting up penlu17
I0926 08:53:08.205495  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.205497  4406 net.cpp:137] Memory required for data: 462560400
I0926 08:53:08.205502  4406 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I0926 08:53:08.205504  4406 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I0926 08:53:08.205507  4406 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I0926 08:53:08.205516  4406 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I0926 08:53:08.205521  4406 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I0926 08:53:08.205544  4406 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I0926 08:53:08.205549  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.205551  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.205554  4406 net.cpp:137] Memory required for data: 472595600
I0926 08:53:08.205556  4406 layer_factory.hpp:77] Creating layer Convolution18
I0926 08:53:08.205562  4406 net.cpp:84] Creating Layer Convolution18
I0926 08:53:08.205564  4406 net.cpp:406] Convolution18 <- Eltwise8_penlu17_0_split_0
I0926 08:53:08.205569  4406 net.cpp:380] Convolution18 -> Convolution18
I0926 08:53:08.206455  4406 net.cpp:122] Setting up Convolution18
I0926 08:53:08.206465  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.206466  4406 net.cpp:137] Memory required for data: 477613200
I0926 08:53:08.206471  4406 layer_factory.hpp:77] Creating layer BatchNorm18
I0926 08:53:08.206476  4406 net.cpp:84] Creating Layer BatchNorm18
I0926 08:53:08.206480  4406 net.cpp:406] BatchNorm18 <- Convolution18
I0926 08:53:08.206482  4406 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0926 08:53:08.206616  4406 net.cpp:122] Setting up BatchNorm18
I0926 08:53:08.206620  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.206622  4406 net.cpp:137] Memory required for data: 482630800
I0926 08:53:08.206627  4406 layer_factory.hpp:77] Creating layer Scale18
I0926 08:53:08.206631  4406 net.cpp:84] Creating Layer Scale18
I0926 08:53:08.206634  4406 net.cpp:406] Scale18 <- Convolution18
I0926 08:53:08.206638  4406 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0926 08:53:08.206663  4406 layer_factory.hpp:77] Creating layer Scale18
I0926 08:53:08.206743  4406 net.cpp:122] Setting up Scale18
I0926 08:53:08.206748  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.206749  4406 net.cpp:137] Memory required for data: 487648400
I0926 08:53:08.206753  4406 layer_factory.hpp:77] Creating layer penlu18
I0926 08:53:08.206759  4406 net.cpp:84] Creating Layer penlu18
I0926 08:53:08.206761  4406 net.cpp:406] penlu18 <- Convolution18
I0926 08:53:08.206765  4406 net.cpp:367] penlu18 -> Convolution18 (in-place)
I0926 08:53:08.206874  4406 net.cpp:122] Setting up penlu18
I0926 08:53:08.206879  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.206881  4406 net.cpp:137] Memory required for data: 492666000
I0926 08:53:08.206885  4406 layer_factory.hpp:77] Creating layer Convolution19
I0926 08:53:08.206892  4406 net.cpp:84] Creating Layer Convolution19
I0926 08:53:08.206894  4406 net.cpp:406] Convolution19 <- Convolution18
I0926 08:53:08.206898  4406 net.cpp:380] Convolution19 -> Convolution19
I0926 08:53:08.207808  4406 net.cpp:122] Setting up Convolution19
I0926 08:53:08.207818  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.207820  4406 net.cpp:137] Memory required for data: 497683600
I0926 08:53:08.207824  4406 layer_factory.hpp:77] Creating layer BatchNorm19
I0926 08:53:08.207829  4406 net.cpp:84] Creating Layer BatchNorm19
I0926 08:53:08.207832  4406 net.cpp:406] BatchNorm19 <- Convolution19
I0926 08:53:08.207836  4406 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0926 08:53:08.207974  4406 net.cpp:122] Setting up BatchNorm19
I0926 08:53:08.207979  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.207981  4406 net.cpp:137] Memory required for data: 502701200
I0926 08:53:08.207986  4406 layer_factory.hpp:77] Creating layer Scale19
I0926 08:53:08.207991  4406 net.cpp:84] Creating Layer Scale19
I0926 08:53:08.207993  4406 net.cpp:406] Scale19 <- Convolution19
I0926 08:53:08.207998  4406 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0926 08:53:08.208024  4406 layer_factory.hpp:77] Creating layer Scale19
I0926 08:53:08.208106  4406 net.cpp:122] Setting up Scale19
I0926 08:53:08.208109  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.208112  4406 net.cpp:137] Memory required for data: 507718800
I0926 08:53:08.208122  4406 layer_factory.hpp:77] Creating layer Eltwise9
I0926 08:53:08.208127  4406 net.cpp:84] Creating Layer Eltwise9
I0926 08:53:08.208130  4406 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I0926 08:53:08.208133  4406 net.cpp:406] Eltwise9 <- Convolution19
I0926 08:53:08.208138  4406 net.cpp:380] Eltwise9 -> Eltwise9
I0926 08:53:08.208154  4406 net.cpp:122] Setting up Eltwise9
I0926 08:53:08.208159  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.208163  4406 net.cpp:137] Memory required for data: 512736400
I0926 08:53:08.208164  4406 layer_factory.hpp:77] Creating layer penlu19
I0926 08:53:08.208168  4406 net.cpp:84] Creating Layer penlu19
I0926 08:53:08.208171  4406 net.cpp:406] penlu19 <- Eltwise9
I0926 08:53:08.208175  4406 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I0926 08:53:08.208287  4406 net.cpp:122] Setting up penlu19
I0926 08:53:08.208292  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.208293  4406 net.cpp:137] Memory required for data: 517754000
I0926 08:53:08.208298  4406 layer_factory.hpp:77] Creating layer Eltwise9_penlu19_0_split
I0926 08:53:08.208302  4406 net.cpp:84] Creating Layer Eltwise9_penlu19_0_split
I0926 08:53:08.208304  4406 net.cpp:406] Eltwise9_penlu19_0_split <- Eltwise9
I0926 08:53:08.208307  4406 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_0
I0926 08:53:08.208312  4406 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_1
I0926 08:53:08.208335  4406 net.cpp:122] Setting up Eltwise9_penlu19_0_split
I0926 08:53:08.208339  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.208343  4406 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I0926 08:53:08.208344  4406 net.cpp:137] Memory required for data: 527789200
I0926 08:53:08.208346  4406 layer_factory.hpp:77] Creating layer Convolution20
I0926 08:53:08.208353  4406 net.cpp:84] Creating Layer Convolution20
I0926 08:53:08.208355  4406 net.cpp:406] Convolution20 <- Eltwise9_penlu19_0_split_0
I0926 08:53:08.208359  4406 net.cpp:380] Convolution20 -> Convolution20
I0926 08:53:08.209616  4406 net.cpp:122] Setting up Convolution20
I0926 08:53:08.209626  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.209630  4406 net.cpp:137] Memory required for data: 530298000
I0926 08:53:08.209635  4406 layer_factory.hpp:77] Creating layer BatchNorm20
I0926 08:53:08.209640  4406 net.cpp:84] Creating Layer BatchNorm20
I0926 08:53:08.209643  4406 net.cpp:406] BatchNorm20 <- Convolution20
I0926 08:53:08.209646  4406 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0926 08:53:08.209790  4406 net.cpp:122] Setting up BatchNorm20
I0926 08:53:08.209794  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.209797  4406 net.cpp:137] Memory required for data: 532806800
I0926 08:53:08.209802  4406 layer_factory.hpp:77] Creating layer Scale20
I0926 08:53:08.209806  4406 net.cpp:84] Creating Layer Scale20
I0926 08:53:08.209808  4406 net.cpp:406] Scale20 <- Convolution20
I0926 08:53:08.209812  4406 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0926 08:53:08.209839  4406 layer_factory.hpp:77] Creating layer Scale20
I0926 08:53:08.209918  4406 net.cpp:122] Setting up Scale20
I0926 08:53:08.209923  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.209924  4406 net.cpp:137] Memory required for data: 535315600
I0926 08:53:08.209928  4406 layer_factory.hpp:77] Creating layer Convolution21
I0926 08:53:08.209935  4406 net.cpp:84] Creating Layer Convolution21
I0926 08:53:08.209939  4406 net.cpp:406] Convolution21 <- Eltwise9_penlu19_0_split_1
I0926 08:53:08.209944  4406 net.cpp:380] Convolution21 -> Convolution21
I0926 08:53:08.211586  4406 net.cpp:122] Setting up Convolution21
I0926 08:53:08.211596  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.211598  4406 net.cpp:137] Memory required for data: 537824400
I0926 08:53:08.211603  4406 layer_factory.hpp:77] Creating layer BatchNorm21
I0926 08:53:08.211608  4406 net.cpp:84] Creating Layer BatchNorm21
I0926 08:53:08.211618  4406 net.cpp:406] BatchNorm21 <- Convolution21
I0926 08:53:08.211623  4406 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0926 08:53:08.211763  4406 net.cpp:122] Setting up BatchNorm21
I0926 08:53:08.211767  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.211771  4406 net.cpp:137] Memory required for data: 540333200
I0926 08:53:08.211776  4406 layer_factory.hpp:77] Creating layer Scale21
I0926 08:53:08.211779  4406 net.cpp:84] Creating Layer Scale21
I0926 08:53:08.211782  4406 net.cpp:406] Scale21 <- Convolution21
I0926 08:53:08.211786  4406 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0926 08:53:08.211812  4406 layer_factory.hpp:77] Creating layer Scale21
I0926 08:53:08.211890  4406 net.cpp:122] Setting up Scale21
I0926 08:53:08.211895  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.211897  4406 net.cpp:137] Memory required for data: 542842000
I0926 08:53:08.211901  4406 layer_factory.hpp:77] Creating layer penlu20
I0926 08:53:08.211906  4406 net.cpp:84] Creating Layer penlu20
I0926 08:53:08.211908  4406 net.cpp:406] penlu20 <- Convolution21
I0926 08:53:08.211912  4406 net.cpp:367] penlu20 -> Convolution21 (in-place)
I0926 08:53:08.212020  4406 net.cpp:122] Setting up penlu20
I0926 08:53:08.212025  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.212028  4406 net.cpp:137] Memory required for data: 545350800
I0926 08:53:08.212031  4406 layer_factory.hpp:77] Creating layer Convolution22
I0926 08:53:08.212038  4406 net.cpp:84] Creating Layer Convolution22
I0926 08:53:08.212040  4406 net.cpp:406] Convolution22 <- Convolution21
I0926 08:53:08.212045  4406 net.cpp:380] Convolution22 -> Convolution22
I0926 08:53:08.213114  4406 net.cpp:122] Setting up Convolution22
I0926 08:53:08.213122  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.213124  4406 net.cpp:137] Memory required for data: 547859600
I0926 08:53:08.213129  4406 layer_factory.hpp:77] Creating layer BatchNorm22
I0926 08:53:08.213135  4406 net.cpp:84] Creating Layer BatchNorm22
I0926 08:53:08.213136  4406 net.cpp:406] BatchNorm22 <- Convolution22
I0926 08:53:08.213140  4406 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I0926 08:53:08.213273  4406 net.cpp:122] Setting up BatchNorm22
I0926 08:53:08.213277  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.213279  4406 net.cpp:137] Memory required for data: 550368400
I0926 08:53:08.213284  4406 layer_factory.hpp:77] Creating layer Scale22
I0926 08:53:08.213289  4406 net.cpp:84] Creating Layer Scale22
I0926 08:53:08.213290  4406 net.cpp:406] Scale22 <- Convolution22
I0926 08:53:08.213294  4406 net.cpp:367] Scale22 -> Convolution22 (in-place)
I0926 08:53:08.213320  4406 layer_factory.hpp:77] Creating layer Scale22
I0926 08:53:08.213395  4406 net.cpp:122] Setting up Scale22
I0926 08:53:08.213399  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.213402  4406 net.cpp:137] Memory required for data: 552877200
I0926 08:53:08.213405  4406 layer_factory.hpp:77] Creating layer Eltwise10
I0926 08:53:08.213410  4406 net.cpp:84] Creating Layer Eltwise10
I0926 08:53:08.213413  4406 net.cpp:406] Eltwise10 <- Convolution20
I0926 08:53:08.213415  4406 net.cpp:406] Eltwise10 <- Convolution22
I0926 08:53:08.213418  4406 net.cpp:380] Eltwise10 -> Eltwise10
I0926 08:53:08.213434  4406 net.cpp:122] Setting up Eltwise10
I0926 08:53:08.213438  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.213439  4406 net.cpp:137] Memory required for data: 555386000
I0926 08:53:08.213441  4406 layer_factory.hpp:77] Creating layer penlu21
I0926 08:53:08.213448  4406 net.cpp:84] Creating Layer penlu21
I0926 08:53:08.213449  4406 net.cpp:406] penlu21 <- Eltwise10
I0926 08:53:08.213452  4406 net.cpp:367] penlu21 -> Eltwise10 (in-place)
I0926 08:53:08.213558  4406 net.cpp:122] Setting up penlu21
I0926 08:53:08.213563  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.213565  4406 net.cpp:137] Memory required for data: 557894800
I0926 08:53:08.213569  4406 layer_factory.hpp:77] Creating layer Eltwise10_penlu21_0_split
I0926 08:53:08.213579  4406 net.cpp:84] Creating Layer Eltwise10_penlu21_0_split
I0926 08:53:08.213582  4406 net.cpp:406] Eltwise10_penlu21_0_split <- Eltwise10
I0926 08:53:08.213587  4406 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_0
I0926 08:53:08.213591  4406 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_1
I0926 08:53:08.213613  4406 net.cpp:122] Setting up Eltwise10_penlu21_0_split
I0926 08:53:08.213618  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.213620  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.213623  4406 net.cpp:137] Memory required for data: 562912400
I0926 08:53:08.213624  4406 layer_factory.hpp:77] Creating layer Convolution23
I0926 08:53:08.213630  4406 net.cpp:84] Creating Layer Convolution23
I0926 08:53:08.213634  4406 net.cpp:406] Convolution23 <- Eltwise10_penlu21_0_split_0
I0926 08:53:08.213637  4406 net.cpp:380] Convolution23 -> Convolution23
I0926 08:53:08.214962  4406 net.cpp:122] Setting up Convolution23
I0926 08:53:08.214970  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.214972  4406 net.cpp:137] Memory required for data: 565421200
I0926 08:53:08.214977  4406 layer_factory.hpp:77] Creating layer BatchNorm23
I0926 08:53:08.214982  4406 net.cpp:84] Creating Layer BatchNorm23
I0926 08:53:08.214985  4406 net.cpp:406] BatchNorm23 <- Convolution23
I0926 08:53:08.214988  4406 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I0926 08:53:08.215124  4406 net.cpp:122] Setting up BatchNorm23
I0926 08:53:08.215129  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.215131  4406 net.cpp:137] Memory required for data: 567930000
I0926 08:53:08.215137  4406 layer_factory.hpp:77] Creating layer Scale23
I0926 08:53:08.215142  4406 net.cpp:84] Creating Layer Scale23
I0926 08:53:08.215143  4406 net.cpp:406] Scale23 <- Convolution23
I0926 08:53:08.215147  4406 net.cpp:367] Scale23 -> Convolution23 (in-place)
I0926 08:53:08.215173  4406 layer_factory.hpp:77] Creating layer Scale23
I0926 08:53:08.215248  4406 net.cpp:122] Setting up Scale23
I0926 08:53:08.215253  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.215255  4406 net.cpp:137] Memory required for data: 570438800
I0926 08:53:08.215260  4406 layer_factory.hpp:77] Creating layer penlu22
I0926 08:53:08.215265  4406 net.cpp:84] Creating Layer penlu22
I0926 08:53:08.215267  4406 net.cpp:406] penlu22 <- Convolution23
I0926 08:53:08.215271  4406 net.cpp:367] penlu22 -> Convolution23 (in-place)
I0926 08:53:08.215375  4406 net.cpp:122] Setting up penlu22
I0926 08:53:08.215380  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.215382  4406 net.cpp:137] Memory required for data: 572947600
I0926 08:53:08.215386  4406 layer_factory.hpp:77] Creating layer Convolution24
I0926 08:53:08.215392  4406 net.cpp:84] Creating Layer Convolution24
I0926 08:53:08.215394  4406 net.cpp:406] Convolution24 <- Convolution23
I0926 08:53:08.215399  4406 net.cpp:380] Convolution24 -> Convolution24
I0926 08:53:08.216431  4406 net.cpp:122] Setting up Convolution24
I0926 08:53:08.216440  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.216442  4406 net.cpp:137] Memory required for data: 575456400
I0926 08:53:08.216447  4406 layer_factory.hpp:77] Creating layer BatchNorm24
I0926 08:53:08.216454  4406 net.cpp:84] Creating Layer BatchNorm24
I0926 08:53:08.216455  4406 net.cpp:406] BatchNorm24 <- Convolution24
I0926 08:53:08.216459  4406 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I0926 08:53:08.216621  4406 net.cpp:122] Setting up BatchNorm24
I0926 08:53:08.216626  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.216629  4406 net.cpp:137] Memory required for data: 577965200
I0926 08:53:08.216634  4406 layer_factory.hpp:77] Creating layer Scale24
I0926 08:53:08.216637  4406 net.cpp:84] Creating Layer Scale24
I0926 08:53:08.216639  4406 net.cpp:406] Scale24 <- Convolution24
I0926 08:53:08.216644  4406 net.cpp:367] Scale24 -> Convolution24 (in-place)
I0926 08:53:08.216677  4406 layer_factory.hpp:77] Creating layer Scale24
I0926 08:53:08.216755  4406 net.cpp:122] Setting up Scale24
I0926 08:53:08.216760  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.216763  4406 net.cpp:137] Memory required for data: 580474000
I0926 08:53:08.216766  4406 layer_factory.hpp:77] Creating layer Eltwise11
I0926 08:53:08.216770  4406 net.cpp:84] Creating Layer Eltwise11
I0926 08:53:08.216773  4406 net.cpp:406] Eltwise11 <- Eltwise10_penlu21_0_split_1
I0926 08:53:08.216775  4406 net.cpp:406] Eltwise11 <- Convolution24
I0926 08:53:08.216779  4406 net.cpp:380] Eltwise11 -> Eltwise11
I0926 08:53:08.216795  4406 net.cpp:122] Setting up Eltwise11
I0926 08:53:08.216799  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.216801  4406 net.cpp:137] Memory required for data: 582982800
I0926 08:53:08.216804  4406 layer_factory.hpp:77] Creating layer penlu23
I0926 08:53:08.216807  4406 net.cpp:84] Creating Layer penlu23
I0926 08:53:08.216810  4406 net.cpp:406] penlu23 <- Eltwise11
I0926 08:53:08.216814  4406 net.cpp:367] penlu23 -> Eltwise11 (in-place)
I0926 08:53:08.216920  4406 net.cpp:122] Setting up penlu23
I0926 08:53:08.216925  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.216928  4406 net.cpp:137] Memory required for data: 585491600
I0926 08:53:08.216931  4406 layer_factory.hpp:77] Creating layer Eltwise11_penlu23_0_split
I0926 08:53:08.216934  4406 net.cpp:84] Creating Layer Eltwise11_penlu23_0_split
I0926 08:53:08.216936  4406 net.cpp:406] Eltwise11_penlu23_0_split <- Eltwise11
I0926 08:53:08.216940  4406 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_0
I0926 08:53:08.216944  4406 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_1
I0926 08:53:08.216967  4406 net.cpp:122] Setting up Eltwise11_penlu23_0_split
I0926 08:53:08.216971  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.216974  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.216975  4406 net.cpp:137] Memory required for data: 590509200
I0926 08:53:08.216977  4406 layer_factory.hpp:77] Creating layer Convolution25
I0926 08:53:08.216984  4406 net.cpp:84] Creating Layer Convolution25
I0926 08:53:08.216986  4406 net.cpp:406] Convolution25 <- Eltwise11_penlu23_0_split_0
I0926 08:53:08.216990  4406 net.cpp:380] Convolution25 -> Convolution25
I0926 08:53:08.218021  4406 net.cpp:122] Setting up Convolution25
I0926 08:53:08.218031  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.218034  4406 net.cpp:137] Memory required for data: 593018000
I0926 08:53:08.218039  4406 layer_factory.hpp:77] Creating layer BatchNorm25
I0926 08:53:08.218042  4406 net.cpp:84] Creating Layer BatchNorm25
I0926 08:53:08.218045  4406 net.cpp:406] BatchNorm25 <- Convolution25
I0926 08:53:08.218050  4406 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I0926 08:53:08.218185  4406 net.cpp:122] Setting up BatchNorm25
I0926 08:53:08.218189  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.218191  4406 net.cpp:137] Memory required for data: 595526800
I0926 08:53:08.218196  4406 layer_factory.hpp:77] Creating layer Scale25
I0926 08:53:08.218200  4406 net.cpp:84] Creating Layer Scale25
I0926 08:53:08.218202  4406 net.cpp:406] Scale25 <- Convolution25
I0926 08:53:08.218206  4406 net.cpp:367] Scale25 -> Convolution25 (in-place)
I0926 08:53:08.218233  4406 layer_factory.hpp:77] Creating layer Scale25
I0926 08:53:08.218309  4406 net.cpp:122] Setting up Scale25
I0926 08:53:08.218314  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.218317  4406 net.cpp:137] Memory required for data: 598035600
I0926 08:53:08.218320  4406 layer_factory.hpp:77] Creating layer penlu24
I0926 08:53:08.218324  4406 net.cpp:84] Creating Layer penlu24
I0926 08:53:08.218327  4406 net.cpp:406] penlu24 <- Convolution25
I0926 08:53:08.218330  4406 net.cpp:367] penlu24 -> Convolution25 (in-place)
I0926 08:53:08.218437  4406 net.cpp:122] Setting up penlu24
I0926 08:53:08.218441  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.218451  4406 net.cpp:137] Memory required for data: 600544400
I0926 08:53:08.218454  4406 layer_factory.hpp:77] Creating layer Convolution26
I0926 08:53:08.218461  4406 net.cpp:84] Creating Layer Convolution26
I0926 08:53:08.218464  4406 net.cpp:406] Convolution26 <- Convolution25
I0926 08:53:08.218468  4406 net.cpp:380] Convolution26 -> Convolution26
I0926 08:53:08.219501  4406 net.cpp:122] Setting up Convolution26
I0926 08:53:08.219509  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.219512  4406 net.cpp:137] Memory required for data: 603053200
I0926 08:53:08.219516  4406 layer_factory.hpp:77] Creating layer BatchNorm26
I0926 08:53:08.219521  4406 net.cpp:84] Creating Layer BatchNorm26
I0926 08:53:08.219524  4406 net.cpp:406] BatchNorm26 <- Convolution26
I0926 08:53:08.219528  4406 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I0926 08:53:08.219666  4406 net.cpp:122] Setting up BatchNorm26
I0926 08:53:08.219669  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.219671  4406 net.cpp:137] Memory required for data: 605562000
I0926 08:53:08.219676  4406 layer_factory.hpp:77] Creating layer Scale26
I0926 08:53:08.219681  4406 net.cpp:84] Creating Layer Scale26
I0926 08:53:08.219682  4406 net.cpp:406] Scale26 <- Convolution26
I0926 08:53:08.219686  4406 net.cpp:367] Scale26 -> Convolution26 (in-place)
I0926 08:53:08.219712  4406 layer_factory.hpp:77] Creating layer Scale26
I0926 08:53:08.219790  4406 net.cpp:122] Setting up Scale26
I0926 08:53:08.219795  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.219797  4406 net.cpp:137] Memory required for data: 608070800
I0926 08:53:08.219801  4406 layer_factory.hpp:77] Creating layer Eltwise12
I0926 08:53:08.219805  4406 net.cpp:84] Creating Layer Eltwise12
I0926 08:53:08.219807  4406 net.cpp:406] Eltwise12 <- Eltwise11_penlu23_0_split_1
I0926 08:53:08.219810  4406 net.cpp:406] Eltwise12 <- Convolution26
I0926 08:53:08.219813  4406 net.cpp:380] Eltwise12 -> Eltwise12
I0926 08:53:08.219830  4406 net.cpp:122] Setting up Eltwise12
I0926 08:53:08.219833  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.219835  4406 net.cpp:137] Memory required for data: 610579600
I0926 08:53:08.219837  4406 layer_factory.hpp:77] Creating layer penlu25
I0926 08:53:08.219842  4406 net.cpp:84] Creating Layer penlu25
I0926 08:53:08.219846  4406 net.cpp:406] penlu25 <- Eltwise12
I0926 08:53:08.219848  4406 net.cpp:367] penlu25 -> Eltwise12 (in-place)
I0926 08:53:08.219959  4406 net.cpp:122] Setting up penlu25
I0926 08:53:08.219964  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.219966  4406 net.cpp:137] Memory required for data: 613088400
I0926 08:53:08.219990  4406 layer_factory.hpp:77] Creating layer Eltwise12_penlu25_0_split
I0926 08:53:08.220000  4406 net.cpp:84] Creating Layer Eltwise12_penlu25_0_split
I0926 08:53:08.220003  4406 net.cpp:406] Eltwise12_penlu25_0_split <- Eltwise12
I0926 08:53:08.220006  4406 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_0
I0926 08:53:08.220015  4406 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_1
I0926 08:53:08.220039  4406 net.cpp:122] Setting up Eltwise12_penlu25_0_split
I0926 08:53:08.220044  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.220046  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.220048  4406 net.cpp:137] Memory required for data: 618106000
I0926 08:53:08.220052  4406 layer_factory.hpp:77] Creating layer Convolution27
I0926 08:53:08.220057  4406 net.cpp:84] Creating Layer Convolution27
I0926 08:53:08.220060  4406 net.cpp:406] Convolution27 <- Eltwise12_penlu25_0_split_0
I0926 08:53:08.220064  4406 net.cpp:380] Convolution27 -> Convolution27
I0926 08:53:08.220780  4406 net.cpp:122] Setting up Convolution27
I0926 08:53:08.220788  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.220790  4406 net.cpp:137] Memory required for data: 620614800
I0926 08:53:08.220795  4406 layer_factory.hpp:77] Creating layer BatchNorm27
I0926 08:53:08.220799  4406 net.cpp:84] Creating Layer BatchNorm27
I0926 08:53:08.220808  4406 net.cpp:406] BatchNorm27 <- Convolution27
I0926 08:53:08.220813  4406 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I0926 08:53:08.220950  4406 net.cpp:122] Setting up BatchNorm27
I0926 08:53:08.220955  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.220957  4406 net.cpp:137] Memory required for data: 623123600
I0926 08:53:08.220963  4406 layer_factory.hpp:77] Creating layer Scale27
I0926 08:53:08.220966  4406 net.cpp:84] Creating Layer Scale27
I0926 08:53:08.220968  4406 net.cpp:406] Scale27 <- Convolution27
I0926 08:53:08.220973  4406 net.cpp:367] Scale27 -> Convolution27 (in-place)
I0926 08:53:08.220999  4406 layer_factory.hpp:77] Creating layer Scale27
I0926 08:53:08.221076  4406 net.cpp:122] Setting up Scale27
I0926 08:53:08.221079  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.221082  4406 net.cpp:137] Memory required for data: 625632400
I0926 08:53:08.221086  4406 layer_factory.hpp:77] Creating layer penlu26
I0926 08:53:08.221091  4406 net.cpp:84] Creating Layer penlu26
I0926 08:53:08.221093  4406 net.cpp:406] penlu26 <- Convolution27
I0926 08:53:08.221097  4406 net.cpp:367] penlu26 -> Convolution27 (in-place)
I0926 08:53:08.221204  4406 net.cpp:122] Setting up penlu26
I0926 08:53:08.221210  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.221211  4406 net.cpp:137] Memory required for data: 628141200
I0926 08:53:08.221215  4406 layer_factory.hpp:77] Creating layer Convolution28
I0926 08:53:08.221221  4406 net.cpp:84] Creating Layer Convolution28
I0926 08:53:08.221225  4406 net.cpp:406] Convolution28 <- Convolution27
I0926 08:53:08.221228  4406 net.cpp:380] Convolution28 -> Convolution28
I0926 08:53:08.222249  4406 net.cpp:122] Setting up Convolution28
I0926 08:53:08.222259  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.222260  4406 net.cpp:137] Memory required for data: 630650000
I0926 08:53:08.222265  4406 layer_factory.hpp:77] Creating layer BatchNorm28
I0926 08:53:08.222270  4406 net.cpp:84] Creating Layer BatchNorm28
I0926 08:53:08.222272  4406 net.cpp:406] BatchNorm28 <- Convolution28
I0926 08:53:08.222276  4406 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I0926 08:53:08.222414  4406 net.cpp:122] Setting up BatchNorm28
I0926 08:53:08.222419  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.222420  4406 net.cpp:137] Memory required for data: 633158800
I0926 08:53:08.222425  4406 layer_factory.hpp:77] Creating layer Scale28
I0926 08:53:08.222429  4406 net.cpp:84] Creating Layer Scale28
I0926 08:53:08.222431  4406 net.cpp:406] Scale28 <- Convolution28
I0926 08:53:08.222434  4406 net.cpp:367] Scale28 -> Convolution28 (in-place)
I0926 08:53:08.222462  4406 layer_factory.hpp:77] Creating layer Scale28
I0926 08:53:08.222540  4406 net.cpp:122] Setting up Scale28
I0926 08:53:08.222545  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.222548  4406 net.cpp:137] Memory required for data: 635667600
I0926 08:53:08.222551  4406 layer_factory.hpp:77] Creating layer Eltwise13
I0926 08:53:08.222555  4406 net.cpp:84] Creating Layer Eltwise13
I0926 08:53:08.222558  4406 net.cpp:406] Eltwise13 <- Eltwise12_penlu25_0_split_1
I0926 08:53:08.222560  4406 net.cpp:406] Eltwise13 <- Convolution28
I0926 08:53:08.222563  4406 net.cpp:380] Eltwise13 -> Eltwise13
I0926 08:53:08.222580  4406 net.cpp:122] Setting up Eltwise13
I0926 08:53:08.222584  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.222586  4406 net.cpp:137] Memory required for data: 638176400
I0926 08:53:08.222589  4406 layer_factory.hpp:77] Creating layer penlu27
I0926 08:53:08.222594  4406 net.cpp:84] Creating Layer penlu27
I0926 08:53:08.222595  4406 net.cpp:406] penlu27 <- Eltwise13
I0926 08:53:08.222599  4406 net.cpp:367] penlu27 -> Eltwise13 (in-place)
I0926 08:53:08.222708  4406 net.cpp:122] Setting up penlu27
I0926 08:53:08.222712  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.222714  4406 net.cpp:137] Memory required for data: 640685200
I0926 08:53:08.222718  4406 layer_factory.hpp:77] Creating layer Eltwise13_penlu27_0_split
I0926 08:53:08.222728  4406 net.cpp:84] Creating Layer Eltwise13_penlu27_0_split
I0926 08:53:08.222731  4406 net.cpp:406] Eltwise13_penlu27_0_split <- Eltwise13
I0926 08:53:08.222735  4406 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_0
I0926 08:53:08.222739  4406 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_1
I0926 08:53:08.222764  4406 net.cpp:122] Setting up Eltwise13_penlu27_0_split
I0926 08:53:08.222767  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.222769  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.222771  4406 net.cpp:137] Memory required for data: 645702800
I0926 08:53:08.222774  4406 layer_factory.hpp:77] Creating layer Convolution29
I0926 08:53:08.222779  4406 net.cpp:84] Creating Layer Convolution29
I0926 08:53:08.222781  4406 net.cpp:406] Convolution29 <- Eltwise13_penlu27_0_split_0
I0926 08:53:08.222786  4406 net.cpp:380] Convolution29 -> Convolution29
I0926 08:53:08.223819  4406 net.cpp:122] Setting up Convolution29
I0926 08:53:08.223826  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.223829  4406 net.cpp:137] Memory required for data: 648211600
I0926 08:53:08.223834  4406 layer_factory.hpp:77] Creating layer BatchNorm29
I0926 08:53:08.223839  4406 net.cpp:84] Creating Layer BatchNorm29
I0926 08:53:08.223841  4406 net.cpp:406] BatchNorm29 <- Convolution29
I0926 08:53:08.223845  4406 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I0926 08:53:08.223981  4406 net.cpp:122] Setting up BatchNorm29
I0926 08:53:08.223985  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.223989  4406 net.cpp:137] Memory required for data: 650720400
I0926 08:53:08.223992  4406 layer_factory.hpp:77] Creating layer Scale29
I0926 08:53:08.223996  4406 net.cpp:84] Creating Layer Scale29
I0926 08:53:08.223999  4406 net.cpp:406] Scale29 <- Convolution29
I0926 08:53:08.224002  4406 net.cpp:367] Scale29 -> Convolution29 (in-place)
I0926 08:53:08.224030  4406 layer_factory.hpp:77] Creating layer Scale29
I0926 08:53:08.224107  4406 net.cpp:122] Setting up Scale29
I0926 08:53:08.224112  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.224114  4406 net.cpp:137] Memory required for data: 653229200
I0926 08:53:08.224117  4406 layer_factory.hpp:77] Creating layer penlu28
I0926 08:53:08.224123  4406 net.cpp:84] Creating Layer penlu28
I0926 08:53:08.224124  4406 net.cpp:406] penlu28 <- Convolution29
I0926 08:53:08.224128  4406 net.cpp:367] penlu28 -> Convolution29 (in-place)
I0926 08:53:08.224238  4406 net.cpp:122] Setting up penlu28
I0926 08:53:08.224242  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.224244  4406 net.cpp:137] Memory required for data: 655738000
I0926 08:53:08.224248  4406 layer_factory.hpp:77] Creating layer Convolution30
I0926 08:53:08.224256  4406 net.cpp:84] Creating Layer Convolution30
I0926 08:53:08.224257  4406 net.cpp:406] Convolution30 <- Convolution29
I0926 08:53:08.224262  4406 net.cpp:380] Convolution30 -> Convolution30
I0926 08:53:08.225298  4406 net.cpp:122] Setting up Convolution30
I0926 08:53:08.225307  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.225311  4406 net.cpp:137] Memory required for data: 658246800
I0926 08:53:08.225314  4406 layer_factory.hpp:77] Creating layer BatchNorm30
I0926 08:53:08.225319  4406 net.cpp:84] Creating Layer BatchNorm30
I0926 08:53:08.225322  4406 net.cpp:406] BatchNorm30 <- Convolution30
I0926 08:53:08.225327  4406 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I0926 08:53:08.225461  4406 net.cpp:122] Setting up BatchNorm30
I0926 08:53:08.225466  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.225467  4406 net.cpp:137] Memory required for data: 660755600
I0926 08:53:08.225472  4406 layer_factory.hpp:77] Creating layer Scale30
I0926 08:53:08.225476  4406 net.cpp:84] Creating Layer Scale30
I0926 08:53:08.225478  4406 net.cpp:406] Scale30 <- Convolution30
I0926 08:53:08.225481  4406 net.cpp:367] Scale30 -> Convolution30 (in-place)
I0926 08:53:08.225518  4406 layer_factory.hpp:77] Creating layer Scale30
I0926 08:53:08.225596  4406 net.cpp:122] Setting up Scale30
I0926 08:53:08.225601  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.225603  4406 net.cpp:137] Memory required for data: 663264400
I0926 08:53:08.225607  4406 layer_factory.hpp:77] Creating layer Eltwise14
I0926 08:53:08.225611  4406 net.cpp:84] Creating Layer Eltwise14
I0926 08:53:08.225615  4406 net.cpp:406] Eltwise14 <- Eltwise13_penlu27_0_split_1
I0926 08:53:08.225617  4406 net.cpp:406] Eltwise14 <- Convolution30
I0926 08:53:08.225620  4406 net.cpp:380] Eltwise14 -> Eltwise14
I0926 08:53:08.225636  4406 net.cpp:122] Setting up Eltwise14
I0926 08:53:08.225639  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.225641  4406 net.cpp:137] Memory required for data: 665773200
I0926 08:53:08.225643  4406 layer_factory.hpp:77] Creating layer penlu29
I0926 08:53:08.225649  4406 net.cpp:84] Creating Layer penlu29
I0926 08:53:08.225651  4406 net.cpp:406] penlu29 <- Eltwise14
I0926 08:53:08.225654  4406 net.cpp:367] penlu29 -> Eltwise14 (in-place)
I0926 08:53:08.225764  4406 net.cpp:122] Setting up penlu29
I0926 08:53:08.225767  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.225769  4406 net.cpp:137] Memory required for data: 668282000
I0926 08:53:08.225774  4406 layer_factory.hpp:77] Creating layer Eltwise14_penlu29_0_split
I0926 08:53:08.225777  4406 net.cpp:84] Creating Layer Eltwise14_penlu29_0_split
I0926 08:53:08.225780  4406 net.cpp:406] Eltwise14_penlu29_0_split <- Eltwise14
I0926 08:53:08.225783  4406 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_0
I0926 08:53:08.225787  4406 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_1
I0926 08:53:08.225810  4406 net.cpp:122] Setting up Eltwise14_penlu29_0_split
I0926 08:53:08.225813  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.225816  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.225818  4406 net.cpp:137] Memory required for data: 673299600
I0926 08:53:08.225821  4406 layer_factory.hpp:77] Creating layer Convolution31
I0926 08:53:08.225826  4406 net.cpp:84] Creating Layer Convolution31
I0926 08:53:08.225829  4406 net.cpp:406] Convolution31 <- Eltwise14_penlu29_0_split_0
I0926 08:53:08.225833  4406 net.cpp:380] Convolution31 -> Convolution31
I0926 08:53:08.226868  4406 net.cpp:122] Setting up Convolution31
I0926 08:53:08.226876  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.226879  4406 net.cpp:137] Memory required for data: 675808400
I0926 08:53:08.226883  4406 layer_factory.hpp:77] Creating layer BatchNorm31
I0926 08:53:08.226888  4406 net.cpp:84] Creating Layer BatchNorm31
I0926 08:53:08.226891  4406 net.cpp:406] BatchNorm31 <- Convolution31
I0926 08:53:08.226896  4406 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I0926 08:53:08.227032  4406 net.cpp:122] Setting up BatchNorm31
I0926 08:53:08.227036  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.227039  4406 net.cpp:137] Memory required for data: 678317200
I0926 08:53:08.227043  4406 layer_factory.hpp:77] Creating layer Scale31
I0926 08:53:08.227047  4406 net.cpp:84] Creating Layer Scale31
I0926 08:53:08.227051  4406 net.cpp:406] Scale31 <- Convolution31
I0926 08:53:08.227053  4406 net.cpp:367] Scale31 -> Convolution31 (in-place)
I0926 08:53:08.227080  4406 layer_factory.hpp:77] Creating layer Scale31
I0926 08:53:08.227159  4406 net.cpp:122] Setting up Scale31
I0926 08:53:08.227164  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.227166  4406 net.cpp:137] Memory required for data: 680826000
I0926 08:53:08.227170  4406 layer_factory.hpp:77] Creating layer penlu30
I0926 08:53:08.227175  4406 net.cpp:84] Creating Layer penlu30
I0926 08:53:08.227179  4406 net.cpp:406] penlu30 <- Convolution31
I0926 08:53:08.227181  4406 net.cpp:367] penlu30 -> Convolution31 (in-place)
I0926 08:53:08.227291  4406 net.cpp:122] Setting up penlu30
I0926 08:53:08.227295  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.227303  4406 net.cpp:137] Memory required for data: 683334800
I0926 08:53:08.227308  4406 layer_factory.hpp:77] Creating layer Convolution32
I0926 08:53:08.227316  4406 net.cpp:84] Creating Layer Convolution32
I0926 08:53:08.227319  4406 net.cpp:406] Convolution32 <- Convolution31
I0926 08:53:08.227322  4406 net.cpp:380] Convolution32 -> Convolution32
I0926 08:53:08.228361  4406 net.cpp:122] Setting up Convolution32
I0926 08:53:08.228370  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.228373  4406 net.cpp:137] Memory required for data: 685843600
I0926 08:53:08.228376  4406 layer_factory.hpp:77] Creating layer BatchNorm32
I0926 08:53:08.228381  4406 net.cpp:84] Creating Layer BatchNorm32
I0926 08:53:08.228385  4406 net.cpp:406] BatchNorm32 <- Convolution32
I0926 08:53:08.228389  4406 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I0926 08:53:08.228530  4406 net.cpp:122] Setting up BatchNorm32
I0926 08:53:08.228535  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.228538  4406 net.cpp:137] Memory required for data: 688352400
I0926 08:53:08.228543  4406 layer_factory.hpp:77] Creating layer Scale32
I0926 08:53:08.228549  4406 net.cpp:84] Creating Layer Scale32
I0926 08:53:08.228550  4406 net.cpp:406] Scale32 <- Convolution32
I0926 08:53:08.228554  4406 net.cpp:367] Scale32 -> Convolution32 (in-place)
I0926 08:53:08.228580  4406 layer_factory.hpp:77] Creating layer Scale32
I0926 08:53:08.228660  4406 net.cpp:122] Setting up Scale32
I0926 08:53:08.228664  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.228667  4406 net.cpp:137] Memory required for data: 690861200
I0926 08:53:08.228670  4406 layer_factory.hpp:77] Creating layer Eltwise15
I0926 08:53:08.228674  4406 net.cpp:84] Creating Layer Eltwise15
I0926 08:53:08.228677  4406 net.cpp:406] Eltwise15 <- Eltwise14_penlu29_0_split_1
I0926 08:53:08.228679  4406 net.cpp:406] Eltwise15 <- Convolution32
I0926 08:53:08.228683  4406 net.cpp:380] Eltwise15 -> Eltwise15
I0926 08:53:08.228700  4406 net.cpp:122] Setting up Eltwise15
I0926 08:53:08.228703  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.228705  4406 net.cpp:137] Memory required for data: 693370000
I0926 08:53:08.228708  4406 layer_factory.hpp:77] Creating layer penlu31
I0926 08:53:08.228713  4406 net.cpp:84] Creating Layer penlu31
I0926 08:53:08.228715  4406 net.cpp:406] penlu31 <- Eltwise15
I0926 08:53:08.228718  4406 net.cpp:367] penlu31 -> Eltwise15 (in-place)
I0926 08:53:08.228828  4406 net.cpp:122] Setting up penlu31
I0926 08:53:08.228833  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.228835  4406 net.cpp:137] Memory required for data: 695878800
I0926 08:53:08.228839  4406 layer_factory.hpp:77] Creating layer Eltwise15_penlu31_0_split
I0926 08:53:08.228842  4406 net.cpp:84] Creating Layer Eltwise15_penlu31_0_split
I0926 08:53:08.228845  4406 net.cpp:406] Eltwise15_penlu31_0_split <- Eltwise15
I0926 08:53:08.228850  4406 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_0
I0926 08:53:08.228854  4406 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_1
I0926 08:53:08.228878  4406 net.cpp:122] Setting up Eltwise15_penlu31_0_split
I0926 08:53:08.228881  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.228883  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.228885  4406 net.cpp:137] Memory required for data: 700896400
I0926 08:53:08.228888  4406 layer_factory.hpp:77] Creating layer Convolution33
I0926 08:53:08.228894  4406 net.cpp:84] Creating Layer Convolution33
I0926 08:53:08.228898  4406 net.cpp:406] Convolution33 <- Eltwise15_penlu31_0_split_0
I0926 08:53:08.228901  4406 net.cpp:380] Convolution33 -> Convolution33
I0926 08:53:08.230247  4406 net.cpp:122] Setting up Convolution33
I0926 08:53:08.230255  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.230258  4406 net.cpp:137] Memory required for data: 703405200
I0926 08:53:08.230263  4406 layer_factory.hpp:77] Creating layer BatchNorm33
I0926 08:53:08.230268  4406 net.cpp:84] Creating Layer BatchNorm33
I0926 08:53:08.230276  4406 net.cpp:406] BatchNorm33 <- Convolution33
I0926 08:53:08.230281  4406 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I0926 08:53:08.230424  4406 net.cpp:122] Setting up BatchNorm33
I0926 08:53:08.230429  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.230432  4406 net.cpp:137] Memory required for data: 705914000
I0926 08:53:08.230437  4406 layer_factory.hpp:77] Creating layer Scale33
I0926 08:53:08.230440  4406 net.cpp:84] Creating Layer Scale33
I0926 08:53:08.230443  4406 net.cpp:406] Scale33 <- Convolution33
I0926 08:53:08.230446  4406 net.cpp:367] Scale33 -> Convolution33 (in-place)
I0926 08:53:08.230473  4406 layer_factory.hpp:77] Creating layer Scale33
I0926 08:53:08.230553  4406 net.cpp:122] Setting up Scale33
I0926 08:53:08.230557  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.230559  4406 net.cpp:137] Memory required for data: 708422800
I0926 08:53:08.230563  4406 layer_factory.hpp:77] Creating layer penlu32
I0926 08:53:08.230568  4406 net.cpp:84] Creating Layer penlu32
I0926 08:53:08.230571  4406 net.cpp:406] penlu32 <- Convolution33
I0926 08:53:08.230574  4406 net.cpp:367] penlu32 -> Convolution33 (in-place)
I0926 08:53:08.230685  4406 net.cpp:122] Setting up penlu32
I0926 08:53:08.230690  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.230691  4406 net.cpp:137] Memory required for data: 710931600
I0926 08:53:08.230695  4406 layer_factory.hpp:77] Creating layer Convolution34
I0926 08:53:08.230702  4406 net.cpp:84] Creating Layer Convolution34
I0926 08:53:08.230705  4406 net.cpp:406] Convolution34 <- Convolution33
I0926 08:53:08.230710  4406 net.cpp:380] Convolution34 -> Convolution34
I0926 08:53:08.231755  4406 net.cpp:122] Setting up Convolution34
I0926 08:53:08.231762  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.231765  4406 net.cpp:137] Memory required for data: 713440400
I0926 08:53:08.231770  4406 layer_factory.hpp:77] Creating layer BatchNorm34
I0926 08:53:08.231775  4406 net.cpp:84] Creating Layer BatchNorm34
I0926 08:53:08.231777  4406 net.cpp:406] BatchNorm34 <- Convolution34
I0926 08:53:08.231782  4406 net.cpp:367] BatchNorm34 -> Convolution34 (in-place)
I0926 08:53:08.231920  4406 net.cpp:122] Setting up BatchNorm34
I0926 08:53:08.231925  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.231927  4406 net.cpp:137] Memory required for data: 715949200
I0926 08:53:08.231931  4406 layer_factory.hpp:77] Creating layer Scale34
I0926 08:53:08.231935  4406 net.cpp:84] Creating Layer Scale34
I0926 08:53:08.231938  4406 net.cpp:406] Scale34 <- Convolution34
I0926 08:53:08.231941  4406 net.cpp:367] Scale34 -> Convolution34 (in-place)
I0926 08:53:08.231967  4406 layer_factory.hpp:77] Creating layer Scale34
I0926 08:53:08.232048  4406 net.cpp:122] Setting up Scale34
I0926 08:53:08.232051  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.232053  4406 net.cpp:137] Memory required for data: 718458000
I0926 08:53:08.232058  4406 layer_factory.hpp:77] Creating layer Eltwise16
I0926 08:53:08.232061  4406 net.cpp:84] Creating Layer Eltwise16
I0926 08:53:08.232064  4406 net.cpp:406] Eltwise16 <- Eltwise15_penlu31_0_split_1
I0926 08:53:08.232066  4406 net.cpp:406] Eltwise16 <- Convolution34
I0926 08:53:08.232069  4406 net.cpp:380] Eltwise16 -> Eltwise16
I0926 08:53:08.232087  4406 net.cpp:122] Setting up Eltwise16
I0926 08:53:08.232090  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.232092  4406 net.cpp:137] Memory required for data: 720966800
I0926 08:53:08.232095  4406 layer_factory.hpp:77] Creating layer penlu33
I0926 08:53:08.232100  4406 net.cpp:84] Creating Layer penlu33
I0926 08:53:08.232102  4406 net.cpp:406] penlu33 <- Eltwise16
I0926 08:53:08.232106  4406 net.cpp:367] penlu33 -> Eltwise16 (in-place)
I0926 08:53:08.232215  4406 net.cpp:122] Setting up penlu33
I0926 08:53:08.232219  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.232221  4406 net.cpp:137] Memory required for data: 723475600
I0926 08:53:08.232234  4406 layer_factory.hpp:77] Creating layer Eltwise16_penlu33_0_split
I0926 08:53:08.232237  4406 net.cpp:84] Creating Layer Eltwise16_penlu33_0_split
I0926 08:53:08.232239  4406 net.cpp:406] Eltwise16_penlu33_0_split <- Eltwise16
I0926 08:53:08.232242  4406 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_0
I0926 08:53:08.232246  4406 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_1
I0926 08:53:08.232271  4406 net.cpp:122] Setting up Eltwise16_penlu33_0_split
I0926 08:53:08.232275  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.232278  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.232280  4406 net.cpp:137] Memory required for data: 728493200
I0926 08:53:08.232282  4406 layer_factory.hpp:77] Creating layer Convolution35
I0926 08:53:08.232287  4406 net.cpp:84] Creating Layer Convolution35
I0926 08:53:08.232290  4406 net.cpp:406] Convolution35 <- Eltwise16_penlu33_0_split_0
I0926 08:53:08.232295  4406 net.cpp:380] Convolution35 -> Convolution35
I0926 08:53:08.233338  4406 net.cpp:122] Setting up Convolution35
I0926 08:53:08.233347  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.233350  4406 net.cpp:137] Memory required for data: 731002000
I0926 08:53:08.233355  4406 layer_factory.hpp:77] Creating layer BatchNorm35
I0926 08:53:08.233359  4406 net.cpp:84] Creating Layer BatchNorm35
I0926 08:53:08.233362  4406 net.cpp:406] BatchNorm35 <- Convolution35
I0926 08:53:08.233366  4406 net.cpp:367] BatchNorm35 -> Convolution35 (in-place)
I0926 08:53:08.233506  4406 net.cpp:122] Setting up BatchNorm35
I0926 08:53:08.233511  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.233513  4406 net.cpp:137] Memory required for data: 733510800
I0926 08:53:08.233518  4406 layer_factory.hpp:77] Creating layer Scale35
I0926 08:53:08.233522  4406 net.cpp:84] Creating Layer Scale35
I0926 08:53:08.233525  4406 net.cpp:406] Scale35 <- Convolution35
I0926 08:53:08.233527  4406 net.cpp:367] Scale35 -> Convolution35 (in-place)
I0926 08:53:08.233554  4406 layer_factory.hpp:77] Creating layer Scale35
I0926 08:53:08.233633  4406 net.cpp:122] Setting up Scale35
I0926 08:53:08.233638  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.233639  4406 net.cpp:137] Memory required for data: 736019600
I0926 08:53:08.233644  4406 layer_factory.hpp:77] Creating layer penlu34
I0926 08:53:08.233649  4406 net.cpp:84] Creating Layer penlu34
I0926 08:53:08.233650  4406 net.cpp:406] penlu34 <- Convolution35
I0926 08:53:08.233654  4406 net.cpp:367] penlu34 -> Convolution35 (in-place)
I0926 08:53:08.233764  4406 net.cpp:122] Setting up penlu34
I0926 08:53:08.233768  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.233772  4406 net.cpp:137] Memory required for data: 738528400
I0926 08:53:08.233775  4406 layer_factory.hpp:77] Creating layer Convolution36
I0926 08:53:08.233781  4406 net.cpp:84] Creating Layer Convolution36
I0926 08:53:08.233783  4406 net.cpp:406] Convolution36 <- Convolution35
I0926 08:53:08.233788  4406 net.cpp:380] Convolution36 -> Convolution36
I0926 08:53:08.234828  4406 net.cpp:122] Setting up Convolution36
I0926 08:53:08.234836  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.234839  4406 net.cpp:137] Memory required for data: 741037200
I0926 08:53:08.234843  4406 layer_factory.hpp:77] Creating layer BatchNorm36
I0926 08:53:08.234848  4406 net.cpp:84] Creating Layer BatchNorm36
I0926 08:53:08.234851  4406 net.cpp:406] BatchNorm36 <- Convolution36
I0926 08:53:08.234854  4406 net.cpp:367] BatchNorm36 -> Convolution36 (in-place)
I0926 08:53:08.234993  4406 net.cpp:122] Setting up BatchNorm36
I0926 08:53:08.234998  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.235000  4406 net.cpp:137] Memory required for data: 743546000
I0926 08:53:08.235005  4406 layer_factory.hpp:77] Creating layer Scale36
I0926 08:53:08.235008  4406 net.cpp:84] Creating Layer Scale36
I0926 08:53:08.235011  4406 net.cpp:406] Scale36 <- Convolution36
I0926 08:53:08.235014  4406 net.cpp:367] Scale36 -> Convolution36 (in-place)
I0926 08:53:08.235050  4406 layer_factory.hpp:77] Creating layer Scale36
I0926 08:53:08.235129  4406 net.cpp:122] Setting up Scale36
I0926 08:53:08.235134  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.235136  4406 net.cpp:137] Memory required for data: 746054800
I0926 08:53:08.235139  4406 layer_factory.hpp:77] Creating layer Eltwise17
I0926 08:53:08.235144  4406 net.cpp:84] Creating Layer Eltwise17
I0926 08:53:08.235147  4406 net.cpp:406] Eltwise17 <- Eltwise16_penlu33_0_split_1
I0926 08:53:08.235150  4406 net.cpp:406] Eltwise17 <- Convolution36
I0926 08:53:08.235153  4406 net.cpp:380] Eltwise17 -> Eltwise17
I0926 08:53:08.235169  4406 net.cpp:122] Setting up Eltwise17
I0926 08:53:08.235173  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.235175  4406 net.cpp:137] Memory required for data: 748563600
I0926 08:53:08.235177  4406 layer_factory.hpp:77] Creating layer penlu35
I0926 08:53:08.235183  4406 net.cpp:84] Creating Layer penlu35
I0926 08:53:08.235184  4406 net.cpp:406] penlu35 <- Eltwise17
I0926 08:53:08.235188  4406 net.cpp:367] penlu35 -> Eltwise17 (in-place)
I0926 08:53:08.235297  4406 net.cpp:122] Setting up penlu35
I0926 08:53:08.235301  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.235303  4406 net.cpp:137] Memory required for data: 751072400
I0926 08:53:08.235308  4406 layer_factory.hpp:77] Creating layer Eltwise17_penlu35_0_split
I0926 08:53:08.235312  4406 net.cpp:84] Creating Layer Eltwise17_penlu35_0_split
I0926 08:53:08.235314  4406 net.cpp:406] Eltwise17_penlu35_0_split <- Eltwise17
I0926 08:53:08.235318  4406 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_0
I0926 08:53:08.235322  4406 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_1
I0926 08:53:08.235344  4406 net.cpp:122] Setting up Eltwise17_penlu35_0_split
I0926 08:53:08.235348  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.235352  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.235353  4406 net.cpp:137] Memory required for data: 756090000
I0926 08:53:08.235355  4406 layer_factory.hpp:77] Creating layer Convolution37
I0926 08:53:08.235361  4406 net.cpp:84] Creating Layer Convolution37
I0926 08:53:08.235364  4406 net.cpp:406] Convolution37 <- Eltwise17_penlu35_0_split_0
I0926 08:53:08.235368  4406 net.cpp:380] Convolution37 -> Convolution37
I0926 08:53:08.236086  4406 net.cpp:122] Setting up Convolution37
I0926 08:53:08.236094  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.236097  4406 net.cpp:137] Memory required for data: 758598800
I0926 08:53:08.236101  4406 layer_factory.hpp:77] Creating layer BatchNorm37
I0926 08:53:08.236105  4406 net.cpp:84] Creating Layer BatchNorm37
I0926 08:53:08.236109  4406 net.cpp:406] BatchNorm37 <- Convolution37
I0926 08:53:08.236111  4406 net.cpp:367] BatchNorm37 -> Convolution37 (in-place)
I0926 08:53:08.236250  4406 net.cpp:122] Setting up BatchNorm37
I0926 08:53:08.236254  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.236256  4406 net.cpp:137] Memory required for data: 761107600
I0926 08:53:08.236261  4406 layer_factory.hpp:77] Creating layer Scale37
I0926 08:53:08.236264  4406 net.cpp:84] Creating Layer Scale37
I0926 08:53:08.236268  4406 net.cpp:406] Scale37 <- Convolution37
I0926 08:53:08.236270  4406 net.cpp:367] Scale37 -> Convolution37 (in-place)
I0926 08:53:08.236296  4406 layer_factory.hpp:77] Creating layer Scale37
I0926 08:53:08.236377  4406 net.cpp:122] Setting up Scale37
I0926 08:53:08.236382  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.236383  4406 net.cpp:137] Memory required for data: 763616400
I0926 08:53:08.236387  4406 layer_factory.hpp:77] Creating layer penlu36
I0926 08:53:08.236392  4406 net.cpp:84] Creating Layer penlu36
I0926 08:53:08.236394  4406 net.cpp:406] penlu36 <- Convolution37
I0926 08:53:08.236398  4406 net.cpp:367] penlu36 -> Convolution37 (in-place)
I0926 08:53:08.236536  4406 net.cpp:122] Setting up penlu36
I0926 08:53:08.236552  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.236569  4406 net.cpp:137] Memory required for data: 766125200
I0926 08:53:08.236573  4406 layer_factory.hpp:77] Creating layer Convolution38
I0926 08:53:08.236582  4406 net.cpp:84] Creating Layer Convolution38
I0926 08:53:08.236583  4406 net.cpp:406] Convolution38 <- Convolution37
I0926 08:53:08.236588  4406 net.cpp:380] Convolution38 -> Convolution38
I0926 08:53:08.237637  4406 net.cpp:122] Setting up Convolution38
I0926 08:53:08.237646  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.237649  4406 net.cpp:137] Memory required for data: 768634000
I0926 08:53:08.237653  4406 layer_factory.hpp:77] Creating layer BatchNorm38
I0926 08:53:08.237658  4406 net.cpp:84] Creating Layer BatchNorm38
I0926 08:53:08.237660  4406 net.cpp:406] BatchNorm38 <- Convolution38
I0926 08:53:08.237664  4406 net.cpp:367] BatchNorm38 -> Convolution38 (in-place)
I0926 08:53:08.237804  4406 net.cpp:122] Setting up BatchNorm38
I0926 08:53:08.237809  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.237812  4406 net.cpp:137] Memory required for data: 771142800
I0926 08:53:08.237817  4406 layer_factory.hpp:77] Creating layer Scale38
I0926 08:53:08.237819  4406 net.cpp:84] Creating Layer Scale38
I0926 08:53:08.237823  4406 net.cpp:406] Scale38 <- Convolution38
I0926 08:53:08.237825  4406 net.cpp:367] Scale38 -> Convolution38 (in-place)
I0926 08:53:08.237854  4406 layer_factory.hpp:77] Creating layer Scale38
I0926 08:53:08.237932  4406 net.cpp:122] Setting up Scale38
I0926 08:53:08.237937  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.237939  4406 net.cpp:137] Memory required for data: 773651600
I0926 08:53:08.237943  4406 layer_factory.hpp:77] Creating layer Eltwise18
I0926 08:53:08.237947  4406 net.cpp:84] Creating Layer Eltwise18
I0926 08:53:08.237949  4406 net.cpp:406] Eltwise18 <- Eltwise17_penlu35_0_split_1
I0926 08:53:08.237952  4406 net.cpp:406] Eltwise18 <- Convolution38
I0926 08:53:08.237956  4406 net.cpp:380] Eltwise18 -> Eltwise18
I0926 08:53:08.237972  4406 net.cpp:122] Setting up Eltwise18
I0926 08:53:08.237977  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.237978  4406 net.cpp:137] Memory required for data: 776160400
I0926 08:53:08.237980  4406 layer_factory.hpp:77] Creating layer penlu37
I0926 08:53:08.237985  4406 net.cpp:84] Creating Layer penlu37
I0926 08:53:08.237987  4406 net.cpp:406] penlu37 <- Eltwise18
I0926 08:53:08.237992  4406 net.cpp:367] penlu37 -> Eltwise18 (in-place)
I0926 08:53:08.238103  4406 net.cpp:122] Setting up penlu37
I0926 08:53:08.238107  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.238109  4406 net.cpp:137] Memory required for data: 778669200
I0926 08:53:08.238113  4406 layer_factory.hpp:77] Creating layer Eltwise18_penlu37_0_split
I0926 08:53:08.238117  4406 net.cpp:84] Creating Layer Eltwise18_penlu37_0_split
I0926 08:53:08.238119  4406 net.cpp:406] Eltwise18_penlu37_0_split <- Eltwise18
I0926 08:53:08.238122  4406 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_0
I0926 08:53:08.238127  4406 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_1
I0926 08:53:08.238150  4406 net.cpp:122] Setting up Eltwise18_penlu37_0_split
I0926 08:53:08.238153  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.238157  4406 net.cpp:129] Top shape: 100 32 14 14 (627200)
I0926 08:53:08.238158  4406 net.cpp:137] Memory required for data: 783686800
I0926 08:53:08.238160  4406 layer_factory.hpp:77] Creating layer Convolution39
I0926 08:53:08.238167  4406 net.cpp:84] Creating Layer Convolution39
I0926 08:53:08.238168  4406 net.cpp:406] Convolution39 <- Eltwise18_penlu37_0_split_0
I0926 08:53:08.238173  4406 net.cpp:380] Convolution39 -> Convolution39
I0926 08:53:08.239061  4406 net.cpp:122] Setting up Convolution39
I0926 08:53:08.239070  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.239073  4406 net.cpp:137] Memory required for data: 784941200
I0926 08:53:08.239078  4406 layer_factory.hpp:77] Creating layer BatchNorm39
I0926 08:53:08.239089  4406 net.cpp:84] Creating Layer BatchNorm39
I0926 08:53:08.239091  4406 net.cpp:406] BatchNorm39 <- Convolution39
I0926 08:53:08.239096  4406 net.cpp:367] BatchNorm39 -> Convolution39 (in-place)
I0926 08:53:08.239233  4406 net.cpp:122] Setting up BatchNorm39
I0926 08:53:08.239238  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.239239  4406 net.cpp:137] Memory required for data: 786195600
I0926 08:53:08.239244  4406 layer_factory.hpp:77] Creating layer Scale39
I0926 08:53:08.239248  4406 net.cpp:84] Creating Layer Scale39
I0926 08:53:08.239250  4406 net.cpp:406] Scale39 <- Convolution39
I0926 08:53:08.239253  4406 net.cpp:367] Scale39 -> Convolution39 (in-place)
I0926 08:53:08.239280  4406 layer_factory.hpp:77] Creating layer Scale39
I0926 08:53:08.239362  4406 net.cpp:122] Setting up Scale39
I0926 08:53:08.239365  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.239367  4406 net.cpp:137] Memory required for data: 787450000
I0926 08:53:08.239372  4406 layer_factory.hpp:77] Creating layer Convolution40
I0926 08:53:08.239378  4406 net.cpp:84] Creating Layer Convolution40
I0926 08:53:08.239382  4406 net.cpp:406] Convolution40 <- Eltwise18_penlu37_0_split_1
I0926 08:53:08.239385  4406 net.cpp:380] Convolution40 -> Convolution40
I0926 08:53:08.241250  4406 net.cpp:122] Setting up Convolution40
I0926 08:53:08.241259  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.241261  4406 net.cpp:137] Memory required for data: 788704400
I0926 08:53:08.241266  4406 layer_factory.hpp:77] Creating layer BatchNorm40
I0926 08:53:08.241271  4406 net.cpp:84] Creating Layer BatchNorm40
I0926 08:53:08.241273  4406 net.cpp:406] BatchNorm40 <- Convolution40
I0926 08:53:08.241278  4406 net.cpp:367] BatchNorm40 -> Convolution40 (in-place)
I0926 08:53:08.241442  4406 net.cpp:122] Setting up BatchNorm40
I0926 08:53:08.241457  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.241459  4406 net.cpp:137] Memory required for data: 789958800
I0926 08:53:08.241464  4406 layer_factory.hpp:77] Creating layer Scale40
I0926 08:53:08.241468  4406 net.cpp:84] Creating Layer Scale40
I0926 08:53:08.241472  4406 net.cpp:406] Scale40 <- Convolution40
I0926 08:53:08.241484  4406 net.cpp:367] Scale40 -> Convolution40 (in-place)
I0926 08:53:08.241513  4406 layer_factory.hpp:77] Creating layer Scale40
I0926 08:53:08.241611  4406 net.cpp:122] Setting up Scale40
I0926 08:53:08.241617  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.241621  4406 net.cpp:137] Memory required for data: 791213200
I0926 08:53:08.241624  4406 layer_factory.hpp:77] Creating layer penlu38
I0926 08:53:08.241631  4406 net.cpp:84] Creating Layer penlu38
I0926 08:53:08.241633  4406 net.cpp:406] penlu38 <- Convolution40
I0926 08:53:08.241636  4406 net.cpp:367] penlu38 -> Convolution40 (in-place)
I0926 08:53:08.241780  4406 net.cpp:122] Setting up penlu38
I0926 08:53:08.241791  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.241794  4406 net.cpp:137] Memory required for data: 792467600
I0926 08:53:08.241802  4406 layer_factory.hpp:77] Creating layer Convolution41
I0926 08:53:08.241822  4406 net.cpp:84] Creating Layer Convolution41
I0926 08:53:08.241825  4406 net.cpp:406] Convolution41 <- Convolution40
I0926 08:53:08.241829  4406 net.cpp:380] Convolution41 -> Convolution41
I0926 08:53:08.243975  4406 net.cpp:122] Setting up Convolution41
I0926 08:53:08.243993  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.243996  4406 net.cpp:137] Memory required for data: 793722000
I0926 08:53:08.244010  4406 layer_factory.hpp:77] Creating layer BatchNorm41
I0926 08:53:08.244016  4406 net.cpp:84] Creating Layer BatchNorm41
I0926 08:53:08.244019  4406 net.cpp:406] BatchNorm41 <- Convolution41
I0926 08:53:08.244024  4406 net.cpp:367] BatchNorm41 -> Convolution41 (in-place)
I0926 08:53:08.244226  4406 net.cpp:122] Setting up BatchNorm41
I0926 08:53:08.244231  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.244233  4406 net.cpp:137] Memory required for data: 794976400
I0926 08:53:08.244238  4406 layer_factory.hpp:77] Creating layer Scale41
I0926 08:53:08.244251  4406 net.cpp:84] Creating Layer Scale41
I0926 08:53:08.244254  4406 net.cpp:406] Scale41 <- Convolution41
I0926 08:53:08.244257  4406 net.cpp:367] Scale41 -> Convolution41 (in-place)
I0926 08:53:08.244287  4406 layer_factory.hpp:77] Creating layer Scale41
I0926 08:53:08.244369  4406 net.cpp:122] Setting up Scale41
I0926 08:53:08.244374  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.244375  4406 net.cpp:137] Memory required for data: 796230800
I0926 08:53:08.244379  4406 layer_factory.hpp:77] Creating layer Eltwise19
I0926 08:53:08.244385  4406 net.cpp:84] Creating Layer Eltwise19
I0926 08:53:08.244396  4406 net.cpp:406] Eltwise19 <- Convolution39
I0926 08:53:08.244400  4406 net.cpp:406] Eltwise19 <- Convolution41
I0926 08:53:08.244403  4406 net.cpp:380] Eltwise19 -> Eltwise19
I0926 08:53:08.244441  4406 net.cpp:122] Setting up Eltwise19
I0926 08:53:08.244446  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.244448  4406 net.cpp:137] Memory required for data: 797485200
I0926 08:53:08.244460  4406 layer_factory.hpp:77] Creating layer penlu39
I0926 08:53:08.244468  4406 net.cpp:84] Creating Layer penlu39
I0926 08:53:08.244472  4406 net.cpp:406] penlu39 <- Eltwise19
I0926 08:53:08.244475  4406 net.cpp:367] penlu39 -> Eltwise19 (in-place)
I0926 08:53:08.244613  4406 net.cpp:122] Setting up penlu39
I0926 08:53:08.244618  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.244621  4406 net.cpp:137] Memory required for data: 798739600
I0926 08:53:08.244626  4406 layer_factory.hpp:77] Creating layer Eltwise19_penlu39_0_split
I0926 08:53:08.244629  4406 net.cpp:84] Creating Layer Eltwise19_penlu39_0_split
I0926 08:53:08.244632  4406 net.cpp:406] Eltwise19_penlu39_0_split <- Eltwise19
I0926 08:53:08.244635  4406 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_0
I0926 08:53:08.244640  4406 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_1
I0926 08:53:08.244663  4406 net.cpp:122] Setting up Eltwise19_penlu39_0_split
I0926 08:53:08.244668  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.244670  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.244673  4406 net.cpp:137] Memory required for data: 801248400
I0926 08:53:08.244674  4406 layer_factory.hpp:77] Creating layer Convolution42
I0926 08:53:08.244681  4406 net.cpp:84] Creating Layer Convolution42
I0926 08:53:08.244683  4406 net.cpp:406] Convolution42 <- Eltwise19_penlu39_0_split_0
I0926 08:53:08.244688  4406 net.cpp:380] Convolution42 -> Convolution42
I0926 08:53:08.246336  4406 net.cpp:122] Setting up Convolution42
I0926 08:53:08.246345  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.246347  4406 net.cpp:137] Memory required for data: 802502800
I0926 08:53:08.246352  4406 layer_factory.hpp:77] Creating layer BatchNorm42
I0926 08:53:08.246357  4406 net.cpp:84] Creating Layer BatchNorm42
I0926 08:53:08.246359  4406 net.cpp:406] BatchNorm42 <- Convolution42
I0926 08:53:08.246364  4406 net.cpp:367] BatchNorm42 -> Convolution42 (in-place)
I0926 08:53:08.246505  4406 net.cpp:122] Setting up BatchNorm42
I0926 08:53:08.246510  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.246511  4406 net.cpp:137] Memory required for data: 803757200
I0926 08:53:08.246516  4406 layer_factory.hpp:77] Creating layer Scale42
I0926 08:53:08.246520  4406 net.cpp:84] Creating Layer Scale42
I0926 08:53:08.246523  4406 net.cpp:406] Scale42 <- Convolution42
I0926 08:53:08.246526  4406 net.cpp:367] Scale42 -> Convolution42 (in-place)
I0926 08:53:08.246556  4406 layer_factory.hpp:77] Creating layer Scale42
I0926 08:53:08.246636  4406 net.cpp:122] Setting up Scale42
I0926 08:53:08.246640  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.246642  4406 net.cpp:137] Memory required for data: 805011600
I0926 08:53:08.246646  4406 layer_factory.hpp:77] Creating layer penlu40
I0926 08:53:08.246651  4406 net.cpp:84] Creating Layer penlu40
I0926 08:53:08.246654  4406 net.cpp:406] penlu40 <- Convolution42
I0926 08:53:08.246665  4406 net.cpp:367] penlu40 -> Convolution42 (in-place)
I0926 08:53:08.246780  4406 net.cpp:122] Setting up penlu40
I0926 08:53:08.246785  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.246788  4406 net.cpp:137] Memory required for data: 806266000
I0926 08:53:08.246791  4406 layer_factory.hpp:77] Creating layer Convolution43
I0926 08:53:08.246798  4406 net.cpp:84] Creating Layer Convolution43
I0926 08:53:08.246801  4406 net.cpp:406] Convolution43 <- Convolution42
I0926 08:53:08.246804  4406 net.cpp:380] Convolution43 -> Convolution43
I0926 08:53:08.249086  4406 net.cpp:122] Setting up Convolution43
I0926 08:53:08.249094  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.249097  4406 net.cpp:137] Memory required for data: 807520400
I0926 08:53:08.249102  4406 layer_factory.hpp:77] Creating layer BatchNorm43
I0926 08:53:08.249109  4406 net.cpp:84] Creating Layer BatchNorm43
I0926 08:53:08.249111  4406 net.cpp:406] BatchNorm43 <- Convolution43
I0926 08:53:08.249114  4406 net.cpp:367] BatchNorm43 -> Convolution43 (in-place)
I0926 08:53:08.249260  4406 net.cpp:122] Setting up BatchNorm43
I0926 08:53:08.249265  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.249267  4406 net.cpp:137] Memory required for data: 808774800
I0926 08:53:08.249272  4406 layer_factory.hpp:77] Creating layer Scale43
I0926 08:53:08.249276  4406 net.cpp:84] Creating Layer Scale43
I0926 08:53:08.249279  4406 net.cpp:406] Scale43 <- Convolution43
I0926 08:53:08.249282  4406 net.cpp:367] Scale43 -> Convolution43 (in-place)
I0926 08:53:08.249310  4406 layer_factory.hpp:77] Creating layer Scale43
I0926 08:53:08.249393  4406 net.cpp:122] Setting up Scale43
I0926 08:53:08.249397  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.249399  4406 net.cpp:137] Memory required for data: 810029200
I0926 08:53:08.249403  4406 layer_factory.hpp:77] Creating layer Eltwise20
I0926 08:53:08.249408  4406 net.cpp:84] Creating Layer Eltwise20
I0926 08:53:08.249413  4406 net.cpp:406] Eltwise20 <- Eltwise19_penlu39_0_split_1
I0926 08:53:08.249415  4406 net.cpp:406] Eltwise20 <- Convolution43
I0926 08:53:08.249418  4406 net.cpp:380] Eltwise20 -> Eltwise20
I0926 08:53:08.249434  4406 net.cpp:122] Setting up Eltwise20
I0926 08:53:08.249439  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.249440  4406 net.cpp:137] Memory required for data: 811283600
I0926 08:53:08.249442  4406 layer_factory.hpp:77] Creating layer penlu41
I0926 08:53:08.249449  4406 net.cpp:84] Creating Layer penlu41
I0926 08:53:08.249450  4406 net.cpp:406] penlu41 <- Eltwise20
I0926 08:53:08.249454  4406 net.cpp:367] penlu41 -> Eltwise20 (in-place)
I0926 08:53:08.249570  4406 net.cpp:122] Setting up penlu41
I0926 08:53:08.249575  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.249577  4406 net.cpp:137] Memory required for data: 812538000
I0926 08:53:08.249581  4406 layer_factory.hpp:77] Creating layer Eltwise20_penlu41_0_split
I0926 08:53:08.249585  4406 net.cpp:84] Creating Layer Eltwise20_penlu41_0_split
I0926 08:53:08.249588  4406 net.cpp:406] Eltwise20_penlu41_0_split <- Eltwise20
I0926 08:53:08.249591  4406 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_0
I0926 08:53:08.249595  4406 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_1
I0926 08:53:08.249620  4406 net.cpp:122] Setting up Eltwise20_penlu41_0_split
I0926 08:53:08.249624  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.249627  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.249629  4406 net.cpp:137] Memory required for data: 815046800
I0926 08:53:08.249631  4406 layer_factory.hpp:77] Creating layer Convolution44
I0926 08:53:08.249636  4406 net.cpp:84] Creating Layer Convolution44
I0926 08:53:08.249639  4406 net.cpp:406] Convolution44 <- Eltwise20_penlu41_0_split_0
I0926 08:53:08.249644  4406 net.cpp:380] Convolution44 -> Convolution44
I0926 08:53:08.251299  4406 net.cpp:122] Setting up Convolution44
I0926 08:53:08.251308  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.251317  4406 net.cpp:137] Memory required for data: 816301200
I0926 08:53:08.251322  4406 layer_factory.hpp:77] Creating layer BatchNorm44
I0926 08:53:08.251327  4406 net.cpp:84] Creating Layer BatchNorm44
I0926 08:53:08.251330  4406 net.cpp:406] BatchNorm44 <- Convolution44
I0926 08:53:08.251334  4406 net.cpp:367] BatchNorm44 -> Convolution44 (in-place)
I0926 08:53:08.251477  4406 net.cpp:122] Setting up BatchNorm44
I0926 08:53:08.251482  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.251484  4406 net.cpp:137] Memory required for data: 817555600
I0926 08:53:08.251488  4406 layer_factory.hpp:77] Creating layer Scale44
I0926 08:53:08.251492  4406 net.cpp:84] Creating Layer Scale44
I0926 08:53:08.251495  4406 net.cpp:406] Scale44 <- Convolution44
I0926 08:53:08.251498  4406 net.cpp:367] Scale44 -> Convolution44 (in-place)
I0926 08:53:08.251526  4406 layer_factory.hpp:77] Creating layer Scale44
I0926 08:53:08.251608  4406 net.cpp:122] Setting up Scale44
I0926 08:53:08.251613  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.251616  4406 net.cpp:137] Memory required for data: 818810000
I0926 08:53:08.251619  4406 layer_factory.hpp:77] Creating layer penlu42
I0926 08:53:08.251624  4406 net.cpp:84] Creating Layer penlu42
I0926 08:53:08.251626  4406 net.cpp:406] penlu42 <- Convolution44
I0926 08:53:08.251631  4406 net.cpp:367] penlu42 -> Convolution44 (in-place)
I0926 08:53:08.251744  4406 net.cpp:122] Setting up penlu42
I0926 08:53:08.251749  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.251750  4406 net.cpp:137] Memory required for data: 820064400
I0926 08:53:08.251754  4406 layer_factory.hpp:77] Creating layer Convolution45
I0926 08:53:08.251761  4406 net.cpp:84] Creating Layer Convolution45
I0926 08:53:08.251763  4406 net.cpp:406] Convolution45 <- Convolution44
I0926 08:53:08.251767  4406 net.cpp:380] Convolution45 -> Convolution45
I0926 08:53:08.253749  4406 net.cpp:122] Setting up Convolution45
I0926 08:53:08.253757  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.253760  4406 net.cpp:137] Memory required for data: 821318800
I0926 08:53:08.253764  4406 layer_factory.hpp:77] Creating layer BatchNorm45
I0926 08:53:08.253770  4406 net.cpp:84] Creating Layer BatchNorm45
I0926 08:53:08.253772  4406 net.cpp:406] BatchNorm45 <- Convolution45
I0926 08:53:08.253777  4406 net.cpp:367] BatchNorm45 -> Convolution45 (in-place)
I0926 08:53:08.253923  4406 net.cpp:122] Setting up BatchNorm45
I0926 08:53:08.253928  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.253931  4406 net.cpp:137] Memory required for data: 822573200
I0926 08:53:08.253935  4406 layer_factory.hpp:77] Creating layer Scale45
I0926 08:53:08.253939  4406 net.cpp:84] Creating Layer Scale45
I0926 08:53:08.253942  4406 net.cpp:406] Scale45 <- Convolution45
I0926 08:53:08.253945  4406 net.cpp:367] Scale45 -> Convolution45 (in-place)
I0926 08:53:08.253973  4406 layer_factory.hpp:77] Creating layer Scale45
I0926 08:53:08.254057  4406 net.cpp:122] Setting up Scale45
I0926 08:53:08.254062  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.254065  4406 net.cpp:137] Memory required for data: 823827600
I0926 08:53:08.254068  4406 layer_factory.hpp:77] Creating layer Eltwise21
I0926 08:53:08.254073  4406 net.cpp:84] Creating Layer Eltwise21
I0926 08:53:08.254076  4406 net.cpp:406] Eltwise21 <- Eltwise20_penlu41_0_split_1
I0926 08:53:08.254078  4406 net.cpp:406] Eltwise21 <- Convolution45
I0926 08:53:08.254081  4406 net.cpp:380] Eltwise21 -> Eltwise21
I0926 08:53:08.254099  4406 net.cpp:122] Setting up Eltwise21
I0926 08:53:08.254103  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.254106  4406 net.cpp:137] Memory required for data: 825082000
I0926 08:53:08.254107  4406 layer_factory.hpp:77] Creating layer penlu43
I0926 08:53:08.254112  4406 net.cpp:84] Creating Layer penlu43
I0926 08:53:08.254115  4406 net.cpp:406] penlu43 <- Eltwise21
I0926 08:53:08.254118  4406 net.cpp:367] penlu43 -> Eltwise21 (in-place)
I0926 08:53:08.254232  4406 net.cpp:122] Setting up penlu43
I0926 08:53:08.254243  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.254245  4406 net.cpp:137] Memory required for data: 826336400
I0926 08:53:08.254250  4406 layer_factory.hpp:77] Creating layer Eltwise21_penlu43_0_split
I0926 08:53:08.254253  4406 net.cpp:84] Creating Layer Eltwise21_penlu43_0_split
I0926 08:53:08.254256  4406 net.cpp:406] Eltwise21_penlu43_0_split <- Eltwise21
I0926 08:53:08.254259  4406 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_0
I0926 08:53:08.254263  4406 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_1
I0926 08:53:08.254288  4406 net.cpp:122] Setting up Eltwise21_penlu43_0_split
I0926 08:53:08.254292  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.254295  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.254297  4406 net.cpp:137] Memory required for data: 828845200
I0926 08:53:08.254299  4406 layer_factory.hpp:77] Creating layer Convolution46
I0926 08:53:08.254305  4406 net.cpp:84] Creating Layer Convolution46
I0926 08:53:08.254308  4406 net.cpp:406] Convolution46 <- Eltwise21_penlu43_0_split_0
I0926 08:53:08.254312  4406 net.cpp:380] Convolution46 -> Convolution46
I0926 08:53:08.255972  4406 net.cpp:122] Setting up Convolution46
I0926 08:53:08.255981  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.255983  4406 net.cpp:137] Memory required for data: 830099600
I0926 08:53:08.255988  4406 layer_factory.hpp:77] Creating layer BatchNorm46
I0926 08:53:08.255993  4406 net.cpp:84] Creating Layer BatchNorm46
I0926 08:53:08.255996  4406 net.cpp:406] BatchNorm46 <- Convolution46
I0926 08:53:08.256000  4406 net.cpp:367] BatchNorm46 -> Convolution46 (in-place)
I0926 08:53:08.256145  4406 net.cpp:122] Setting up BatchNorm46
I0926 08:53:08.256150  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.256152  4406 net.cpp:137] Memory required for data: 831354000
I0926 08:53:08.256157  4406 layer_factory.hpp:77] Creating layer Scale46
I0926 08:53:08.256161  4406 net.cpp:84] Creating Layer Scale46
I0926 08:53:08.256165  4406 net.cpp:406] Scale46 <- Convolution46
I0926 08:53:08.256167  4406 net.cpp:367] Scale46 -> Convolution46 (in-place)
I0926 08:53:08.256196  4406 layer_factory.hpp:77] Creating layer Scale46
I0926 08:53:08.256278  4406 net.cpp:122] Setting up Scale46
I0926 08:53:08.256283  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.256284  4406 net.cpp:137] Memory required for data: 832608400
I0926 08:53:08.256289  4406 layer_factory.hpp:77] Creating layer penlu44
I0926 08:53:08.256294  4406 net.cpp:84] Creating Layer penlu44
I0926 08:53:08.256296  4406 net.cpp:406] penlu44 <- Convolution46
I0926 08:53:08.256300  4406 net.cpp:367] penlu44 -> Convolution46 (in-place)
I0926 08:53:08.256417  4406 net.cpp:122] Setting up penlu44
I0926 08:53:08.256420  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.256422  4406 net.cpp:137] Memory required for data: 833862800
I0926 08:53:08.256427  4406 layer_factory.hpp:77] Creating layer Convolution47
I0926 08:53:08.256433  4406 net.cpp:84] Creating Layer Convolution47
I0926 08:53:08.256435  4406 net.cpp:406] Convolution47 <- Convolution46
I0926 08:53:08.256439  4406 net.cpp:380] Convolution47 -> Convolution47
I0926 08:53:08.258105  4406 net.cpp:122] Setting up Convolution47
I0926 08:53:08.258112  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.258116  4406 net.cpp:137] Memory required for data: 835117200
I0926 08:53:08.258119  4406 layer_factory.hpp:77] Creating layer BatchNorm47
I0926 08:53:08.258124  4406 net.cpp:84] Creating Layer BatchNorm47
I0926 08:53:08.258127  4406 net.cpp:406] BatchNorm47 <- Convolution47
I0926 08:53:08.258131  4406 net.cpp:367] BatchNorm47 -> Convolution47 (in-place)
I0926 08:53:08.258278  4406 net.cpp:122] Setting up BatchNorm47
I0926 08:53:08.258282  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.258285  4406 net.cpp:137] Memory required for data: 836371600
I0926 08:53:08.258289  4406 layer_factory.hpp:77] Creating layer Scale47
I0926 08:53:08.258301  4406 net.cpp:84] Creating Layer Scale47
I0926 08:53:08.258303  4406 net.cpp:406] Scale47 <- Convolution47
I0926 08:53:08.258306  4406 net.cpp:367] Scale47 -> Convolution47 (in-place)
I0926 08:53:08.258337  4406 layer_factory.hpp:77] Creating layer Scale47
I0926 08:53:08.258419  4406 net.cpp:122] Setting up Scale47
I0926 08:53:08.258424  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.258425  4406 net.cpp:137] Memory required for data: 837626000
I0926 08:53:08.258430  4406 layer_factory.hpp:77] Creating layer Eltwise22
I0926 08:53:08.258435  4406 net.cpp:84] Creating Layer Eltwise22
I0926 08:53:08.258436  4406 net.cpp:406] Eltwise22 <- Eltwise21_penlu43_0_split_1
I0926 08:53:08.258440  4406 net.cpp:406] Eltwise22 <- Convolution47
I0926 08:53:08.258443  4406 net.cpp:380] Eltwise22 -> Eltwise22
I0926 08:53:08.258460  4406 net.cpp:122] Setting up Eltwise22
I0926 08:53:08.258463  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.258466  4406 net.cpp:137] Memory required for data: 838880400
I0926 08:53:08.258467  4406 layer_factory.hpp:77] Creating layer penlu45
I0926 08:53:08.258473  4406 net.cpp:84] Creating Layer penlu45
I0926 08:53:08.258476  4406 net.cpp:406] penlu45 <- Eltwise22
I0926 08:53:08.258479  4406 net.cpp:367] penlu45 -> Eltwise22 (in-place)
I0926 08:53:08.258592  4406 net.cpp:122] Setting up penlu45
I0926 08:53:08.258596  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.258599  4406 net.cpp:137] Memory required for data: 840134800
I0926 08:53:08.258602  4406 layer_factory.hpp:77] Creating layer Eltwise22_penlu45_0_split
I0926 08:53:08.258606  4406 net.cpp:84] Creating Layer Eltwise22_penlu45_0_split
I0926 08:53:08.258608  4406 net.cpp:406] Eltwise22_penlu45_0_split <- Eltwise22
I0926 08:53:08.258612  4406 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_0
I0926 08:53:08.258616  4406 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_1
I0926 08:53:08.258641  4406 net.cpp:122] Setting up Eltwise22_penlu45_0_split
I0926 08:53:08.258644  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.258647  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.258649  4406 net.cpp:137] Memory required for data: 842643600
I0926 08:53:08.258651  4406 layer_factory.hpp:77] Creating layer Convolution48
I0926 08:53:08.258657  4406 net.cpp:84] Creating Layer Convolution48
I0926 08:53:08.258659  4406 net.cpp:406] Convolution48 <- Eltwise22_penlu45_0_split_0
I0926 08:53:08.258663  4406 net.cpp:380] Convolution48 -> Convolution48
I0926 08:53:08.260301  4406 net.cpp:122] Setting up Convolution48
I0926 08:53:08.260310  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.260313  4406 net.cpp:137] Memory required for data: 843898000
I0926 08:53:08.260316  4406 layer_factory.hpp:77] Creating layer BatchNorm48
I0926 08:53:08.260321  4406 net.cpp:84] Creating Layer BatchNorm48
I0926 08:53:08.260325  4406 net.cpp:406] BatchNorm48 <- Convolution48
I0926 08:53:08.260329  4406 net.cpp:367] BatchNorm48 -> Convolution48 (in-place)
I0926 08:53:08.260473  4406 net.cpp:122] Setting up BatchNorm48
I0926 08:53:08.260478  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.260479  4406 net.cpp:137] Memory required for data: 845152400
I0926 08:53:08.260484  4406 layer_factory.hpp:77] Creating layer Scale48
I0926 08:53:08.260488  4406 net.cpp:84] Creating Layer Scale48
I0926 08:53:08.260491  4406 net.cpp:406] Scale48 <- Convolution48
I0926 08:53:08.260499  4406 net.cpp:367] Scale48 -> Convolution48 (in-place)
I0926 08:53:08.260551  4406 layer_factory.hpp:77] Creating layer Scale48
I0926 08:53:08.260635  4406 net.cpp:122] Setting up Scale48
I0926 08:53:08.260639  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.260641  4406 net.cpp:137] Memory required for data: 846406800
I0926 08:53:08.260645  4406 layer_factory.hpp:77] Creating layer penlu46
I0926 08:53:08.260651  4406 net.cpp:84] Creating Layer penlu46
I0926 08:53:08.260654  4406 net.cpp:406] penlu46 <- Convolution48
I0926 08:53:08.260658  4406 net.cpp:367] penlu46 -> Convolution48 (in-place)
I0926 08:53:08.260782  4406 net.cpp:122] Setting up penlu46
I0926 08:53:08.260787  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.260788  4406 net.cpp:137] Memory required for data: 847661200
I0926 08:53:08.260792  4406 layer_factory.hpp:77] Creating layer Convolution49
I0926 08:53:08.260799  4406 net.cpp:84] Creating Layer Convolution49
I0926 08:53:08.260802  4406 net.cpp:406] Convolution49 <- Convolution48
I0926 08:53:08.260805  4406 net.cpp:380] Convolution49 -> Convolution49
I0926 08:53:08.262758  4406 net.cpp:122] Setting up Convolution49
I0926 08:53:08.262766  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.262768  4406 net.cpp:137] Memory required for data: 848915600
I0926 08:53:08.262773  4406 layer_factory.hpp:77] Creating layer BatchNorm49
I0926 08:53:08.262778  4406 net.cpp:84] Creating Layer BatchNorm49
I0926 08:53:08.262780  4406 net.cpp:406] BatchNorm49 <- Convolution49
I0926 08:53:08.262784  4406 net.cpp:367] BatchNorm49 -> Convolution49 (in-place)
I0926 08:53:08.262933  4406 net.cpp:122] Setting up BatchNorm49
I0926 08:53:08.262938  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.262939  4406 net.cpp:137] Memory required for data: 850170000
I0926 08:53:08.262943  4406 layer_factory.hpp:77] Creating layer Scale49
I0926 08:53:08.262948  4406 net.cpp:84] Creating Layer Scale49
I0926 08:53:08.262950  4406 net.cpp:406] Scale49 <- Convolution49
I0926 08:53:08.262954  4406 net.cpp:367] Scale49 -> Convolution49 (in-place)
I0926 08:53:08.262982  4406 layer_factory.hpp:77] Creating layer Scale49
I0926 08:53:08.263065  4406 net.cpp:122] Setting up Scale49
I0926 08:53:08.263070  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.263072  4406 net.cpp:137] Memory required for data: 851424400
I0926 08:53:08.263077  4406 layer_factory.hpp:77] Creating layer Eltwise23
I0926 08:53:08.263080  4406 net.cpp:84] Creating Layer Eltwise23
I0926 08:53:08.263083  4406 net.cpp:406] Eltwise23 <- Eltwise22_penlu45_0_split_1
I0926 08:53:08.263087  4406 net.cpp:406] Eltwise23 <- Convolution49
I0926 08:53:08.263089  4406 net.cpp:380] Eltwise23 -> Eltwise23
I0926 08:53:08.263108  4406 net.cpp:122] Setting up Eltwise23
I0926 08:53:08.263111  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.263113  4406 net.cpp:137] Memory required for data: 852678800
I0926 08:53:08.263115  4406 layer_factory.hpp:77] Creating layer penlu47
I0926 08:53:08.263120  4406 net.cpp:84] Creating Layer penlu47
I0926 08:53:08.263123  4406 net.cpp:406] penlu47 <- Eltwise23
I0926 08:53:08.263126  4406 net.cpp:367] penlu47 -> Eltwise23 (in-place)
I0926 08:53:08.263245  4406 net.cpp:122] Setting up penlu47
I0926 08:53:08.263250  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.263252  4406 net.cpp:137] Memory required for data: 853933200
I0926 08:53:08.263257  4406 layer_factory.hpp:77] Creating layer Eltwise23_penlu47_0_split
I0926 08:53:08.263260  4406 net.cpp:84] Creating Layer Eltwise23_penlu47_0_split
I0926 08:53:08.263262  4406 net.cpp:406] Eltwise23_penlu47_0_split <- Eltwise23
I0926 08:53:08.263265  4406 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_0
I0926 08:53:08.263270  4406 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_1
I0926 08:53:08.263295  4406 net.cpp:122] Setting up Eltwise23_penlu47_0_split
I0926 08:53:08.263298  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.263301  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.263303  4406 net.cpp:137] Memory required for data: 856442000
I0926 08:53:08.263306  4406 layer_factory.hpp:77] Creating layer Convolution50
I0926 08:53:08.263312  4406 net.cpp:84] Creating Layer Convolution50
I0926 08:53:08.263314  4406 net.cpp:406] Convolution50 <- Eltwise23_penlu47_0_split_0
I0926 08:53:08.263319  4406 net.cpp:380] Convolution50 -> Convolution50
I0926 08:53:08.264981  4406 net.cpp:122] Setting up Convolution50
I0926 08:53:08.264991  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.264993  4406 net.cpp:137] Memory required for data: 857696400
I0926 08:53:08.265004  4406 layer_factory.hpp:77] Creating layer BatchNorm50
I0926 08:53:08.265010  4406 net.cpp:84] Creating Layer BatchNorm50
I0926 08:53:08.265013  4406 net.cpp:406] BatchNorm50 <- Convolution50
I0926 08:53:08.265017  4406 net.cpp:367] BatchNorm50 -> Convolution50 (in-place)
I0926 08:53:08.265163  4406 net.cpp:122] Setting up BatchNorm50
I0926 08:53:08.265168  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.265171  4406 net.cpp:137] Memory required for data: 858950800
I0926 08:53:08.265175  4406 layer_factory.hpp:77] Creating layer Scale50
I0926 08:53:08.265178  4406 net.cpp:84] Creating Layer Scale50
I0926 08:53:08.265182  4406 net.cpp:406] Scale50 <- Convolution50
I0926 08:53:08.265185  4406 net.cpp:367] Scale50 -> Convolution50 (in-place)
I0926 08:53:08.265213  4406 layer_factory.hpp:77] Creating layer Scale50
I0926 08:53:08.265297  4406 net.cpp:122] Setting up Scale50
I0926 08:53:08.265302  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.270692  4406 net.cpp:137] Memory required for data: 860205200
I0926 08:53:08.270699  4406 layer_factory.hpp:77] Creating layer penlu48
I0926 08:53:08.270705  4406 net.cpp:84] Creating Layer penlu48
I0926 08:53:08.270709  4406 net.cpp:406] penlu48 <- Convolution50
I0926 08:53:08.270712  4406 net.cpp:367] penlu48 -> Convolution50 (in-place)
I0926 08:53:08.270845  4406 net.cpp:122] Setting up penlu48
I0926 08:53:08.270850  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.270853  4406 net.cpp:137] Memory required for data: 861459600
I0926 08:53:08.270858  4406 layer_factory.hpp:77] Creating layer Convolution51
I0926 08:53:08.270864  4406 net.cpp:84] Creating Layer Convolution51
I0926 08:53:08.270867  4406 net.cpp:406] Convolution51 <- Convolution50
I0926 08:53:08.270871  4406 net.cpp:380] Convolution51 -> Convolution51
I0926 08:53:08.273414  4406 net.cpp:122] Setting up Convolution51
I0926 08:53:08.273423  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.273427  4406 net.cpp:137] Memory required for data: 862714000
I0926 08:53:08.273430  4406 layer_factory.hpp:77] Creating layer BatchNorm51
I0926 08:53:08.273437  4406 net.cpp:84] Creating Layer BatchNorm51
I0926 08:53:08.273439  4406 net.cpp:406] BatchNorm51 <- Convolution51
I0926 08:53:08.273443  4406 net.cpp:367] BatchNorm51 -> Convolution51 (in-place)
I0926 08:53:08.273597  4406 net.cpp:122] Setting up BatchNorm51
I0926 08:53:08.273602  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.273603  4406 net.cpp:137] Memory required for data: 863968400
I0926 08:53:08.273608  4406 layer_factory.hpp:77] Creating layer Scale51
I0926 08:53:08.273612  4406 net.cpp:84] Creating Layer Scale51
I0926 08:53:08.273614  4406 net.cpp:406] Scale51 <- Convolution51
I0926 08:53:08.273618  4406 net.cpp:367] Scale51 -> Convolution51 (in-place)
I0926 08:53:08.273648  4406 layer_factory.hpp:77] Creating layer Scale51
I0926 08:53:08.273733  4406 net.cpp:122] Setting up Scale51
I0926 08:53:08.273738  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.273741  4406 net.cpp:137] Memory required for data: 865222800
I0926 08:53:08.273744  4406 layer_factory.hpp:77] Creating layer Eltwise24
I0926 08:53:08.273748  4406 net.cpp:84] Creating Layer Eltwise24
I0926 08:53:08.273751  4406 net.cpp:406] Eltwise24 <- Eltwise23_penlu47_0_split_1
I0926 08:53:08.273754  4406 net.cpp:406] Eltwise24 <- Convolution51
I0926 08:53:08.273757  4406 net.cpp:380] Eltwise24 -> Eltwise24
I0926 08:53:08.273777  4406 net.cpp:122] Setting up Eltwise24
I0926 08:53:08.273780  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.273782  4406 net.cpp:137] Memory required for data: 866477200
I0926 08:53:08.273784  4406 layer_factory.hpp:77] Creating layer penlu49
I0926 08:53:08.273789  4406 net.cpp:84] Creating Layer penlu49
I0926 08:53:08.273792  4406 net.cpp:406] penlu49 <- Eltwise24
I0926 08:53:08.273795  4406 net.cpp:367] penlu49 -> Eltwise24 (in-place)
I0926 08:53:08.273916  4406 net.cpp:122] Setting up penlu49
I0926 08:53:08.273919  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.273927  4406 net.cpp:137] Memory required for data: 867731600
I0926 08:53:08.273932  4406 layer_factory.hpp:77] Creating layer Eltwise24_penlu49_0_split
I0926 08:53:08.273936  4406 net.cpp:84] Creating Layer Eltwise24_penlu49_0_split
I0926 08:53:08.273938  4406 net.cpp:406] Eltwise24_penlu49_0_split <- Eltwise24
I0926 08:53:08.273942  4406 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_0
I0926 08:53:08.273947  4406 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_1
I0926 08:53:08.273993  4406 net.cpp:122] Setting up Eltwise24_penlu49_0_split
I0926 08:53:08.273998  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.274000  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.274003  4406 net.cpp:137] Memory required for data: 870240400
I0926 08:53:08.274004  4406 layer_factory.hpp:77] Creating layer Convolution52
I0926 08:53:08.274010  4406 net.cpp:84] Creating Layer Convolution52
I0926 08:53:08.274013  4406 net.cpp:406] Convolution52 <- Eltwise24_penlu49_0_split_0
I0926 08:53:08.274019  4406 net.cpp:380] Convolution52 -> Convolution52
I0926 08:53:08.275882  4406 net.cpp:122] Setting up Convolution52
I0926 08:53:08.275890  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.275893  4406 net.cpp:137] Memory required for data: 871494800
I0926 08:53:08.275897  4406 layer_factory.hpp:77] Creating layer BatchNorm52
I0926 08:53:08.275902  4406 net.cpp:84] Creating Layer BatchNorm52
I0926 08:53:08.275905  4406 net.cpp:406] BatchNorm52 <- Convolution52
I0926 08:53:08.275909  4406 net.cpp:367] BatchNorm52 -> Convolution52 (in-place)
I0926 08:53:08.276057  4406 net.cpp:122] Setting up BatchNorm52
I0926 08:53:08.276062  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.276064  4406 net.cpp:137] Memory required for data: 872749200
I0926 08:53:08.276068  4406 layer_factory.hpp:77] Creating layer Scale52
I0926 08:53:08.276072  4406 net.cpp:84] Creating Layer Scale52
I0926 08:53:08.276075  4406 net.cpp:406] Scale52 <- Convolution52
I0926 08:53:08.276078  4406 net.cpp:367] Scale52 -> Convolution52 (in-place)
I0926 08:53:08.276108  4406 layer_factory.hpp:77] Creating layer Scale52
I0926 08:53:08.276192  4406 net.cpp:122] Setting up Scale52
I0926 08:53:08.276197  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.276199  4406 net.cpp:137] Memory required for data: 874003600
I0926 08:53:08.276203  4406 layer_factory.hpp:77] Creating layer penlu50
I0926 08:53:08.276207  4406 net.cpp:84] Creating Layer penlu50
I0926 08:53:08.276211  4406 net.cpp:406] penlu50 <- Convolution52
I0926 08:53:08.276214  4406 net.cpp:367] penlu50 -> Convolution52 (in-place)
I0926 08:53:08.276332  4406 net.cpp:122] Setting up penlu50
I0926 08:53:08.276336  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.276338  4406 net.cpp:137] Memory required for data: 875258000
I0926 08:53:08.276386  4406 layer_factory.hpp:77] Creating layer Convolution53
I0926 08:53:08.276408  4406 net.cpp:84] Creating Layer Convolution53
I0926 08:53:08.276410  4406 net.cpp:406] Convolution53 <- Convolution52
I0926 08:53:08.276414  4406 net.cpp:380] Convolution53 -> Convolution53
I0926 08:53:08.278403  4406 net.cpp:122] Setting up Convolution53
I0926 08:53:08.278414  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.278415  4406 net.cpp:137] Memory required for data: 876512400
I0926 08:53:08.278420  4406 layer_factory.hpp:77] Creating layer BatchNorm53
I0926 08:53:08.278425  4406 net.cpp:84] Creating Layer BatchNorm53
I0926 08:53:08.278427  4406 net.cpp:406] BatchNorm53 <- Convolution53
I0926 08:53:08.278431  4406 net.cpp:367] BatchNorm53 -> Convolution53 (in-place)
I0926 08:53:08.278581  4406 net.cpp:122] Setting up BatchNorm53
I0926 08:53:08.278586  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.278589  4406 net.cpp:137] Memory required for data: 877766800
I0926 08:53:08.278594  4406 layer_factory.hpp:77] Creating layer Scale53
I0926 08:53:08.278597  4406 net.cpp:84] Creating Layer Scale53
I0926 08:53:08.278606  4406 net.cpp:406] Scale53 <- Convolution53
I0926 08:53:08.278610  4406 net.cpp:367] Scale53 -> Convolution53 (in-place)
I0926 08:53:08.278641  4406 layer_factory.hpp:77] Creating layer Scale53
I0926 08:53:08.278726  4406 net.cpp:122] Setting up Scale53
I0926 08:53:08.278731  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.278734  4406 net.cpp:137] Memory required for data: 879021200
I0926 08:53:08.278738  4406 layer_factory.hpp:77] Creating layer Eltwise25
I0926 08:53:08.278743  4406 net.cpp:84] Creating Layer Eltwise25
I0926 08:53:08.278744  4406 net.cpp:406] Eltwise25 <- Eltwise24_penlu49_0_split_1
I0926 08:53:08.278748  4406 net.cpp:406] Eltwise25 <- Convolution53
I0926 08:53:08.278750  4406 net.cpp:380] Eltwise25 -> Eltwise25
I0926 08:53:08.278769  4406 net.cpp:122] Setting up Eltwise25
I0926 08:53:08.278772  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.278774  4406 net.cpp:137] Memory required for data: 880275600
I0926 08:53:08.278777  4406 layer_factory.hpp:77] Creating layer penlu51
I0926 08:53:08.278782  4406 net.cpp:84] Creating Layer penlu51
I0926 08:53:08.278784  4406 net.cpp:406] penlu51 <- Eltwise25
I0926 08:53:08.278789  4406 net.cpp:367] penlu51 -> Eltwise25 (in-place)
I0926 08:53:08.278906  4406 net.cpp:122] Setting up penlu51
I0926 08:53:08.278910  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.278913  4406 net.cpp:137] Memory required for data: 881530000
I0926 08:53:08.278918  4406 layer_factory.hpp:77] Creating layer Eltwise25_penlu51_0_split
I0926 08:53:08.278920  4406 net.cpp:84] Creating Layer Eltwise25_penlu51_0_split
I0926 08:53:08.278923  4406 net.cpp:406] Eltwise25_penlu51_0_split <- Eltwise25
I0926 08:53:08.278926  4406 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_0
I0926 08:53:08.278930  4406 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_1
I0926 08:53:08.278955  4406 net.cpp:122] Setting up Eltwise25_penlu51_0_split
I0926 08:53:08.278959  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.278962  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.278964  4406 net.cpp:137] Memory required for data: 884038800
I0926 08:53:08.278966  4406 layer_factory.hpp:77] Creating layer Convolution54
I0926 08:53:08.278971  4406 net.cpp:84] Creating Layer Convolution54
I0926 08:53:08.278975  4406 net.cpp:406] Convolution54 <- Eltwise25_penlu51_0_split_0
I0926 08:53:08.278978  4406 net.cpp:380] Convolution54 -> Convolution54
I0926 08:53:08.281141  4406 net.cpp:122] Setting up Convolution54
I0926 08:53:08.281152  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.281154  4406 net.cpp:137] Memory required for data: 885293200
I0926 08:53:08.281158  4406 layer_factory.hpp:77] Creating layer BatchNorm54
I0926 08:53:08.281164  4406 net.cpp:84] Creating Layer BatchNorm54
I0926 08:53:08.281167  4406 net.cpp:406] BatchNorm54 <- Convolution54
I0926 08:53:08.281172  4406 net.cpp:367] BatchNorm54 -> Convolution54 (in-place)
I0926 08:53:08.281322  4406 net.cpp:122] Setting up BatchNorm54
I0926 08:53:08.281327  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.281328  4406 net.cpp:137] Memory required for data: 886547600
I0926 08:53:08.281333  4406 layer_factory.hpp:77] Creating layer Scale54
I0926 08:53:08.281338  4406 net.cpp:84] Creating Layer Scale54
I0926 08:53:08.281342  4406 net.cpp:406] Scale54 <- Convolution54
I0926 08:53:08.281344  4406 net.cpp:367] Scale54 -> Convolution54 (in-place)
I0926 08:53:08.281373  4406 layer_factory.hpp:77] Creating layer Scale54
I0926 08:53:08.281460  4406 net.cpp:122] Setting up Scale54
I0926 08:53:08.281464  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.281466  4406 net.cpp:137] Memory required for data: 887802000
I0926 08:53:08.281471  4406 layer_factory.hpp:77] Creating layer penlu52
I0926 08:53:08.281476  4406 net.cpp:84] Creating Layer penlu52
I0926 08:53:08.281478  4406 net.cpp:406] penlu52 <- Convolution54
I0926 08:53:08.281481  4406 net.cpp:367] penlu52 -> Convolution54 (in-place)
I0926 08:53:08.281610  4406 net.cpp:122] Setting up penlu52
I0926 08:53:08.281615  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.281616  4406 net.cpp:137] Memory required for data: 889056400
I0926 08:53:08.281620  4406 layer_factory.hpp:77] Creating layer Convolution55
I0926 08:53:08.281627  4406 net.cpp:84] Creating Layer Convolution55
I0926 08:53:08.281630  4406 net.cpp:406] Convolution55 <- Convolution54
I0926 08:53:08.281633  4406 net.cpp:380] Convolution55 -> Convolution55
I0926 08:53:08.283592  4406 net.cpp:122] Setting up Convolution55
I0926 08:53:08.283601  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.283602  4406 net.cpp:137] Memory required for data: 890310800
I0926 08:53:08.283607  4406 layer_factory.hpp:77] Creating layer BatchNorm55
I0926 08:53:08.283612  4406 net.cpp:84] Creating Layer BatchNorm55
I0926 08:53:08.283614  4406 net.cpp:406] BatchNorm55 <- Convolution55
I0926 08:53:08.283619  4406 net.cpp:367] BatchNorm55 -> Convolution55 (in-place)
I0926 08:53:08.283772  4406 net.cpp:122] Setting up BatchNorm55
I0926 08:53:08.283777  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.283779  4406 net.cpp:137] Memory required for data: 891565200
I0926 08:53:08.283785  4406 layer_factory.hpp:77] Creating layer Scale55
I0926 08:53:08.283789  4406 net.cpp:84] Creating Layer Scale55
I0926 08:53:08.283792  4406 net.cpp:406] Scale55 <- Convolution55
I0926 08:53:08.283795  4406 net.cpp:367] Scale55 -> Convolution55 (in-place)
I0926 08:53:08.283824  4406 layer_factory.hpp:77] Creating layer Scale55
I0926 08:53:08.283910  4406 net.cpp:122] Setting up Scale55
I0926 08:53:08.283915  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.283917  4406 net.cpp:137] Memory required for data: 892819600
I0926 08:53:08.283921  4406 layer_factory.hpp:77] Creating layer Eltwise26
I0926 08:53:08.283924  4406 net.cpp:84] Creating Layer Eltwise26
I0926 08:53:08.283927  4406 net.cpp:406] Eltwise26 <- Eltwise25_penlu51_0_split_1
I0926 08:53:08.283931  4406 net.cpp:406] Eltwise26 <- Convolution55
I0926 08:53:08.283933  4406 net.cpp:380] Eltwise26 -> Eltwise26
I0926 08:53:08.283952  4406 net.cpp:122] Setting up Eltwise26
I0926 08:53:08.283957  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.283958  4406 net.cpp:137] Memory required for data: 894074000
I0926 08:53:08.283960  4406 layer_factory.hpp:77] Creating layer penlu53
I0926 08:53:08.283964  4406 net.cpp:84] Creating Layer penlu53
I0926 08:53:08.283967  4406 net.cpp:406] penlu53 <- Eltwise26
I0926 08:53:08.283970  4406 net.cpp:367] penlu53 -> Eltwise26 (in-place)
I0926 08:53:08.284092  4406 net.cpp:122] Setting up penlu53
I0926 08:53:08.284096  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.284098  4406 net.cpp:137] Memory required for data: 895328400
I0926 08:53:08.284103  4406 layer_factory.hpp:77] Creating layer Eltwise26_penlu53_0_split
I0926 08:53:08.284106  4406 net.cpp:84] Creating Layer Eltwise26_penlu53_0_split
I0926 08:53:08.284108  4406 net.cpp:406] Eltwise26_penlu53_0_split <- Eltwise26
I0926 08:53:08.284112  4406 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_0
I0926 08:53:08.284116  4406 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_1
I0926 08:53:08.284142  4406 net.cpp:122] Setting up Eltwise26_penlu53_0_split
I0926 08:53:08.284145  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.284148  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.284150  4406 net.cpp:137] Memory required for data: 897837200
I0926 08:53:08.284152  4406 layer_factory.hpp:77] Creating layer Convolution56
I0926 08:53:08.284159  4406 net.cpp:84] Creating Layer Convolution56
I0926 08:53:08.284162  4406 net.cpp:406] Convolution56 <- Eltwise26_penlu53_0_split_0
I0926 08:53:08.284166  4406 net.cpp:380] Convolution56 -> Convolution56
I0926 08:53:08.285831  4406 net.cpp:122] Setting up Convolution56
I0926 08:53:08.285840  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.285842  4406 net.cpp:137] Memory required for data: 899091600
I0926 08:53:08.285853  4406 layer_factory.hpp:77] Creating layer BatchNorm56
I0926 08:53:08.285858  4406 net.cpp:84] Creating Layer BatchNorm56
I0926 08:53:08.285861  4406 net.cpp:406] BatchNorm56 <- Convolution56
I0926 08:53:08.285866  4406 net.cpp:367] BatchNorm56 -> Convolution56 (in-place)
I0926 08:53:08.286017  4406 net.cpp:122] Setting up BatchNorm56
I0926 08:53:08.286021  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.286023  4406 net.cpp:137] Memory required for data: 900346000
I0926 08:53:08.286028  4406 layer_factory.hpp:77] Creating layer Scale56
I0926 08:53:08.286033  4406 net.cpp:84] Creating Layer Scale56
I0926 08:53:08.286036  4406 net.cpp:406] Scale56 <- Convolution56
I0926 08:53:08.286038  4406 net.cpp:367] Scale56 -> Convolution56 (in-place)
I0926 08:53:08.286068  4406 layer_factory.hpp:77] Creating layer Scale56
I0926 08:53:08.301340  4406 net.cpp:122] Setting up Scale56
I0926 08:53:08.301350  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.301353  4406 net.cpp:137] Memory required for data: 901600400
I0926 08:53:08.301358  4406 layer_factory.hpp:77] Creating layer penlu54
I0926 08:53:08.301364  4406 net.cpp:84] Creating Layer penlu54
I0926 08:53:08.301367  4406 net.cpp:406] penlu54 <- Convolution56
I0926 08:53:08.301373  4406 net.cpp:367] penlu54 -> Convolution56 (in-place)
I0926 08:53:08.301511  4406 net.cpp:122] Setting up penlu54
I0926 08:53:08.301515  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.301517  4406 net.cpp:137] Memory required for data: 902854800
I0926 08:53:08.301522  4406 layer_factory.hpp:77] Creating layer Convolution57
I0926 08:53:08.301529  4406 net.cpp:84] Creating Layer Convolution57
I0926 08:53:08.301532  4406 net.cpp:406] Convolution57 <- Convolution56
I0926 08:53:08.301537  4406 net.cpp:380] Convolution57 -> Convolution57
I0926 08:53:08.303437  4406 net.cpp:122] Setting up Convolution57
I0926 08:53:08.303447  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.303449  4406 net.cpp:137] Memory required for data: 904109200
I0926 08:53:08.303454  4406 layer_factory.hpp:77] Creating layer BatchNorm57
I0926 08:53:08.303460  4406 net.cpp:84] Creating Layer BatchNorm57
I0926 08:53:08.303463  4406 net.cpp:406] BatchNorm57 <- Convolution57
I0926 08:53:08.303467  4406 net.cpp:367] BatchNorm57 -> Convolution57 (in-place)
I0926 08:53:08.303638  4406 net.cpp:122] Setting up BatchNorm57
I0926 08:53:08.303643  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.303645  4406 net.cpp:137] Memory required for data: 905363600
I0926 08:53:08.303650  4406 layer_factory.hpp:77] Creating layer Scale57
I0926 08:53:08.303655  4406 net.cpp:84] Creating Layer Scale57
I0926 08:53:08.303658  4406 net.cpp:406] Scale57 <- Convolution57
I0926 08:53:08.303661  4406 net.cpp:367] Scale57 -> Convolution57 (in-place)
I0926 08:53:08.303692  4406 layer_factory.hpp:77] Creating layer Scale57
I0926 08:53:08.303783  4406 net.cpp:122] Setting up Scale57
I0926 08:53:08.303788  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.303791  4406 net.cpp:137] Memory required for data: 906618000
I0926 08:53:08.303794  4406 layer_factory.hpp:77] Creating layer Eltwise27
I0926 08:53:08.303799  4406 net.cpp:84] Creating Layer Eltwise27
I0926 08:53:08.303802  4406 net.cpp:406] Eltwise27 <- Eltwise26_penlu53_0_split_1
I0926 08:53:08.303805  4406 net.cpp:406] Eltwise27 <- Convolution57
I0926 08:53:08.303808  4406 net.cpp:380] Eltwise27 -> Eltwise27
I0926 08:53:08.303828  4406 net.cpp:122] Setting up Eltwise27
I0926 08:53:08.303831  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.303833  4406 net.cpp:137] Memory required for data: 907872400
I0926 08:53:08.303835  4406 layer_factory.hpp:77] Creating layer penlu55
I0926 08:53:08.303841  4406 net.cpp:84] Creating Layer penlu55
I0926 08:53:08.303843  4406 net.cpp:406] penlu55 <- Eltwise27
I0926 08:53:08.303846  4406 net.cpp:367] penlu55 -> Eltwise27 (in-place)
I0926 08:53:08.303969  4406 net.cpp:122] Setting up penlu55
I0926 08:53:08.303974  4406 net.cpp:129] Top shape: 100 64 7 7 (313600)
I0926 08:53:08.303983  4406 net.cpp:137] Memory required for data: 909126800
I0926 08:53:08.303988  4406 layer_factory.hpp:77] Creating layer Pooling1
I0926 08:53:08.303992  4406 net.cpp:84] Creating Layer Pooling1
I0926 08:53:08.303995  4406 net.cpp:406] Pooling1 <- Eltwise27
I0926 08:53:08.303999  4406 net.cpp:380] Pooling1 -> Pooling1
I0926 08:53:08.304487  4406 net.cpp:122] Setting up Pooling1
I0926 08:53:08.304505  4406 net.cpp:129] Top shape: 100 64 1 1 (6400)
I0926 08:53:08.304512  4406 net.cpp:137] Memory required for data: 909152400
I0926 08:53:08.304517  4406 layer_factory.hpp:77] Creating layer InnerProduct1
I0926 08:53:08.304527  4406 net.cpp:84] Creating Layer InnerProduct1
I0926 08:53:08.304530  4406 net.cpp:406] InnerProduct1 <- Pooling1
I0926 08:53:08.304535  4406 net.cpp:380] InnerProduct1 -> InnerProduct1
I0926 08:53:08.304652  4406 net.cpp:122] Setting up InnerProduct1
I0926 08:53:08.304657  4406 net.cpp:129] Top shape: 100 10 (1000)
I0926 08:53:08.304659  4406 net.cpp:137] Memory required for data: 909156400
I0926 08:53:08.304663  4406 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0926 08:53:08.304668  4406 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0926 08:53:08.304671  4406 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I0926 08:53:08.304673  4406 net.cpp:406] SoftmaxWithLoss1 <- Data2
I0926 08:53:08.304679  4406 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0926 08:53:08.304685  4406 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0926 08:53:08.304924  4406 net.cpp:122] Setting up SoftmaxWithLoss1
I0926 08:53:08.304929  4406 net.cpp:129] Top shape: (1)
I0926 08:53:08.304932  4406 net.cpp:132]     with loss weight 1
I0926 08:53:08.304944  4406 net.cpp:137] Memory required for data: 909156404
I0926 08:53:08.304947  4406 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0926 08:53:08.304950  4406 net.cpp:198] InnerProduct1 needs backward computation.
I0926 08:53:08.304952  4406 net.cpp:198] Pooling1 needs backward computation.
I0926 08:53:08.304955  4406 net.cpp:198] penlu55 needs backward computation.
I0926 08:53:08.304956  4406 net.cpp:198] Eltwise27 needs backward computation.
I0926 08:53:08.304960  4406 net.cpp:198] Scale57 needs backward computation.
I0926 08:53:08.304961  4406 net.cpp:198] BatchNorm57 needs backward computation.
I0926 08:53:08.304963  4406 net.cpp:198] Convolution57 needs backward computation.
I0926 08:53:08.304965  4406 net.cpp:198] penlu54 needs backward computation.
I0926 08:53:08.304967  4406 net.cpp:198] Scale56 needs backward computation.
I0926 08:53:08.304970  4406 net.cpp:198] BatchNorm56 needs backward computation.
I0926 08:53:08.304971  4406 net.cpp:198] Convolution56 needs backward computation.
I0926 08:53:08.304973  4406 net.cpp:198] Eltwise26_penlu53_0_split needs backward computation.
I0926 08:53:08.304976  4406 net.cpp:198] penlu53 needs backward computation.
I0926 08:53:08.304978  4406 net.cpp:198] Eltwise26 needs backward computation.
I0926 08:53:08.304980  4406 net.cpp:198] Scale55 needs backward computation.
I0926 08:53:08.304982  4406 net.cpp:198] BatchNorm55 needs backward computation.
I0926 08:53:08.304985  4406 net.cpp:198] Convolution55 needs backward computation.
I0926 08:53:08.304987  4406 net.cpp:198] penlu52 needs backward computation.
I0926 08:53:08.304989  4406 net.cpp:198] Scale54 needs backward computation.
I0926 08:53:08.304991  4406 net.cpp:198] BatchNorm54 needs backward computation.
I0926 08:53:08.304993  4406 net.cpp:198] Convolution54 needs backward computation.
I0926 08:53:08.304996  4406 net.cpp:198] Eltwise25_penlu51_0_split needs backward computation.
I0926 08:53:08.304998  4406 net.cpp:198] penlu51 needs backward computation.
I0926 08:53:08.305001  4406 net.cpp:198] Eltwise25 needs backward computation.
I0926 08:53:08.305006  4406 net.cpp:198] Scale53 needs backward computation.
I0926 08:53:08.305007  4406 net.cpp:198] BatchNorm53 needs backward computation.
I0926 08:53:08.305009  4406 net.cpp:198] Convolution53 needs backward computation.
I0926 08:53:08.305011  4406 net.cpp:198] penlu50 needs backward computation.
I0926 08:53:08.305021  4406 net.cpp:198] Scale52 needs backward computation.
I0926 08:53:08.305023  4406 net.cpp:198] BatchNorm52 needs backward computation.
I0926 08:53:08.305025  4406 net.cpp:198] Convolution52 needs backward computation.
I0926 08:53:08.305028  4406 net.cpp:198] Eltwise24_penlu49_0_split needs backward computation.
I0926 08:53:08.305032  4406 net.cpp:198] penlu49 needs backward computation.
I0926 08:53:08.305033  4406 net.cpp:198] Eltwise24 needs backward computation.
I0926 08:53:08.305037  4406 net.cpp:198] Scale51 needs backward computation.
I0926 08:53:08.305038  4406 net.cpp:198] BatchNorm51 needs backward computation.
I0926 08:53:08.305042  4406 net.cpp:198] Convolution51 needs backward computation.
I0926 08:53:08.305043  4406 net.cpp:198] penlu48 needs backward computation.
I0926 08:53:08.305047  4406 net.cpp:198] Scale50 needs backward computation.
I0926 08:53:08.305048  4406 net.cpp:198] BatchNorm50 needs backward computation.
I0926 08:53:08.305050  4406 net.cpp:198] Convolution50 needs backward computation.
I0926 08:53:08.305053  4406 net.cpp:198] Eltwise23_penlu47_0_split needs backward computation.
I0926 08:53:08.305055  4406 net.cpp:198] penlu47 needs backward computation.
I0926 08:53:08.305058  4406 net.cpp:198] Eltwise23 needs backward computation.
I0926 08:53:08.305060  4406 net.cpp:198] Scale49 needs backward computation.
I0926 08:53:08.305063  4406 net.cpp:198] BatchNorm49 needs backward computation.
I0926 08:53:08.305065  4406 net.cpp:198] Convolution49 needs backward computation.
I0926 08:53:08.305068  4406 net.cpp:198] penlu46 needs backward computation.
I0926 08:53:08.305070  4406 net.cpp:198] Scale48 needs backward computation.
I0926 08:53:08.305073  4406 net.cpp:198] BatchNorm48 needs backward computation.
I0926 08:53:08.305074  4406 net.cpp:198] Convolution48 needs backward computation.
I0926 08:53:08.305078  4406 net.cpp:198] Eltwise22_penlu45_0_split needs backward computation.
I0926 08:53:08.305080  4406 net.cpp:198] penlu45 needs backward computation.
I0926 08:53:08.305083  4406 net.cpp:198] Eltwise22 needs backward computation.
I0926 08:53:08.305086  4406 net.cpp:198] Scale47 needs backward computation.
I0926 08:53:08.305088  4406 net.cpp:198] BatchNorm47 needs backward computation.
I0926 08:53:08.305090  4406 net.cpp:198] Convolution47 needs backward computation.
I0926 08:53:08.305094  4406 net.cpp:198] penlu44 needs backward computation.
I0926 08:53:08.305095  4406 net.cpp:198] Scale46 needs backward computation.
I0926 08:53:08.305097  4406 net.cpp:198] BatchNorm46 needs backward computation.
I0926 08:53:08.305099  4406 net.cpp:198] Convolution46 needs backward computation.
I0926 08:53:08.305102  4406 net.cpp:198] Eltwise21_penlu43_0_split needs backward computation.
I0926 08:53:08.305104  4406 net.cpp:198] penlu43 needs backward computation.
I0926 08:53:08.305107  4406 net.cpp:198] Eltwise21 needs backward computation.
I0926 08:53:08.305109  4406 net.cpp:198] Scale45 needs backward computation.
I0926 08:53:08.305112  4406 net.cpp:198] BatchNorm45 needs backward computation.
I0926 08:53:08.305114  4406 net.cpp:198] Convolution45 needs backward computation.
I0926 08:53:08.305117  4406 net.cpp:198] penlu42 needs backward computation.
I0926 08:53:08.305119  4406 net.cpp:198] Scale44 needs backward computation.
I0926 08:53:08.305121  4406 net.cpp:198] BatchNorm44 needs backward computation.
I0926 08:53:08.305124  4406 net.cpp:198] Convolution44 needs backward computation.
I0926 08:53:08.305126  4406 net.cpp:198] Eltwise20_penlu41_0_split needs backward computation.
I0926 08:53:08.305128  4406 net.cpp:198] penlu41 needs backward computation.
I0926 08:53:08.305131  4406 net.cpp:198] Eltwise20 needs backward computation.
I0926 08:53:08.305133  4406 net.cpp:198] Scale43 needs backward computation.
I0926 08:53:08.305136  4406 net.cpp:198] BatchNorm43 needs backward computation.
I0926 08:53:08.305138  4406 net.cpp:198] Convolution43 needs backward computation.
I0926 08:53:08.305140  4406 net.cpp:198] penlu40 needs backward computation.
I0926 08:53:08.305143  4406 net.cpp:198] Scale42 needs backward computation.
I0926 08:53:08.305148  4406 net.cpp:198] BatchNorm42 needs backward computation.
I0926 08:53:08.305150  4406 net.cpp:198] Convolution42 needs backward computation.
I0926 08:53:08.305153  4406 net.cpp:198] Eltwise19_penlu39_0_split needs backward computation.
I0926 08:53:08.305156  4406 net.cpp:198] penlu39 needs backward computation.
I0926 08:53:08.305158  4406 net.cpp:198] Eltwise19 needs backward computation.
I0926 08:53:08.305161  4406 net.cpp:198] Scale41 needs backward computation.
I0926 08:53:08.305163  4406 net.cpp:198] BatchNorm41 needs backward computation.
I0926 08:53:08.305166  4406 net.cpp:198] Convolution41 needs backward computation.
I0926 08:53:08.305177  4406 net.cpp:198] penlu38 needs backward computation.
I0926 08:53:08.305181  4406 net.cpp:198] Scale40 needs backward computation.
I0926 08:53:08.305182  4406 net.cpp:198] BatchNorm40 needs backward computation.
I0926 08:53:08.305184  4406 net.cpp:198] Convolution40 needs backward computation.
I0926 08:53:08.305187  4406 net.cpp:198] Scale39 needs backward computation.
I0926 08:53:08.305189  4406 net.cpp:198] BatchNorm39 needs backward computation.
I0926 08:53:08.305191  4406 net.cpp:198] Convolution39 needs backward computation.
I0926 08:53:08.305207  4406 net.cpp:198] Eltwise18_penlu37_0_split needs backward computation.
I0926 08:53:08.305210  4406 net.cpp:198] penlu37 needs backward computation.
I0926 08:53:08.305213  4406 net.cpp:198] Eltwise18 needs backward computation.
I0926 08:53:08.305224  4406 net.cpp:198] Scale38 needs backward computation.
I0926 08:53:08.305227  4406 net.cpp:198] BatchNorm38 needs backward computation.
I0926 08:53:08.305229  4406 net.cpp:198] Convolution38 needs backward computation.
I0926 08:53:08.305232  4406 net.cpp:198] penlu36 needs backward computation.
I0926 08:53:08.305234  4406 net.cpp:198] Scale37 needs backward computation.
I0926 08:53:08.305236  4406 net.cpp:198] BatchNorm37 needs backward computation.
I0926 08:53:08.305238  4406 net.cpp:198] Convolution37 needs backward computation.
I0926 08:53:08.305241  4406 net.cpp:198] Eltwise17_penlu35_0_split needs backward computation.
I0926 08:53:08.305243  4406 net.cpp:198] penlu35 needs backward computation.
I0926 08:53:08.305245  4406 net.cpp:198] Eltwise17 needs backward computation.
I0926 08:53:08.305248  4406 net.cpp:198] Scale36 needs backward computation.
I0926 08:53:08.305250  4406 net.cpp:198] BatchNorm36 needs backward computation.
I0926 08:53:08.305253  4406 net.cpp:198] Convolution36 needs backward computation.
I0926 08:53:08.305255  4406 net.cpp:198] penlu34 needs backward computation.
I0926 08:53:08.305258  4406 net.cpp:198] Scale35 needs backward computation.
I0926 08:53:08.305259  4406 net.cpp:198] BatchNorm35 needs backward computation.
I0926 08:53:08.305270  4406 net.cpp:198] Convolution35 needs backward computation.
I0926 08:53:08.305274  4406 net.cpp:198] Eltwise16_penlu33_0_split needs backward computation.
I0926 08:53:08.305275  4406 net.cpp:198] penlu33 needs backward computation.
I0926 08:53:08.305277  4406 net.cpp:198] Eltwise16 needs backward computation.
I0926 08:53:08.305280  4406 net.cpp:198] Scale34 needs backward computation.
I0926 08:53:08.305282  4406 net.cpp:198] BatchNorm34 needs backward computation.
I0926 08:53:08.305285  4406 net.cpp:198] Convolution34 needs backward computation.
I0926 08:53:08.305287  4406 net.cpp:198] penlu32 needs backward computation.
I0926 08:53:08.305289  4406 net.cpp:198] Scale33 needs backward computation.
I0926 08:53:08.305291  4406 net.cpp:198] BatchNorm33 needs backward computation.
I0926 08:53:08.305294  4406 net.cpp:198] Convolution33 needs backward computation.
I0926 08:53:08.305296  4406 net.cpp:198] Eltwise15_penlu31_0_split needs backward computation.
I0926 08:53:08.305299  4406 net.cpp:198] penlu31 needs backward computation.
I0926 08:53:08.305301  4406 net.cpp:198] Eltwise15 needs backward computation.
I0926 08:53:08.305304  4406 net.cpp:198] Scale32 needs backward computation.
I0926 08:53:08.305305  4406 net.cpp:198] BatchNorm32 needs backward computation.
I0926 08:53:08.305310  4406 net.cpp:198] Convolution32 needs backward computation.
I0926 08:53:08.305313  4406 net.cpp:198] penlu30 needs backward computation.
I0926 08:53:08.305315  4406 net.cpp:198] Scale31 needs backward computation.
I0926 08:53:08.305317  4406 net.cpp:198] BatchNorm31 needs backward computation.
I0926 08:53:08.305320  4406 net.cpp:198] Convolution31 needs backward computation.
I0926 08:53:08.305322  4406 net.cpp:198] Eltwise14_penlu29_0_split needs backward computation.
I0926 08:53:08.305325  4406 net.cpp:198] penlu29 needs backward computation.
I0926 08:53:08.305327  4406 net.cpp:198] Eltwise14 needs backward computation.
I0926 08:53:08.305330  4406 net.cpp:198] Scale30 needs backward computation.
I0926 08:53:08.331576  4406 net.cpp:198] BatchNorm30 needs backward computation.
I0926 08:53:08.331584  4406 net.cpp:198] Convolution30 needs backward computation.
I0926 08:53:08.331588  4406 net.cpp:198] penlu28 needs backward computation.
I0926 08:53:08.331590  4406 net.cpp:198] Scale29 needs backward computation.
I0926 08:53:08.331593  4406 net.cpp:198] BatchNorm29 needs backward computation.
I0926 08:53:08.331594  4406 net.cpp:198] Convolution29 needs backward computation.
I0926 08:53:08.331598  4406 net.cpp:198] Eltwise13_penlu27_0_split needs backward computation.
I0926 08:53:08.331601  4406 net.cpp:198] penlu27 needs backward computation.
I0926 08:53:08.331604  4406 net.cpp:198] Eltwise13 needs backward computation.
I0926 08:53:08.331606  4406 net.cpp:198] Scale28 needs backward computation.
I0926 08:53:08.331609  4406 net.cpp:198] BatchNorm28 needs backward computation.
I0926 08:53:08.331612  4406 net.cpp:198] Convolution28 needs backward computation.
I0926 08:53:08.331615  4406 net.cpp:198] penlu26 needs backward computation.
I0926 08:53:08.331617  4406 net.cpp:198] Scale27 needs backward computation.
I0926 08:53:08.331620  4406 net.cpp:198] BatchNorm27 needs backward computation.
I0926 08:53:08.331622  4406 net.cpp:198] Convolution27 needs backward computation.
I0926 08:53:08.331625  4406 net.cpp:198] Eltwise12_penlu25_0_split needs backward computation.
I0926 08:53:08.331629  4406 net.cpp:198] penlu25 needs backward computation.
I0926 08:53:08.331630  4406 net.cpp:198] Eltwise12 needs backward computation.
I0926 08:53:08.331634  4406 net.cpp:198] Scale26 needs backward computation.
I0926 08:53:08.331636  4406 net.cpp:198] BatchNorm26 needs backward computation.
I0926 08:53:08.331638  4406 net.cpp:198] Convolution26 needs backward computation.
I0926 08:53:08.331641  4406 net.cpp:198] penlu24 needs backward computation.
I0926 08:53:08.331643  4406 net.cpp:198] Scale25 needs backward computation.
I0926 08:53:08.331646  4406 net.cpp:198] BatchNorm25 needs backward computation.
I0926 08:53:08.331648  4406 net.cpp:198] Convolution25 needs backward computation.
I0926 08:53:08.331651  4406 net.cpp:198] Eltwise11_penlu23_0_split needs backward computation.
I0926 08:53:08.331655  4406 net.cpp:198] penlu23 needs backward computation.
I0926 08:53:08.331656  4406 net.cpp:198] Eltwise11 needs backward computation.
I0926 08:53:08.331660  4406 net.cpp:198] Scale24 needs backward computation.
I0926 08:53:08.331662  4406 net.cpp:198] BatchNorm24 needs backward computation.
I0926 08:53:08.331665  4406 net.cpp:198] Convolution24 needs backward computation.
I0926 08:53:08.331667  4406 net.cpp:198] penlu22 needs backward computation.
I0926 08:53:08.331670  4406 net.cpp:198] Scale23 needs backward computation.
I0926 08:53:08.331672  4406 net.cpp:198] BatchNorm23 needs backward computation.
I0926 08:53:08.331674  4406 net.cpp:198] Convolution23 needs backward computation.
I0926 08:53:08.331677  4406 net.cpp:198] Eltwise10_penlu21_0_split needs backward computation.
I0926 08:53:08.331681  4406 net.cpp:198] penlu21 needs backward computation.
I0926 08:53:08.331682  4406 net.cpp:198] Eltwise10 needs backward computation.
I0926 08:53:08.331686  4406 net.cpp:198] Scale22 needs backward computation.
I0926 08:53:08.331688  4406 net.cpp:198] BatchNorm22 needs backward computation.
I0926 08:53:08.331691  4406 net.cpp:198] Convolution22 needs backward computation.
I0926 08:53:08.331701  4406 net.cpp:198] penlu20 needs backward computation.
I0926 08:53:08.331704  4406 net.cpp:198] Scale21 needs backward computation.
I0926 08:53:08.331707  4406 net.cpp:198] BatchNorm21 needs backward computation.
I0926 08:53:08.331709  4406 net.cpp:198] Convolution21 needs backward computation.
I0926 08:53:08.331712  4406 net.cpp:198] Scale20 needs backward computation.
I0926 08:53:08.331714  4406 net.cpp:198] BatchNorm20 needs backward computation.
I0926 08:53:08.331717  4406 net.cpp:198] Convolution20 needs backward computation.
I0926 08:53:08.331719  4406 net.cpp:198] Eltwise9_penlu19_0_split needs backward computation.
I0926 08:53:08.331722  4406 net.cpp:198] penlu19 needs backward computation.
I0926 08:53:08.331725  4406 net.cpp:198] Eltwise9 needs backward computation.
I0926 08:53:08.331728  4406 net.cpp:198] Scale19 needs backward computation.
I0926 08:53:08.331730  4406 net.cpp:198] BatchNorm19 needs backward computation.
I0926 08:53:08.331733  4406 net.cpp:198] Convolution19 needs backward computation.
I0926 08:53:08.331735  4406 net.cpp:198] penlu18 needs backward computation.
I0926 08:53:08.331738  4406 net.cpp:198] Scale18 needs backward computation.
I0926 08:53:08.331740  4406 net.cpp:198] BatchNorm18 needs backward computation.
I0926 08:53:08.331743  4406 net.cpp:198] Convolution18 needs backward computation.
I0926 08:53:08.331748  4406 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I0926 08:53:08.331750  4406 net.cpp:198] penlu17 needs backward computation.
I0926 08:53:08.331753  4406 net.cpp:198] Eltwise8 needs backward computation.
I0926 08:53:08.331755  4406 net.cpp:198] Scale17 needs backward computation.
I0926 08:53:08.331758  4406 net.cpp:198] BatchNorm17 needs backward computation.
I0926 08:53:08.331760  4406 net.cpp:198] Convolution17 needs backward computation.
I0926 08:53:08.331763  4406 net.cpp:198] penlu16 needs backward computation.
I0926 08:53:08.331765  4406 net.cpp:198] Scale16 needs backward computation.
I0926 08:53:08.331768  4406 net.cpp:198] BatchNorm16 needs backward computation.
I0926 08:53:08.331770  4406 net.cpp:198] Convolution16 needs backward computation.
I0926 08:53:08.331773  4406 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I0926 08:53:08.331776  4406 net.cpp:198] penlu15 needs backward computation.
I0926 08:53:08.331779  4406 net.cpp:198] Eltwise7 needs backward computation.
I0926 08:53:08.331781  4406 net.cpp:198] Scale15 needs backward computation.
I0926 08:53:08.331784  4406 net.cpp:198] BatchNorm15 needs backward computation.
I0926 08:53:08.331786  4406 net.cpp:198] Convolution15 needs backward computation.
I0926 08:53:08.331789  4406 net.cpp:198] penlu14 needs backward computation.
I0926 08:53:08.331791  4406 net.cpp:198] Scale14 needs backward computation.
I0926 08:53:08.331794  4406 net.cpp:198] BatchNorm14 needs backward computation.
I0926 08:53:08.331796  4406 net.cpp:198] Convolution14 needs backward computation.
I0926 08:53:08.331799  4406 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I0926 08:53:08.331801  4406 net.cpp:198] penlu13 needs backward computation.
I0926 08:53:08.331804  4406 net.cpp:198] Eltwise6 needs backward computation.
I0926 08:53:08.331807  4406 net.cpp:198] Scale13 needs backward computation.
I0926 08:53:08.331810  4406 net.cpp:198] BatchNorm13 needs backward computation.
I0926 08:53:08.331812  4406 net.cpp:198] Convolution13 needs backward computation.
I0926 08:53:08.331815  4406 net.cpp:198] penlu12 needs backward computation.
I0926 08:53:08.331817  4406 net.cpp:198] Scale12 needs backward computation.
I0926 08:53:08.331820  4406 net.cpp:198] BatchNorm12 needs backward computation.
I0926 08:53:08.331822  4406 net.cpp:198] Convolution12 needs backward computation.
I0926 08:53:08.331825  4406 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I0926 08:53:08.331827  4406 net.cpp:198] penlu11 needs backward computation.
I0926 08:53:08.331830  4406 net.cpp:198] Eltwise5 needs backward computation.
I0926 08:53:08.331833  4406 net.cpp:198] Scale11 needs backward computation.
I0926 08:53:08.331838  4406 net.cpp:198] BatchNorm11 needs backward computation.
I0926 08:53:08.331841  4406 net.cpp:198] Convolution11 needs backward computation.
I0926 08:53:08.331845  4406 net.cpp:198] penlu10 needs backward computation.
I0926 08:53:08.331846  4406 net.cpp:198] Scale10 needs backward computation.
I0926 08:53:08.331849  4406 net.cpp:198] BatchNorm10 needs backward computation.
I0926 08:53:08.331851  4406 net.cpp:198] Convolution10 needs backward computation.
I0926 08:53:08.331854  4406 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I0926 08:53:08.333703  4406 net.cpp:198] penlu9 needs backward computation.
I0926 08:53:08.333711  4406 net.cpp:198] Eltwise4 needs backward computation.
I0926 08:53:08.333715  4406 net.cpp:198] Scale9 needs backward computation.
I0926 08:53:08.333717  4406 net.cpp:198] BatchNorm9 needs backward computation.
I0926 08:53:08.333720  4406 net.cpp:198] Convolution9 needs backward computation.
I0926 08:53:08.333724  4406 net.cpp:198] penlu8 needs backward computation.
I0926 08:53:08.333725  4406 net.cpp:198] Scale8 needs backward computation.
I0926 08:53:08.333729  4406 net.cpp:198] BatchNorm8 needs backward computation.
I0926 08:53:08.333730  4406 net.cpp:198] Convolution8 needs backward computation.
I0926 08:53:08.333734  4406 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I0926 08:53:08.333736  4406 net.cpp:198] penlu7 needs backward computation.
I0926 08:53:08.333739  4406 net.cpp:198] Eltwise3 needs backward computation.
I0926 08:53:08.333741  4406 net.cpp:198] Scale7 needs backward computation.
I0926 08:53:08.333745  4406 net.cpp:198] BatchNorm7 needs backward computation.
I0926 08:53:08.333746  4406 net.cpp:198] Convolution7 needs backward computation.
I0926 08:53:08.333748  4406 net.cpp:198] penlu6 needs backward computation.
I0926 08:53:08.333751  4406 net.cpp:198] Scale6 needs backward computation.
I0926 08:53:08.333753  4406 net.cpp:198] BatchNorm6 needs backward computation.
I0926 08:53:08.333755  4406 net.cpp:198] Convolution6 needs backward computation.
I0926 08:53:08.333758  4406 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I0926 08:53:08.333761  4406 net.cpp:198] penlu5 needs backward computation.
I0926 08:53:08.333763  4406 net.cpp:198] Eltwise2 needs backward computation.
I0926 08:53:08.333776  4406 net.cpp:198] Scale5 needs backward computation.
I0926 08:53:08.333778  4406 net.cpp:198] BatchNorm5 needs backward computation.
I0926 08:53:08.333781  4406 net.cpp:198] Convolution5 needs backward computation.
I0926 08:53:08.333783  4406 net.cpp:198] penlu4 needs backward computation.
I0926 08:53:08.333787  4406 net.cpp:198] Scale4 needs backward computation.
I0926 08:53:08.333789  4406 net.cpp:198] BatchNorm4 needs backward computation.
I0926 08:53:08.333791  4406 net.cpp:198] Convolution4 needs backward computation.
I0926 08:53:08.333794  4406 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I0926 08:53:08.333797  4406 net.cpp:198] penlu3 needs backward computation.
I0926 08:53:08.333799  4406 net.cpp:198] Eltwise1 needs backward computation.
I0926 08:53:08.333802  4406 net.cpp:198] Scale3 needs backward computation.
I0926 08:53:08.333804  4406 net.cpp:198] BatchNorm3 needs backward computation.
I0926 08:53:08.333806  4406 net.cpp:198] Convolution3 needs backward computation.
I0926 08:53:08.333809  4406 net.cpp:198] penlu2 needs backward computation.
I0926 08:53:08.333811  4406 net.cpp:198] Scale2 needs backward computation.
I0926 08:53:08.333813  4406 net.cpp:198] BatchNorm2 needs backward computation.
I0926 08:53:08.333817  4406 net.cpp:198] Convolution2 needs backward computation.
I0926 08:53:08.333818  4406 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I0926 08:53:08.333822  4406 net.cpp:198] penlu1 needs backward computation.
I0926 08:53:08.333823  4406 net.cpp:198] Scale1 needs backward computation.
I0926 08:53:08.333827  4406 net.cpp:198] BatchNorm1 needs backward computation.
I0926 08:53:08.333828  4406 net.cpp:198] Convolution1 needs backward computation.
I0926 08:53:08.333837  4406 net.cpp:200] Data1 does not need backward computation.
I0926 08:53:08.333839  4406 net.cpp:242] This network produces output SoftmaxWithLoss1
I0926 08:53:08.333925  4406 net.cpp:255] Network initialization done.
I0926 08:53:08.338614  4406 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0926 08:53:08.338626  4406 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0926 08:53:08.338631  4406 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/PENLU/neural/resnet/res56_penlu_train_test.prototxt
I0926 08:53:08.338822  4406 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I0926 08:53:08.340034  4406 net.cpp:51] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution17"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution19"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu20"
  type: "PENLU"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution20"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu21"
  type: "PENLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu22"
  type: "PENLU"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 2
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      
I0926 08:53:08.395112  4406 layer_factory.hpp:77] Creating layer Data1
I0926 08:53:08.395159  4406 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/PENLU/data/cifar10/cifar10_test_lmdb
I0926 08:53:08.395170  4406 net.cpp:84] Creating Layer Data1
I0926 08:53:08.395175  4406 net.cpp:380] Data1 -> Data1
I0926 08:53:08.395182  4406 net.cpp:380] Data1 -> Data2
I0926 08:53:08.395190  4406 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/PENLU/data/cifar10/mean.binaryproto
I0926 08:53:08.395359  4406 data_layer.cpp:45] output data size: 100,3,32,32
I0926 08:53:08.399438  4406 net.cpp:122] Setting up Data1
I0926 08:53:08.399459  4406 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0926 08:53:08.399463  4406 net.cpp:129] Top shape: 100 (100)
I0926 08:53:08.399466  4406 net.cpp:137] Memory required for data: 1229200
I0926 08:53:08.399471  4406 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0926 08:53:08.399480  4406 net.cpp:84] Creating Layer Data2_Data1_1_split
I0926 08:53:08.399483  4406 net.cpp:406] Data2_Data1_1_split <- Data2
I0926 08:53:08.399487  4406 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0926 08:53:08.399495  4406 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0926 08:53:08.399575  4406 net.cpp:122] Setting up Data2_Data1_1_split
I0926 08:53:08.399585  4406 net.cpp:129] Top shape: 100 (100)
I0926 08:53:08.399588  4406 net.cpp:129] Top shape: 100 (100)
I0926 08:53:08.399590  4406 net.cpp:137] Memory required for data: 1230000
I0926 08:53:08.399593  4406 layer_factory.hpp:77] Creating layer Convolution1
I0926 08:53:08.399603  4406 net.cpp:84] Creating Layer Convolution1
I0926 08:53:08.399606  4406 net.cpp:406] Convolution1 <- Data1
I0926 08:53:08.399611  4406 net.cpp:380] Convolution1 -> Convolution1
I0926 08:53:08.400847  4406 net.cpp:122] Setting up Convolution1
I0926 08:53:08.400858  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.400876  4406 net.cpp:137] Memory required for data: 7783600
I0926 08:53:08.400884  4406 layer_factory.hpp:77] Creating layer BatchNorm1
I0926 08:53:08.400892  4406 net.cpp:84] Creating Layer BatchNorm1
I0926 08:53:08.400893  4406 net.cpp:406] BatchNorm1 <- Convolution1
I0926 08:53:08.400897  4406 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0926 08:53:08.401051  4406 net.cpp:122] Setting up BatchNorm1
I0926 08:53:08.401057  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.401059  4406 net.cpp:137] Memory required for data: 14337200
I0926 08:53:08.401067  4406 layer_factory.hpp:77] Creating layer Scale1
I0926 08:53:08.401072  4406 net.cpp:84] Creating Layer Scale1
I0926 08:53:08.401074  4406 net.cpp:406] Scale1 <- Convolution1
I0926 08:53:08.401082  4406 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0926 08:53:08.401114  4406 layer_factory.hpp:77] Creating layer Scale1
I0926 08:53:08.401201  4406 net.cpp:122] Setting up Scale1
I0926 08:53:08.401206  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.401207  4406 net.cpp:137] Memory required for data: 20890800
I0926 08:53:08.401212  4406 layer_factory.hpp:77] Creating layer penlu1
I0926 08:53:08.401218  4406 net.cpp:84] Creating Layer penlu1
I0926 08:53:08.401221  4406 net.cpp:406] penlu1 <- Convolution1
I0926 08:53:08.401226  4406 net.cpp:367] penlu1 -> Convolution1 (in-place)
I0926 08:53:08.401850  4406 net.cpp:122] Setting up penlu1
I0926 08:53:08.401859  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.401861  4406 net.cpp:137] Memory required for data: 27444400
I0926 08:53:08.401868  4406 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I0926 08:53:08.401875  4406 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I0926 08:53:08.401877  4406 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I0926 08:53:08.401881  4406 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I0926 08:53:08.401885  4406 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I0926 08:53:08.401913  4406 net.cpp:122] Setting up Convolution1_penlu1_0_split
I0926 08:53:08.401917  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.422559  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.422567  4406 net.cpp:137] Memory required for data: 40551600
I0926 08:53:08.422570  4406 layer_factory.hpp:77] Creating layer Convolution2
I0926 08:53:08.422579  4406 net.cpp:84] Creating Layer Convolution2
I0926 08:53:08.422581  4406 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I0926 08:53:08.422588  4406 net.cpp:380] Convolution2 -> Convolution2
I0926 08:53:08.423720  4406 net.cpp:122] Setting up Convolution2
I0926 08:53:08.423730  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.423733  4406 net.cpp:137] Memory required for data: 47105200
I0926 08:53:08.423738  4406 layer_factory.hpp:77] Creating layer BatchNorm2
I0926 08:53:08.423746  4406 net.cpp:84] Creating Layer BatchNorm2
I0926 08:53:08.423749  4406 net.cpp:406] BatchNorm2 <- Convolution2
I0926 08:53:08.423754  4406 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0926 08:53:08.423939  4406 net.cpp:122] Setting up BatchNorm2
I0926 08:53:08.423946  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.423949  4406 net.cpp:137] Memory required for data: 53658800
I0926 08:53:08.423955  4406 layer_factory.hpp:77] Creating layer Scale2
I0926 08:53:08.423961  4406 net.cpp:84] Creating Layer Scale2
I0926 08:53:08.423964  4406 net.cpp:406] Scale2 <- Convolution2
I0926 08:53:08.423969  4406 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0926 08:53:08.424005  4406 layer_factory.hpp:77] Creating layer Scale2
I0926 08:53:08.424103  4406 net.cpp:122] Setting up Scale2
I0926 08:53:08.424113  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.424118  4406 net.cpp:137] Memory required for data: 60212400
I0926 08:53:08.424129  4406 layer_factory.hpp:77] Creating layer penlu2
I0926 08:53:08.424139  4406 net.cpp:84] Creating Layer penlu2
I0926 08:53:08.424156  4406 net.cpp:406] penlu2 <- Convolution2
I0926 08:53:08.424166  4406 net.cpp:367] penlu2 -> Convolution2 (in-place)
I0926 08:53:08.424391  4406 net.cpp:122] Setting up penlu2
I0926 08:53:08.424409  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.424412  4406 net.cpp:137] Memory required for data: 66766000
I0926 08:53:08.424417  4406 layer_factory.hpp:77] Creating layer Convolution3
I0926 08:53:08.424424  4406 net.cpp:84] Creating Layer Convolution3
I0926 08:53:08.424427  4406 net.cpp:406] Convolution3 <- Convolution2
I0926 08:53:08.424432  4406 net.cpp:380] Convolution3 -> Convolution3
I0926 08:53:08.425500  4406 net.cpp:122] Setting up Convolution3
I0926 08:53:08.425510  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.425513  4406 net.cpp:137] Memory required for data: 73319600
I0926 08:53:08.425518  4406 layer_factory.hpp:77] Creating layer BatchNorm3
I0926 08:53:08.425523  4406 net.cpp:84] Creating Layer BatchNorm3
I0926 08:53:08.425525  4406 net.cpp:406] BatchNorm3 <- Convolution3
I0926 08:53:08.425529  4406 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0926 08:53:08.425680  4406 net.cpp:122] Setting up BatchNorm3
I0926 08:53:08.425684  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.425688  4406 net.cpp:137] Memory required for data: 79873200
I0926 08:53:08.425691  4406 layer_factory.hpp:77] Creating layer Scale3
I0926 08:53:08.425695  4406 net.cpp:84] Creating Layer Scale3
I0926 08:53:08.425698  4406 net.cpp:406] Scale3 <- Convolution3
I0926 08:53:08.425703  4406 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0926 08:53:08.425731  4406 layer_factory.hpp:77] Creating layer Scale3
I0926 08:53:08.425815  4406 net.cpp:122] Setting up Scale3
I0926 08:53:08.425819  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.425822  4406 net.cpp:137] Memory required for data: 86426800
I0926 08:53:08.425825  4406 layer_factory.hpp:77] Creating layer Eltwise1
I0926 08:53:08.425830  4406 net.cpp:84] Creating Layer Eltwise1
I0926 08:53:08.425832  4406 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I0926 08:53:08.425835  4406 net.cpp:406] Eltwise1 <- Convolution3
I0926 08:53:08.425839  4406 net.cpp:380] Eltwise1 -> Eltwise1
I0926 08:53:08.425858  4406 net.cpp:122] Setting up Eltwise1
I0926 08:53:08.425871  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.425873  4406 net.cpp:137] Memory required for data: 92980400
I0926 08:53:08.425875  4406 layer_factory.hpp:77] Creating layer penlu3
I0926 08:53:08.425882  4406 net.cpp:84] Creating Layer penlu3
I0926 08:53:08.425884  4406 net.cpp:406] penlu3 <- Eltwise1
I0926 08:53:08.425887  4406 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I0926 08:53:08.426070  4406 net.cpp:122] Setting up penlu3
I0926 08:53:08.426075  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.426077  4406 net.cpp:137] Memory required for data: 99534000
I0926 08:53:08.426082  4406 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I0926 08:53:08.426089  4406 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I0926 08:53:08.426090  4406 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I0926 08:53:08.426095  4406 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I0926 08:53:08.426098  4406 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I0926 08:53:08.426125  4406 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I0926 08:53:08.426128  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.426131  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.426133  4406 net.cpp:137] Memory required for data: 112641200
I0926 08:53:08.426136  4406 layer_factory.hpp:77] Creating layer Convolution4
I0926 08:53:08.426141  4406 net.cpp:84] Creating Layer Convolution4
I0926 08:53:08.426143  4406 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I0926 08:53:08.426147  4406 net.cpp:380] Convolution4 -> Convolution4
I0926 08:53:08.427295  4406 net.cpp:122] Setting up Convolution4
I0926 08:53:08.427312  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.427316  4406 net.cpp:137] Memory required for data: 119194800
I0926 08:53:08.427320  4406 layer_factory.hpp:77] Creating layer BatchNorm4
I0926 08:53:08.427326  4406 net.cpp:84] Creating Layer BatchNorm4
I0926 08:53:08.427330  4406 net.cpp:406] BatchNorm4 <- Convolution4
I0926 08:53:08.427333  4406 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0926 08:53:08.427489  4406 net.cpp:122] Setting up BatchNorm4
I0926 08:53:08.427495  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.427496  4406 net.cpp:137] Memory required for data: 125748400
I0926 08:53:08.427503  4406 layer_factory.hpp:77] Creating layer Scale4
I0926 08:53:08.427508  4406 net.cpp:84] Creating Layer Scale4
I0926 08:53:08.427510  4406 net.cpp:406] Scale4 <- Convolution4
I0926 08:53:08.427515  4406 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0926 08:53:08.427546  4406 layer_factory.hpp:77] Creating layer Scale4
I0926 08:53:08.427631  4406 net.cpp:122] Setting up Scale4
I0926 08:53:08.427636  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.427639  4406 net.cpp:137] Memory required for data: 132302000
I0926 08:53:08.427642  4406 layer_factory.hpp:77] Creating layer penlu4
I0926 08:53:08.427649  4406 net.cpp:84] Creating Layer penlu4
I0926 08:53:08.427650  4406 net.cpp:406] penlu4 <- Convolution4
I0926 08:53:08.427654  4406 net.cpp:367] penlu4 -> Convolution4 (in-place)
I0926 08:53:08.427783  4406 net.cpp:122] Setting up penlu4
I0926 08:53:08.427788  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.427790  4406 net.cpp:137] Memory required for data: 138855600
I0926 08:53:08.427794  4406 layer_factory.hpp:77] Creating layer Convolution5
I0926 08:53:08.427801  4406 net.cpp:84] Creating Layer Convolution5
I0926 08:53:08.427803  4406 net.cpp:406] Convolution5 <- Convolution4
I0926 08:53:08.427808  4406 net.cpp:380] Convolution5 -> Convolution5
I0926 08:53:08.428766  4406 net.cpp:122] Setting up Convolution5
I0926 08:53:08.428776  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.428778  4406 net.cpp:137] Memory required for data: 145409200
I0926 08:53:08.428783  4406 layer_factory.hpp:77] Creating layer BatchNorm5
I0926 08:53:08.428788  4406 net.cpp:84] Creating Layer BatchNorm5
I0926 08:53:08.428791  4406 net.cpp:406] BatchNorm5 <- Convolution5
I0926 08:53:08.428795  4406 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0926 08:53:08.428949  4406 net.cpp:122] Setting up BatchNorm5
I0926 08:53:08.428954  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.428956  4406 net.cpp:137] Memory required for data: 151962800
I0926 08:53:08.428961  4406 layer_factory.hpp:77] Creating layer Scale5
I0926 08:53:08.428966  4406 net.cpp:84] Creating Layer Scale5
I0926 08:53:08.428968  4406 net.cpp:406] Scale5 <- Convolution5
I0926 08:53:08.428972  4406 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0926 08:53:08.429002  4406 layer_factory.hpp:77] Creating layer Scale5
I0926 08:53:08.429086  4406 net.cpp:122] Setting up Scale5
I0926 08:53:08.429090  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.429093  4406 net.cpp:137] Memory required for data: 158516400
I0926 08:53:08.429096  4406 layer_factory.hpp:77] Creating layer Eltwise2
I0926 08:53:08.429100  4406 net.cpp:84] Creating Layer Eltwise2
I0926 08:53:08.429103  4406 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I0926 08:53:08.429105  4406 net.cpp:406] Eltwise2 <- Convolution5
I0926 08:53:08.429110  4406 net.cpp:380] Eltwise2 -> Eltwise2
I0926 08:53:08.429127  4406 net.cpp:122] Setting up Eltwise2
I0926 08:53:08.429131  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.429132  4406 net.cpp:137] Memory required for data: 165070000
I0926 08:53:08.429136  4406 layer_factory.hpp:77] Creating layer penlu5
I0926 08:53:08.429141  4406 net.cpp:84] Creating Layer penlu5
I0926 08:53:08.429143  4406 net.cpp:406] penlu5 <- Eltwise2
I0926 08:53:08.429147  4406 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I0926 08:53:08.429275  4406 net.cpp:122] Setting up penlu5
I0926 08:53:08.429286  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.429289  4406 net.cpp:137] Memory required for data: 171623600
I0926 08:53:08.429293  4406 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I0926 08:53:08.429297  4406 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I0926 08:53:08.429299  4406 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I0926 08:53:08.429302  4406 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I0926 08:53:08.429306  4406 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I0926 08:53:08.429335  4406 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I0926 08:53:08.429339  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.429342  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.429344  4406 net.cpp:137] Memory required for data: 184730800
I0926 08:53:08.429347  4406 layer_factory.hpp:77] Creating layer Convolution6
I0926 08:53:08.429353  4406 net.cpp:84] Creating Layer Convolution6
I0926 08:53:08.429355  4406 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I0926 08:53:08.429359  4406 net.cpp:380] Convolution6 -> Convolution6
I0926 08:53:08.430296  4406 net.cpp:122] Setting up Convolution6
I0926 08:53:08.430305  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.430310  4406 net.cpp:137] Memory required for data: 191284400
I0926 08:53:08.430313  4406 layer_factory.hpp:77] Creating layer BatchNorm6
I0926 08:53:08.430318  4406 net.cpp:84] Creating Layer BatchNorm6
I0926 08:53:08.430320  4406 net.cpp:406] BatchNorm6 <- Convolution6
I0926 08:53:08.430325  4406 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0926 08:53:08.430477  4406 net.cpp:122] Setting up BatchNorm6
I0926 08:53:08.430482  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.430485  4406 net.cpp:137] Memory required for data: 197838000
I0926 08:53:08.430488  4406 layer_factory.hpp:77] Creating layer Scale6
I0926 08:53:08.430492  4406 net.cpp:84] Creating Layer Scale6
I0926 08:53:08.430495  4406 net.cpp:406] Scale6 <- Convolution6
I0926 08:53:08.430498  4406 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0926 08:53:08.430528  4406 layer_factory.hpp:77] Creating layer Scale6
I0926 08:53:08.430615  4406 net.cpp:122] Setting up Scale6
I0926 08:53:08.430619  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.430621  4406 net.cpp:137] Memory required for data: 204391600
I0926 08:53:08.430625  4406 layer_factory.hpp:77] Creating layer penlu6
I0926 08:53:08.430631  4406 net.cpp:84] Creating Layer penlu6
I0926 08:53:08.430634  4406 net.cpp:406] penlu6 <- Convolution6
I0926 08:53:08.430637  4406 net.cpp:367] penlu6 -> Convolution6 (in-place)
I0926 08:53:08.430764  4406 net.cpp:122] Setting up penlu6
I0926 08:53:08.430769  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.430771  4406 net.cpp:137] Memory required for data: 210945200
I0926 08:53:08.430775  4406 layer_factory.hpp:77] Creating layer Convolution7
I0926 08:53:08.430783  4406 net.cpp:84] Creating Layer Convolution7
I0926 08:53:08.430784  4406 net.cpp:406] Convolution7 <- Convolution6
I0926 08:53:08.430788  4406 net.cpp:380] Convolution7 -> Convolution7
I0926 08:53:08.431720  4406 net.cpp:122] Setting up Convolution7
I0926 08:53:08.431728  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.431731  4406 net.cpp:137] Memory required for data: 217498800
I0926 08:53:08.431735  4406 layer_factory.hpp:77] Creating layer BatchNorm7
I0926 08:53:08.431742  4406 net.cpp:84] Creating Layer BatchNorm7
I0926 08:53:08.431746  4406 net.cpp:406] BatchNorm7 <- Convolution7
I0926 08:53:08.431749  4406 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0926 08:53:08.431900  4406 net.cpp:122] Setting up BatchNorm7
I0926 08:53:08.431903  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.431905  4406 net.cpp:137] Memory required for data: 224052400
I0926 08:53:08.431915  4406 layer_factory.hpp:77] Creating layer Scale7
I0926 08:53:08.431921  4406 net.cpp:84] Creating Layer Scale7
I0926 08:53:08.431929  4406 net.cpp:406] Scale7 <- Convolution7
I0926 08:53:08.431933  4406 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0926 08:53:08.431965  4406 layer_factory.hpp:77] Creating layer Scale7
I0926 08:53:08.432052  4406 net.cpp:122] Setting up Scale7
I0926 08:53:08.432057  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.432060  4406 net.cpp:137] Memory required for data: 230606000
I0926 08:53:08.432063  4406 layer_factory.hpp:77] Creating layer Eltwise3
I0926 08:53:08.432067  4406 net.cpp:84] Creating Layer Eltwise3
I0926 08:53:08.432070  4406 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I0926 08:53:08.432073  4406 net.cpp:406] Eltwise3 <- Convolution7
I0926 08:53:08.432076  4406 net.cpp:380] Eltwise3 -> Eltwise3
I0926 08:53:08.432095  4406 net.cpp:122] Setting up Eltwise3
I0926 08:53:08.432098  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.432101  4406 net.cpp:137] Memory required for data: 237159600
I0926 08:53:08.432102  4406 layer_factory.hpp:77] Creating layer penlu7
I0926 08:53:08.432107  4406 net.cpp:84] Creating Layer penlu7
I0926 08:53:08.432109  4406 net.cpp:406] penlu7 <- Eltwise3
I0926 08:53:08.432113  4406 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I0926 08:53:08.432241  4406 net.cpp:122] Setting up penlu7
I0926 08:53:08.432245  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.432247  4406 net.cpp:137] Memory required for data: 243713200
I0926 08:53:08.432252  4406 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I0926 08:53:08.432255  4406 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I0926 08:53:08.432257  4406 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I0926 08:53:08.432261  4406 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I0926 08:53:08.432265  4406 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I0926 08:53:08.432291  4406 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I0926 08:53:08.432294  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.432297  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.453099  4406 net.cpp:137] Memory required for data: 256820400
I0926 08:53:08.453107  4406 layer_factory.hpp:77] Creating layer Convolution8
I0926 08:53:08.453115  4406 net.cpp:84] Creating Layer Convolution8
I0926 08:53:08.453119  4406 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I0926 08:53:08.453126  4406 net.cpp:380] Convolution8 -> Convolution8
I0926 08:53:08.454196  4406 net.cpp:122] Setting up Convolution8
I0926 08:53:08.454207  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.454210  4406 net.cpp:137] Memory required for data: 263374000
I0926 08:53:08.454216  4406 layer_factory.hpp:77] Creating layer BatchNorm8
I0926 08:53:08.454221  4406 net.cpp:84] Creating Layer BatchNorm8
I0926 08:53:08.454224  4406 net.cpp:406] BatchNorm8 <- Convolution8
I0926 08:53:08.454228  4406 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0926 08:53:08.454414  4406 net.cpp:122] Setting up BatchNorm8
I0926 08:53:08.454422  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.454424  4406 net.cpp:137] Memory required for data: 269927600
I0926 08:53:08.454430  4406 layer_factory.hpp:77] Creating layer Scale8
I0926 08:53:08.454437  4406 net.cpp:84] Creating Layer Scale8
I0926 08:53:08.454439  4406 net.cpp:406] Scale8 <- Convolution8
I0926 08:53:08.454443  4406 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0926 08:53:08.454479  4406 layer_factory.hpp:77] Creating layer Scale8
I0926 08:53:08.454588  4406 net.cpp:122] Setting up Scale8
I0926 08:53:08.454598  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.454602  4406 net.cpp:137] Memory required for data: 276481200
I0926 08:53:08.454610  4406 layer_factory.hpp:77] Creating layer penlu8
I0926 08:53:08.454622  4406 net.cpp:84] Creating Layer penlu8
I0926 08:53:08.454627  4406 net.cpp:406] penlu8 <- Convolution8
I0926 08:53:08.454632  4406 net.cpp:367] penlu8 -> Convolution8 (in-place)
I0926 08:53:08.454809  4406 net.cpp:122] Setting up penlu8
I0926 08:53:08.454826  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.454829  4406 net.cpp:137] Memory required for data: 283034800
I0926 08:53:08.454834  4406 layer_factory.hpp:77] Creating layer Convolution9
I0926 08:53:08.454843  4406 net.cpp:84] Creating Layer Convolution9
I0926 08:53:08.454845  4406 net.cpp:406] Convolution9 <- Convolution8
I0926 08:53:08.454859  4406 net.cpp:380] Convolution9 -> Convolution9
I0926 08:53:08.455896  4406 net.cpp:122] Setting up Convolution9
I0926 08:53:08.455905  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.455909  4406 net.cpp:137] Memory required for data: 289588400
I0926 08:53:08.455914  4406 layer_factory.hpp:77] Creating layer BatchNorm9
I0926 08:53:08.455917  4406 net.cpp:84] Creating Layer BatchNorm9
I0926 08:53:08.455920  4406 net.cpp:406] BatchNorm9 <- Convolution9
I0926 08:53:08.455924  4406 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0926 08:53:08.456079  4406 net.cpp:122] Setting up BatchNorm9
I0926 08:53:08.456084  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.456085  4406 net.cpp:137] Memory required for data: 296142000
I0926 08:53:08.456090  4406 layer_factory.hpp:77] Creating layer Scale9
I0926 08:53:08.456094  4406 net.cpp:84] Creating Layer Scale9
I0926 08:53:08.456096  4406 net.cpp:406] Scale9 <- Convolution9
I0926 08:53:08.456099  4406 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0926 08:53:08.456130  4406 layer_factory.hpp:77] Creating layer Scale9
I0926 08:53:08.456215  4406 net.cpp:122] Setting up Scale9
I0926 08:53:08.456220  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.456223  4406 net.cpp:137] Memory required for data: 302695600
I0926 08:53:08.456236  4406 layer_factory.hpp:77] Creating layer Eltwise4
I0926 08:53:08.456240  4406 net.cpp:84] Creating Layer Eltwise4
I0926 08:53:08.456243  4406 net.cpp:406] Eltwise4 <- Eltwise3_penlu7_0_split_1
I0926 08:53:08.456246  4406 net.cpp:406] Eltwise4 <- Convolution9
I0926 08:53:08.456250  4406 net.cpp:380] Eltwise4 -> Eltwise4
I0926 08:53:08.456277  4406 net.cpp:122] Setting up Eltwise4
I0926 08:53:08.456282  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.456284  4406 net.cpp:137] Memory required for data: 309249200
I0926 08:53:08.456286  4406 layer_factory.hpp:77] Creating layer penlu9
I0926 08:53:08.456293  4406 net.cpp:84] Creating Layer penlu9
I0926 08:53:08.456295  4406 net.cpp:406] penlu9 <- Eltwise4
I0926 08:53:08.456308  4406 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I0926 08:53:08.456481  4406 net.cpp:122] Setting up penlu9
I0926 08:53:08.456486  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.456488  4406 net.cpp:137] Memory required for data: 315802800
I0926 08:53:08.456493  4406 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I0926 08:53:08.456518  4406 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I0926 08:53:08.456522  4406 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I0926 08:53:08.456526  4406 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I0926 08:53:08.456540  4406 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I0926 08:53:08.456568  4406 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I0926 08:53:08.456573  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.456575  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.456578  4406 net.cpp:137] Memory required for data: 328910000
I0926 08:53:08.456580  4406 layer_factory.hpp:77] Creating layer Convolution10
I0926 08:53:08.456586  4406 net.cpp:84] Creating Layer Convolution10
I0926 08:53:08.456589  4406 net.cpp:406] Convolution10 <- Eltwise4_penlu9_0_split_0
I0926 08:53:08.456593  4406 net.cpp:380] Convolution10 -> Convolution10
I0926 08:53:08.457818  4406 net.cpp:122] Setting up Convolution10
I0926 08:53:08.457828  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.457830  4406 net.cpp:137] Memory required for data: 335463600
I0926 08:53:08.457834  4406 layer_factory.hpp:77] Creating layer BatchNorm10
I0926 08:53:08.457847  4406 net.cpp:84] Creating Layer BatchNorm10
I0926 08:53:08.457850  4406 net.cpp:406] BatchNorm10 <- Convolution10
I0926 08:53:08.457854  4406 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0926 08:53:08.458010  4406 net.cpp:122] Setting up BatchNorm10
I0926 08:53:08.458015  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.458017  4406 net.cpp:137] Memory required for data: 342017200
I0926 08:53:08.458022  4406 layer_factory.hpp:77] Creating layer Scale10
I0926 08:53:08.458026  4406 net.cpp:84] Creating Layer Scale10
I0926 08:53:08.458029  4406 net.cpp:406] Scale10 <- Convolution10
I0926 08:53:08.458034  4406 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0926 08:53:08.458062  4406 layer_factory.hpp:77] Creating layer Scale10
I0926 08:53:08.458149  4406 net.cpp:122] Setting up Scale10
I0926 08:53:08.458154  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.458156  4406 net.cpp:137] Memory required for data: 348570800
I0926 08:53:08.458160  4406 layer_factory.hpp:77] Creating layer penlu10
I0926 08:53:08.458165  4406 net.cpp:84] Creating Layer penlu10
I0926 08:53:08.458168  4406 net.cpp:406] penlu10 <- Convolution10
I0926 08:53:08.458171  4406 net.cpp:367] penlu10 -> Convolution10 (in-place)
I0926 08:53:08.458299  4406 net.cpp:122] Setting up penlu10
I0926 08:53:08.458303  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.458305  4406 net.cpp:137] Memory required for data: 355124400
I0926 08:53:08.458310  4406 layer_factory.hpp:77] Creating layer Convolution11
I0926 08:53:08.458317  4406 net.cpp:84] Creating Layer Convolution11
I0926 08:53:08.458319  4406 net.cpp:406] Convolution11 <- Convolution10
I0926 08:53:08.458324  4406 net.cpp:380] Convolution11 -> Convolution11
I0926 08:53:08.459584  4406 net.cpp:122] Setting up Convolution11
I0926 08:53:08.459592  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.459595  4406 net.cpp:137] Memory required for data: 361678000
I0926 08:53:08.459599  4406 layer_factory.hpp:77] Creating layer BatchNorm11
I0926 08:53:08.459605  4406 net.cpp:84] Creating Layer BatchNorm11
I0926 08:53:08.459607  4406 net.cpp:406] BatchNorm11 <- Convolution11
I0926 08:53:08.459611  4406 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0926 08:53:08.459770  4406 net.cpp:122] Setting up BatchNorm11
I0926 08:53:08.459774  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.459777  4406 net.cpp:137] Memory required for data: 368231600
I0926 08:53:08.459781  4406 layer_factory.hpp:77] Creating layer Scale11
I0926 08:53:08.459785  4406 net.cpp:84] Creating Layer Scale11
I0926 08:53:08.459789  4406 net.cpp:406] Scale11 <- Convolution11
I0926 08:53:08.459792  4406 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0926 08:53:08.459822  4406 layer_factory.hpp:77] Creating layer Scale11
I0926 08:53:08.459908  4406 net.cpp:122] Setting up Scale11
I0926 08:53:08.459913  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.459914  4406 net.cpp:137] Memory required for data: 374785200
I0926 08:53:08.459918  4406 layer_factory.hpp:77] Creating layer Eltwise5
I0926 08:53:08.459923  4406 net.cpp:84] Creating Layer Eltwise5
I0926 08:53:08.459925  4406 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I0926 08:53:08.459928  4406 net.cpp:406] Eltwise5 <- Convolution11
I0926 08:53:08.459931  4406 net.cpp:380] Eltwise5 -> Eltwise5
I0926 08:53:08.459949  4406 net.cpp:122] Setting up Eltwise5
I0926 08:53:08.459954  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.459955  4406 net.cpp:137] Memory required for data: 381338800
I0926 08:53:08.459957  4406 layer_factory.hpp:77] Creating layer penlu11
I0926 08:53:08.459962  4406 net.cpp:84] Creating Layer penlu11
I0926 08:53:08.459965  4406 net.cpp:406] penlu11 <- Eltwise5
I0926 08:53:08.459969  4406 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I0926 08:53:08.460099  4406 net.cpp:122] Setting up penlu11
I0926 08:53:08.460103  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.460105  4406 net.cpp:137] Memory required for data: 387892400
I0926 08:53:08.460116  4406 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I0926 08:53:08.460120  4406 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I0926 08:53:08.460124  4406 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I0926 08:53:08.460126  4406 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I0926 08:53:08.460131  4406 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I0926 08:53:08.460158  4406 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I0926 08:53:08.460162  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.460165  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.460167  4406 net.cpp:137] Memory required for data: 400999600
I0926 08:53:08.460170  4406 layer_factory.hpp:77] Creating layer Convolution12
I0926 08:53:08.460175  4406 net.cpp:84] Creating Layer Convolution12
I0926 08:53:08.460177  4406 net.cpp:406] Convolution12 <- Eltwise5_penlu11_0_split_0
I0926 08:53:08.460182  4406 net.cpp:380] Convolution12 -> Convolution12
I0926 08:53:08.461169  4406 net.cpp:122] Setting up Convolution12
I0926 08:53:08.461179  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.461181  4406 net.cpp:137] Memory required for data: 407553200
I0926 08:53:08.461186  4406 layer_factory.hpp:77] Creating layer BatchNorm12
I0926 08:53:08.461191  4406 net.cpp:84] Creating Layer BatchNorm12
I0926 08:53:08.461194  4406 net.cpp:406] BatchNorm12 <- Convolution12
I0926 08:53:08.461197  4406 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0926 08:53:08.461354  4406 net.cpp:122] Setting up BatchNorm12
I0926 08:53:08.461359  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.461361  4406 net.cpp:137] Memory required for data: 414106800
I0926 08:53:08.461366  4406 layer_factory.hpp:77] Creating layer Scale12
I0926 08:53:08.461370  4406 net.cpp:84] Creating Layer Scale12
I0926 08:53:08.461372  4406 net.cpp:406] Scale12 <- Convolution12
I0926 08:53:08.461375  4406 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0926 08:53:08.461407  4406 layer_factory.hpp:77] Creating layer Scale12
I0926 08:53:08.461494  4406 net.cpp:122] Setting up Scale12
I0926 08:53:08.461499  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.461501  4406 net.cpp:137] Memory required for data: 420660400
I0926 08:53:08.461505  4406 layer_factory.hpp:77] Creating layer penlu12
I0926 08:53:08.461511  4406 net.cpp:84] Creating Layer penlu12
I0926 08:53:08.461513  4406 net.cpp:406] penlu12 <- Convolution12
I0926 08:53:08.461518  4406 net.cpp:367] penlu12 -> Convolution12 (in-place)
I0926 08:53:08.461645  4406 net.cpp:122] Setting up penlu12
I0926 08:53:08.461649  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.461652  4406 net.cpp:137] Memory required for data: 427214000
I0926 08:53:08.461655  4406 layer_factory.hpp:77] Creating layer Convolution13
I0926 08:53:08.461663  4406 net.cpp:84] Creating Layer Convolution13
I0926 08:53:08.461664  4406 net.cpp:406] Convolution13 <- Convolution12
I0926 08:53:08.461668  4406 net.cpp:380] Convolution13 -> Convolution13
I0926 08:53:08.462635  4406 net.cpp:122] Setting up Convolution13
I0926 08:53:08.462643  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.462646  4406 net.cpp:137] Memory required for data: 433767600
I0926 08:53:08.462651  4406 layer_factory.hpp:77] Creating layer BatchNorm13
I0926 08:53:08.462656  4406 net.cpp:84] Creating Layer BatchNorm13
I0926 08:53:08.462657  4406 net.cpp:406] BatchNorm13 <- Convolution13
I0926 08:53:08.462661  4406 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0926 08:53:08.462817  4406 net.cpp:122] Setting up BatchNorm13
I0926 08:53:08.462821  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.462824  4406 net.cpp:137] Memory required for data: 440321200
I0926 08:53:08.462828  4406 layer_factory.hpp:77] Creating layer Scale13
I0926 08:53:08.462832  4406 net.cpp:84] Creating Layer Scale13
I0926 08:53:08.462836  4406 net.cpp:406] Scale13 <- Convolution13
I0926 08:53:08.462846  4406 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0926 08:53:08.462878  4406 layer_factory.hpp:77] Creating layer Scale13
I0926 08:53:08.462966  4406 net.cpp:122] Setting up Scale13
I0926 08:53:08.462970  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.462972  4406 net.cpp:137] Memory required for data: 446874800
I0926 08:53:08.462976  4406 layer_factory.hpp:77] Creating layer Eltwise6
I0926 08:53:08.462985  4406 net.cpp:84] Creating Layer Eltwise6
I0926 08:53:08.462987  4406 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I0926 08:53:08.462990  4406 net.cpp:406] Eltwise6 <- Convolution13
I0926 08:53:08.462994  4406 net.cpp:380] Eltwise6 -> Eltwise6
I0926 08:53:08.463013  4406 net.cpp:122] Setting up Eltwise6
I0926 08:53:08.463018  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.463021  4406 net.cpp:137] Memory required for data: 453428400
I0926 08:53:08.463022  4406 layer_factory.hpp:77] Creating layer penlu13
I0926 08:53:08.463027  4406 net.cpp:84] Creating Layer penlu13
I0926 08:53:08.463029  4406 net.cpp:406] penlu13 <- Eltwise6
I0926 08:53:08.463033  4406 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I0926 08:53:08.463177  4406 net.cpp:122] Setting up penlu13
I0926 08:53:08.463181  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.463183  4406 net.cpp:137] Memory required for data: 459982000
I0926 08:53:08.463208  4406 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I0926 08:53:08.463212  4406 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I0926 08:53:08.463227  4406 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I0926 08:53:08.463230  4406 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I0926 08:53:08.463244  4406 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I0926 08:53:08.463274  4406 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I0926 08:53:08.463277  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.463279  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.483515  4406 net.cpp:137] Memory required for data: 473089200
I0926 08:53:08.483523  4406 layer_factory.hpp:77] Creating layer Convolution14
I0926 08:53:08.483533  4406 net.cpp:84] Creating Layer Convolution14
I0926 08:53:08.483536  4406 net.cpp:406] Convolution14 <- Eltwise6_penlu13_0_split_0
I0926 08:53:08.483542  4406 net.cpp:380] Convolution14 -> Convolution14
I0926 08:53:08.484618  4406 net.cpp:122] Setting up Convolution14
I0926 08:53:08.484628  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.484632  4406 net.cpp:137] Memory required for data: 479642800
I0926 08:53:08.484637  4406 layer_factory.hpp:77] Creating layer BatchNorm14
I0926 08:53:08.484642  4406 net.cpp:84] Creating Layer BatchNorm14
I0926 08:53:08.484644  4406 net.cpp:406] BatchNorm14 <- Convolution14
I0926 08:53:08.484648  4406 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0926 08:53:08.484844  4406 net.cpp:122] Setting up BatchNorm14
I0926 08:53:08.484861  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.484864  4406 net.cpp:137] Memory required for data: 486196400
I0926 08:53:08.484869  4406 layer_factory.hpp:77] Creating layer Scale14
I0926 08:53:08.484874  4406 net.cpp:84] Creating Layer Scale14
I0926 08:53:08.484877  4406 net.cpp:406] Scale14 <- Convolution14
I0926 08:53:08.484881  4406 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0926 08:53:08.484925  4406 layer_factory.hpp:77] Creating layer Scale14
I0926 08:53:08.485047  4406 net.cpp:122] Setting up Scale14
I0926 08:53:08.485057  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.485061  4406 net.cpp:137] Memory required for data: 492750000
I0926 08:53:08.485069  4406 layer_factory.hpp:77] Creating layer penlu14
I0926 08:53:08.485080  4406 net.cpp:84] Creating Layer penlu14
I0926 08:53:08.485085  4406 net.cpp:406] penlu14 <- Convolution14
I0926 08:53:08.485091  4406 net.cpp:367] penlu14 -> Convolution14 (in-place)
I0926 08:53:08.485244  4406 net.cpp:122] Setting up penlu14
I0926 08:53:08.485256  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.485260  4406 net.cpp:137] Memory required for data: 499303600
I0926 08:53:08.485263  4406 layer_factory.hpp:77] Creating layer Convolution15
I0926 08:53:08.485271  4406 net.cpp:84] Creating Layer Convolution15
I0926 08:53:08.485275  4406 net.cpp:406] Convolution15 <- Convolution14
I0926 08:53:08.485278  4406 net.cpp:380] Convolution15 -> Convolution15
I0926 08:53:08.486328  4406 net.cpp:122] Setting up Convolution15
I0926 08:53:08.486337  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.486340  4406 net.cpp:137] Memory required for data: 505857200
I0926 08:53:08.486344  4406 layer_factory.hpp:77] Creating layer BatchNorm15
I0926 08:53:08.486349  4406 net.cpp:84] Creating Layer BatchNorm15
I0926 08:53:08.486351  4406 net.cpp:406] BatchNorm15 <- Convolution15
I0926 08:53:08.486356  4406 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0926 08:53:08.486512  4406 net.cpp:122] Setting up BatchNorm15
I0926 08:53:08.486516  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.486519  4406 net.cpp:137] Memory required for data: 512410800
I0926 08:53:08.486523  4406 layer_factory.hpp:77] Creating layer Scale15
I0926 08:53:08.486527  4406 net.cpp:84] Creating Layer Scale15
I0926 08:53:08.486531  4406 net.cpp:406] Scale15 <- Convolution15
I0926 08:53:08.486533  4406 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0926 08:53:08.486564  4406 layer_factory.hpp:77] Creating layer Scale15
I0926 08:53:08.486673  4406 net.cpp:122] Setting up Scale15
I0926 08:53:08.486678  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.486680  4406 net.cpp:137] Memory required for data: 518964400
I0926 08:53:08.486685  4406 layer_factory.hpp:77] Creating layer Eltwise7
I0926 08:53:08.486688  4406 net.cpp:84] Creating Layer Eltwise7
I0926 08:53:08.486691  4406 net.cpp:406] Eltwise7 <- Eltwise6_penlu13_0_split_1
I0926 08:53:08.486694  4406 net.cpp:406] Eltwise7 <- Convolution15
I0926 08:53:08.486698  4406 net.cpp:380] Eltwise7 -> Eltwise7
I0926 08:53:08.486726  4406 net.cpp:122] Setting up Eltwise7
I0926 08:53:08.486740  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.486742  4406 net.cpp:137] Memory required for data: 525518000
I0926 08:53:08.486744  4406 layer_factory.hpp:77] Creating layer penlu15
I0926 08:53:08.486750  4406 net.cpp:84] Creating Layer penlu15
I0926 08:53:08.486752  4406 net.cpp:406] penlu15 <- Eltwise7
I0926 08:53:08.486757  4406 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I0926 08:53:08.486909  4406 net.cpp:122] Setting up penlu15
I0926 08:53:08.486913  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.486915  4406 net.cpp:137] Memory required for data: 532071600
I0926 08:53:08.486919  4406 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I0926 08:53:08.486923  4406 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I0926 08:53:08.486925  4406 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I0926 08:53:08.486929  4406 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I0926 08:53:08.486933  4406 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I0926 08:53:08.486959  4406 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I0926 08:53:08.486963  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.486966  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.486968  4406 net.cpp:137] Memory required for data: 545178800
I0926 08:53:08.486970  4406 layer_factory.hpp:77] Creating layer Convolution16
I0926 08:53:08.486977  4406 net.cpp:84] Creating Layer Convolution16
I0926 08:53:08.486979  4406 net.cpp:406] Convolution16 <- Eltwise7_penlu15_0_split_0
I0926 08:53:08.486994  4406 net.cpp:380] Convolution16 -> Convolution16
I0926 08:53:08.487669  4406 net.cpp:122] Setting up Convolution16
I0926 08:53:08.487676  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.487679  4406 net.cpp:137] Memory required for data: 551732400
I0926 08:53:08.487682  4406 layer_factory.hpp:77] Creating layer BatchNorm16
I0926 08:53:08.487694  4406 net.cpp:84] Creating Layer BatchNorm16
I0926 08:53:08.487699  4406 net.cpp:406] BatchNorm16 <- Convolution16
I0926 08:53:08.487701  4406 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0926 08:53:08.487860  4406 net.cpp:122] Setting up BatchNorm16
I0926 08:53:08.487864  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.487866  4406 net.cpp:137] Memory required for data: 558286000
I0926 08:53:08.487871  4406 layer_factory.hpp:77] Creating layer Scale16
I0926 08:53:08.487875  4406 net.cpp:84] Creating Layer Scale16
I0926 08:53:08.487877  4406 net.cpp:406] Scale16 <- Convolution16
I0926 08:53:08.487880  4406 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0926 08:53:08.487912  4406 layer_factory.hpp:77] Creating layer Scale16
I0926 08:53:08.487999  4406 net.cpp:122] Setting up Scale16
I0926 08:53:08.488004  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.488006  4406 net.cpp:137] Memory required for data: 564839600
I0926 08:53:08.488010  4406 layer_factory.hpp:77] Creating layer penlu16
I0926 08:53:08.488015  4406 net.cpp:84] Creating Layer penlu16
I0926 08:53:08.488018  4406 net.cpp:406] penlu16 <- Convolution16
I0926 08:53:08.488021  4406 net.cpp:367] penlu16 -> Convolution16 (in-place)
I0926 08:53:08.488152  4406 net.cpp:122] Setting up penlu16
I0926 08:53:08.488155  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.488157  4406 net.cpp:137] Memory required for data: 571393200
I0926 08:53:08.488162  4406 layer_factory.hpp:77] Creating layer Convolution17
I0926 08:53:08.488168  4406 net.cpp:84] Creating Layer Convolution17
I0926 08:53:08.488170  4406 net.cpp:406] Convolution17 <- Convolution16
I0926 08:53:08.488174  4406 net.cpp:380] Convolution17 -> Convolution17
I0926 08:53:08.489145  4406 net.cpp:122] Setting up Convolution17
I0926 08:53:08.489154  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.489156  4406 net.cpp:137] Memory required for data: 577946800
I0926 08:53:08.489161  4406 layer_factory.hpp:77] Creating layer BatchNorm17
I0926 08:53:08.489166  4406 net.cpp:84] Creating Layer BatchNorm17
I0926 08:53:08.489168  4406 net.cpp:406] BatchNorm17 <- Convolution17
I0926 08:53:08.489173  4406 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0926 08:53:08.489329  4406 net.cpp:122] Setting up BatchNorm17
I0926 08:53:08.489333  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.489336  4406 net.cpp:137] Memory required for data: 584500400
I0926 08:53:08.489341  4406 layer_factory.hpp:77] Creating layer Scale17
I0926 08:53:08.489344  4406 net.cpp:84] Creating Layer Scale17
I0926 08:53:08.489347  4406 net.cpp:406] Scale17 <- Convolution17
I0926 08:53:08.489351  4406 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0926 08:53:08.489382  4406 layer_factory.hpp:77] Creating layer Scale17
I0926 08:53:08.489470  4406 net.cpp:122] Setting up Scale17
I0926 08:53:08.489475  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.489476  4406 net.cpp:137] Memory required for data: 591054000
I0926 08:53:08.489480  4406 layer_factory.hpp:77] Creating layer Eltwise8
I0926 08:53:08.489485  4406 net.cpp:84] Creating Layer Eltwise8
I0926 08:53:08.489487  4406 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I0926 08:53:08.489490  4406 net.cpp:406] Eltwise8 <- Convolution17
I0926 08:53:08.489493  4406 net.cpp:380] Eltwise8 -> Eltwise8
I0926 08:53:08.489512  4406 net.cpp:122] Setting up Eltwise8
I0926 08:53:08.489516  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.489518  4406 net.cpp:137] Memory required for data: 597607600
I0926 08:53:08.489521  4406 layer_factory.hpp:77] Creating layer penlu17
I0926 08:53:08.489526  4406 net.cpp:84] Creating Layer penlu17
I0926 08:53:08.489527  4406 net.cpp:406] penlu17 <- Eltwise8
I0926 08:53:08.489531  4406 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I0926 08:53:08.489666  4406 net.cpp:122] Setting up penlu17
I0926 08:53:08.489671  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.489673  4406 net.cpp:137] Memory required for data: 604161200
I0926 08:53:08.489683  4406 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I0926 08:53:08.489688  4406 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I0926 08:53:08.489691  4406 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I0926 08:53:08.489694  4406 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I0926 08:53:08.489699  4406 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I0926 08:53:08.489727  4406 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I0926 08:53:08.489730  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.489733  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.489735  4406 net.cpp:137] Memory required for data: 617268400
I0926 08:53:08.489737  4406 layer_factory.hpp:77] Creating layer Convolution18
I0926 08:53:08.489743  4406 net.cpp:84] Creating Layer Convolution18
I0926 08:53:08.489747  4406 net.cpp:406] Convolution18 <- Eltwise8_penlu17_0_split_0
I0926 08:53:08.489750  4406 net.cpp:380] Convolution18 -> Convolution18
I0926 08:53:08.490737  4406 net.cpp:122] Setting up Convolution18
I0926 08:53:08.490746  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.490749  4406 net.cpp:137] Memory required for data: 623822000
I0926 08:53:08.490753  4406 layer_factory.hpp:77] Creating layer BatchNorm18
I0926 08:53:08.490758  4406 net.cpp:84] Creating Layer BatchNorm18
I0926 08:53:08.490761  4406 net.cpp:406] BatchNorm18 <- Convolution18
I0926 08:53:08.490764  4406 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0926 08:53:08.490919  4406 net.cpp:122] Setting up BatchNorm18
I0926 08:53:08.490923  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.490926  4406 net.cpp:137] Memory required for data: 630375600
I0926 08:53:08.490931  4406 layer_factory.hpp:77] Creating layer Scale18
I0926 08:53:08.490934  4406 net.cpp:84] Creating Layer Scale18
I0926 08:53:08.490937  4406 net.cpp:406] Scale18 <- Convolution18
I0926 08:53:08.490941  4406 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0926 08:53:08.490970  4406 layer_factory.hpp:77] Creating layer Scale18
I0926 08:53:08.491057  4406 net.cpp:122] Setting up Scale18
I0926 08:53:08.491061  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.491065  4406 net.cpp:137] Memory required for data: 636929200
I0926 08:53:08.491067  4406 layer_factory.hpp:77] Creating layer penlu18
I0926 08:53:08.491072  4406 net.cpp:84] Creating Layer penlu18
I0926 08:53:08.491075  4406 net.cpp:406] penlu18 <- Convolution18
I0926 08:53:08.491080  4406 net.cpp:367] penlu18 -> Convolution18 (in-place)
I0926 08:53:08.491211  4406 net.cpp:122] Setting up penlu18
I0926 08:53:08.491216  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.491219  4406 net.cpp:137] Memory required for data: 643482800
I0926 08:53:08.491222  4406 layer_factory.hpp:77] Creating layer Convolution19
I0926 08:53:08.491230  4406 net.cpp:84] Creating Layer Convolution19
I0926 08:53:08.491231  4406 net.cpp:406] Convolution19 <- Convolution18
I0926 08:53:08.491236  4406 net.cpp:380] Convolution19 -> Convolution19
I0926 08:53:08.492182  4406 net.cpp:122] Setting up Convolution19
I0926 08:53:08.492192  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.492193  4406 net.cpp:137] Memory required for data: 650036400
I0926 08:53:08.492198  4406 layer_factory.hpp:77] Creating layer BatchNorm19
I0926 08:53:08.492203  4406 net.cpp:84] Creating Layer BatchNorm19
I0926 08:53:08.492207  4406 net.cpp:406] BatchNorm19 <- Convolution19
I0926 08:53:08.492209  4406 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0926 08:53:08.492370  4406 net.cpp:122] Setting up BatchNorm19
I0926 08:53:08.492374  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.492377  4406 net.cpp:137] Memory required for data: 656590000
I0926 08:53:08.492382  4406 layer_factory.hpp:77] Creating layer Scale19
I0926 08:53:08.492385  4406 net.cpp:84] Creating Layer Scale19
I0926 08:53:08.492388  4406 net.cpp:406] Scale19 <- Convolution19
I0926 08:53:08.492398  4406 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0926 08:53:08.492431  4406 layer_factory.hpp:77] Creating layer Scale19
I0926 08:53:08.492533  4406 net.cpp:122] Setting up Scale19
I0926 08:53:08.492539  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.492542  4406 net.cpp:137] Memory required for data: 663143600
I0926 08:53:08.492545  4406 layer_factory.hpp:77] Creating layer Eltwise9
I0926 08:53:08.492550  4406 net.cpp:84] Creating Layer Eltwise9
I0926 08:53:08.492553  4406 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I0926 08:53:08.492557  4406 net.cpp:406] Eltwise9 <- Convolution19
I0926 08:53:08.492559  4406 net.cpp:380] Eltwise9 -> Eltwise9
I0926 08:53:08.492578  4406 net.cpp:122] Setting up Eltwise9
I0926 08:53:08.492583  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.492584  4406 net.cpp:137] Memory required for data: 669697200
I0926 08:53:08.492586  4406 layer_factory.hpp:77] Creating layer penlu19
I0926 08:53:08.492591  4406 net.cpp:84] Creating Layer penlu19
I0926 08:53:08.492594  4406 net.cpp:406] penlu19 <- Eltwise9
I0926 08:53:08.492597  4406 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I0926 08:53:08.492733  4406 net.cpp:122] Setting up penlu19
I0926 08:53:08.492738  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.492739  4406 net.cpp:137] Memory required for data: 676250800
I0926 08:53:08.492743  4406 layer_factory.hpp:77] Creating layer Eltwise9_penlu19_0_split
I0926 08:53:08.492748  4406 net.cpp:84] Creating Layer Eltwise9_penlu19_0_split
I0926 08:53:08.492749  4406 net.cpp:406] Eltwise9_penlu19_0_split <- Eltwise9
I0926 08:53:08.492753  4406 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_0
I0926 08:53:08.492756  4406 net.cpp:380] Eltwise9_penlu19_0_split -> Eltwise9_penlu19_0_split_1
I0926 08:53:08.492785  4406 net.cpp:122] Setting up Eltwise9_penlu19_0_split
I0926 08:53:08.513939  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.513947  4406 net.cpp:129] Top shape: 100 16 32 32 (1638400)
I0926 08:53:08.513949  4406 net.cpp:137] Memory required for data: 689358000
I0926 08:53:08.513952  4406 layer_factory.hpp:77] Creating layer Convolution20
I0926 08:53:08.513962  4406 net.cpp:84] Creating Layer Convolution20
I0926 08:53:08.513965  4406 net.cpp:406] Convolution20 <- Eltwise9_penlu19_0_split_0
I0926 08:53:08.513970  4406 net.cpp:380] Convolution20 -> Convolution20
I0926 08:53:08.515655  4406 net.cpp:122] Setting up Convolution20
I0926 08:53:08.515666  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.515669  4406 net.cpp:137] Memory required for data: 692634800
I0926 08:53:08.515673  4406 layer_factory.hpp:77] Creating layer BatchNorm20
I0926 08:53:08.515679  4406 net.cpp:84] Creating Layer BatchNorm20
I0926 08:53:08.515682  4406 net.cpp:406] BatchNorm20 <- Convolution20
I0926 08:53:08.515686  4406 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0926 08:53:08.515827  4406 net.cpp:122] Setting up BatchNorm20
I0926 08:53:08.515831  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.515833  4406 net.cpp:137] Memory required for data: 695911600
I0926 08:53:08.515838  4406 layer_factory.hpp:77] Creating layer Scale20
I0926 08:53:08.515843  4406 net.cpp:84] Creating Layer Scale20
I0926 08:53:08.515846  4406 net.cpp:406] Scale20 <- Convolution20
I0926 08:53:08.515848  4406 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0926 08:53:08.515875  4406 layer_factory.hpp:77] Creating layer Scale20
I0926 08:53:08.515949  4406 net.cpp:122] Setting up Scale20
I0926 08:53:08.515954  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.515955  4406 net.cpp:137] Memory required for data: 699188400
I0926 08:53:08.515959  4406 layer_factory.hpp:77] Creating layer Convolution21
I0926 08:53:08.515967  4406 net.cpp:84] Creating Layer Convolution21
I0926 08:53:08.515969  4406 net.cpp:406] Convolution21 <- Eltwise9_penlu19_0_split_1
I0926 08:53:08.515974  4406 net.cpp:380] Convolution21 -> Convolution21
I0926 08:53:08.517094  4406 net.cpp:122] Setting up Convolution21
I0926 08:53:08.517112  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.517114  4406 net.cpp:137] Memory required for data: 702465200
I0926 08:53:08.517119  4406 layer_factory.hpp:77] Creating layer BatchNorm21
I0926 08:53:08.517124  4406 net.cpp:84] Creating Layer BatchNorm21
I0926 08:53:08.517127  4406 net.cpp:406] BatchNorm21 <- Convolution21
I0926 08:53:08.517132  4406 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0926 08:53:08.517256  4406 net.cpp:122] Setting up BatchNorm21
I0926 08:53:08.517259  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.517262  4406 net.cpp:137] Memory required for data: 705742000
I0926 08:53:08.517266  4406 layer_factory.hpp:77] Creating layer Scale21
I0926 08:53:08.517271  4406 net.cpp:84] Creating Layer Scale21
I0926 08:53:08.517273  4406 net.cpp:406] Scale21 <- Convolution21
I0926 08:53:08.517287  4406 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0926 08:53:08.517314  4406 layer_factory.hpp:77] Creating layer Scale21
I0926 08:53:08.517415  4406 net.cpp:122] Setting up Scale21
I0926 08:53:08.517418  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.517421  4406 net.cpp:137] Memory required for data: 709018800
I0926 08:53:08.517424  4406 layer_factory.hpp:77] Creating layer penlu20
I0926 08:53:08.517431  4406 net.cpp:84] Creating Layer penlu20
I0926 08:53:08.517432  4406 net.cpp:406] penlu20 <- Convolution21
I0926 08:53:08.517436  4406 net.cpp:367] penlu20 -> Convolution21 (in-place)
I0926 08:53:08.517539  4406 net.cpp:122] Setting up penlu20
I0926 08:53:08.517544  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.517546  4406 net.cpp:137] Memory required for data: 712295600
I0926 08:53:08.517550  4406 layer_factory.hpp:77] Creating layer Convolution22
I0926 08:53:08.517556  4406 net.cpp:84] Creating Layer Convolution22
I0926 08:53:08.517560  4406 net.cpp:406] Convolution22 <- Convolution21
I0926 08:53:08.517563  4406 net.cpp:380] Convolution22 -> Convolution22
I0926 08:53:08.518661  4406 net.cpp:122] Setting up Convolution22
I0926 08:53:08.518669  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.518672  4406 net.cpp:137] Memory required for data: 715572400
I0926 08:53:08.518676  4406 layer_factory.hpp:77] Creating layer BatchNorm22
I0926 08:53:08.518682  4406 net.cpp:84] Creating Layer BatchNorm22
I0926 08:53:08.518684  4406 net.cpp:406] BatchNorm22 <- Convolution22
I0926 08:53:08.518688  4406 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I0926 08:53:08.518810  4406 net.cpp:122] Setting up BatchNorm22
I0926 08:53:08.518815  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.518816  4406 net.cpp:137] Memory required for data: 718849200
I0926 08:53:08.518821  4406 layer_factory.hpp:77] Creating layer Scale22
I0926 08:53:08.518826  4406 net.cpp:84] Creating Layer Scale22
I0926 08:53:08.518828  4406 net.cpp:406] Scale22 <- Convolution22
I0926 08:53:08.518831  4406 net.cpp:367] Scale22 -> Convolution22 (in-place)
I0926 08:53:08.518857  4406 layer_factory.hpp:77] Creating layer Scale22
I0926 08:53:08.518929  4406 net.cpp:122] Setting up Scale22
I0926 08:53:08.518934  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.518935  4406 net.cpp:137] Memory required for data: 722126000
I0926 08:53:08.518939  4406 layer_factory.hpp:77] Creating layer Eltwise10
I0926 08:53:08.518944  4406 net.cpp:84] Creating Layer Eltwise10
I0926 08:53:08.518947  4406 net.cpp:406] Eltwise10 <- Convolution20
I0926 08:53:08.518949  4406 net.cpp:406] Eltwise10 <- Convolution22
I0926 08:53:08.518954  4406 net.cpp:380] Eltwise10 -> Eltwise10
I0926 08:53:08.518965  4406 net.cpp:122] Setting up Eltwise10
I0926 08:53:08.518968  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.518970  4406 net.cpp:137] Memory required for data: 725402800
I0926 08:53:08.518972  4406 layer_factory.hpp:77] Creating layer penlu21
I0926 08:53:08.518977  4406 net.cpp:84] Creating Layer penlu21
I0926 08:53:08.518980  4406 net.cpp:406] penlu21 <- Eltwise10
I0926 08:53:08.518983  4406 net.cpp:367] penlu21 -> Eltwise10 (in-place)
I0926 08:53:08.519095  4406 net.cpp:122] Setting up penlu21
I0926 08:53:08.519099  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.519101  4406 net.cpp:137] Memory required for data: 728679600
I0926 08:53:08.519106  4406 layer_factory.hpp:77] Creating layer Eltwise10_penlu21_0_split
I0926 08:53:08.519110  4406 net.cpp:84] Creating Layer Eltwise10_penlu21_0_split
I0926 08:53:08.519112  4406 net.cpp:406] Eltwise10_penlu21_0_split <- Eltwise10
I0926 08:53:08.519115  4406 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_0
I0926 08:53:08.519119  4406 net.cpp:380] Eltwise10_penlu21_0_split -> Eltwise10_penlu21_0_split_1
I0926 08:53:08.519142  4406 net.cpp:122] Setting up Eltwise10_penlu21_0_split
I0926 08:53:08.519145  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.519148  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.519150  4406 net.cpp:137] Memory required for data: 735233200
I0926 08:53:08.519152  4406 layer_factory.hpp:77] Creating layer Convolution23
I0926 08:53:08.519158  4406 net.cpp:84] Creating Layer Convolution23
I0926 08:53:08.519161  4406 net.cpp:406] Convolution23 <- Eltwise10_penlu21_0_split_0
I0926 08:53:08.519165  4406 net.cpp:380] Convolution23 -> Convolution23
I0926 08:53:08.520233  4406 net.cpp:122] Setting up Convolution23
I0926 08:53:08.520242  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.520244  4406 net.cpp:137] Memory required for data: 738510000
I0926 08:53:08.520249  4406 layer_factory.hpp:77] Creating layer BatchNorm23
I0926 08:53:08.520254  4406 net.cpp:84] Creating Layer BatchNorm23
I0926 08:53:08.520257  4406 net.cpp:406] BatchNorm23 <- Convolution23
I0926 08:53:08.520261  4406 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I0926 08:53:08.520380  4406 net.cpp:122] Setting up BatchNorm23
I0926 08:53:08.520385  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.520386  4406 net.cpp:137] Memory required for data: 741786800
I0926 08:53:08.520391  4406 layer_factory.hpp:77] Creating layer Scale23
I0926 08:53:08.520395  4406 net.cpp:84] Creating Layer Scale23
I0926 08:53:08.520398  4406 net.cpp:406] Scale23 <- Convolution23
I0926 08:53:08.520401  4406 net.cpp:367] Scale23 -> Convolution23 (in-place)
I0926 08:53:08.520426  4406 layer_factory.hpp:77] Creating layer Scale23
I0926 08:53:08.520512  4406 net.cpp:122] Setting up Scale23
I0926 08:53:08.520521  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.520525  4406 net.cpp:137] Memory required for data: 745063600
I0926 08:53:08.520529  4406 layer_factory.hpp:77] Creating layer penlu22
I0926 08:53:08.520534  4406 net.cpp:84] Creating Layer penlu22
I0926 08:53:08.520537  4406 net.cpp:406] penlu22 <- Convolution23
I0926 08:53:08.520541  4406 net.cpp:367] penlu22 -> Convolution23 (in-place)
I0926 08:53:08.520659  4406 net.cpp:122] Setting up penlu22
I0926 08:53:08.520664  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.520666  4406 net.cpp:137] Memory required for data: 748340400
I0926 08:53:08.520670  4406 layer_factory.hpp:77] Creating layer Convolution24
I0926 08:53:08.520678  4406 net.cpp:84] Creating Layer Convolution24
I0926 08:53:08.520680  4406 net.cpp:406] Convolution24 <- Convolution23
I0926 08:53:08.520684  4406 net.cpp:380] Convolution24 -> Convolution24
I0926 08:53:08.521751  4406 net.cpp:122] Setting up Convolution24
I0926 08:53:08.521760  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.521764  4406 net.cpp:137] Memory required for data: 751617200
I0926 08:53:08.521767  4406 layer_factory.hpp:77] Creating layer BatchNorm24
I0926 08:53:08.521772  4406 net.cpp:84] Creating Layer BatchNorm24
I0926 08:53:08.521775  4406 net.cpp:406] BatchNorm24 <- Convolution24
I0926 08:53:08.521780  4406 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I0926 08:53:08.521903  4406 net.cpp:122] Setting up BatchNorm24
I0926 08:53:08.521908  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.521909  4406 net.cpp:137] Memory required for data: 754894000
I0926 08:53:08.521921  4406 layer_factory.hpp:77] Creating layer Scale24
I0926 08:53:08.521925  4406 net.cpp:84] Creating Layer Scale24
I0926 08:53:08.521929  4406 net.cpp:406] Scale24 <- Convolution24
I0926 08:53:08.521932  4406 net.cpp:367] Scale24 -> Convolution24 (in-place)
I0926 08:53:08.521958  4406 layer_factory.hpp:77] Creating layer Scale24
I0926 08:53:08.522029  4406 net.cpp:122] Setting up Scale24
I0926 08:53:08.522033  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.522035  4406 net.cpp:137] Memory required for data: 758170800
I0926 08:53:08.522039  4406 layer_factory.hpp:77] Creating layer Eltwise11
I0926 08:53:08.522043  4406 net.cpp:84] Creating Layer Eltwise11
I0926 08:53:08.522045  4406 net.cpp:406] Eltwise11 <- Eltwise10_penlu21_0_split_1
I0926 08:53:08.522048  4406 net.cpp:406] Eltwise11 <- Convolution24
I0926 08:53:08.522053  4406 net.cpp:380] Eltwise11 -> Eltwise11
I0926 08:53:08.522063  4406 net.cpp:122] Setting up Eltwise11
I0926 08:53:08.522068  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.522069  4406 net.cpp:137] Memory required for data: 761447600
I0926 08:53:08.522071  4406 layer_factory.hpp:77] Creating layer penlu23
I0926 08:53:08.522076  4406 net.cpp:84] Creating Layer penlu23
I0926 08:53:08.522079  4406 net.cpp:406] penlu23 <- Eltwise11
I0926 08:53:08.522083  4406 net.cpp:367] penlu23 -> Eltwise11 (in-place)
I0926 08:53:08.522189  4406 net.cpp:122] Setting up penlu23
I0926 08:53:08.522193  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.522195  4406 net.cpp:137] Memory required for data: 764724400
I0926 08:53:08.522199  4406 layer_factory.hpp:77] Creating layer Eltwise11_penlu23_0_split
I0926 08:53:08.522204  4406 net.cpp:84] Creating Layer Eltwise11_penlu23_0_split
I0926 08:53:08.522207  4406 net.cpp:406] Eltwise11_penlu23_0_split <- Eltwise11
I0926 08:53:08.522209  4406 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_0
I0926 08:53:08.522213  4406 net.cpp:380] Eltwise11_penlu23_0_split -> Eltwise11_penlu23_0_split_1
I0926 08:53:08.522236  4406 net.cpp:122] Setting up Eltwise11_penlu23_0_split
I0926 08:53:08.522240  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.522243  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.522244  4406 net.cpp:137] Memory required for data: 771278000
I0926 08:53:08.522246  4406 layer_factory.hpp:77] Creating layer Convolution25
I0926 08:53:08.522253  4406 net.cpp:84] Creating Layer Convolution25
I0926 08:53:08.522254  4406 net.cpp:406] Convolution25 <- Eltwise11_penlu23_0_split_0
I0926 08:53:08.522259  4406 net.cpp:380] Convolution25 -> Convolution25
I0926 08:53:08.523334  4406 net.cpp:122] Setting up Convolution25
I0926 08:53:08.523342  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.523345  4406 net.cpp:137] Memory required for data: 774554800
I0926 08:53:08.523350  4406 layer_factory.hpp:77] Creating layer BatchNorm25
I0926 08:53:08.523355  4406 net.cpp:84] Creating Layer BatchNorm25
I0926 08:53:08.523357  4406 net.cpp:406] BatchNorm25 <- Convolution25
I0926 08:53:08.523361  4406 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I0926 08:53:08.523489  4406 net.cpp:122] Setting up BatchNorm25
I0926 08:53:08.523494  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.523495  4406 net.cpp:137] Memory required for data: 777831600
I0926 08:53:08.523500  4406 layer_factory.hpp:77] Creating layer Scale25
I0926 08:53:08.523504  4406 net.cpp:84] Creating Layer Scale25
I0926 08:53:08.523507  4406 net.cpp:406] Scale25 <- Convolution25
I0926 08:53:08.523510  4406 net.cpp:367] Scale25 -> Convolution25 (in-place)
I0926 08:53:08.523535  4406 layer_factory.hpp:77] Creating layer Scale25
I0926 08:53:08.523607  4406 net.cpp:122] Setting up Scale25
I0926 08:53:08.523612  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.523613  4406 net.cpp:137] Memory required for data: 781108400
I0926 08:53:08.523617  4406 layer_factory.hpp:77] Creating layer penlu24
I0926 08:53:08.523622  4406 net.cpp:84] Creating Layer penlu24
I0926 08:53:08.523632  4406 net.cpp:406] penlu24 <- Convolution25
I0926 08:53:08.523635  4406 net.cpp:367] penlu24 -> Convolution25 (in-place)
I0926 08:53:08.523739  4406 net.cpp:122] Setting up penlu24
I0926 08:53:08.523743  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.523746  4406 net.cpp:137] Memory required for data: 784385200
I0926 08:53:08.523751  4406 layer_factory.hpp:77] Creating layer Convolution26
I0926 08:53:08.523756  4406 net.cpp:84] Creating Layer Convolution26
I0926 08:53:08.523759  4406 net.cpp:406] Convolution26 <- Convolution25
I0926 08:53:08.523763  4406 net.cpp:380] Convolution26 -> Convolution26
I0926 08:53:08.524557  4406 net.cpp:122] Setting up Convolution26
I0926 08:53:08.524566  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.524569  4406 net.cpp:137] Memory required for data: 787662000
I0926 08:53:08.524574  4406 layer_factory.hpp:77] Creating layer BatchNorm26
I0926 08:53:08.524577  4406 net.cpp:84] Creating Layer BatchNorm26
I0926 08:53:08.524580  4406 net.cpp:406] BatchNorm26 <- Convolution26
I0926 08:53:08.524585  4406 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I0926 08:53:08.524713  4406 net.cpp:122] Setting up BatchNorm26
I0926 08:53:08.524716  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.524718  4406 net.cpp:137] Memory required for data: 790938800
I0926 08:53:08.524724  4406 layer_factory.hpp:77] Creating layer Scale26
I0926 08:53:08.524727  4406 net.cpp:84] Creating Layer Scale26
I0926 08:53:08.524729  4406 net.cpp:406] Scale26 <- Convolution26
I0926 08:53:08.524734  4406 net.cpp:367] Scale26 -> Convolution26 (in-place)
I0926 08:53:08.524758  4406 layer_factory.hpp:77] Creating layer Scale26
I0926 08:53:08.544900  4406 net.cpp:122] Setting up Scale26
I0926 08:53:08.544909  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.544912  4406 net.cpp:137] Memory required for data: 794215600
I0926 08:53:08.544917  4406 layer_factory.hpp:77] Creating layer Eltwise12
I0926 08:53:08.544922  4406 net.cpp:84] Creating Layer Eltwise12
I0926 08:53:08.544926  4406 net.cpp:406] Eltwise12 <- Eltwise11_penlu23_0_split_1
I0926 08:53:08.544930  4406 net.cpp:406] Eltwise12 <- Convolution26
I0926 08:53:08.544934  4406 net.cpp:380] Eltwise12 -> Eltwise12
I0926 08:53:08.544948  4406 net.cpp:122] Setting up Eltwise12
I0926 08:53:08.544952  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.544955  4406 net.cpp:137] Memory required for data: 797492400
I0926 08:53:08.544957  4406 layer_factory.hpp:77] Creating layer penlu25
I0926 08:53:08.544971  4406 net.cpp:84] Creating Layer penlu25
I0926 08:53:08.544975  4406 net.cpp:406] penlu25 <- Eltwise12
I0926 08:53:08.544978  4406 net.cpp:367] penlu25 -> Eltwise12 (in-place)
I0926 08:53:08.545101  4406 net.cpp:122] Setting up penlu25
I0926 08:53:08.545106  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.545109  4406 net.cpp:137] Memory required for data: 800769200
I0926 08:53:08.545133  4406 layer_factory.hpp:77] Creating layer Eltwise12_penlu25_0_split
I0926 08:53:08.545138  4406 net.cpp:84] Creating Layer Eltwise12_penlu25_0_split
I0926 08:53:08.545140  4406 net.cpp:406] Eltwise12_penlu25_0_split <- Eltwise12
I0926 08:53:08.545145  4406 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_0
I0926 08:53:08.545150  4406 net.cpp:380] Eltwise12_penlu25_0_split -> Eltwise12_penlu25_0_split_1
I0926 08:53:08.545176  4406 net.cpp:122] Setting up Eltwise12_penlu25_0_split
I0926 08:53:08.545181  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.545183  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.545186  4406 net.cpp:137] Memory required for data: 807322800
I0926 08:53:08.545188  4406 layer_factory.hpp:77] Creating layer Convolution27
I0926 08:53:08.545195  4406 net.cpp:84] Creating Layer Convolution27
I0926 08:53:08.545197  4406 net.cpp:406] Convolution27 <- Eltwise12_penlu25_0_split_0
I0926 08:53:08.545202  4406 net.cpp:380] Convolution27 -> Convolution27
I0926 08:53:08.546919  4406 net.cpp:122] Setting up Convolution27
I0926 08:53:08.546936  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.546939  4406 net.cpp:137] Memory required for data: 810599600
I0926 08:53:08.546944  4406 layer_factory.hpp:77] Creating layer BatchNorm27
I0926 08:53:08.546950  4406 net.cpp:84] Creating Layer BatchNorm27
I0926 08:53:08.546953  4406 net.cpp:406] BatchNorm27 <- Convolution27
I0926 08:53:08.546957  4406 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I0926 08:53:08.547087  4406 net.cpp:122] Setting up BatchNorm27
I0926 08:53:08.547092  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.547094  4406 net.cpp:137] Memory required for data: 813876400
I0926 08:53:08.547099  4406 layer_factory.hpp:77] Creating layer Scale27
I0926 08:53:08.547103  4406 net.cpp:84] Creating Layer Scale27
I0926 08:53:08.547106  4406 net.cpp:406] Scale27 <- Convolution27
I0926 08:53:08.547109  4406 net.cpp:367] Scale27 -> Convolution27 (in-place)
I0926 08:53:08.547135  4406 layer_factory.hpp:77] Creating layer Scale27
I0926 08:53:08.547209  4406 net.cpp:122] Setting up Scale27
I0926 08:53:08.547214  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.547216  4406 net.cpp:137] Memory required for data: 817153200
I0926 08:53:08.547220  4406 layer_factory.hpp:77] Creating layer penlu26
I0926 08:53:08.547225  4406 net.cpp:84] Creating Layer penlu26
I0926 08:53:08.547228  4406 net.cpp:406] penlu26 <- Convolution27
I0926 08:53:08.547232  4406 net.cpp:367] penlu26 -> Convolution27 (in-place)
I0926 08:53:08.547340  4406 net.cpp:122] Setting up penlu26
I0926 08:53:08.547344  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.547348  4406 net.cpp:137] Memory required for data: 820430000
I0926 08:53:08.547351  4406 layer_factory.hpp:77] Creating layer Convolution28
I0926 08:53:08.547358  4406 net.cpp:84] Creating Layer Convolution28
I0926 08:53:08.547361  4406 net.cpp:406] Convolution28 <- Convolution27
I0926 08:53:08.547365  4406 net.cpp:380] Convolution28 -> Convolution28
I0926 08:53:08.548872  4406 net.cpp:122] Setting up Convolution28
I0926 08:53:08.548882  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.548884  4406 net.cpp:137] Memory required for data: 823706800
I0926 08:53:08.548889  4406 layer_factory.hpp:77] Creating layer BatchNorm28
I0926 08:53:08.548894  4406 net.cpp:84] Creating Layer BatchNorm28
I0926 08:53:08.548897  4406 net.cpp:406] BatchNorm28 <- Convolution28
I0926 08:53:08.548902  4406 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I0926 08:53:08.549036  4406 net.cpp:122] Setting up BatchNorm28
I0926 08:53:08.549041  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.549042  4406 net.cpp:137] Memory required for data: 826983600
I0926 08:53:08.549047  4406 layer_factory.hpp:77] Creating layer Scale28
I0926 08:53:08.549052  4406 net.cpp:84] Creating Layer Scale28
I0926 08:53:08.549054  4406 net.cpp:406] Scale28 <- Convolution28
I0926 08:53:08.549057  4406 net.cpp:367] Scale28 -> Convolution28 (in-place)
I0926 08:53:08.549084  4406 layer_factory.hpp:77] Creating layer Scale28
I0926 08:53:08.549160  4406 net.cpp:122] Setting up Scale28
I0926 08:53:08.549165  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.549167  4406 net.cpp:137] Memory required for data: 830260400
I0926 08:53:08.549171  4406 layer_factory.hpp:77] Creating layer Eltwise13
I0926 08:53:08.549175  4406 net.cpp:84] Creating Layer Eltwise13
I0926 08:53:08.549180  4406 net.cpp:406] Eltwise13 <- Eltwise12_penlu25_0_split_1
I0926 08:53:08.549182  4406 net.cpp:406] Eltwise13 <- Convolution28
I0926 08:53:08.549185  4406 net.cpp:380] Eltwise13 -> Eltwise13
I0926 08:53:08.549197  4406 net.cpp:122] Setting up Eltwise13
I0926 08:53:08.549201  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.549203  4406 net.cpp:137] Memory required for data: 833537200
I0926 08:53:08.549206  4406 layer_factory.hpp:77] Creating layer penlu27
I0926 08:53:08.549211  4406 net.cpp:84] Creating Layer penlu27
I0926 08:53:08.549213  4406 net.cpp:406] penlu27 <- Eltwise13
I0926 08:53:08.549217  4406 net.cpp:367] penlu27 -> Eltwise13 (in-place)
I0926 08:53:08.549337  4406 net.cpp:122] Setting up penlu27
I0926 08:53:08.549342  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.549345  4406 net.cpp:137] Memory required for data: 836814000
I0926 08:53:08.549348  4406 layer_factory.hpp:77] Creating layer Eltwise13_penlu27_0_split
I0926 08:53:08.549352  4406 net.cpp:84] Creating Layer Eltwise13_penlu27_0_split
I0926 08:53:08.549355  4406 net.cpp:406] Eltwise13_penlu27_0_split <- Eltwise13
I0926 08:53:08.549360  4406 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_0
I0926 08:53:08.549363  4406 net.cpp:380] Eltwise13_penlu27_0_split -> Eltwise13_penlu27_0_split_1
I0926 08:53:08.549386  4406 net.cpp:122] Setting up Eltwise13_penlu27_0_split
I0926 08:53:08.549391  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.549393  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.549396  4406 net.cpp:137] Memory required for data: 843367600
I0926 08:53:08.549397  4406 layer_factory.hpp:77] Creating layer Convolution29
I0926 08:53:08.549403  4406 net.cpp:84] Creating Layer Convolution29
I0926 08:53:08.549407  4406 net.cpp:406] Convolution29 <- Eltwise13_penlu27_0_split_0
I0926 08:53:08.549410  4406 net.cpp:380] Convolution29 -> Convolution29
I0926 08:53:08.550513  4406 net.cpp:122] Setting up Convolution29
I0926 08:53:08.550521  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.550524  4406 net.cpp:137] Memory required for data: 846644400
I0926 08:53:08.550529  4406 layer_factory.hpp:77] Creating layer BatchNorm29
I0926 08:53:08.550534  4406 net.cpp:84] Creating Layer BatchNorm29
I0926 08:53:08.550537  4406 net.cpp:406] BatchNorm29 <- Convolution29
I0926 08:53:08.550542  4406 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I0926 08:53:08.550671  4406 net.cpp:122] Setting up BatchNorm29
I0926 08:53:08.550675  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.550678  4406 net.cpp:137] Memory required for data: 849921200
I0926 08:53:08.550683  4406 layer_factory.hpp:77] Creating layer Scale29
I0926 08:53:08.550686  4406 net.cpp:84] Creating Layer Scale29
I0926 08:53:08.550689  4406 net.cpp:406] Scale29 <- Convolution29
I0926 08:53:08.550693  4406 net.cpp:367] Scale29 -> Convolution29 (in-place)
I0926 08:53:08.550719  4406 layer_factory.hpp:77] Creating layer Scale29
I0926 08:53:08.550796  4406 net.cpp:122] Setting up Scale29
I0926 08:53:08.550801  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.550802  4406 net.cpp:137] Memory required for data: 853198000
I0926 08:53:08.550806  4406 layer_factory.hpp:77] Creating layer penlu28
I0926 08:53:08.550812  4406 net.cpp:84] Creating Layer penlu28
I0926 08:53:08.550815  4406 net.cpp:406] penlu28 <- Convolution29
I0926 08:53:08.550818  4406 net.cpp:367] penlu28 -> Convolution29 (in-place)
I0926 08:53:08.550925  4406 net.cpp:122] Setting up penlu28
I0926 08:53:08.550930  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.550932  4406 net.cpp:137] Memory required for data: 856474800
I0926 08:53:08.550936  4406 layer_factory.hpp:77] Creating layer Convolution30
I0926 08:53:08.550943  4406 net.cpp:84] Creating Layer Convolution30
I0926 08:53:08.550945  4406 net.cpp:406] Convolution30 <- Convolution29
I0926 08:53:08.550951  4406 net.cpp:380] Convolution30 -> Convolution30
I0926 08:53:08.552052  4406 net.cpp:122] Setting up Convolution30
I0926 08:53:08.552060  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.552063  4406 net.cpp:137] Memory required for data: 859751600
I0926 08:53:08.552067  4406 layer_factory.hpp:77] Creating layer BatchNorm30
I0926 08:53:08.552073  4406 net.cpp:84] Creating Layer BatchNorm30
I0926 08:53:08.552075  4406 net.cpp:406] BatchNorm30 <- Convolution30
I0926 08:53:08.552079  4406 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I0926 08:53:08.552204  4406 net.cpp:122] Setting up BatchNorm30
I0926 08:53:08.552209  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.552211  4406 net.cpp:137] Memory required for data: 863028400
I0926 08:53:08.552222  4406 layer_factory.hpp:77] Creating layer Scale30
I0926 08:53:08.552228  4406 net.cpp:84] Creating Layer Scale30
I0926 08:53:08.552232  4406 net.cpp:406] Scale30 <- Convolution30
I0926 08:53:08.552235  4406 net.cpp:367] Scale30 -> Convolution30 (in-place)
I0926 08:53:08.552261  4406 layer_factory.hpp:77] Creating layer Scale30
I0926 08:53:08.552335  4406 net.cpp:122] Setting up Scale30
I0926 08:53:08.552340  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.552341  4406 net.cpp:137] Memory required for data: 866305200
I0926 08:53:08.552345  4406 layer_factory.hpp:77] Creating layer Eltwise14
I0926 08:53:08.552350  4406 net.cpp:84] Creating Layer Eltwise14
I0926 08:53:08.552352  4406 net.cpp:406] Eltwise14 <- Eltwise13_penlu27_0_split_1
I0926 08:53:08.552356  4406 net.cpp:406] Eltwise14 <- Convolution30
I0926 08:53:08.552358  4406 net.cpp:380] Eltwise14 -> Eltwise14
I0926 08:53:08.552371  4406 net.cpp:122] Setting up Eltwise14
I0926 08:53:08.552374  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.552376  4406 net.cpp:137] Memory required for data: 869582000
I0926 08:53:08.552379  4406 layer_factory.hpp:77] Creating layer penlu29
I0926 08:53:08.552384  4406 net.cpp:84] Creating Layer penlu29
I0926 08:53:08.552386  4406 net.cpp:406] penlu29 <- Eltwise14
I0926 08:53:08.552390  4406 net.cpp:367] penlu29 -> Eltwise14 (in-place)
I0926 08:53:08.552505  4406 net.cpp:122] Setting up penlu29
I0926 08:53:08.552513  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.552515  4406 net.cpp:137] Memory required for data: 872858800
I0926 08:53:08.552520  4406 layer_factory.hpp:77] Creating layer Eltwise14_penlu29_0_split
I0926 08:53:08.552525  4406 net.cpp:84] Creating Layer Eltwise14_penlu29_0_split
I0926 08:53:08.552526  4406 net.cpp:406] Eltwise14_penlu29_0_split <- Eltwise14
I0926 08:53:08.552531  4406 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_0
I0926 08:53:08.552534  4406 net.cpp:380] Eltwise14_penlu29_0_split -> Eltwise14_penlu29_0_split_1
I0926 08:53:08.552559  4406 net.cpp:122] Setting up Eltwise14_penlu29_0_split
I0926 08:53:08.552563  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.552567  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.552568  4406 net.cpp:137] Memory required for data: 879412400
I0926 08:53:08.552570  4406 layer_factory.hpp:77] Creating layer Convolution31
I0926 08:53:08.552577  4406 net.cpp:84] Creating Layer Convolution31
I0926 08:53:08.552579  4406 net.cpp:406] Convolution31 <- Eltwise14_penlu29_0_split_0
I0926 08:53:08.552583  4406 net.cpp:380] Convolution31 -> Convolution31
I0926 08:53:08.553683  4406 net.cpp:122] Setting up Convolution31
I0926 08:53:08.553692  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.553694  4406 net.cpp:137] Memory required for data: 882689200
I0926 08:53:08.553699  4406 layer_factory.hpp:77] Creating layer BatchNorm31
I0926 08:53:08.553704  4406 net.cpp:84] Creating Layer BatchNorm31
I0926 08:53:08.553707  4406 net.cpp:406] BatchNorm31 <- Convolution31
I0926 08:53:08.553711  4406 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I0926 08:53:08.553836  4406 net.cpp:122] Setting up BatchNorm31
I0926 08:53:08.553840  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.553843  4406 net.cpp:137] Memory required for data: 885966000
I0926 08:53:08.553848  4406 layer_factory.hpp:77] Creating layer Scale31
I0926 08:53:08.553851  4406 net.cpp:84] Creating Layer Scale31
I0926 08:53:08.553854  4406 net.cpp:406] Scale31 <- Convolution31
I0926 08:53:08.553858  4406 net.cpp:367] Scale31 -> Convolution31 (in-place)
I0926 08:53:08.553884  4406 layer_factory.hpp:77] Creating layer Scale31
I0926 08:53:08.553956  4406 net.cpp:122] Setting up Scale31
I0926 08:53:08.553961  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.553962  4406 net.cpp:137] Memory required for data: 889242800
I0926 08:53:08.553966  4406 layer_factory.hpp:77] Creating layer penlu30
I0926 08:53:08.553972  4406 net.cpp:84] Creating Layer penlu30
I0926 08:53:08.553980  4406 net.cpp:406] penlu30 <- Convolution31
I0926 08:53:08.553985  4406 net.cpp:367] penlu30 -> Convolution31 (in-place)
I0926 08:53:08.554091  4406 net.cpp:122] Setting up penlu30
I0926 08:53:08.554095  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.554097  4406 net.cpp:137] Memory required for data: 892519600
I0926 08:53:08.554102  4406 layer_factory.hpp:77] Creating layer Convolution32
I0926 08:53:08.554110  4406 net.cpp:84] Creating Layer Convolution32
I0926 08:53:08.554111  4406 net.cpp:406] Convolution32 <- Convolution31
I0926 08:53:08.554116  4406 net.cpp:380] Convolution32 -> Convolution32
I0926 08:53:08.555232  4406 net.cpp:122] Setting up Convolution32
I0926 08:53:08.555240  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.555243  4406 net.cpp:137] Memory required for data: 895796400
I0926 08:53:08.555248  4406 layer_factory.hpp:77] Creating layer BatchNorm32
I0926 08:53:08.555253  4406 net.cpp:84] Creating Layer BatchNorm32
I0926 08:53:08.555256  4406 net.cpp:406] BatchNorm32 <- Convolution32
I0926 08:53:08.555261  4406 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I0926 08:53:08.555385  4406 net.cpp:122] Setting up BatchNorm32
I0926 08:53:08.555390  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.555392  4406 net.cpp:137] Memory required for data: 899073200
I0926 08:53:08.555397  4406 layer_factory.hpp:77] Creating layer Scale32
I0926 08:53:08.555402  4406 net.cpp:84] Creating Layer Scale32
I0926 08:53:08.555403  4406 net.cpp:406] Scale32 <- Convolution32
I0926 08:53:08.575824  4406 net.cpp:367] Scale32 -> Convolution32 (in-place)
I0926 08:53:08.575868  4406 layer_factory.hpp:77] Creating layer Scale32
I0926 08:53:08.575958  4406 net.cpp:122] Setting up Scale32
I0926 08:53:08.575963  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.575965  4406 net.cpp:137] Memory required for data: 902350000
I0926 08:53:08.575970  4406 layer_factory.hpp:77] Creating layer Eltwise15
I0926 08:53:08.575975  4406 net.cpp:84] Creating Layer Eltwise15
I0926 08:53:08.575978  4406 net.cpp:406] Eltwise15 <- Eltwise14_penlu29_0_split_1
I0926 08:53:08.575983  4406 net.cpp:406] Eltwise15 <- Convolution32
I0926 08:53:08.575986  4406 net.cpp:380] Eltwise15 -> Eltwise15
I0926 08:53:08.575999  4406 net.cpp:122] Setting up Eltwise15
I0926 08:53:08.576004  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.576006  4406 net.cpp:137] Memory required for data: 905626800
I0926 08:53:08.576009  4406 layer_factory.hpp:77] Creating layer penlu31
I0926 08:53:08.576014  4406 net.cpp:84] Creating Layer penlu31
I0926 08:53:08.576016  4406 net.cpp:406] penlu31 <- Eltwise15
I0926 08:53:08.576021  4406 net.cpp:367] penlu31 -> Eltwise15 (in-place)
I0926 08:53:08.576144  4406 net.cpp:122] Setting up penlu31
I0926 08:53:08.576149  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.576151  4406 net.cpp:137] Memory required for data: 908903600
I0926 08:53:08.576156  4406 layer_factory.hpp:77] Creating layer Eltwise15_penlu31_0_split
I0926 08:53:08.576161  4406 net.cpp:84] Creating Layer Eltwise15_penlu31_0_split
I0926 08:53:08.576164  4406 net.cpp:406] Eltwise15_penlu31_0_split <- Eltwise15
I0926 08:53:08.576167  4406 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_0
I0926 08:53:08.576172  4406 net.cpp:380] Eltwise15_penlu31_0_split -> Eltwise15_penlu31_0_split_1
I0926 08:53:08.576197  4406 net.cpp:122] Setting up Eltwise15_penlu31_0_split
I0926 08:53:08.576201  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.576205  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.576207  4406 net.cpp:137] Memory required for data: 915457200
I0926 08:53:08.576210  4406 layer_factory.hpp:77] Creating layer Convolution33
I0926 08:53:08.576215  4406 net.cpp:84] Creating Layer Convolution33
I0926 08:53:08.576218  4406 net.cpp:406] Convolution33 <- Eltwise15_penlu31_0_split_0
I0926 08:53:08.576223  4406 net.cpp:380] Convolution33 -> Convolution33
I0926 08:53:08.577476  4406 net.cpp:122] Setting up Convolution33
I0926 08:53:08.577491  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.577504  4406 net.cpp:137] Memory required for data: 918734000
I0926 08:53:08.577509  4406 layer_factory.hpp:77] Creating layer BatchNorm33
I0926 08:53:08.577514  4406 net.cpp:84] Creating Layer BatchNorm33
I0926 08:53:08.577517  4406 net.cpp:406] BatchNorm33 <- Convolution33
I0926 08:53:08.577522  4406 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I0926 08:53:08.577656  4406 net.cpp:122] Setting up BatchNorm33
I0926 08:53:08.577661  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.577662  4406 net.cpp:137] Memory required for data: 922010800
I0926 08:53:08.577668  4406 layer_factory.hpp:77] Creating layer Scale33
I0926 08:53:08.577672  4406 net.cpp:84] Creating Layer Scale33
I0926 08:53:08.577674  4406 net.cpp:406] Scale33 <- Convolution33
I0926 08:53:08.577678  4406 net.cpp:367] Scale33 -> Convolution33 (in-place)
I0926 08:53:08.577705  4406 layer_factory.hpp:77] Creating layer Scale33
I0926 08:53:08.577780  4406 net.cpp:122] Setting up Scale33
I0926 08:53:08.577785  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.577786  4406 net.cpp:137] Memory required for data: 925287600
I0926 08:53:08.577790  4406 layer_factory.hpp:77] Creating layer penlu32
I0926 08:53:08.577797  4406 net.cpp:84] Creating Layer penlu32
I0926 08:53:08.577800  4406 net.cpp:406] penlu32 <- Convolution33
I0926 08:53:08.577813  4406 net.cpp:367] penlu32 -> Convolution33 (in-place)
I0926 08:53:08.577965  4406 net.cpp:122] Setting up penlu32
I0926 08:53:08.577968  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.577970  4406 net.cpp:137] Memory required for data: 928564400
I0926 08:53:08.577986  4406 layer_factory.hpp:77] Creating layer Convolution34
I0926 08:53:08.578001  4406 net.cpp:84] Creating Layer Convolution34
I0926 08:53:08.578003  4406 net.cpp:406] Convolution34 <- Convolution33
I0926 08:53:08.578007  4406 net.cpp:380] Convolution34 -> Convolution34
I0926 08:53:08.579784  4406 net.cpp:122] Setting up Convolution34
I0926 08:53:08.579793  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.579795  4406 net.cpp:137] Memory required for data: 931841200
I0926 08:53:08.579800  4406 layer_factory.hpp:77] Creating layer BatchNorm34
I0926 08:53:08.579807  4406 net.cpp:84] Creating Layer BatchNorm34
I0926 08:53:08.579809  4406 net.cpp:406] BatchNorm34 <- Convolution34
I0926 08:53:08.579813  4406 net.cpp:367] BatchNorm34 -> Convolution34 (in-place)
I0926 08:53:08.579939  4406 net.cpp:122] Setting up BatchNorm34
I0926 08:53:08.579944  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.579946  4406 net.cpp:137] Memory required for data: 935118000
I0926 08:53:08.579952  4406 layer_factory.hpp:77] Creating layer Scale34
I0926 08:53:08.579957  4406 net.cpp:84] Creating Layer Scale34
I0926 08:53:08.579958  4406 net.cpp:406] Scale34 <- Convolution34
I0926 08:53:08.579962  4406 net.cpp:367] Scale34 -> Convolution34 (in-place)
I0926 08:53:08.579989  4406 layer_factory.hpp:77] Creating layer Scale34
I0926 08:53:08.580063  4406 net.cpp:122] Setting up Scale34
I0926 08:53:08.580068  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.580070  4406 net.cpp:137] Memory required for data: 938394800
I0926 08:53:08.580075  4406 layer_factory.hpp:77] Creating layer Eltwise16
I0926 08:53:08.580080  4406 net.cpp:84] Creating Layer Eltwise16
I0926 08:53:08.580082  4406 net.cpp:406] Eltwise16 <- Eltwise15_penlu31_0_split_1
I0926 08:53:08.580085  4406 net.cpp:406] Eltwise16 <- Convolution34
I0926 08:53:08.580088  4406 net.cpp:380] Eltwise16 -> Eltwise16
I0926 08:53:08.580101  4406 net.cpp:122] Setting up Eltwise16
I0926 08:53:08.580104  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.580106  4406 net.cpp:137] Memory required for data: 941671600
I0926 08:53:08.580108  4406 layer_factory.hpp:77] Creating layer penlu33
I0926 08:53:08.580113  4406 net.cpp:84] Creating Layer penlu33
I0926 08:53:08.580116  4406 net.cpp:406] penlu33 <- Eltwise16
I0926 08:53:08.580128  4406 net.cpp:367] penlu33 -> Eltwise16 (in-place)
I0926 08:53:08.580240  4406 net.cpp:122] Setting up penlu33
I0926 08:53:08.580245  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.580246  4406 net.cpp:137] Memory required for data: 944948400
I0926 08:53:08.580250  4406 layer_factory.hpp:77] Creating layer Eltwise16_penlu33_0_split
I0926 08:53:08.580255  4406 net.cpp:84] Creating Layer Eltwise16_penlu33_0_split
I0926 08:53:08.580256  4406 net.cpp:406] Eltwise16_penlu33_0_split <- Eltwise16
I0926 08:53:08.580260  4406 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_0
I0926 08:53:08.580265  4406 net.cpp:380] Eltwise16_penlu33_0_split -> Eltwise16_penlu33_0_split_1
I0926 08:53:08.580287  4406 net.cpp:122] Setting up Eltwise16_penlu33_0_split
I0926 08:53:08.580291  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.580293  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.580296  4406 net.cpp:137] Memory required for data: 951502000
I0926 08:53:08.580297  4406 layer_factory.hpp:77] Creating layer Convolution35
I0926 08:53:08.580304  4406 net.cpp:84] Creating Layer Convolution35
I0926 08:53:08.580307  4406 net.cpp:406] Convolution35 <- Eltwise16_penlu33_0_split_0
I0926 08:53:08.580310  4406 net.cpp:380] Convolution35 -> Convolution35
I0926 08:53:08.581428  4406 net.cpp:122] Setting up Convolution35
I0926 08:53:08.581436  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.581439  4406 net.cpp:137] Memory required for data: 954778800
I0926 08:53:08.581444  4406 layer_factory.hpp:77] Creating layer BatchNorm35
I0926 08:53:08.581449  4406 net.cpp:84] Creating Layer BatchNorm35
I0926 08:53:08.581451  4406 net.cpp:406] BatchNorm35 <- Convolution35
I0926 08:53:08.581455  4406 net.cpp:367] BatchNorm35 -> Convolution35 (in-place)
I0926 08:53:08.581585  4406 net.cpp:122] Setting up BatchNorm35
I0926 08:53:08.581589  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.581593  4406 net.cpp:137] Memory required for data: 958055600
I0926 08:53:08.581596  4406 layer_factory.hpp:77] Creating layer Scale35
I0926 08:53:08.581600  4406 net.cpp:84] Creating Layer Scale35
I0926 08:53:08.581604  4406 net.cpp:406] Scale35 <- Convolution35
I0926 08:53:08.581606  4406 net.cpp:367] Scale35 -> Convolution35 (in-place)
I0926 08:53:08.581632  4406 layer_factory.hpp:77] Creating layer Scale35
I0926 08:53:08.581708  4406 net.cpp:122] Setting up Scale35
I0926 08:53:08.581713  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.581715  4406 net.cpp:137] Memory required for data: 961332400
I0926 08:53:08.581718  4406 layer_factory.hpp:77] Creating layer penlu34
I0926 08:53:08.581724  4406 net.cpp:84] Creating Layer penlu34
I0926 08:53:08.581727  4406 net.cpp:406] penlu34 <- Convolution35
I0926 08:53:08.581730  4406 net.cpp:367] penlu34 -> Convolution35 (in-place)
I0926 08:53:08.581836  4406 net.cpp:122] Setting up penlu34
I0926 08:53:08.581841  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.581843  4406 net.cpp:137] Memory required for data: 964609200
I0926 08:53:08.581847  4406 layer_factory.hpp:77] Creating layer Convolution36
I0926 08:53:08.581854  4406 net.cpp:84] Creating Layer Convolution36
I0926 08:53:08.581857  4406 net.cpp:406] Convolution36 <- Convolution35
I0926 08:53:08.581861  4406 net.cpp:380] Convolution36 -> Convolution36
I0926 08:53:08.582612  4406 net.cpp:122] Setting up Convolution36
I0926 08:53:08.582620  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.582623  4406 net.cpp:137] Memory required for data: 967886000
I0926 08:53:08.582628  4406 layer_factory.hpp:77] Creating layer BatchNorm36
I0926 08:53:08.582631  4406 net.cpp:84] Creating Layer BatchNorm36
I0926 08:53:08.582634  4406 net.cpp:406] BatchNorm36 <- Convolution36
I0926 08:53:08.582638  4406 net.cpp:367] BatchNorm36 -> Convolution36 (in-place)
I0926 08:53:08.582768  4406 net.cpp:122] Setting up BatchNorm36
I0926 08:53:08.582772  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.582774  4406 net.cpp:137] Memory required for data: 971162800
I0926 08:53:08.582785  4406 layer_factory.hpp:77] Creating layer Scale36
I0926 08:53:08.582790  4406 net.cpp:84] Creating Layer Scale36
I0926 08:53:08.582792  4406 net.cpp:406] Scale36 <- Convolution36
I0926 08:53:08.582795  4406 net.cpp:367] Scale36 -> Convolution36 (in-place)
I0926 08:53:08.582823  4406 layer_factory.hpp:77] Creating layer Scale36
I0926 08:53:08.582897  4406 net.cpp:122] Setting up Scale36
I0926 08:53:08.582901  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.582903  4406 net.cpp:137] Memory required for data: 974439600
I0926 08:53:08.582907  4406 layer_factory.hpp:77] Creating layer Eltwise17
I0926 08:53:08.582911  4406 net.cpp:84] Creating Layer Eltwise17
I0926 08:53:08.582913  4406 net.cpp:406] Eltwise17 <- Eltwise16_penlu33_0_split_1
I0926 08:53:08.582916  4406 net.cpp:406] Eltwise17 <- Convolution36
I0926 08:53:08.582921  4406 net.cpp:380] Eltwise17 -> Eltwise17
I0926 08:53:08.582932  4406 net.cpp:122] Setting up Eltwise17
I0926 08:53:08.582936  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.582937  4406 net.cpp:137] Memory required for data: 977716400
I0926 08:53:08.582939  4406 layer_factory.hpp:77] Creating layer penlu35
I0926 08:53:08.582945  4406 net.cpp:84] Creating Layer penlu35
I0926 08:53:08.582947  4406 net.cpp:406] penlu35 <- Eltwise17
I0926 08:53:08.582952  4406 net.cpp:367] penlu35 -> Eltwise17 (in-place)
I0926 08:53:08.583060  4406 net.cpp:122] Setting up penlu35
I0926 08:53:08.583065  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.583066  4406 net.cpp:137] Memory required for data: 980993200
I0926 08:53:08.583070  4406 layer_factory.hpp:77] Creating layer Eltwise17_penlu35_0_split
I0926 08:53:08.583073  4406 net.cpp:84] Creating Layer Eltwise17_penlu35_0_split
I0926 08:53:08.583076  4406 net.cpp:406] Eltwise17_penlu35_0_split <- Eltwise17
I0926 08:53:08.583079  4406 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_0
I0926 08:53:08.583083  4406 net.cpp:380] Eltwise17_penlu35_0_split -> Eltwise17_penlu35_0_split_1
I0926 08:53:08.583106  4406 net.cpp:122] Setting up Eltwise17_penlu35_0_split
I0926 08:53:08.583111  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.583113  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.583115  4406 net.cpp:137] Memory required for data: 987546800
I0926 08:53:08.583117  4406 layer_factory.hpp:77] Creating layer Convolution37
I0926 08:53:08.583124  4406 net.cpp:84] Creating Layer Convolution37
I0926 08:53:08.583127  4406 net.cpp:406] Convolution37 <- Eltwise17_penlu35_0_split_0
I0926 08:53:08.583130  4406 net.cpp:380] Convolution37 -> Convolution37
I0926 08:53:08.584211  4406 net.cpp:122] Setting up Convolution37
I0926 08:53:08.584219  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.584223  4406 net.cpp:137] Memory required for data: 990823600
I0926 08:53:08.584226  4406 layer_factory.hpp:77] Creating layer BatchNorm37
I0926 08:53:08.584231  4406 net.cpp:84] Creating Layer BatchNorm37
I0926 08:53:08.584234  4406 net.cpp:406] BatchNorm37 <- Convolution37
I0926 08:53:08.584239  4406 net.cpp:367] BatchNorm37 -> Convolution37 (in-place)
I0926 08:53:08.584368  4406 net.cpp:122] Setting up BatchNorm37
I0926 08:53:08.584373  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.584375  4406 net.cpp:137] Memory required for data: 994100400
I0926 08:53:08.584379  4406 layer_factory.hpp:77] Creating layer Scale37
I0926 08:53:08.584383  4406 net.cpp:84] Creating Layer Scale37
I0926 08:53:08.584386  4406 net.cpp:406] Scale37 <- Convolution37
I0926 08:53:08.584389  4406 net.cpp:367] Scale37 -> Convolution37 (in-place)
I0926 08:53:08.584415  4406 layer_factory.hpp:77] Creating layer Scale37
I0926 08:53:08.584491  4406 net.cpp:122] Setting up Scale37
I0926 08:53:08.584511  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.584516  4406 net.cpp:137] Memory required for data: 997377200
I0926 08:53:08.584520  4406 layer_factory.hpp:77] Creating layer penlu36
I0926 08:53:08.584525  4406 net.cpp:84] Creating Layer penlu36
I0926 08:53:08.584543  4406 net.cpp:406] penlu36 <- Convolution37
I0926 08:53:08.584548  4406 net.cpp:367] penlu36 -> Convolution37 (in-place)
I0926 08:53:08.584657  4406 net.cpp:122] Setting up penlu36
I0926 08:53:08.584662  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.584664  4406 net.cpp:137] Memory required for data: 1000654000
I0926 08:53:08.584668  4406 layer_factory.hpp:77] Creating layer Convolution38
I0926 08:53:08.584676  4406 net.cpp:84] Creating Layer Convolution38
I0926 08:53:08.584678  4406 net.cpp:406] Convolution38 <- Convolution37
I0926 08:53:08.584682  4406 net.cpp:380] Convolution38 -> Convolution38
I0926 08:53:08.586123  4406 net.cpp:122] Setting up Convolution38
I0926 08:53:08.586133  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.586135  4406 net.cpp:137] Memory required for data: 1003930800
I0926 08:53:08.586139  4406 layer_factory.hpp:77] Creating layer BatchNorm38
I0926 08:53:08.586145  4406 net.cpp:84] Creating Layer BatchNorm38
I0926 08:53:08.586148  4406 net.cpp:406] BatchNorm38 <- Convolution38
I0926 08:53:08.586151  4406 net.cpp:367] BatchNorm38 -> Convolution38 (in-place)
I0926 08:53:08.586297  4406 net.cpp:122] Setting up BatchNorm38
I0926 08:53:08.586302  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.586303  4406 net.cpp:137] Memory required for data: 1007207600
I0926 08:53:08.606189  4406 layer_factory.hpp:77] Creating layer Scale38
I0926 08:53:08.606200  4406 net.cpp:84] Creating Layer Scale38
I0926 08:53:08.606204  4406 net.cpp:406] Scale38 <- Convolution38
I0926 08:53:08.606209  4406 net.cpp:367] Scale38 -> Convolution38 (in-place)
I0926 08:53:08.606251  4406 layer_factory.hpp:77] Creating layer Scale38
I0926 08:53:08.606343  4406 net.cpp:122] Setting up Scale38
I0926 08:53:08.606348  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.606351  4406 net.cpp:137] Memory required for data: 1010484400
I0926 08:53:08.606355  4406 layer_factory.hpp:77] Creating layer Eltwise18
I0926 08:53:08.606360  4406 net.cpp:84] Creating Layer Eltwise18
I0926 08:53:08.606364  4406 net.cpp:406] Eltwise18 <- Eltwise17_penlu35_0_split_1
I0926 08:53:08.606367  4406 net.cpp:406] Eltwise18 <- Convolution38
I0926 08:53:08.606371  4406 net.cpp:380] Eltwise18 -> Eltwise18
I0926 08:53:08.606384  4406 net.cpp:122] Setting up Eltwise18
I0926 08:53:08.606389  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.606391  4406 net.cpp:137] Memory required for data: 1013761200
I0926 08:53:08.606393  4406 layer_factory.hpp:77] Creating layer penlu37
I0926 08:53:08.606400  4406 net.cpp:84] Creating Layer penlu37
I0926 08:53:08.606402  4406 net.cpp:406] penlu37 <- Eltwise18
I0926 08:53:08.606406  4406 net.cpp:367] penlu37 -> Eltwise18 (in-place)
I0926 08:53:08.606531  4406 net.cpp:122] Setting up penlu37
I0926 08:53:08.606536  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.606539  4406 net.cpp:137] Memory required for data: 1017038000
I0926 08:53:08.606544  4406 layer_factory.hpp:77] Creating layer Eltwise18_penlu37_0_split
I0926 08:53:08.606547  4406 net.cpp:84] Creating Layer Eltwise18_penlu37_0_split
I0926 08:53:08.606550  4406 net.cpp:406] Eltwise18_penlu37_0_split <- Eltwise18
I0926 08:53:08.606554  4406 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_0
I0926 08:53:08.606559  4406 net.cpp:380] Eltwise18_penlu37_0_split -> Eltwise18_penlu37_0_split_1
I0926 08:53:08.606585  4406 net.cpp:122] Setting up Eltwise18_penlu37_0_split
I0926 08:53:08.606588  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.606591  4406 net.cpp:129] Top shape: 100 32 16 16 (819200)
I0926 08:53:08.606593  4406 net.cpp:137] Memory required for data: 1023591600
I0926 08:53:08.606596  4406 layer_factory.hpp:77] Creating layer Convolution39
I0926 08:53:08.606603  4406 net.cpp:84] Creating Layer Convolution39
I0926 08:53:08.606606  4406 net.cpp:406] Convolution39 <- Eltwise18_penlu37_0_split_0
I0926 08:53:08.606611  4406 net.cpp:380] Convolution39 -> Convolution39
I0926 08:53:08.607750  4406 net.cpp:122] Setting up Convolution39
I0926 08:53:08.607760  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.607764  4406 net.cpp:137] Memory required for data: 1025230000
I0926 08:53:08.607767  4406 layer_factory.hpp:77] Creating layer BatchNorm39
I0926 08:53:08.607774  4406 net.cpp:84] Creating Layer BatchNorm39
I0926 08:53:08.607776  4406 net.cpp:406] BatchNorm39 <- Convolution39
I0926 08:53:08.607780  4406 net.cpp:367] BatchNorm39 -> Convolution39 (in-place)
I0926 08:53:08.607923  4406 net.cpp:122] Setting up BatchNorm39
I0926 08:53:08.607928  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.607930  4406 net.cpp:137] Memory required for data: 1026868400
I0926 08:53:08.607935  4406 layer_factory.hpp:77] Creating layer Scale39
I0926 08:53:08.607939  4406 net.cpp:84] Creating Layer Scale39
I0926 08:53:08.607942  4406 net.cpp:406] Scale39 <- Convolution39
I0926 08:53:08.607946  4406 net.cpp:367] Scale39 -> Convolution39 (in-place)
I0926 08:53:08.607975  4406 layer_factory.hpp:77] Creating layer Scale39
I0926 08:53:08.608057  4406 net.cpp:122] Setting up Scale39
I0926 08:53:08.608062  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.608063  4406 net.cpp:137] Memory required for data: 1028506800
I0926 08:53:08.608067  4406 layer_factory.hpp:77] Creating layer Convolution40
I0926 08:53:08.608074  4406 net.cpp:84] Creating Layer Convolution40
I0926 08:53:08.608077  4406 net.cpp:406] Convolution40 <- Eltwise18_penlu37_0_split_1
I0926 08:53:08.608083  4406 net.cpp:380] Convolution40 -> Convolution40
I0926 08:53:08.609501  4406 net.cpp:122] Setting up Convolution40
I0926 08:53:08.609522  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.609525  4406 net.cpp:137] Memory required for data: 1030145200
I0926 08:53:08.609530  4406 layer_factory.hpp:77] Creating layer BatchNorm40
I0926 08:53:08.609534  4406 net.cpp:84] Creating Layer BatchNorm40
I0926 08:53:08.609537  4406 net.cpp:406] BatchNorm40 <- Convolution40
I0926 08:53:08.609542  4406 net.cpp:367] BatchNorm40 -> Convolution40 (in-place)
I0926 08:53:08.609691  4406 net.cpp:122] Setting up BatchNorm40
I0926 08:53:08.609696  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.609699  4406 net.cpp:137] Memory required for data: 1031783600
I0926 08:53:08.609704  4406 layer_factory.hpp:77] Creating layer Scale40
I0926 08:53:08.609707  4406 net.cpp:84] Creating Layer Scale40
I0926 08:53:08.609709  4406 net.cpp:406] Scale40 <- Convolution40
I0926 08:53:08.609714  4406 net.cpp:367] Scale40 -> Convolution40 (in-place)
I0926 08:53:08.609740  4406 layer_factory.hpp:77] Creating layer Scale40
I0926 08:53:08.609828  4406 net.cpp:122] Setting up Scale40
I0926 08:53:08.609833  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.609835  4406 net.cpp:137] Memory required for data: 1033422000
I0926 08:53:08.609839  4406 layer_factory.hpp:77] Creating layer penlu38
I0926 08:53:08.609844  4406 net.cpp:84] Creating Layer penlu38
I0926 08:53:08.609848  4406 net.cpp:406] penlu38 <- Convolution40
I0926 08:53:08.609860  4406 net.cpp:367] penlu38 -> Convolution40 (in-place)
I0926 08:53:08.609997  4406 net.cpp:122] Setting up penlu38
I0926 08:53:08.610002  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.610004  4406 net.cpp:137] Memory required for data: 1035060400
I0926 08:53:08.610008  4406 layer_factory.hpp:77] Creating layer Convolution41
I0926 08:53:08.610015  4406 net.cpp:84] Creating Layer Convolution41
I0926 08:53:08.610018  4406 net.cpp:406] Convolution41 <- Convolution40
I0926 08:53:08.610023  4406 net.cpp:380] Convolution41 -> Convolution41
I0926 08:53:08.611716  4406 net.cpp:122] Setting up Convolution41
I0926 08:53:08.611724  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.611726  4406 net.cpp:137] Memory required for data: 1036698800
I0926 08:53:08.611732  4406 layer_factory.hpp:77] Creating layer BatchNorm41
I0926 08:53:08.611737  4406 net.cpp:84] Creating Layer BatchNorm41
I0926 08:53:08.611739  4406 net.cpp:406] BatchNorm41 <- Convolution41
I0926 08:53:08.611749  4406 net.cpp:367] BatchNorm41 -> Convolution41 (in-place)
I0926 08:53:08.611886  4406 net.cpp:122] Setting up BatchNorm41
I0926 08:53:08.611891  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.611893  4406 net.cpp:137] Memory required for data: 1038337200
I0926 08:53:08.611898  4406 layer_factory.hpp:77] Creating layer Scale41
I0926 08:53:08.611902  4406 net.cpp:84] Creating Layer Scale41
I0926 08:53:08.611904  4406 net.cpp:406] Scale41 <- Convolution41
I0926 08:53:08.611907  4406 net.cpp:367] Scale41 -> Convolution41 (in-place)
I0926 08:53:08.611937  4406 layer_factory.hpp:77] Creating layer Scale41
I0926 08:53:08.612015  4406 net.cpp:122] Setting up Scale41
I0926 08:53:08.612020  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.612023  4406 net.cpp:137] Memory required for data: 1039975600
I0926 08:53:08.612026  4406 layer_factory.hpp:77] Creating layer Eltwise19
I0926 08:53:08.612030  4406 net.cpp:84] Creating Layer Eltwise19
I0926 08:53:08.612033  4406 net.cpp:406] Eltwise19 <- Convolution39
I0926 08:53:08.612036  4406 net.cpp:406] Eltwise19 <- Convolution41
I0926 08:53:08.612040  4406 net.cpp:380] Eltwise19 -> Eltwise19
I0926 08:53:08.612056  4406 net.cpp:122] Setting up Eltwise19
I0926 08:53:08.612059  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.612061  4406 net.cpp:137] Memory required for data: 1041614000
I0926 08:53:08.612064  4406 layer_factory.hpp:77] Creating layer penlu39
I0926 08:53:08.612069  4406 net.cpp:84] Creating Layer penlu39
I0926 08:53:08.612071  4406 net.cpp:406] penlu39 <- Eltwise19
I0926 08:53:08.612076  4406 net.cpp:367] penlu39 -> Eltwise19 (in-place)
I0926 08:53:08.612191  4406 net.cpp:122] Setting up penlu39
I0926 08:53:08.612196  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.612198  4406 net.cpp:137] Memory required for data: 1043252400
I0926 08:53:08.612202  4406 layer_factory.hpp:77] Creating layer Eltwise19_penlu39_0_split
I0926 08:53:08.612207  4406 net.cpp:84] Creating Layer Eltwise19_penlu39_0_split
I0926 08:53:08.612210  4406 net.cpp:406] Eltwise19_penlu39_0_split <- Eltwise19
I0926 08:53:08.612213  4406 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_0
I0926 08:53:08.612217  4406 net.cpp:380] Eltwise19_penlu39_0_split -> Eltwise19_penlu39_0_split_1
I0926 08:53:08.612241  4406 net.cpp:122] Setting up Eltwise19_penlu39_0_split
I0926 08:53:08.612244  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.612247  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.612249  4406 net.cpp:137] Memory required for data: 1046529200
I0926 08:53:08.612251  4406 layer_factory.hpp:77] Creating layer Convolution42
I0926 08:53:08.612257  4406 net.cpp:84] Creating Layer Convolution42
I0926 08:53:08.612259  4406 net.cpp:406] Convolution42 <- Eltwise19_penlu39_0_split_0
I0926 08:53:08.612264  4406 net.cpp:380] Convolution42 -> Convolution42
I0926 08:53:08.613965  4406 net.cpp:122] Setting up Convolution42
I0926 08:53:08.613975  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.613976  4406 net.cpp:137] Memory required for data: 1048167600
I0926 08:53:08.613981  4406 layer_factory.hpp:77] Creating layer BatchNorm42
I0926 08:53:08.613986  4406 net.cpp:84] Creating Layer BatchNorm42
I0926 08:53:08.613989  4406 net.cpp:406] BatchNorm42 <- Convolution42
I0926 08:53:08.613992  4406 net.cpp:367] BatchNorm42 -> Convolution42 (in-place)
I0926 08:53:08.614128  4406 net.cpp:122] Setting up BatchNorm42
I0926 08:53:08.614133  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.614135  4406 net.cpp:137] Memory required for data: 1049806000
I0926 08:53:08.614140  4406 layer_factory.hpp:77] Creating layer Scale42
I0926 08:53:08.614143  4406 net.cpp:84] Creating Layer Scale42
I0926 08:53:08.614146  4406 net.cpp:406] Scale42 <- Convolution42
I0926 08:53:08.614150  4406 net.cpp:367] Scale42 -> Convolution42 (in-place)
I0926 08:53:08.614177  4406 layer_factory.hpp:77] Creating layer Scale42
I0926 08:53:08.614256  4406 net.cpp:122] Setting up Scale42
I0926 08:53:08.614260  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.614269  4406 net.cpp:137] Memory required for data: 1051444400
I0926 08:53:08.614274  4406 layer_factory.hpp:77] Creating layer penlu40
I0926 08:53:08.614279  4406 net.cpp:84] Creating Layer penlu40
I0926 08:53:08.614282  4406 net.cpp:406] penlu40 <- Convolution42
I0926 08:53:08.614286  4406 net.cpp:367] penlu40 -> Convolution42 (in-place)
I0926 08:53:08.614401  4406 net.cpp:122] Setting up penlu40
I0926 08:53:08.614406  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.614408  4406 net.cpp:137] Memory required for data: 1053082800
I0926 08:53:08.614413  4406 layer_factory.hpp:77] Creating layer Convolution43
I0926 08:53:08.614420  4406 net.cpp:84] Creating Layer Convolution43
I0926 08:53:08.614423  4406 net.cpp:406] Convolution43 <- Convolution42
I0926 08:53:08.614426  4406 net.cpp:380] Convolution43 -> Convolution43
I0926 08:53:08.616132  4406 net.cpp:122] Setting up Convolution43
I0926 08:53:08.616142  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.616143  4406 net.cpp:137] Memory required for data: 1054721200
I0926 08:53:08.616148  4406 layer_factory.hpp:77] Creating layer BatchNorm43
I0926 08:53:08.616154  4406 net.cpp:84] Creating Layer BatchNorm43
I0926 08:53:08.616158  4406 net.cpp:406] BatchNorm43 <- Convolution43
I0926 08:53:08.616161  4406 net.cpp:367] BatchNorm43 -> Convolution43 (in-place)
I0926 08:53:08.616309  4406 net.cpp:122] Setting up BatchNorm43
I0926 08:53:08.616315  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.616317  4406 net.cpp:137] Memory required for data: 1056359600
I0926 08:53:08.616323  4406 layer_factory.hpp:77] Creating layer Scale43
I0926 08:53:08.616328  4406 net.cpp:84] Creating Layer Scale43
I0926 08:53:08.616330  4406 net.cpp:406] Scale43 <- Convolution43
I0926 08:53:08.616334  4406 net.cpp:367] Scale43 -> Convolution43 (in-place)
I0926 08:53:08.616361  4406 layer_factory.hpp:77] Creating layer Scale43
I0926 08:53:08.616441  4406 net.cpp:122] Setting up Scale43
I0926 08:53:08.616444  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.616446  4406 net.cpp:137] Memory required for data: 1057998000
I0926 08:53:08.616451  4406 layer_factory.hpp:77] Creating layer Eltwise20
I0926 08:53:08.616454  4406 net.cpp:84] Creating Layer Eltwise20
I0926 08:53:08.616457  4406 net.cpp:406] Eltwise20 <- Eltwise19_penlu39_0_split_1
I0926 08:53:08.616461  4406 net.cpp:406] Eltwise20 <- Convolution43
I0926 08:53:08.616464  4406 net.cpp:380] Eltwise20 -> Eltwise20
I0926 08:53:08.616480  4406 net.cpp:122] Setting up Eltwise20
I0926 08:53:08.616484  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.616487  4406 net.cpp:137] Memory required for data: 1059636400
I0926 08:53:08.616488  4406 layer_factory.hpp:77] Creating layer penlu41
I0926 08:53:08.616494  4406 net.cpp:84] Creating Layer penlu41
I0926 08:53:08.616503  4406 net.cpp:406] penlu41 <- Eltwise20
I0926 08:53:08.616508  4406 net.cpp:367] penlu41 -> Eltwise20 (in-place)
I0926 08:53:08.616627  4406 net.cpp:122] Setting up penlu41
I0926 08:53:08.616632  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.616634  4406 net.cpp:137] Memory required for data: 1061274800
I0926 08:53:08.616639  4406 layer_factory.hpp:77] Creating layer Eltwise20_penlu41_0_split
I0926 08:53:08.616642  4406 net.cpp:84] Creating Layer Eltwise20_penlu41_0_split
I0926 08:53:08.616645  4406 net.cpp:406] Eltwise20_penlu41_0_split <- Eltwise20
I0926 08:53:08.616649  4406 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_0
I0926 08:53:08.616653  4406 net.cpp:380] Eltwise20_penlu41_0_split -> Eltwise20_penlu41_0_split_1
I0926 08:53:08.616677  4406 net.cpp:122] Setting up Eltwise20_penlu41_0_split
I0926 08:53:08.616680  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.616683  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.616684  4406 net.cpp:137] Memory required for data: 1064551600
I0926 08:53:08.616688  4406 layer_factory.hpp:77] Creating layer Convolution44
I0926 08:53:08.616693  4406 net.cpp:84] Creating Layer Convolution44
I0926 08:53:08.616703  4406 net.cpp:406] Convolution44 <- Eltwise20_penlu41_0_split_0
I0926 08:53:08.616706  4406 net.cpp:380] Convolution44 -> Convolution44
I0926 08:53:08.618716  4406 net.cpp:122] Setting up Convolution44
I0926 08:53:08.618724  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.618727  4406 net.cpp:137] Memory required for data: 1066190000
I0926 08:53:08.618731  4406 layer_factory.hpp:77] Creating layer BatchNorm44
I0926 08:53:08.618736  4406 net.cpp:84] Creating Layer BatchNorm44
I0926 08:53:08.618739  4406 net.cpp:406] BatchNorm44 <- Convolution44
I0926 08:53:08.618743  4406 net.cpp:367] BatchNorm44 -> Convolution44 (in-place)
I0926 08:53:08.618885  4406 net.cpp:122] Setting up BatchNorm44
I0926 08:53:08.618890  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.618891  4406 net.cpp:137] Memory required for data: 1067828400
I0926 08:53:08.618896  4406 layer_factory.hpp:77] Creating layer Scale44
I0926 08:53:08.618901  4406 net.cpp:84] Creating Layer Scale44
I0926 08:53:08.618902  4406 net.cpp:406] Scale44 <- Convolution44
I0926 08:53:08.618906  4406 net.cpp:367] Scale44 -> Convolution44 (in-place)
I0926 08:53:08.618933  4406 layer_factory.hpp:77] Creating layer Scale44
I0926 08:53:08.619012  4406 net.cpp:122] Setting up Scale44
I0926 08:53:08.619017  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.619019  4406 net.cpp:137] Memory required for data: 1069466800
I0926 08:53:08.619024  4406 layer_factory.hpp:77] Creating layer penlu42
I0926 08:53:08.619029  4406 net.cpp:84] Creating Layer penlu42
I0926 08:53:08.619031  4406 net.cpp:406] penlu42 <- Convolution44
I0926 08:53:08.619035  4406 net.cpp:367] penlu42 -> Convolution44 (in-place)
I0926 08:53:08.619148  4406 net.cpp:122] Setting up penlu42
I0926 08:53:08.619153  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.619155  4406 net.cpp:137] Memory required for data: 1071105200
I0926 08:53:08.619159  4406 layer_factory.hpp:77] Creating layer Convolution45
I0926 08:53:08.619166  4406 net.cpp:84] Creating Layer Convolution45
I0926 08:53:08.619168  4406 net.cpp:406] Convolution45 <- Convolution44
I0926 08:53:08.619173  4406 net.cpp:380] Convolution45 -> Convolution45
I0926 08:53:08.620883  4406 net.cpp:122] Setting up Convolution45
I0926 08:53:08.620893  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.620896  4406 net.cpp:137] Memory required for data: 1072743600
I0926 08:53:08.620900  4406 layer_factory.hpp:77] Creating layer BatchNorm45
I0926 08:53:08.620904  4406 net.cpp:84] Creating Layer BatchNorm45
I0926 08:53:08.620908  4406 net.cpp:406] BatchNorm45 <- Convolution45
I0926 08:53:08.620911  4406 net.cpp:367] BatchNorm45 -> Convolution45 (in-place)
I0926 08:53:08.621049  4406 net.cpp:122] Setting up BatchNorm45
I0926 08:53:08.621053  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.621057  4406 net.cpp:137] Memory required for data: 1074382000
I0926 08:53:08.621060  4406 layer_factory.hpp:77] Creating layer Scale45
I0926 08:53:08.621064  4406 net.cpp:84] Creating Layer Scale45
I0926 08:53:08.621068  4406 net.cpp:406] Scale45 <- Convolution45
I0926 08:53:08.621070  4406 net.cpp:367] Scale45 -> Convolution45 (in-place)
I0926 08:53:08.621098  4406 layer_factory.hpp:77] Creating layer Scale45
I0926 08:53:08.621176  4406 net.cpp:122] Setting up Scale45
I0926 08:53:08.621181  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.621182  4406 net.cpp:137] Memory required for data: 1076020400
I0926 08:53:08.621186  4406 layer_factory.hpp:77] Creating layer Eltwise21
I0926 08:53:08.621189  4406 net.cpp:84] Creating Layer Eltwise21
I0926 08:53:08.621192  4406 net.cpp:406] Eltwise21 <- Eltwise20_penlu41_0_split_1
I0926 08:53:08.621196  4406 net.cpp:406] Eltwise21 <- Convolution45
I0926 08:53:08.621198  4406 net.cpp:380] Eltwise21 -> Eltwise21
I0926 08:53:08.621215  4406 net.cpp:122] Setting up Eltwise21
I0926 08:53:08.621219  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.621220  4406 net.cpp:137] Memory required for data: 1077658800
I0926 08:53:08.621229  4406 layer_factory.hpp:77] Creating layer penlu43
I0926 08:53:08.621235  4406 net.cpp:84] Creating Layer penlu43
I0926 08:53:08.621238  4406 net.cpp:406] penlu43 <- Eltwise21
I0926 08:53:08.621243  4406 net.cpp:367] penlu43 -> Eltwise21 (in-place)
I0926 08:53:08.621359  4406 net.cpp:122] Setting up penlu43
I0926 08:53:08.621363  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.621366  4406 net.cpp:137] Memory required for data: 1079297200
I0926 08:53:08.621371  4406 layer_factory.hpp:77] Creating layer Eltwise21_penlu43_0_split
I0926 08:53:08.621373  4406 net.cpp:84] Creating Layer Eltwise21_penlu43_0_split
I0926 08:53:08.621376  4406 net.cpp:406] Eltwise21_penlu43_0_split <- Eltwise21
I0926 08:53:08.621379  4406 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_0
I0926 08:53:08.621383  4406 net.cpp:380] Eltwise21_penlu43_0_split -> Eltwise21_penlu43_0_split_1
I0926 08:53:08.621407  4406 net.cpp:122] Setting up Eltwise21_penlu43_0_split
I0926 08:53:08.621412  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.621414  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.638686  4406 net.cpp:137] Memory required for data: 1082574000
I0926 08:53:08.638695  4406 layer_factory.hpp:77] Creating layer Convolution46
I0926 08:53:08.638703  4406 net.cpp:84] Creating Layer Convolution46
I0926 08:53:08.638707  4406 net.cpp:406] Convolution46 <- Eltwise21_penlu43_0_split_0
I0926 08:53:08.638715  4406 net.cpp:380] Convolution46 -> Convolution46
I0926 08:53:08.640638  4406 net.cpp:122] Setting up Convolution46
I0926 08:53:08.640648  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.640651  4406 net.cpp:137] Memory required for data: 1084212400
I0926 08:53:08.640656  4406 layer_factory.hpp:77] Creating layer BatchNorm46
I0926 08:53:08.640661  4406 net.cpp:84] Creating Layer BatchNorm46
I0926 08:53:08.640663  4406 net.cpp:406] BatchNorm46 <- Convolution46
I0926 08:53:08.640668  4406 net.cpp:367] BatchNorm46 -> Convolution46 (in-place)
I0926 08:53:08.640811  4406 net.cpp:122] Setting up BatchNorm46
I0926 08:53:08.640815  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.640817  4406 net.cpp:137] Memory required for data: 1085850800
I0926 08:53:08.640822  4406 layer_factory.hpp:77] Creating layer Scale46
I0926 08:53:08.640826  4406 net.cpp:84] Creating Layer Scale46
I0926 08:53:08.640830  4406 net.cpp:406] Scale46 <- Convolution46
I0926 08:53:08.640833  4406 net.cpp:367] Scale46 -> Convolution46 (in-place)
I0926 08:53:08.640861  4406 layer_factory.hpp:77] Creating layer Scale46
I0926 08:53:08.640943  4406 net.cpp:122] Setting up Scale46
I0926 08:53:08.640946  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.640949  4406 net.cpp:137] Memory required for data: 1087489200
I0926 08:53:08.640954  4406 layer_factory.hpp:77] Creating layer penlu44
I0926 08:53:08.640959  4406 net.cpp:84] Creating Layer penlu44
I0926 08:53:08.640960  4406 net.cpp:406] penlu44 <- Convolution46
I0926 08:53:08.640964  4406 net.cpp:367] penlu44 -> Convolution46 (in-place)
I0926 08:53:08.641083  4406 net.cpp:122] Setting up penlu44
I0926 08:53:08.641088  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.641089  4406 net.cpp:137] Memory required for data: 1089127600
I0926 08:53:08.641093  4406 layer_factory.hpp:77] Creating layer Convolution47
I0926 08:53:08.641099  4406 net.cpp:84] Creating Layer Convolution47
I0926 08:53:08.641103  4406 net.cpp:406] Convolution47 <- Convolution46
I0926 08:53:08.641106  4406 net.cpp:380] Convolution47 -> Convolution47
I0926 08:53:08.642799  4406 net.cpp:122] Setting up Convolution47
I0926 08:53:08.642808  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.642812  4406 net.cpp:137] Memory required for data: 1090766000
I0926 08:53:08.642817  4406 layer_factory.hpp:77] Creating layer BatchNorm47
I0926 08:53:08.642820  4406 net.cpp:84] Creating Layer BatchNorm47
I0926 08:53:08.642823  4406 net.cpp:406] BatchNorm47 <- Convolution47
I0926 08:53:08.642827  4406 net.cpp:367] BatchNorm47 -> Convolution47 (in-place)
I0926 08:53:08.642974  4406 net.cpp:122] Setting up BatchNorm47
I0926 08:53:08.642979  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.642982  4406 net.cpp:137] Memory required for data: 1092404400
I0926 08:53:08.642987  4406 layer_factory.hpp:77] Creating layer Scale47
I0926 08:53:08.642990  4406 net.cpp:84] Creating Layer Scale47
I0926 08:53:08.642993  4406 net.cpp:406] Scale47 <- Convolution47
I0926 08:53:08.642997  4406 net.cpp:367] Scale47 -> Convolution47 (in-place)
I0926 08:53:08.643024  4406 layer_factory.hpp:77] Creating layer Scale47
I0926 08:53:08.643103  4406 net.cpp:122] Setting up Scale47
I0926 08:53:08.643107  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.643110  4406 net.cpp:137] Memory required for data: 1094042800
I0926 08:53:08.643113  4406 layer_factory.hpp:77] Creating layer Eltwise22
I0926 08:53:08.643117  4406 net.cpp:84] Creating Layer Eltwise22
I0926 08:53:08.643121  4406 net.cpp:406] Eltwise22 <- Eltwise21_penlu43_0_split_1
I0926 08:53:08.643123  4406 net.cpp:406] Eltwise22 <- Convolution47
I0926 08:53:08.643126  4406 net.cpp:380] Eltwise22 -> Eltwise22
I0926 08:53:08.643143  4406 net.cpp:122] Setting up Eltwise22
I0926 08:53:08.643147  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.643149  4406 net.cpp:137] Memory required for data: 1095681200
I0926 08:53:08.643151  4406 layer_factory.hpp:77] Creating layer penlu45
I0926 08:53:08.643157  4406 net.cpp:84] Creating Layer penlu45
I0926 08:53:08.643158  4406 net.cpp:406] penlu45 <- Eltwise22
I0926 08:53:08.643162  4406 net.cpp:367] penlu45 -> Eltwise22 (in-place)
I0926 08:53:08.643277  4406 net.cpp:122] Setting up penlu45
I0926 08:53:08.643282  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.643285  4406 net.cpp:137] Memory required for data: 1097319600
I0926 08:53:08.643288  4406 layer_factory.hpp:77] Creating layer Eltwise22_penlu45_0_split
I0926 08:53:08.643292  4406 net.cpp:84] Creating Layer Eltwise22_penlu45_0_split
I0926 08:53:08.643295  4406 net.cpp:406] Eltwise22_penlu45_0_split <- Eltwise22
I0926 08:53:08.643297  4406 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_0
I0926 08:53:08.643302  4406 net.cpp:380] Eltwise22_penlu45_0_split -> Eltwise22_penlu45_0_split_1
I0926 08:53:08.643326  4406 net.cpp:122] Setting up Eltwise22_penlu45_0_split
I0926 08:53:08.643329  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.643332  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.643334  4406 net.cpp:137] Memory required for data: 1100596400
I0926 08:53:08.643337  4406 layer_factory.hpp:77] Creating layer Convolution48
I0926 08:53:08.643342  4406 net.cpp:84] Creating Layer Convolution48
I0926 08:53:08.643344  4406 net.cpp:406] Convolution48 <- Eltwise22_penlu45_0_split_0
I0926 08:53:08.643348  4406 net.cpp:380] Convolution48 -> Convolution48
I0926 08:53:08.645360  4406 net.cpp:122] Setting up Convolution48
I0926 08:53:08.645368  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.645371  4406 net.cpp:137] Memory required for data: 1102234800
I0926 08:53:08.645375  4406 layer_factory.hpp:77] Creating layer BatchNorm48
I0926 08:53:08.645381  4406 net.cpp:84] Creating Layer BatchNorm48
I0926 08:53:08.645385  4406 net.cpp:406] BatchNorm48 <- Convolution48
I0926 08:53:08.645387  4406 net.cpp:367] BatchNorm48 -> Convolution48 (in-place)
I0926 08:53:08.645530  4406 net.cpp:122] Setting up BatchNorm48
I0926 08:53:08.645534  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.645537  4406 net.cpp:137] Memory required for data: 1103873200
I0926 08:53:08.645541  4406 layer_factory.hpp:77] Creating layer Scale48
I0926 08:53:08.645546  4406 net.cpp:84] Creating Layer Scale48
I0926 08:53:08.645550  4406 net.cpp:406] Scale48 <- Convolution48
I0926 08:53:08.645552  4406 net.cpp:367] Scale48 -> Convolution48 (in-place)
I0926 08:53:08.645581  4406 layer_factory.hpp:77] Creating layer Scale48
I0926 08:53:08.645661  4406 net.cpp:122] Setting up Scale48
I0926 08:53:08.645666  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.645674  4406 net.cpp:137] Memory required for data: 1105511600
I0926 08:53:08.645679  4406 layer_factory.hpp:77] Creating layer penlu46
I0926 08:53:08.645685  4406 net.cpp:84] Creating Layer penlu46
I0926 08:53:08.645687  4406 net.cpp:406] penlu46 <- Convolution48
I0926 08:53:08.645691  4406 net.cpp:367] penlu46 -> Convolution48 (in-place)
I0926 08:53:08.645807  4406 net.cpp:122] Setting up penlu46
I0926 08:53:08.645812  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.645814  4406 net.cpp:137] Memory required for data: 1107150000
I0926 08:53:08.645818  4406 layer_factory.hpp:77] Creating layer Convolution49
I0926 08:53:08.645825  4406 net.cpp:84] Creating Layer Convolution49
I0926 08:53:08.645828  4406 net.cpp:406] Convolution49 <- Convolution48
I0926 08:53:08.645831  4406 net.cpp:380] Convolution49 -> Convolution49
I0926 08:53:08.647850  4406 net.cpp:122] Setting up Convolution49
I0926 08:53:08.647857  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.647861  4406 net.cpp:137] Memory required for data: 1108788400
I0926 08:53:08.647866  4406 layer_factory.hpp:77] Creating layer BatchNorm49
I0926 08:53:08.647871  4406 net.cpp:84] Creating Layer BatchNorm49
I0926 08:53:08.647873  4406 net.cpp:406] BatchNorm49 <- Convolution49
I0926 08:53:08.647876  4406 net.cpp:367] BatchNorm49 -> Convolution49 (in-place)
I0926 08:53:08.648016  4406 net.cpp:122] Setting up BatchNorm49
I0926 08:53:08.648020  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.648022  4406 net.cpp:137] Memory required for data: 1110426800
I0926 08:53:08.648027  4406 layer_factory.hpp:77] Creating layer Scale49
I0926 08:53:08.648031  4406 net.cpp:84] Creating Layer Scale49
I0926 08:53:08.648035  4406 net.cpp:406] Scale49 <- Convolution49
I0926 08:53:08.648037  4406 net.cpp:367] Scale49 -> Convolution49 (in-place)
I0926 08:53:08.648066  4406 layer_factory.hpp:77] Creating layer Scale49
I0926 08:53:08.648145  4406 net.cpp:122] Setting up Scale49
I0926 08:53:08.648149  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.648151  4406 net.cpp:137] Memory required for data: 1112065200
I0926 08:53:08.648155  4406 layer_factory.hpp:77] Creating layer Eltwise23
I0926 08:53:08.648160  4406 net.cpp:84] Creating Layer Eltwise23
I0926 08:53:08.648164  4406 net.cpp:406] Eltwise23 <- Eltwise22_penlu45_0_split_1
I0926 08:53:08.648166  4406 net.cpp:406] Eltwise23 <- Convolution49
I0926 08:53:08.648169  4406 net.cpp:380] Eltwise23 -> Eltwise23
I0926 08:53:08.648186  4406 net.cpp:122] Setting up Eltwise23
I0926 08:53:08.648190  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.648192  4406 net.cpp:137] Memory required for data: 1113703600
I0926 08:53:08.648195  4406 layer_factory.hpp:77] Creating layer penlu47
I0926 08:53:08.648200  4406 net.cpp:84] Creating Layer penlu47
I0926 08:53:08.648202  4406 net.cpp:406] penlu47 <- Eltwise23
I0926 08:53:08.648205  4406 net.cpp:367] penlu47 -> Eltwise23 (in-place)
I0926 08:53:08.648322  4406 net.cpp:122] Setting up penlu47
I0926 08:53:08.648327  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.648329  4406 net.cpp:137] Memory required for data: 1115342000
I0926 08:53:08.648334  4406 layer_factory.hpp:77] Creating layer Eltwise23_penlu47_0_split
I0926 08:53:08.648337  4406 net.cpp:84] Creating Layer Eltwise23_penlu47_0_split
I0926 08:53:08.648340  4406 net.cpp:406] Eltwise23_penlu47_0_split <- Eltwise23
I0926 08:53:08.648344  4406 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_0
I0926 08:53:08.648347  4406 net.cpp:380] Eltwise23_penlu47_0_split -> Eltwise23_penlu47_0_split_1
I0926 08:53:08.648371  4406 net.cpp:122] Setting up Eltwise23_penlu47_0_split
I0926 08:53:08.648375  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.648378  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.648380  4406 net.cpp:137] Memory required for data: 1118618800
I0926 08:53:08.648382  4406 layer_factory.hpp:77] Creating layer Convolution50
I0926 08:53:08.648387  4406 net.cpp:84] Creating Layer Convolution50
I0926 08:53:08.648396  4406 net.cpp:406] Convolution50 <- Eltwise23_penlu47_0_split_0
I0926 08:53:08.648402  4406 net.cpp:380] Convolution50 -> Convolution50
I0926 08:53:08.650934  4406 net.cpp:122] Setting up Convolution50
I0926 08:53:08.650943  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.650946  4406 net.cpp:137] Memory required for data: 1120257200
I0926 08:53:08.650951  4406 layer_factory.hpp:77] Creating layer BatchNorm50
I0926 08:53:08.650956  4406 net.cpp:84] Creating Layer BatchNorm50
I0926 08:53:08.650959  4406 net.cpp:406] BatchNorm50 <- Convolution50
I0926 08:53:08.650962  4406 net.cpp:367] BatchNorm50 -> Convolution50 (in-place)
I0926 08:53:08.651108  4406 net.cpp:122] Setting up BatchNorm50
I0926 08:53:08.651113  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.651115  4406 net.cpp:137] Memory required for data: 1121895600
I0926 08:53:08.651119  4406 layer_factory.hpp:77] Creating layer Scale50
I0926 08:53:08.651124  4406 net.cpp:84] Creating Layer Scale50
I0926 08:53:08.651126  4406 net.cpp:406] Scale50 <- Convolution50
I0926 08:53:08.651129  4406 net.cpp:367] Scale50 -> Convolution50 (in-place)
I0926 08:53:08.651160  4406 layer_factory.hpp:77] Creating layer Scale50
I0926 08:53:08.651239  4406 net.cpp:122] Setting up Scale50
I0926 08:53:08.651244  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.651247  4406 net.cpp:137] Memory required for data: 1123534000
I0926 08:53:08.651250  4406 layer_factory.hpp:77] Creating layer penlu48
I0926 08:53:08.651255  4406 net.cpp:84] Creating Layer penlu48
I0926 08:53:08.651258  4406 net.cpp:406] penlu48 <- Convolution50
I0926 08:53:08.651262  4406 net.cpp:367] penlu48 -> Convolution50 (in-place)
I0926 08:53:08.651378  4406 net.cpp:122] Setting up penlu48
I0926 08:53:08.651382  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.651384  4406 net.cpp:137] Memory required for data: 1125172400
I0926 08:53:08.651389  4406 layer_factory.hpp:77] Creating layer Convolution51
I0926 08:53:08.651396  4406 net.cpp:84] Creating Layer Convolution51
I0926 08:53:08.651398  4406 net.cpp:406] Convolution51 <- Convolution50
I0926 08:53:08.651404  4406 net.cpp:380] Convolution51 -> Convolution51
I0926 08:53:08.653103  4406 net.cpp:122] Setting up Convolution51
I0926 08:53:08.653112  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.653115  4406 net.cpp:137] Memory required for data: 1126810800
I0926 08:53:08.653120  4406 layer_factory.hpp:77] Creating layer BatchNorm51
I0926 08:53:08.653126  4406 net.cpp:84] Creating Layer BatchNorm51
I0926 08:53:08.653127  4406 net.cpp:406] BatchNorm51 <- Convolution51
I0926 08:53:08.653131  4406 net.cpp:367] BatchNorm51 -> Convolution51 (in-place)
I0926 08:53:08.653268  4406 net.cpp:122] Setting up BatchNorm51
I0926 08:53:08.653273  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.653275  4406 net.cpp:137] Memory required for data: 1128449200
I0926 08:53:08.653280  4406 layer_factory.hpp:77] Creating layer Scale51
I0926 08:53:08.653285  4406 net.cpp:84] Creating Layer Scale51
I0926 08:53:08.653286  4406 net.cpp:406] Scale51 <- Convolution51
I0926 08:53:08.653290  4406 net.cpp:367] Scale51 -> Convolution51 (in-place)
I0926 08:53:08.653318  4406 layer_factory.hpp:77] Creating layer Scale51
I0926 08:53:08.653398  4406 net.cpp:122] Setting up Scale51
I0926 08:53:08.653401  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.653403  4406 net.cpp:137] Memory required for data: 1130087600
I0926 08:53:08.653408  4406 layer_factory.hpp:77] Creating layer Eltwise24
I0926 08:53:08.653412  4406 net.cpp:84] Creating Layer Eltwise24
I0926 08:53:08.653415  4406 net.cpp:406] Eltwise24 <- Eltwise23_penlu47_0_split_1
I0926 08:53:08.653419  4406 net.cpp:406] Eltwise24 <- Convolution51
I0926 08:53:08.653421  4406 net.cpp:380] Eltwise24 -> Eltwise24
I0926 08:53:08.653437  4406 net.cpp:122] Setting up Eltwise24
I0926 08:53:08.653441  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.653443  4406 net.cpp:137] Memory required for data: 1131726000
I0926 08:53:08.653452  4406 layer_factory.hpp:77] Creating layer penlu49
I0926 08:53:08.653458  4406 net.cpp:84] Creating Layer penlu49
I0926 08:53:08.653460  4406 net.cpp:406] penlu49 <- Eltwise24
I0926 08:53:08.653465  4406 net.cpp:367] penlu49 -> Eltwise24 (in-place)
I0926 08:53:08.653579  4406 net.cpp:122] Setting up penlu49
I0926 08:53:08.653584  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.653586  4406 net.cpp:137] Memory required for data: 1133364400
I0926 08:53:08.653591  4406 layer_factory.hpp:77] Creating layer Eltwise24_penlu49_0_split
I0926 08:53:08.653595  4406 net.cpp:84] Creating Layer Eltwise24_penlu49_0_split
I0926 08:53:08.653599  4406 net.cpp:406] Eltwise24_penlu49_0_split <- Eltwise24
I0926 08:53:08.653601  4406 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_0
I0926 08:53:08.653605  4406 net.cpp:380] Eltwise24_penlu49_0_split -> Eltwise24_penlu49_0_split_1
I0926 08:53:08.653640  4406 net.cpp:122] Setting up Eltwise24_penlu49_0_split
I0926 08:53:08.666849  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.666860  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.666863  4406 net.cpp:137] Memory required for data: 1136641200
I0926 08:53:08.666867  4406 layer_factory.hpp:77] Creating layer Convolution52
I0926 08:53:08.666877  4406 net.cpp:84] Creating Layer Convolution52
I0926 08:53:08.666880  4406 net.cpp:406] Convolution52 <- Eltwise24_penlu49_0_split_0
I0926 08:53:08.666885  4406 net.cpp:380] Convolution52 -> Convolution52
I0926 08:53:08.669402  4406 net.cpp:122] Setting up Convolution52
I0926 08:53:08.669411  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.669414  4406 net.cpp:137] Memory required for data: 1138279600
I0926 08:53:08.669420  4406 layer_factory.hpp:77] Creating layer BatchNorm52
I0926 08:53:08.669425  4406 net.cpp:84] Creating Layer BatchNorm52
I0926 08:53:08.669427  4406 net.cpp:406] BatchNorm52 <- Convolution52
I0926 08:53:08.669431  4406 net.cpp:367] BatchNorm52 -> Convolution52 (in-place)
I0926 08:53:08.669577  4406 net.cpp:122] Setting up BatchNorm52
I0926 08:53:08.669582  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.669584  4406 net.cpp:137] Memory required for data: 1139918000
I0926 08:53:08.669589  4406 layer_factory.hpp:77] Creating layer Scale52
I0926 08:53:08.669596  4406 net.cpp:84] Creating Layer Scale52
I0926 08:53:08.669598  4406 net.cpp:406] Scale52 <- Convolution52
I0926 08:53:08.669602  4406 net.cpp:367] Scale52 -> Convolution52 (in-place)
I0926 08:53:08.669632  4406 layer_factory.hpp:77] Creating layer Scale52
I0926 08:53:08.669716  4406 net.cpp:122] Setting up Scale52
I0926 08:53:08.669720  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.669723  4406 net.cpp:137] Memory required for data: 1141556400
I0926 08:53:08.669726  4406 layer_factory.hpp:77] Creating layer penlu50
I0926 08:53:08.669746  4406 net.cpp:84] Creating Layer penlu50
I0926 08:53:08.669749  4406 net.cpp:406] penlu50 <- Convolution52
I0926 08:53:08.669754  4406 net.cpp:367] penlu50 -> Convolution52 (in-place)
I0926 08:53:08.669875  4406 net.cpp:122] Setting up penlu50
I0926 08:53:08.669880  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.669883  4406 net.cpp:137] Memory required for data: 1143194800
I0926 08:53:08.669924  4406 layer_factory.hpp:77] Creating layer Convolution53
I0926 08:53:08.669930  4406 net.cpp:84] Creating Layer Convolution53
I0926 08:53:08.669934  4406 net.cpp:406] Convolution53 <- Convolution52
I0926 08:53:08.669937  4406 net.cpp:380] Convolution53 -> Convolution53
I0926 08:53:08.671763  4406 net.cpp:122] Setting up Convolution53
I0926 08:53:08.671772  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.671774  4406 net.cpp:137] Memory required for data: 1144833200
I0926 08:53:08.671779  4406 layer_factory.hpp:77] Creating layer BatchNorm53
I0926 08:53:08.671784  4406 net.cpp:84] Creating Layer BatchNorm53
I0926 08:53:08.671787  4406 net.cpp:406] BatchNorm53 <- Convolution53
I0926 08:53:08.671792  4406 net.cpp:367] BatchNorm53 -> Convolution53 (in-place)
I0926 08:53:08.671941  4406 net.cpp:122] Setting up BatchNorm53
I0926 08:53:08.671946  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.671947  4406 net.cpp:137] Memory required for data: 1146471600
I0926 08:53:08.671952  4406 layer_factory.hpp:77] Creating layer Scale53
I0926 08:53:08.671957  4406 net.cpp:84] Creating Layer Scale53
I0926 08:53:08.671960  4406 net.cpp:406] Scale53 <- Convolution53
I0926 08:53:08.671963  4406 net.cpp:367] Scale53 -> Convolution53 (in-place)
I0926 08:53:08.671991  4406 layer_factory.hpp:77] Creating layer Scale53
I0926 08:53:08.672075  4406 net.cpp:122] Setting up Scale53
I0926 08:53:08.672080  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.672081  4406 net.cpp:137] Memory required for data: 1148110000
I0926 08:53:08.672086  4406 layer_factory.hpp:77] Creating layer Eltwise25
I0926 08:53:08.672089  4406 net.cpp:84] Creating Layer Eltwise25
I0926 08:53:08.672092  4406 net.cpp:406] Eltwise25 <- Eltwise24_penlu49_0_split_1
I0926 08:53:08.672096  4406 net.cpp:406] Eltwise25 <- Convolution53
I0926 08:53:08.672099  4406 net.cpp:380] Eltwise25 -> Eltwise25
I0926 08:53:08.672116  4406 net.cpp:122] Setting up Eltwise25
I0926 08:53:08.672119  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.672122  4406 net.cpp:137] Memory required for data: 1149748400
I0926 08:53:08.672123  4406 layer_factory.hpp:77] Creating layer penlu51
I0926 08:53:08.672129  4406 net.cpp:84] Creating Layer penlu51
I0926 08:53:08.672132  4406 net.cpp:406] penlu51 <- Eltwise25
I0926 08:53:08.672135  4406 net.cpp:367] penlu51 -> Eltwise25 (in-place)
I0926 08:53:08.672252  4406 net.cpp:122] Setting up penlu51
I0926 08:53:08.672256  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.672258  4406 net.cpp:137] Memory required for data: 1151386800
I0926 08:53:08.672263  4406 layer_factory.hpp:77] Creating layer Eltwise25_penlu51_0_split
I0926 08:53:08.672267  4406 net.cpp:84] Creating Layer Eltwise25_penlu51_0_split
I0926 08:53:08.672269  4406 net.cpp:406] Eltwise25_penlu51_0_split <- Eltwise25
I0926 08:53:08.672272  4406 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_0
I0926 08:53:08.672277  4406 net.cpp:380] Eltwise25_penlu51_0_split -> Eltwise25_penlu51_0_split_1
I0926 08:53:08.672300  4406 net.cpp:122] Setting up Eltwise25_penlu51_0_split
I0926 08:53:08.672304  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.672307  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.672309  4406 net.cpp:137] Memory required for data: 1154663600
I0926 08:53:08.672312  4406 layer_factory.hpp:77] Creating layer Convolution54
I0926 08:53:08.672318  4406 net.cpp:84] Creating Layer Convolution54
I0926 08:53:08.672320  4406 net.cpp:406] Convolution54 <- Eltwise25_penlu51_0_split_0
I0926 08:53:08.672324  4406 net.cpp:380] Convolution54 -> Convolution54
I0926 08:53:08.674391  4406 net.cpp:122] Setting up Convolution54
I0926 08:53:08.674399  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.674402  4406 net.cpp:137] Memory required for data: 1156302000
I0926 08:53:08.674407  4406 layer_factory.hpp:77] Creating layer BatchNorm54
I0926 08:53:08.674412  4406 net.cpp:84] Creating Layer BatchNorm54
I0926 08:53:08.674414  4406 net.cpp:406] BatchNorm54 <- Convolution54
I0926 08:53:08.674418  4406 net.cpp:367] BatchNorm54 -> Convolution54 (in-place)
I0926 08:53:08.674576  4406 net.cpp:122] Setting up BatchNorm54
I0926 08:53:08.674581  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.674582  4406 net.cpp:137] Memory required for data: 1157940400
I0926 08:53:08.674587  4406 layer_factory.hpp:77] Creating layer Scale54
I0926 08:53:08.674592  4406 net.cpp:84] Creating Layer Scale54
I0926 08:53:08.674593  4406 net.cpp:406] Scale54 <- Convolution54
I0926 08:53:08.674597  4406 net.cpp:367] Scale54 -> Convolution54 (in-place)
I0926 08:53:08.674625  4406 layer_factory.hpp:77] Creating layer Scale54
I0926 08:53:08.674706  4406 net.cpp:122] Setting up Scale54
I0926 08:53:08.674711  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.674721  4406 net.cpp:137] Memory required for data: 1159578800
I0926 08:53:08.674724  4406 layer_factory.hpp:77] Creating layer penlu52
I0926 08:53:08.674731  4406 net.cpp:84] Creating Layer penlu52
I0926 08:53:08.674733  4406 net.cpp:406] penlu52 <- Convolution54
I0926 08:53:08.674737  4406 net.cpp:367] penlu52 -> Convolution54 (in-place)
I0926 08:53:08.674856  4406 net.cpp:122] Setting up penlu52
I0926 08:53:08.674861  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.674863  4406 net.cpp:137] Memory required for data: 1161217200
I0926 08:53:08.674868  4406 layer_factory.hpp:77] Creating layer Convolution55
I0926 08:53:08.674875  4406 net.cpp:84] Creating Layer Convolution55
I0926 08:53:08.674876  4406 net.cpp:406] Convolution55 <- Convolution54
I0926 08:53:08.674881  4406 net.cpp:380] Convolution55 -> Convolution55
I0926 08:53:08.676635  4406 net.cpp:122] Setting up Convolution55
I0926 08:53:08.676645  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.676647  4406 net.cpp:137] Memory required for data: 1162855600
I0926 08:53:08.676651  4406 layer_factory.hpp:77] Creating layer BatchNorm55
I0926 08:53:08.676656  4406 net.cpp:84] Creating Layer BatchNorm55
I0926 08:53:08.676659  4406 net.cpp:406] BatchNorm55 <- Convolution55
I0926 08:53:08.676664  4406 net.cpp:367] BatchNorm55 -> Convolution55 (in-place)
I0926 08:53:08.676810  4406 net.cpp:122] Setting up BatchNorm55
I0926 08:53:08.676815  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.676816  4406 net.cpp:137] Memory required for data: 1164494000
I0926 08:53:08.676821  4406 layer_factory.hpp:77] Creating layer Scale55
I0926 08:53:08.676826  4406 net.cpp:84] Creating Layer Scale55
I0926 08:53:08.676827  4406 net.cpp:406] Scale55 <- Convolution55
I0926 08:53:08.676831  4406 net.cpp:367] Scale55 -> Convolution55 (in-place)
I0926 08:53:08.676859  4406 layer_factory.hpp:77] Creating layer Scale55
I0926 08:53:08.676957  4406 net.cpp:122] Setting up Scale55
I0926 08:53:08.676964  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.676966  4406 net.cpp:137] Memory required for data: 1166132400
I0926 08:53:08.676970  4406 layer_factory.hpp:77] Creating layer Eltwise26
I0926 08:53:08.676975  4406 net.cpp:84] Creating Layer Eltwise26
I0926 08:53:08.676977  4406 net.cpp:406] Eltwise26 <- Eltwise25_penlu51_0_split_1
I0926 08:53:08.676980  4406 net.cpp:406] Eltwise26 <- Convolution55
I0926 08:53:08.676985  4406 net.cpp:380] Eltwise26 -> Eltwise26
I0926 08:53:08.677001  4406 net.cpp:122] Setting up Eltwise26
I0926 08:53:08.677006  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.677008  4406 net.cpp:137] Memory required for data: 1167770800
I0926 08:53:08.677011  4406 layer_factory.hpp:77] Creating layer penlu53
I0926 08:53:08.677016  4406 net.cpp:84] Creating Layer penlu53
I0926 08:53:08.677017  4406 net.cpp:406] penlu53 <- Eltwise26
I0926 08:53:08.677021  4406 net.cpp:367] penlu53 -> Eltwise26 (in-place)
I0926 08:53:08.677145  4406 net.cpp:122] Setting up penlu53
I0926 08:53:08.677150  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.677151  4406 net.cpp:137] Memory required for data: 1169409200
I0926 08:53:08.677155  4406 layer_factory.hpp:77] Creating layer Eltwise26_penlu53_0_split
I0926 08:53:08.677160  4406 net.cpp:84] Creating Layer Eltwise26_penlu53_0_split
I0926 08:53:08.677161  4406 net.cpp:406] Eltwise26_penlu53_0_split <- Eltwise26
I0926 08:53:08.677165  4406 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_0
I0926 08:53:08.677170  4406 net.cpp:380] Eltwise26_penlu53_0_split -> Eltwise26_penlu53_0_split_1
I0926 08:53:08.677193  4406 net.cpp:122] Setting up Eltwise26_penlu53_0_split
I0926 08:53:08.677197  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.677201  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.677202  4406 net.cpp:137] Memory required for data: 1172686000
I0926 08:53:08.677204  4406 layer_factory.hpp:77] Creating layer Convolution56
I0926 08:53:08.677209  4406 net.cpp:84] Creating Layer Convolution56
I0926 08:53:08.677219  4406 net.cpp:406] Convolution56 <- Eltwise26_penlu53_0_split_0
I0926 08:53:08.677224  4406 net.cpp:380] Convolution56 -> Convolution56
I0926 08:53:08.678922  4406 net.cpp:122] Setting up Convolution56
I0926 08:53:08.678931  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.678935  4406 net.cpp:137] Memory required for data: 1174324400
I0926 08:53:08.678938  4406 layer_factory.hpp:77] Creating layer BatchNorm56
I0926 08:53:08.678943  4406 net.cpp:84] Creating Layer BatchNorm56
I0926 08:53:08.678946  4406 net.cpp:406] BatchNorm56 <- Convolution56
I0926 08:53:08.678951  4406 net.cpp:367] BatchNorm56 -> Convolution56 (in-place)
I0926 08:53:08.679096  4406 net.cpp:122] Setting up BatchNorm56
I0926 08:53:08.679101  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.679103  4406 net.cpp:137] Memory required for data: 1175962800
I0926 08:53:08.679108  4406 layer_factory.hpp:77] Creating layer Scale56
I0926 08:53:08.679112  4406 net.cpp:84] Creating Layer Scale56
I0926 08:53:08.679114  4406 net.cpp:406] Scale56 <- Convolution56
I0926 08:53:08.679117  4406 net.cpp:367] Scale56 -> Convolution56 (in-place)
I0926 08:53:08.679147  4406 layer_factory.hpp:77] Creating layer Scale56
I0926 08:53:08.679229  4406 net.cpp:122] Setting up Scale56
I0926 08:53:08.679234  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.679235  4406 net.cpp:137] Memory required for data: 1177601200
I0926 08:53:08.679239  4406 layer_factory.hpp:77] Creating layer penlu54
I0926 08:53:08.679244  4406 net.cpp:84] Creating Layer penlu54
I0926 08:53:08.679247  4406 net.cpp:406] penlu54 <- Convolution56
I0926 08:53:08.679251  4406 net.cpp:367] penlu54 -> Convolution56 (in-place)
I0926 08:53:08.679373  4406 net.cpp:122] Setting up penlu54
I0926 08:53:08.679376  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.679378  4406 net.cpp:137] Memory required for data: 1179239600
I0926 08:53:08.679383  4406 layer_factory.hpp:77] Creating layer Convolution57
I0926 08:53:08.679389  4406 net.cpp:84] Creating Layer Convolution57
I0926 08:53:08.679391  4406 net.cpp:406] Convolution57 <- Convolution56
I0926 08:53:08.679396  4406 net.cpp:380] Convolution57 -> Convolution57
I0926 08:53:08.681112  4406 net.cpp:122] Setting up Convolution57
I0926 08:53:08.681120  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.681123  4406 net.cpp:137] Memory required for data: 1180878000
I0926 08:53:08.681128  4406 layer_factory.hpp:77] Creating layer BatchNorm57
I0926 08:53:08.681133  4406 net.cpp:84] Creating Layer BatchNorm57
I0926 08:53:08.681136  4406 net.cpp:406] BatchNorm57 <- Convolution57
I0926 08:53:08.681139  4406 net.cpp:367] BatchNorm57 -> Convolution57 (in-place)
I0926 08:53:08.681282  4406 net.cpp:122] Setting up BatchNorm57
I0926 08:53:08.681287  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.681288  4406 net.cpp:137] Memory required for data: 1182516400
I0926 08:53:08.681293  4406 layer_factory.hpp:77] Creating layer Scale57
I0926 08:53:08.681298  4406 net.cpp:84] Creating Layer Scale57
I0926 08:53:08.681299  4406 net.cpp:406] Scale57 <- Convolution57
I0926 08:53:08.681303  4406 net.cpp:367] Scale57 -> Convolution57 (in-place)
I0926 08:53:08.681332  4406 layer_factory.hpp:77] Creating layer Scale57
I0926 08:53:08.681414  4406 net.cpp:122] Setting up Scale57
I0926 08:53:08.681419  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.681421  4406 net.cpp:137] Memory required for data: 1184154800
I0926 08:53:08.681426  4406 layer_factory.hpp:77] Creating layer Eltwise27
I0926 08:53:08.681428  4406 net.cpp:84] Creating Layer Eltwise27
I0926 08:53:08.681432  4406 net.cpp:406] Eltwise27 <- Eltwise26_penlu53_0_split_1
I0926 08:53:08.681434  4406 net.cpp:406] Eltwise27 <- Convolution57
I0926 08:53:08.681437  4406 net.cpp:380] Eltwise27 -> Eltwise27
I0926 08:53:08.681455  4406 net.cpp:122] Setting up Eltwise27
I0926 08:53:08.681458  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.681460  4406 net.cpp:137] Memory required for data: 1185793200
I0926 08:53:08.681469  4406 layer_factory.hpp:77] Creating layer penlu55
I0926 08:53:08.681475  4406 net.cpp:84] Creating Layer penlu55
I0926 08:53:08.681478  4406 net.cpp:406] penlu55 <- Eltwise27
I0926 08:53:08.681481  4406 net.cpp:367] penlu55 -> Eltwise27 (in-place)
I0926 08:53:08.681602  4406 net.cpp:122] Setting up penlu55
I0926 08:53:08.681607  4406 net.cpp:129] Top shape: 100 64 8 8 (409600)
I0926 08:53:08.681609  4406 net.cpp:137] Memory required for data: 1187431600
I0926 08:53:08.681613  4406 layer_factory.hpp:77] Creating layer Pooling1
I0926 08:53:08.681620  4406 net.cpp:84] Creating Layer Pooling1
I0926 08:53:08.681622  4406 net.cpp:406] Pooling1 <- Eltwise27
I0926 08:53:08.681627  4406 net.cpp:380] Pooling1 -> Pooling1
I0926 08:53:08.682103  4406 net.cpp:122] Setting up Pooling1
I0926 08:53:08.682112  4406 net.cpp:129] Top shape: 100 64 1 1 (6400)
I0926 08:53:08.699308  4406 net.cpp:137] Memory required for data: 1187457200
I0926 08:53:08.699318  4406 layer_factory.hpp:77] Creating layer InnerProduct1
I0926 08:53:08.699326  4406 net.cpp:84] Creating Layer InnerProduct1
I0926 08:53:08.699328  4406 net.cpp:406] InnerProduct1 <- Pooling1
I0926 08:53:08.699335  4406 net.cpp:380] InnerProduct1 -> InnerProduct1
I0926 08:53:08.699466  4406 net.cpp:122] Setting up InnerProduct1
I0926 08:53:08.699472  4406 net.cpp:129] Top shape: 100 10 (1000)
I0926 08:53:08.699475  4406 net.cpp:137] Memory required for data: 1187461200
I0926 08:53:08.699478  4406 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0926 08:53:08.699482  4406 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I0926 08:53:08.699484  4406 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0926 08:53:08.699488  4406 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0926 08:53:08.699492  4406 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0926 08:53:08.699519  4406 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I0926 08:53:08.699523  4406 net.cpp:129] Top shape: 100 10 (1000)
I0926 08:53:08.699525  4406 net.cpp:129] Top shape: 100 10 (1000)
I0926 08:53:08.699527  4406 net.cpp:137] Memory required for data: 1187469200
I0926 08:53:08.699529  4406 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0926 08:53:08.699533  4406 net.cpp:84] Creating Layer SoftmaxWithLoss1
I0926 08:53:08.699537  4406 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0926 08:53:08.699539  4406 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0926 08:53:08.699543  4406 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0926 08:53:08.699548  4406 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0926 08:53:08.699751  4406 net.cpp:122] Setting up SoftmaxWithLoss1
I0926 08:53:08.699759  4406 net.cpp:129] Top shape: (1)
I0926 08:53:08.699760  4406 net.cpp:132]     with loss weight 1
I0926 08:53:08.699767  4406 net.cpp:137] Memory required for data: 1187469204
I0926 08:53:08.699770  4406 layer_factory.hpp:77] Creating layer Accuracy1
I0926 08:53:08.699779  4406 net.cpp:84] Creating Layer Accuracy1
I0926 08:53:08.699781  4406 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0926 08:53:08.699784  4406 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I0926 08:53:08.699789  4406 net.cpp:380] Accuracy1 -> Accuracy1
I0926 08:53:08.699795  4406 net.cpp:122] Setting up Accuracy1
I0926 08:53:08.699797  4406 net.cpp:129] Top shape: (1)
I0926 08:53:08.699800  4406 net.cpp:137] Memory required for data: 1187469208
I0926 08:53:08.699802  4406 net.cpp:200] Accuracy1 does not need backward computation.
I0926 08:53:08.699805  4406 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I0926 08:53:08.699806  4406 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0926 08:53:08.699810  4406 net.cpp:198] InnerProduct1 needs backward computation.
I0926 08:53:08.699811  4406 net.cpp:198] Pooling1 needs backward computation.
I0926 08:53:08.699813  4406 net.cpp:198] penlu55 needs backward computation.
I0926 08:53:08.699822  4406 net.cpp:198] Eltwise27 needs backward computation.
I0926 08:53:08.699826  4406 net.cpp:198] Scale57 needs backward computation.
I0926 08:53:08.699827  4406 net.cpp:198] BatchNorm57 needs backward computation.
I0926 08:53:08.699829  4406 net.cpp:198] Convolution57 needs backward computation.
I0926 08:53:08.699831  4406 net.cpp:198] penlu54 needs backward computation.
I0926 08:53:08.699833  4406 net.cpp:198] Scale56 needs backward computation.
I0926 08:53:08.699836  4406 net.cpp:198] BatchNorm56 needs backward computation.
I0926 08:53:08.699837  4406 net.cpp:198] Convolution56 needs backward computation.
I0926 08:53:08.699839  4406 net.cpp:198] Eltwise26_penlu53_0_split needs backward computation.
I0926 08:53:08.699842  4406 net.cpp:198] penlu53 needs backward computation.
I0926 08:53:08.699844  4406 net.cpp:198] Eltwise26 needs backward computation.
I0926 08:53:08.699846  4406 net.cpp:198] Scale55 needs backward computation.
I0926 08:53:08.699848  4406 net.cpp:198] BatchNorm55 needs backward computation.
I0926 08:53:08.699851  4406 net.cpp:198] Convolution55 needs backward computation.
I0926 08:53:08.699852  4406 net.cpp:198] penlu52 needs backward computation.
I0926 08:53:08.699854  4406 net.cpp:198] Scale54 needs backward computation.
I0926 08:53:08.699856  4406 net.cpp:198] BatchNorm54 needs backward computation.
I0926 08:53:08.699858  4406 net.cpp:198] Convolution54 needs backward computation.
I0926 08:53:08.699861  4406 net.cpp:198] Eltwise25_penlu51_0_split needs backward computation.
I0926 08:53:08.699862  4406 net.cpp:198] penlu51 needs backward computation.
I0926 08:53:08.699864  4406 net.cpp:198] Eltwise25 needs backward computation.
I0926 08:53:08.699867  4406 net.cpp:198] Scale53 needs backward computation.
I0926 08:53:08.699869  4406 net.cpp:198] BatchNorm53 needs backward computation.
I0926 08:53:08.699872  4406 net.cpp:198] Convolution53 needs backward computation.
I0926 08:53:08.699873  4406 net.cpp:198] penlu50 needs backward computation.
I0926 08:53:08.699875  4406 net.cpp:198] Scale52 needs backward computation.
I0926 08:53:08.699877  4406 net.cpp:198] BatchNorm52 needs backward computation.
I0926 08:53:08.699879  4406 net.cpp:198] Convolution52 needs backward computation.
I0926 08:53:08.699882  4406 net.cpp:198] Eltwise24_penlu49_0_split needs backward computation.
I0926 08:53:08.699884  4406 net.cpp:198] penlu49 needs backward computation.
I0926 08:53:08.699887  4406 net.cpp:198] Eltwise24 needs backward computation.
I0926 08:53:08.699889  4406 net.cpp:198] Scale51 needs backward computation.
I0926 08:53:08.699892  4406 net.cpp:198] BatchNorm51 needs backward computation.
I0926 08:53:08.699893  4406 net.cpp:198] Convolution51 needs backward computation.
I0926 08:53:08.699895  4406 net.cpp:198] penlu48 needs backward computation.
I0926 08:53:08.699898  4406 net.cpp:198] Scale50 needs backward computation.
I0926 08:53:08.699899  4406 net.cpp:198] BatchNorm50 needs backward computation.
I0926 08:53:08.699901  4406 net.cpp:198] Convolution50 needs backward computation.
I0926 08:53:08.699903  4406 net.cpp:198] Eltwise23_penlu47_0_split needs backward computation.
I0926 08:53:08.699906  4406 net.cpp:198] penlu47 needs backward computation.
I0926 08:53:08.699908  4406 net.cpp:198] Eltwise23 needs backward computation.
I0926 08:53:08.699911  4406 net.cpp:198] Scale49 needs backward computation.
I0926 08:53:08.699913  4406 net.cpp:198] BatchNorm49 needs backward computation.
I0926 08:53:08.699915  4406 net.cpp:198] Convolution49 needs backward computation.
I0926 08:53:08.699918  4406 net.cpp:198] penlu46 needs backward computation.
I0926 08:53:08.699919  4406 net.cpp:198] Scale48 needs backward computation.
I0926 08:53:08.699921  4406 net.cpp:198] BatchNorm48 needs backward computation.
I0926 08:53:08.699923  4406 net.cpp:198] Convolution48 needs backward computation.
I0926 08:53:08.699926  4406 net.cpp:198] Eltwise22_penlu45_0_split needs backward computation.
I0926 08:53:08.699928  4406 net.cpp:198] penlu45 needs backward computation.
I0926 08:53:08.699931  4406 net.cpp:198] Eltwise22 needs backward computation.
I0926 08:53:08.699936  4406 net.cpp:198] Scale47 needs backward computation.
I0926 08:53:08.699940  4406 net.cpp:198] BatchNorm47 needs backward computation.
I0926 08:53:08.699942  4406 net.cpp:198] Convolution47 needs backward computation.
I0926 08:53:08.699944  4406 net.cpp:198] penlu44 needs backward computation.
I0926 08:53:08.699946  4406 net.cpp:198] Scale46 needs backward computation.
I0926 08:53:08.699949  4406 net.cpp:198] BatchNorm46 needs backward computation.
I0926 08:53:08.699950  4406 net.cpp:198] Convolution46 needs backward computation.
I0926 08:53:08.699954  4406 net.cpp:198] Eltwise21_penlu43_0_split needs backward computation.
I0926 08:53:08.699955  4406 net.cpp:198] penlu43 needs backward computation.
I0926 08:53:08.699959  4406 net.cpp:198] Eltwise21 needs backward computation.
I0926 08:53:08.699960  4406 net.cpp:198] Scale45 needs backward computation.
I0926 08:53:08.699971  4406 net.cpp:198] BatchNorm45 needs backward computation.
I0926 08:53:08.699973  4406 net.cpp:198] Convolution45 needs backward computation.
I0926 08:53:08.699976  4406 net.cpp:198] penlu42 needs backward computation.
I0926 08:53:08.699978  4406 net.cpp:198] Scale44 needs backward computation.
I0926 08:53:08.699980  4406 net.cpp:198] BatchNorm44 needs backward computation.
I0926 08:53:08.699983  4406 net.cpp:198] Convolution44 needs backward computation.
I0926 08:53:08.699985  4406 net.cpp:198] Eltwise20_penlu41_0_split needs backward computation.
I0926 08:53:08.699988  4406 net.cpp:198] penlu41 needs backward computation.
I0926 08:53:08.699990  4406 net.cpp:198] Eltwise20 needs backward computation.
I0926 08:53:08.699993  4406 net.cpp:198] Scale43 needs backward computation.
I0926 08:53:08.699996  4406 net.cpp:198] BatchNorm43 needs backward computation.
I0926 08:53:08.699998  4406 net.cpp:198] Convolution43 needs backward computation.
I0926 08:53:08.700001  4406 net.cpp:198] penlu40 needs backward computation.
I0926 08:53:08.700003  4406 net.cpp:198] Scale42 needs backward computation.
I0926 08:53:08.700006  4406 net.cpp:198] BatchNorm42 needs backward computation.
I0926 08:53:08.700008  4406 net.cpp:198] Convolution42 needs backward computation.
I0926 08:53:08.700011  4406 net.cpp:198] Eltwise19_penlu39_0_split needs backward computation.
I0926 08:53:08.700013  4406 net.cpp:198] penlu39 needs backward computation.
I0926 08:53:08.700016  4406 net.cpp:198] Eltwise19 needs backward computation.
I0926 08:53:08.700019  4406 net.cpp:198] Scale41 needs backward computation.
I0926 08:53:08.700021  4406 net.cpp:198] BatchNorm41 needs backward computation.
I0926 08:53:08.700024  4406 net.cpp:198] Convolution41 needs backward computation.
I0926 08:53:08.700026  4406 net.cpp:198] penlu38 needs backward computation.
I0926 08:53:08.700029  4406 net.cpp:198] Scale40 needs backward computation.
I0926 08:53:08.700031  4406 net.cpp:198] BatchNorm40 needs backward computation.
I0926 08:53:08.700033  4406 net.cpp:198] Convolution40 needs backward computation.
I0926 08:53:08.700037  4406 net.cpp:198] Scale39 needs backward computation.
I0926 08:53:08.700038  4406 net.cpp:198] BatchNorm39 needs backward computation.
I0926 08:53:08.700042  4406 net.cpp:198] Convolution39 needs backward computation.
I0926 08:53:08.700043  4406 net.cpp:198] Eltwise18_penlu37_0_split needs backward computation.
I0926 08:53:08.700047  4406 net.cpp:198] penlu37 needs backward computation.
I0926 08:53:08.700049  4406 net.cpp:198] Eltwise18 needs backward computation.
I0926 08:53:08.700052  4406 net.cpp:198] Scale38 needs backward computation.
I0926 08:53:08.700054  4406 net.cpp:198] BatchNorm38 needs backward computation.
I0926 08:53:08.700057  4406 net.cpp:198] Convolution38 needs backward computation.
I0926 08:53:08.700059  4406 net.cpp:198] penlu36 needs backward computation.
I0926 08:53:08.700062  4406 net.cpp:198] Scale37 needs backward computation.
I0926 08:53:08.700064  4406 net.cpp:198] BatchNorm37 needs backward computation.
I0926 08:53:08.700067  4406 net.cpp:198] Convolution37 needs backward computation.
I0926 08:53:08.700073  4406 net.cpp:198] Eltwise17_penlu35_0_split needs backward computation.
I0926 08:53:08.700075  4406 net.cpp:198] penlu35 needs backward computation.
I0926 08:53:08.700078  4406 net.cpp:198] Eltwise17 needs backward computation.
I0926 08:53:08.700080  4406 net.cpp:198] Scale36 needs backward computation.
I0926 08:53:08.700083  4406 net.cpp:198] BatchNorm36 needs backward computation.
I0926 08:53:08.700085  4406 net.cpp:198] Convolution36 needs backward computation.
I0926 08:53:08.700088  4406 net.cpp:198] penlu34 needs backward computation.
I0926 08:53:08.700090  4406 net.cpp:198] Scale35 needs backward computation.
I0926 08:53:08.700093  4406 net.cpp:198] BatchNorm35 needs backward computation.
I0926 08:53:08.700095  4406 net.cpp:198] Convolution35 needs backward computation.
I0926 08:53:08.700098  4406 net.cpp:198] Eltwise16_penlu33_0_split needs backward computation.
I0926 08:53:08.700100  4406 net.cpp:198] penlu33 needs backward computation.
I0926 08:53:08.700103  4406 net.cpp:198] Eltwise16 needs backward computation.
I0926 08:53:08.700105  4406 net.cpp:198] Scale34 needs backward computation.
I0926 08:53:08.700109  4406 net.cpp:198] BatchNorm34 needs backward computation.
I0926 08:53:08.700110  4406 net.cpp:198] Convolution34 needs backward computation.
I0926 08:53:08.700114  4406 net.cpp:198] penlu32 needs backward computation.
I0926 08:53:08.700115  4406 net.cpp:198] Scale33 needs backward computation.
I0926 08:53:08.700119  4406 net.cpp:198] BatchNorm33 needs backward computation.
I0926 08:53:08.700120  4406 net.cpp:198] Convolution33 needs backward computation.
I0926 08:53:08.700124  4406 net.cpp:198] Eltwise15_penlu31_0_split needs backward computation.
I0926 08:53:08.700125  4406 net.cpp:198] penlu31 needs backward computation.
I0926 08:53:08.700127  4406 net.cpp:198] Eltwise15 needs backward computation.
I0926 08:53:08.700130  4406 net.cpp:198] Scale32 needs backward computation.
I0926 08:53:08.700132  4406 net.cpp:198] BatchNorm32 needs backward computation.
I0926 08:53:08.700135  4406 net.cpp:198] Convolution32 needs backward computation.
I0926 08:53:08.700139  4406 net.cpp:198] penlu30 needs backward computation.
I0926 08:53:08.700140  4406 net.cpp:198] Scale31 needs backward computation.
I0926 08:53:08.700142  4406 net.cpp:198] BatchNorm31 needs backward computation.
I0926 08:53:08.700145  4406 net.cpp:198] Convolution31 needs backward computation.
I0926 08:53:08.700147  4406 net.cpp:198] Eltwise14_penlu29_0_split needs backward computation.
I0926 08:53:08.700150  4406 net.cpp:198] penlu29 needs backward computation.
I0926 08:53:08.700152  4406 net.cpp:198] Eltwise14 needs backward computation.
I0926 08:53:08.700155  4406 net.cpp:198] Scale30 needs backward computation.
I0926 08:53:08.700157  4406 net.cpp:198] BatchNorm30 needs backward computation.
I0926 08:53:08.700160  4406 net.cpp:198] Convolution30 needs backward computation.
I0926 08:53:08.700163  4406 net.cpp:198] penlu28 needs backward computation.
I0926 08:53:08.700165  4406 net.cpp:198] Scale29 needs backward computation.
I0926 08:53:08.700167  4406 net.cpp:198] BatchNorm29 needs backward computation.
I0926 08:53:08.700170  4406 net.cpp:198] Convolution29 needs backward computation.
I0926 08:53:08.700173  4406 net.cpp:198] Eltwise13_penlu27_0_split needs backward computation.
I0926 08:53:08.700176  4406 net.cpp:198] penlu27 needs backward computation.
I0926 08:53:08.700178  4406 net.cpp:198] Eltwise13 needs backward computation.
I0926 08:53:08.700181  4406 net.cpp:198] Scale28 needs backward computation.
I0926 08:53:08.700183  4406 net.cpp:198] BatchNorm28 needs backward computation.
I0926 08:53:08.700186  4406 net.cpp:198] Convolution28 needs backward computation.
I0926 08:53:08.700188  4406 net.cpp:198] penlu26 needs backward computation.
I0926 08:53:08.700191  4406 net.cpp:198] Scale27 needs backward computation.
I0926 08:53:08.700193  4406 net.cpp:198] BatchNorm27 needs backward computation.
I0926 08:53:08.700196  4406 net.cpp:198] Convolution27 needs backward computation.
I0926 08:53:08.700198  4406 net.cpp:198] Eltwise12_penlu25_0_split needs backward computation.
I0926 08:53:08.700203  4406 net.cpp:198] penlu25 needs backward computation.
I0926 08:53:08.700206  4406 net.cpp:198] Eltwise12 needs backward computation.
I0926 08:53:08.700211  4406 net.cpp:198] Scale26 needs backward computation.
I0926 08:53:08.700212  4406 net.cpp:198] BatchNorm26 needs backward computation.
I0926 08:53:08.700215  4406 net.cpp:198] Convolution26 needs backward computation.
I0926 08:53:08.700217  4406 net.cpp:198] penlu24 needs backward computation.
I0926 08:53:08.700220  4406 net.cpp:198] Scale25 needs backward computation.
I0926 08:53:08.700222  4406 net.cpp:198] BatchNorm25 needs backward computation.
I0926 08:53:08.727506  4406 net.cpp:198] Convolution25 needs backward computation.
I0926 08:53:08.727517  4406 net.cpp:198] Eltwise11_penlu23_0_split needs backward computation.
I0926 08:53:08.727520  4406 net.cpp:198] penlu23 needs backward computation.
I0926 08:53:08.727524  4406 net.cpp:198] Eltwise11 needs backward computation.
I0926 08:53:08.727527  4406 net.cpp:198] Scale24 needs backward computation.
I0926 08:53:08.727530  4406 net.cpp:198] BatchNorm24 needs backward computation.
I0926 08:53:08.727532  4406 net.cpp:198] Convolution24 needs backward computation.
I0926 08:53:08.727535  4406 net.cpp:198] penlu22 needs backward computation.
I0926 08:53:08.727537  4406 net.cpp:198] Scale23 needs backward computation.
I0926 08:53:08.727540  4406 net.cpp:198] BatchNorm23 needs backward computation.
I0926 08:53:08.727542  4406 net.cpp:198] Convolution23 needs backward computation.
I0926 08:53:08.727545  4406 net.cpp:198] Eltwise10_penlu21_0_split needs backward computation.
I0926 08:53:08.727548  4406 net.cpp:198] penlu21 needs backward computation.
I0926 08:53:08.727550  4406 net.cpp:198] Eltwise10 needs backward computation.
I0926 08:53:08.727553  4406 net.cpp:198] Scale22 needs backward computation.
I0926 08:53:08.727556  4406 net.cpp:198] BatchNorm22 needs backward computation.
I0926 08:53:08.727558  4406 net.cpp:198] Convolution22 needs backward computation.
I0926 08:53:08.727561  4406 net.cpp:198] penlu20 needs backward computation.
I0926 08:53:08.727565  4406 net.cpp:198] Scale21 needs backward computation.
I0926 08:53:08.727566  4406 net.cpp:198] BatchNorm21 needs backward computation.
I0926 08:53:08.727568  4406 net.cpp:198] Convolution21 needs backward computation.
I0926 08:53:08.727571  4406 net.cpp:198] Scale20 needs backward computation.
I0926 08:53:08.727574  4406 net.cpp:198] BatchNorm20 needs backward computation.
I0926 08:53:08.727576  4406 net.cpp:198] Convolution20 needs backward computation.
I0926 08:53:08.727579  4406 net.cpp:198] Eltwise9_penlu19_0_split needs backward computation.
I0926 08:53:08.727582  4406 net.cpp:198] penlu19 needs backward computation.
I0926 08:53:08.727584  4406 net.cpp:198] Eltwise9 needs backward computation.
I0926 08:53:08.727587  4406 net.cpp:198] Scale19 needs backward computation.
I0926 08:53:08.727591  4406 net.cpp:198] BatchNorm19 needs backward computation.
I0926 08:53:08.727593  4406 net.cpp:198] Convolution19 needs backward computation.
I0926 08:53:08.727596  4406 net.cpp:198] penlu18 needs backward computation.
I0926 08:53:08.727598  4406 net.cpp:198] Scale18 needs backward computation.
I0926 08:53:08.727600  4406 net.cpp:198] BatchNorm18 needs backward computation.
I0926 08:53:08.727603  4406 net.cpp:198] Convolution18 needs backward computation.
I0926 08:53:08.727605  4406 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I0926 08:53:08.727608  4406 net.cpp:198] penlu17 needs backward computation.
I0926 08:53:08.727612  4406 net.cpp:198] Eltwise8 needs backward computation.
I0926 08:53:08.727613  4406 net.cpp:198] Scale17 needs backward computation.
I0926 08:53:08.727617  4406 net.cpp:198] BatchNorm17 needs backward computation.
I0926 08:53:08.727619  4406 net.cpp:198] Convolution17 needs backward computation.
I0926 08:53:08.727622  4406 net.cpp:198] penlu16 needs backward computation.
I0926 08:53:08.727624  4406 net.cpp:198] Scale16 needs backward computation.
I0926 08:53:08.727627  4406 net.cpp:198] BatchNorm16 needs backward computation.
I0926 08:53:08.727636  4406 net.cpp:198] Convolution16 needs backward computation.
I0926 08:53:08.727639  4406 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I0926 08:53:08.727643  4406 net.cpp:198] penlu15 needs backward computation.
I0926 08:53:08.727645  4406 net.cpp:198] Eltwise7 needs backward computation.
I0926 08:53:08.727648  4406 net.cpp:198] Scale15 needs backward computation.
I0926 08:53:08.727650  4406 net.cpp:198] BatchNorm15 needs backward computation.
I0926 08:53:08.727653  4406 net.cpp:198] Convolution15 needs backward computation.
I0926 08:53:08.727656  4406 net.cpp:198] penlu14 needs backward computation.
I0926 08:53:08.727659  4406 net.cpp:198] Scale14 needs backward computation.
I0926 08:53:08.727661  4406 net.cpp:198] BatchNorm14 needs backward computation.
I0926 08:53:08.727663  4406 net.cpp:198] Convolution14 needs backward computation.
I0926 08:53:08.727666  4406 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I0926 08:53:08.727669  4406 net.cpp:198] penlu13 needs backward computation.
I0926 08:53:08.727671  4406 net.cpp:198] Eltwise6 needs backward computation.
I0926 08:53:08.727675  4406 net.cpp:198] Scale13 needs backward computation.
I0926 08:53:08.727677  4406 net.cpp:198] BatchNorm13 needs backward computation.
I0926 08:53:08.727680  4406 net.cpp:198] Convolution13 needs backward computation.
I0926 08:53:08.727682  4406 net.cpp:198] penlu12 needs backward computation.
I0926 08:53:08.727684  4406 net.cpp:198] Scale12 needs backward computation.
I0926 08:53:08.727687  4406 net.cpp:198] BatchNorm12 needs backward computation.
I0926 08:53:08.727689  4406 net.cpp:198] Convolution12 needs backward computation.
I0926 08:53:08.727692  4406 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I0926 08:53:08.727696  4406 net.cpp:198] penlu11 needs backward computation.
I0926 08:53:08.727699  4406 net.cpp:198] Eltwise5 needs backward computation.
I0926 08:53:08.727702  4406 net.cpp:198] Scale11 needs backward computation.
I0926 08:53:08.727705  4406 net.cpp:198] BatchNorm11 needs backward computation.
I0926 08:53:08.727707  4406 net.cpp:198] Convolution11 needs backward computation.
I0926 08:53:08.727710  4406 net.cpp:198] penlu10 needs backward computation.
I0926 08:53:08.727712  4406 net.cpp:198] Scale10 needs backward computation.
I0926 08:53:08.727715  4406 net.cpp:198] BatchNorm10 needs backward computation.
I0926 08:53:08.727717  4406 net.cpp:198] Convolution10 needs backward computation.
I0926 08:53:08.727720  4406 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I0926 08:53:08.727722  4406 net.cpp:198] penlu9 needs backward computation.
I0926 08:53:08.727725  4406 net.cpp:198] Eltwise4 needs backward computation.
I0926 08:53:08.727728  4406 net.cpp:198] Scale9 needs backward computation.
I0926 08:53:08.727731  4406 net.cpp:198] BatchNorm9 needs backward computation.
I0926 08:53:08.727735  4406 net.cpp:198] Convolution9 needs backward computation.
I0926 08:53:08.727737  4406 net.cpp:198] penlu8 needs backward computation.
I0926 08:53:08.727740  4406 net.cpp:198] Scale8 needs backward computation.
I0926 08:53:08.727742  4406 net.cpp:198] BatchNorm8 needs backward computation.
I0926 08:53:08.727744  4406 net.cpp:198] Convolution8 needs backward computation.
I0926 08:53:08.727747  4406 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I0926 08:53:08.727751  4406 net.cpp:198] penlu7 needs backward computation.
I0926 08:53:08.727753  4406 net.cpp:198] Eltwise3 needs backward computation.
I0926 08:53:08.727756  4406 net.cpp:198] Scale7 needs backward computation.
I0926 08:53:08.727758  4406 net.cpp:198] BatchNorm7 needs backward computation.
I0926 08:53:08.727761  4406 net.cpp:198] Convolution7 needs backward computation.
I0926 08:53:08.727763  4406 net.cpp:198] penlu6 needs backward computation.
I0926 08:53:08.727766  4406 net.cpp:198] Scale6 needs backward computation.
I0926 08:53:08.727768  4406 net.cpp:198] BatchNorm6 needs backward computation.
I0926 08:53:08.727771  4406 net.cpp:198] Convolution6 needs backward computation.
I0926 08:53:08.727777  4406 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I0926 08:53:08.727780  4406 net.cpp:198] penlu5 needs backward computation.
I0926 08:53:08.727783  4406 net.cpp:198] Eltwise2 needs backward computation.
I0926 08:53:08.727787  4406 net.cpp:198] Scale5 needs backward computation.
I0926 08:53:08.727788  4406 net.cpp:198] BatchNorm5 needs backward computation.
I0926 08:53:08.727792  4406 net.cpp:198] Convolution5 needs backward computation.
I0926 08:53:08.729604  4406 net.cpp:198] penlu4 needs backward computation.
I0926 08:53:08.729610  4406 net.cpp:198] Scale4 needs backward computation.
I0926 08:53:08.729614  4406 net.cpp:198] BatchNorm4 needs backward computation.
I0926 08:53:08.729615  4406 net.cpp:198] Convolution4 needs backward computation.
I0926 08:53:08.729619  4406 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I0926 08:53:08.729621  4406 net.cpp:198] penlu3 needs backward computation.
I0926 08:53:08.729624  4406 net.cpp:198] Eltwise1 needs backward computation.
I0926 08:53:08.729627  4406 net.cpp:198] Scale3 needs backward computation.
I0926 08:53:08.729629  4406 net.cpp:198] BatchNorm3 needs backward computation.
I0926 08:53:08.729632  4406 net.cpp:198] Convolution3 needs backward computation.
I0926 08:53:08.729635  4406 net.cpp:198] penlu2 needs backward computation.
I0926 08:53:08.729637  4406 net.cpp:198] Scale2 needs backward computation.
I0926 08:53:08.729640  4406 net.cpp:198] BatchNorm2 needs backward computation.
I0926 08:53:08.729642  4406 net.cpp:198] Convolution2 needs backward computation.
I0926 08:53:08.729645  4406 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I0926 08:53:08.729647  4406 net.cpp:198] penlu1 needs backward computation.
I0926 08:53:08.729650  4406 net.cpp:198] Scale1 needs backward computation.
I0926 08:53:08.729652  4406 net.cpp:198] BatchNorm1 needs backward computation.
I0926 08:53:08.729655  4406 net.cpp:198] Convolution1 needs backward computation.
I0926 08:53:08.729658  4406 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I0926 08:53:08.729661  4406 net.cpp:200] Data1 does not need backward computation.
I0926 08:53:08.729663  4406 net.cpp:242] This network produces output Accuracy1
I0926 08:53:08.729666  4406 net.cpp:242] This network produces output SoftmaxWithLoss1
I0926 08:53:08.729763  4406 net.cpp:255] Network initialization done.
I0926 08:53:08.730602  4406 solver.cpp:56] Solver scaffolding done.
I0926 08:53:08.744774  4406 caffe.cpp:248] Starting Optimization
I0926 08:53:08.744783  4406 solver.cpp:272] Solving resnet_cifar10
I0926 08:53:08.744786  4406 solver.cpp:273] Learning Rate Policy: multistep
I0926 08:53:08.751417  4406 solver.cpp:330] Iteration 0, Testing net (#0)
I0926 08:53:12.209813  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:53:12.349802  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.1
I0926 08:53:12.349838  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I0926 08:53:12.550392  4406 solver.cpp:218] Iteration 0 (-1.49197e-32 iter/s, 3.80553s/100 iters), loss = 2.30804
I0926 08:53:12.550423  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.30804 (* 1 = 2.30804 loss)
I0926 08:53:12.550438  4406 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I0926 08:53:27.170593  4406 solver.cpp:218] Iteration 100 (6.83989 iter/s, 14.6201s/100 iters), loss = 1.62666
I0926 08:53:27.170631  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.62666 (* 1 = 1.62666 loss)
I0926 08:53:27.170637  4406 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I0926 08:53:41.770275  4406 solver.cpp:218] Iteration 200 (6.84951 iter/s, 14.5996s/100 iters), loss = 1.69664
I0926 08:53:41.770371  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.69664 (* 1 = 1.69664 loss)
I0926 08:53:41.770378  4406 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I0926 08:53:56.376350  4406 solver.cpp:218] Iteration 300 (6.84654 iter/s, 14.6059s/100 iters), loss = 1.42135
I0926 08:53:56.376391  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.42135 (* 1 = 1.42135 loss)
I0926 08:53:56.376397  4406 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I0926 08:54:10.983386  4406 solver.cpp:218] Iteration 400 (6.84606 iter/s, 14.6069s/100 iters), loss = 1.19573
I0926 08:54:10.983424  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.19573 (* 1 = 1.19573 loss)
I0926 08:54:10.983430  4406 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I0926 08:54:24.859841  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:54:25.444854  4406 solver.cpp:330] Iteration 500, Testing net (#0)
I0926 08:54:28.873646  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:54:29.016360  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.434
I0926 08:54:29.016396  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.71451 (* 1 = 1.71451 loss)
I0926 08:54:29.162722  4406 solver.cpp:218] Iteration 500 (5.50078 iter/s, 18.1792s/100 iters), loss = 1.3939
I0926 08:54:29.162755  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.3939 (* 1 = 1.3939 loss)
I0926 08:54:29.162761  4406 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I0926 08:54:43.801025  4406 solver.cpp:218] Iteration 600 (6.83143 iter/s, 14.6382s/100 iters), loss = 1.1872
I0926 08:54:43.801054  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.1872 (* 1 = 1.1872 loss)
I0926 08:54:43.801060  4406 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I0926 08:54:58.442235  4406 solver.cpp:218] Iteration 700 (6.83007 iter/s, 14.6411s/100 iters), loss = 1.21931
I0926 08:54:58.442315  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.21931 (* 1 = 1.21931 loss)
I0926 08:54:58.442332  4406 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I0926 08:55:13.080335  4406 solver.cpp:218] Iteration 800 (6.83155 iter/s, 14.638s/100 iters), loss = 1.17623
I0926 08:55:13.080365  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.17623 (* 1 = 1.17623 loss)
I0926 08:55:13.080373  4406 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I0926 08:55:27.713038  4406 solver.cpp:218] Iteration 900 (6.83404 iter/s, 14.6326s/100 iters), loss = 1.0528
I0926 08:55:27.713068  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.0528 (* 1 = 1.0528 loss)
I0926 08:55:27.713073  4406 sgd_solver.cpp:46] MultiStep Status: Iteration 900, step = 1
I0926 08:55:27.713076  4406 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I0926 08:55:41.618024  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:55:42.203428  4406 solver.cpp:330] Iteration 1000, Testing net (#0)
I0926 08:55:45.630635  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:55:45.773921  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3817
I0926 08:55:45.773947  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.99273 (* 1 = 1.99273 loss)
I0926 08:55:45.919208  4406 solver.cpp:218] Iteration 1000 (5.49267 iter/s, 18.2061s/100 iters), loss = 0.981734
I0926 08:55:45.919240  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.981734 (* 1 = 0.981734 loss)
I0926 08:55:45.919247  4406 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I0926 08:56:00.537438  4406 solver.cpp:218] Iteration 1100 (6.84081 iter/s, 14.6181s/100 iters), loss = 0.948404
I0926 08:56:00.537477  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.948404 (* 1 = 0.948404 loss)
I0926 08:56:00.537484  4406 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I0926 08:56:15.169806  4406 solver.cpp:218] Iteration 1200 (6.8342 iter/s, 14.6323s/100 iters), loss = 1.02004
I0926 08:56:15.169946  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.02004 (* 1 = 1.02004 loss)
I0926 08:56:15.169955  4406 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I0926 08:56:29.808166  4406 solver.cpp:218] Iteration 1300 (6.83145 iter/s, 14.6382s/100 iters), loss = 1.04327
I0926 08:56:29.808207  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.04327 (* 1 = 1.04327 loss)
I0926 08:56:29.808212  4406 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I0926 08:56:44.445737  4406 solver.cpp:218] Iteration 1400 (6.83177 iter/s, 14.6375s/100 iters), loss = 0.908745
I0926 08:56:44.445777  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.908745 (* 1 = 0.908745 loss)
I0926 08:56:44.445783  4406 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I0926 08:56:58.359452  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:56:58.943959  4406 solver.cpp:330] Iteration 1500, Testing net (#0)
I0926 08:57:02.373718  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:57:02.516618  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4619
I0926 08:57:02.516654  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.59782 (* 1 = 1.59782 loss)
I0926 08:57:02.662657  4406 solver.cpp:218] Iteration 1500 (5.48943 iter/s, 18.2168s/100 iters), loss = 0.838966
I0926 08:57:02.662686  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.838966 (* 1 = 0.838966 loss)
I0926 08:57:02.662693  4406 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I0926 08:57:17.286592  4406 solver.cpp:218] Iteration 1600 (6.83814 iter/s, 14.6239s/100 iters), loss = 0.863916
I0926 08:57:17.286634  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.863916 (* 1 = 0.863916 loss)
I0926 08:57:17.286640  4406 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I0926 08:57:31.914868  4406 solver.cpp:218] Iteration 1700 (6.83612 iter/s, 14.6282s/100 iters), loss = 0.918241
I0926 08:57:31.914985  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.918241 (* 1 = 0.918241 loss)
I0926 08:57:31.914993  4406 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I0926 08:57:46.540755  4406 solver.cpp:218] Iteration 1800 (6.83726 iter/s, 14.6257s/100 iters), loss = 0.925274
I0926 08:57:46.540796  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.925274 (* 1 = 0.925274 loss)
I0926 08:57:46.540803  4406 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I0926 08:58:01.172468  4406 solver.cpp:218] Iteration 1900 (6.83451 iter/s, 14.6316s/100 iters), loss = 0.792954
I0926 08:58:01.172511  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.792954 (* 1 = 0.792954 loss)
I0926 08:58:01.172518  4406 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I0926 08:58:15.080993  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:58:15.666911  4406 solver.cpp:330] Iteration 2000, Testing net (#0)
I0926 08:58:19.095024  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:58:19.237263  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.577
I0926 08:58:19.237299  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.20986 (* 1 = 1.20986 loss)
I0926 08:58:19.382592  4406 solver.cpp:218] Iteration 2000 (5.49148 iter/s, 18.21s/100 iters), loss = 0.857794
I0926 08:58:19.382624  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.857794 (* 1 = 0.857794 loss)
I0926 08:58:19.382632  4406 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I0926 08:58:34.026012  4406 solver.cpp:218] Iteration 2100 (6.82904 iter/s, 14.6433s/100 iters), loss = 0.862208
I0926 08:58:34.026052  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.862208 (* 1 = 0.862208 loss)
I0926 08:58:34.026058  4406 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I0926 08:58:48.666944  4406 solver.cpp:218] Iteration 2200 (6.8302 iter/s, 14.6409s/100 iters), loss = 0.765864
I0926 08:58:48.667048  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.765864 (* 1 = 0.765864 loss)
I0926 08:58:48.667057  4406 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I0926 08:59:03.300829  4406 solver.cpp:218] Iteration 2300 (6.83352 iter/s, 14.6338s/100 iters), loss = 0.934641
I0926 08:59:03.300869  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.934641 (* 1 = 0.934641 loss)
I0926 08:59:03.300875  4406 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I0926 08:59:17.936941  4406 solver.cpp:218] Iteration 2400 (6.83245 iter/s, 14.636s/100 iters), loss = 0.77197
I0926 08:59:17.936980  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.77197 (* 1 = 0.77197 loss)
I0926 08:59:17.936986  4406 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I0926 08:59:31.849634  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:59:32.436005  4406 solver.cpp:330] Iteration 2500, Testing net (#0)
I0926 08:59:35.863409  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 08:59:36.006222  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5825
I0926 08:59:36.006258  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.16459 (* 1 = 1.16459 loss)
I0926 08:59:36.152539  4406 solver.cpp:218] Iteration 2500 (5.48983 iter/s, 18.2155s/100 iters), loss = 0.839373
I0926 08:59:36.152565  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.839373 (* 1 = 0.839373 loss)
I0926 08:59:36.152572  4406 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I0926 08:59:50.778466  4406 solver.cpp:218] Iteration 2600 (6.8372 iter/s, 14.6259s/100 iters), loss = 0.800159
I0926 08:59:50.778497  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.800159 (* 1 = 0.800159 loss)
I0926 08:59:50.778503  4406 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I0926 09:00:05.412523  4406 solver.cpp:218] Iteration 2700 (6.83341 iter/s, 14.634s/100 iters), loss = 0.809489
I0926 09:00:05.412629  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.809489 (* 1 = 0.809489 loss)
I0926 09:00:05.412637  4406 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I0926 09:00:20.050384  4406 solver.cpp:218] Iteration 2800 (6.83167 iter/s, 14.6377s/100 iters), loss = 0.935269
I0926 09:00:20.050423  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.935269 (* 1 = 0.935269 loss)
I0926 09:00:20.050429  4406 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I0926 09:00:34.680763  4406 solver.cpp:218] Iteration 2900 (6.83513 iter/s, 14.6303s/100 iters), loss = 0.755632
I0926 09:00:34.680802  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.755632 (* 1 = 0.755632 loss)
I0926 09:00:34.680809  4406 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I0926 09:00:48.585568  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:00:49.170464  4406 solver.cpp:330] Iteration 3000, Testing net (#0)
I0926 09:00:52.596249  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:00:52.738915  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.628
I0926 09:00:52.738951  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.04147 (* 1 = 1.04147 loss)
I0926 09:00:52.883781  4406 solver.cpp:218] Iteration 3000 (5.49362 iter/s, 18.2029s/100 iters), loss = 0.788101
I0926 09:00:52.883812  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.788101 (* 1 = 0.788101 loss)
I0926 09:00:52.883819  4406 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I0926 09:01:07.513455  4406 solver.cpp:218] Iteration 3100 (6.83545 iter/s, 14.6296s/100 iters), loss = 0.714274
I0926 09:01:07.513485  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.714274 (* 1 = 0.714274 loss)
I0926 09:01:07.513491  4406 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I0926 09:01:22.142067  4406 solver.cpp:218] Iteration 3200 (6.83595 iter/s, 14.6285s/100 iters), loss = 0.7803
I0926 09:01:22.142208  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.7803 (* 1 = 0.7803 loss)
I0926 09:01:22.142216  4406 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I0926 09:01:36.771181  4406 solver.cpp:218] Iteration 3300 (6.83577 iter/s, 14.6289s/100 iters), loss = 0.857309
I0926 09:01:36.771210  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.857309 (* 1 = 0.857309 loss)
I0926 09:01:36.771216  4406 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I0926 09:01:51.401471  4406 solver.cpp:218] Iteration 3400 (6.83517 iter/s, 14.6302s/100 iters), loss = 0.682791
I0926 09:01:51.401501  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.682791 (* 1 = 0.682791 loss)
I0926 09:01:51.401509  4406 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I0926 09:02:05.307955  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:02:05.894506  4406 solver.cpp:330] Iteration 3500, Testing net (#0)
I0926 09:02:09.323091  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:02:09.466017  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5472
I0926 09:02:09.466043  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.27842 (* 1 = 1.27842 loss)
I0926 09:02:09.611526  4406 solver.cpp:218] Iteration 3500 (5.49149 iter/s, 18.21s/100 iters), loss = 0.793281
I0926 09:02:09.611558  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.793281 (* 1 = 0.793281 loss)
I0926 09:02:09.611565  4406 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I0926 09:02:24.244537  4406 solver.cpp:218] Iteration 3600 (6.8339 iter/s, 14.6329s/100 iters), loss = 0.647766
I0926 09:02:24.244568  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.647766 (* 1 = 0.647766 loss)
I0926 09:02:24.244575  4406 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I0926 09:02:38.874105  4406 solver.cpp:218] Iteration 3700 (6.8355 iter/s, 14.6295s/100 iters), loss = 0.688514
I0926 09:02:38.874253  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.688514 (* 1 = 0.688514 loss)
I0926 09:02:38.874260  4406 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I0926 09:02:53.503185  4406 solver.cpp:218] Iteration 3800 (6.83578 iter/s, 14.6289s/100 iters), loss = 0.894663
I0926 09:02:53.503216  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.894663 (* 1 = 0.894663 loss)
I0926 09:02:53.503222  4406 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I0926 09:03:08.136438  4406 solver.cpp:218] Iteration 3900 (6.83378 iter/s, 14.6332s/100 iters), loss = 0.677207
I0926 09:03:08.136471  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.677207 (* 1 = 0.677207 loss)
I0926 09:03:08.136476  4406 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I0926 09:03:22.038841  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:03:22.622536  4406 solver.cpp:330] Iteration 4000, Testing net (#0)
I0926 09:03:26.053367  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:03:26.196419  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6453
I0926 09:03:26.196455  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.989103 (* 1 = 0.989103 loss)
I0926 09:03:26.341904  4406 solver.cpp:218] Iteration 4000 (5.49288 iter/s, 18.2054s/100 iters), loss = 0.736986
I0926 09:03:26.341936  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.736986 (* 1 = 0.736986 loss)
I0926 09:03:26.341943  4406 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I0926 09:03:40.960912  4406 solver.cpp:218] Iteration 4100 (6.84044 iter/s, 14.6189s/100 iters), loss = 0.594047
I0926 09:03:40.960942  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.594047 (* 1 = 0.594047 loss)
I0926 09:03:40.960947  4406 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I0926 09:03:55.584573  4406 solver.cpp:218] Iteration 4200 (6.83826 iter/s, 14.6236s/100 iters), loss = 0.669957
I0926 09:03:55.584686  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.669957 (* 1 = 0.669957 loss)
I0926 09:03:55.584692  4406 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I0926 09:04:10.217280  4406 solver.cpp:218] Iteration 4300 (6.83407 iter/s, 14.6326s/100 iters), loss = 0.795332
I0926 09:04:10.217310  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.795332 (* 1 = 0.795332 loss)
I0926 09:04:10.217316  4406 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I0926 09:04:24.852162  4406 solver.cpp:218] Iteration 4400 (6.83302 iter/s, 14.6348s/100 iters), loss = 0.705144
I0926 09:04:24.852192  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.705144 (* 1 = 0.705144 loss)
I0926 09:04:24.852198  4406 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I0926 09:04:38.756336  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:04:39.341073  4406 solver.cpp:330] Iteration 4500, Testing net (#0)
I0926 09:04:42.769996  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:04:42.912978  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6638
I0926 09:04:42.913014  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.932591 (* 1 = 0.932591 loss)
I0926 09:04:43.058593  4406 solver.cpp:218] Iteration 4500 (5.49259 iter/s, 18.2064s/100 iters), loss = 0.720721
I0926 09:04:43.058626  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.720721 (* 1 = 0.720721 loss)
I0926 09:04:43.058634  4406 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I0926 09:04:57.693174  4406 solver.cpp:218] Iteration 4600 (6.83316 iter/s, 14.6345s/100 iters), loss = 0.594017
I0926 09:04:57.693205  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.594017 (* 1 = 0.594017 loss)
I0926 09:04:57.693212  4406 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I0926 09:05:12.326977  4406 solver.cpp:218] Iteration 4700 (6.83352 iter/s, 14.6337s/100 iters), loss = 0.587745
I0926 09:05:12.327087  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.587745 (* 1 = 0.587745 loss)
I0926 09:05:12.327106  4406 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I0926 09:05:26.965481  4406 solver.cpp:218] Iteration 4800 (6.83137 iter/s, 14.6384s/100 iters), loss = 0.757848
I0926 09:05:26.965512  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.757848 (* 1 = 0.757848 loss)
I0926 09:05:26.965517  4406 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I0926 09:05:41.604517  4406 solver.cpp:218] Iteration 4900 (6.83108 iter/s, 14.639s/100 iters), loss = 0.668077
I0926 09:05:41.604558  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.668077 (* 1 = 0.668077 loss)
I0926 09:05:41.604564  4406 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I0926 09:05:55.518306  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:05:56.103271  4406 solver.cpp:330] Iteration 5000, Testing net (#0)
I0926 09:05:59.532819  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:05:59.675716  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6848
I0926 09:05:59.675741  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.867461 (* 1 = 0.867461 loss)
I0926 09:05:59.821338  4406 solver.cpp:218] Iteration 5000 (5.48946 iter/s, 18.2167s/100 iters), loss = 0.662653
I0926 09:05:59.821368  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.662653 (* 1 = 0.662653 loss)
I0926 09:05:59.821375  4406 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I0926 09:06:14.446858  4406 solver.cpp:218] Iteration 5100 (6.83739 iter/s, 14.6255s/100 iters), loss = 0.52998
I0926 09:06:14.446888  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.52998 (* 1 = 0.52998 loss)
I0926 09:06:14.446893  4406 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I0926 09:06:29.142561  4406 solver.cpp:218] Iteration 5200 (6.80474 iter/s, 14.6956s/100 iters), loss = 0.513532
I0926 09:06:29.142685  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.513532 (* 1 = 0.513532 loss)
I0926 09:06:29.142693  4406 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I0926 09:06:43.901113  4406 solver.cpp:218] Iteration 5300 (6.7758 iter/s, 14.7584s/100 iters), loss = 0.790653
I0926 09:06:43.901147  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.790653 (* 1 = 0.790653 loss)
I0926 09:06:43.901155  4406 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I0926 09:06:58.581640  4406 solver.cpp:218] Iteration 5400 (6.81178 iter/s, 14.6805s/100 iters), loss = 0.721525
I0926 09:06:58.581670  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.721525 (* 1 = 0.721525 loss)
I0926 09:06:58.581676  4406 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I0926 09:07:12.525634  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:07:13.110945  4406 solver.cpp:330] Iteration 5500, Testing net (#0)
I0926 09:07:16.534210  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:07:16.677202  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7062
I0926 09:07:16.677237  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.813759 (* 1 = 0.813759 loss)
I0926 09:07:16.822274  4406 solver.cpp:218] Iteration 5500 (5.48229 iter/s, 18.2406s/100 iters), loss = 0.752587
I0926 09:07:16.822305  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.752587 (* 1 = 0.752587 loss)
I0926 09:07:16.822311  4406 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I0926 09:07:31.497768  4406 solver.cpp:218] Iteration 5600 (6.81411 iter/s, 14.6754s/100 iters), loss = 0.533858
I0926 09:07:31.497797  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.533858 (* 1 = 0.533858 loss)
I0926 09:07:31.497803  4406 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I0926 09:07:46.258235  4406 solver.cpp:218] Iteration 5700 (6.77488 iter/s, 14.7604s/100 iters), loss = 0.518427
I0926 09:07:46.258352  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.518427 (* 1 = 0.518427 loss)
I0926 09:07:46.258370  4406 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I0926 09:08:01.010004  4406 solver.cpp:218] Iteration 5800 (6.77891 iter/s, 14.7516s/100 iters), loss = 0.774448
I0926 09:08:01.010033  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.774448 (* 1 = 0.774448 loss)
I0926 09:08:01.010040  4406 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I0926 09:08:15.663970  4406 solver.cpp:218] Iteration 5900 (6.82412 iter/s, 14.6539s/100 iters), loss = 0.637592
I0926 09:08:15.664000  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.637592 (* 1 = 0.637592 loss)
I0926 09:08:15.664007  4406 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I0926 09:08:29.565500  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:08:30.149816  4406 solver.cpp:330] Iteration 6000, Testing net (#0)
I0926 09:08:33.573565  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:08:33.716775  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7117
I0926 09:08:33.716801  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.799013 (* 1 = 0.799013 loss)
I0926 09:08:33.860929  4406 solver.cpp:218] Iteration 6000 (5.49544 iter/s, 18.1969s/100 iters), loss = 0.6893
I0926 09:08:33.860957  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.6893 (* 1 = 0.6893 loss)
I0926 09:08:33.860963  4406 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I0926 09:08:48.491930  4406 solver.cpp:218] Iteration 6100 (6.83483 iter/s, 14.6309s/100 iters), loss = 0.535787
I0926 09:08:48.491960  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.535787 (* 1 = 0.535787 loss)
I0926 09:08:48.491966  4406 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I0926 09:09:03.129050  4406 solver.cpp:218] Iteration 6200 (6.83197 iter/s, 14.6371s/100 iters), loss = 0.690371
I0926 09:09:03.129173  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.690371 (* 1 = 0.690371 loss)
I0926 09:09:03.129180  4406 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I0926 09:09:17.827821  4406 solver.cpp:218] Iteration 6300 (6.80336 iter/s, 14.6986s/100 iters), loss = 0.638252
I0926 09:09:17.827852  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.638252 (* 1 = 0.638252 loss)
I0926 09:09:17.827857  4406 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I0926 09:09:32.602078  4406 solver.cpp:218] Iteration 6400 (6.76856 iter/s, 14.7742s/100 iters), loss = 0.55124
I0926 09:09:32.602108  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.55124 (* 1 = 0.55124 loss)
I0926 09:09:32.602114  4406 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I0926 09:09:46.544903  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:09:47.129784  4406 solver.cpp:330] Iteration 6500, Testing net (#0)
I0926 09:09:50.562602  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:09:50.706904  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.71
I0926 09:09:50.706933  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.803375 (* 1 = 0.803375 loss)
I0926 09:09:50.851913  4406 solver.cpp:218] Iteration 6500 (5.47952 iter/s, 18.2498s/100 iters), loss = 0.618062
I0926 09:09:50.851941  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.618062 (* 1 = 0.618062 loss)
I0926 09:09:50.851948  4406 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I0926 09:10:05.514200  4406 solver.cpp:218] Iteration 6600 (6.82025 iter/s, 14.6622s/100 iters), loss = 0.49259
I0926 09:10:05.514230  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.49259 (* 1 = 0.49259 loss)
I0926 09:10:05.514236  4406 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I0926 09:10:20.242352  4406 solver.cpp:218] Iteration 6700 (6.78975 iter/s, 14.7281s/100 iters), loss = 0.575952
I0926 09:10:20.242494  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.575952 (* 1 = 0.575952 loss)
I0926 09:10:20.242502  4406 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I0926 09:10:34.956032  4406 solver.cpp:218] Iteration 6800 (6.79648 iter/s, 14.7135s/100 iters), loss = 0.608762
I0926 09:10:34.956063  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.608762 (* 1 = 0.608762 loss)
I0926 09:10:34.956068  4406 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I0926 09:10:49.769696  4406 solver.cpp:218] Iteration 6900 (6.75055 iter/s, 14.8136s/100 iters), loss = 0.564212
I0926 09:10:49.769726  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.564212 (* 1 = 0.564212 loss)
I0926 09:10:49.769733  4406 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I0926 09:11:03.772971  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:11:04.373884  4406 solver.cpp:330] Iteration 7000, Testing net (#0)
I0926 09:11:07.840922  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:11:07.983942  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7495
I0926 09:11:07.983978  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.717289 (* 1 = 0.717289 loss)
I0926 09:11:08.128594  4406 solver.cpp:218] Iteration 7000 (5.44697 iter/s, 18.3588s/100 iters), loss = 0.547089
I0926 09:11:08.128620  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.547089 (* 1 = 0.547089 loss)
I0926 09:11:08.128628  4406 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I0926 09:11:22.968425  4406 solver.cpp:218] Iteration 7100 (6.73865 iter/s, 14.8398s/100 iters), loss = 0.45463
I0926 09:11:22.968464  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.45463 (* 1 = 0.45463 loss)
I0926 09:11:22.968472  4406 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I0926 09:11:37.808023  4406 solver.cpp:218] Iteration 7200 (6.73876 iter/s, 14.8395s/100 iters), loss = 0.490175
I0926 09:11:37.808166  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.490175 (* 1 = 0.490175 loss)
I0926 09:11:37.808176  4406 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I0926 09:11:52.636843  4406 solver.cpp:218] Iteration 7300 (6.74371 iter/s, 14.8286s/100 iters), loss = 0.672238
I0926 09:11:52.636890  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.672238 (* 1 = 0.672238 loss)
I0926 09:11:52.636898  4406 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I0926 09:12:07.473171  4406 solver.cpp:218] Iteration 7400 (6.74025 iter/s, 14.8362s/100 iters), loss = 0.541144
I0926 09:12:07.473203  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.541144 (* 1 = 0.541144 loss)
I0926 09:12:07.473211  4406 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I0926 09:12:21.591289  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:12:22.190091  4406 solver.cpp:330] Iteration 7500, Testing net (#0)
I0926 09:12:25.646723  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:12:25.789710  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7582
I0926 09:12:25.789746  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.688748 (* 1 = 0.688748 loss)
I0926 09:12:25.934363  4406 solver.cpp:218] Iteration 7500 (5.41679 iter/s, 18.4611s/100 iters), loss = 0.545787
I0926 09:12:25.934394  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.545787 (* 1 = 0.545787 loss)
I0926 09:12:25.934401  4406 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I0926 09:12:40.735860  4406 solver.cpp:218] Iteration 7600 (6.7561 iter/s, 14.8014s/100 iters), loss = 0.44352
I0926 09:12:40.735894  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44352 (* 1 = 0.44352 loss)
I0926 09:12:40.735901  4406 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I0926 09:12:55.517171  4406 solver.cpp:218] Iteration 7700 (6.76533 iter/s, 14.7812s/100 iters), loss = 0.577606
I0926 09:12:55.517300  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.577606 (* 1 = 0.577606 loss)
I0926 09:12:55.517308  4406 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I0926 09:13:10.358909  4406 solver.cpp:218] Iteration 7800 (6.73783 iter/s, 14.8416s/100 iters), loss = 0.569426
I0926 09:13:10.358940  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.569426 (* 1 = 0.569426 loss)
I0926 09:13:10.358947  4406 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I0926 09:13:25.175568  4406 solver.cpp:218] Iteration 7900 (6.74919 iter/s, 14.8166s/100 iters), loss = 0.559846
I0926 09:13:25.175599  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.559846 (* 1 = 0.559846 loss)
I0926 09:13:25.175606  4406 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I0926 09:13:39.246999  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:13:39.835544  4406 solver.cpp:330] Iteration 8000, Testing net (#0)
I0926 09:13:43.314688  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:13:43.464956  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7526
I0926 09:13:43.464982  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.694286 (* 1 = 0.694286 loss)
I0926 09:13:43.613952  4406 solver.cpp:218] Iteration 8000 (5.42349 iter/s, 18.4383s/100 iters), loss = 0.507105
I0926 09:13:43.613999  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.507105 (* 1 = 0.507105 loss)
I0926 09:13:43.614006  4406 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I0926 09:13:58.433198  4406 solver.cpp:218] Iteration 8100 (6.74802 iter/s, 14.8192s/100 iters), loss = 0.419872
I0926 09:13:58.433234  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419872 (* 1 = 0.419872 loss)
I0926 09:13:58.433243  4406 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I0926 09:14:13.282932  4406 solver.cpp:218] Iteration 8200 (6.73416 iter/s, 14.8497s/100 iters), loss = 0.535391
I0926 09:14:13.283026  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.535391 (* 1 = 0.535391 loss)
I0926 09:14:13.283035  4406 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I0926 09:14:28.116292  4406 solver.cpp:218] Iteration 8300 (6.74162 iter/s, 14.8332s/100 iters), loss = 0.555764
I0926 09:14:28.116328  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.555764 (* 1 = 0.555764 loss)
I0926 09:14:28.116334  4406 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I0926 09:14:42.882172  4406 solver.cpp:218] Iteration 8400 (6.7724 iter/s, 14.7658s/100 iters), loss = 0.522102
I0926 09:14:42.882205  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.522102 (* 1 = 0.522102 loss)
I0926 09:14:42.882211  4406 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I0926 09:14:56.986582  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:14:57.582119  4406 solver.cpp:330] Iteration 8500, Testing net (#0)
I0926 09:15:01.064801  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:15:01.209695  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7597
I0926 09:15:01.209722  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.68268 (* 1 = 0.68268 loss)
I0926 09:15:01.354696  4406 solver.cpp:218] Iteration 8500 (5.41347 iter/s, 18.4724s/100 iters), loss = 0.623472
I0926 09:15:01.354748  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.623472 (* 1 = 0.623472 loss)
I0926 09:15:01.354768  4406 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I0926 09:15:16.160513  4406 solver.cpp:218] Iteration 8600 (6.75414 iter/s, 14.8057s/100 iters), loss = 0.415336
I0926 09:15:16.160548  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.415336 (* 1 = 0.415336 loss)
I0926 09:15:16.160555  4406 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I0926 09:15:31.012557  4406 solver.cpp:218] Iteration 8700 (6.73311 iter/s, 14.852s/100 iters), loss = 0.654312
I0926 09:15:31.012681  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.654312 (* 1 = 0.654312 loss)
I0926 09:15:31.012691  4406 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I0926 09:15:45.823921  4406 solver.cpp:218] Iteration 8800 (6.75164 iter/s, 14.8112s/100 iters), loss = 0.66927
I0926 09:15:45.823954  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.66927 (* 1 = 0.66927 loss)
I0926 09:15:45.823961  4406 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I0926 09:16:00.641937  4406 solver.cpp:218] Iteration 8900 (6.74857 iter/s, 14.8179s/100 iters), loss = 0.513175
I0926 09:16:00.641966  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.513175 (* 1 = 0.513175 loss)
I0926 09:16:00.641973  4406 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I0926 09:16:14.657574  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:16:15.245452  4406 solver.cpp:330] Iteration 9000, Testing net (#0)
I0926 09:16:18.701520  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:16:18.844369  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7522
I0926 09:16:18.844393  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.731487 (* 1 = 0.731487 loss)
I0926 09:16:18.988464  4406 solver.cpp:218] Iteration 9000 (5.45064 iter/s, 18.3465s/100 iters), loss = 0.571667
I0926 09:16:18.988497  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.571667 (* 1 = 0.571667 loss)
I0926 09:16:18.988504  4406 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I0926 09:16:33.648079  4406 solver.cpp:218] Iteration 9100 (6.82149 iter/s, 14.6596s/100 iters), loss = 0.460585
I0926 09:16:33.648109  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.460585 (* 1 = 0.460585 loss)
I0926 09:16:33.648115  4406 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I0926 09:16:48.372407  4406 solver.cpp:218] Iteration 9200 (6.79151 iter/s, 14.7243s/100 iters), loss = 0.441262
I0926 09:16:48.372527  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.441262 (* 1 = 0.441262 loss)
I0926 09:16:48.372535  4406 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I0926 09:17:03.084306  4406 solver.cpp:218] Iteration 9300 (6.79729 iter/s, 14.7117s/100 iters), loss = 0.606807
I0926 09:17:03.084334  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.606807 (* 1 = 0.606807 loss)
I0926 09:17:03.084341  4406 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I0926 09:17:17.868829  4406 solver.cpp:218] Iteration 9400 (6.76386 iter/s, 14.7845s/100 iters), loss = 0.448483
I0926 09:17:17.868870  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.448483 (* 1 = 0.448483 loss)
I0926 09:17:17.868875  4406 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I0926 09:17:31.923718  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:17:32.509212  4406 solver.cpp:330] Iteration 9500, Testing net (#0)
I0926 09:17:35.936228  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:17:36.079005  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.77
I0926 09:17:36.079030  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.664552 (* 1 = 0.664552 loss)
I0926 09:17:36.223520  4406 solver.cpp:218] Iteration 9500 (5.44822 iter/s, 18.3546s/100 iters), loss = 0.523263
I0926 09:17:36.223548  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.523263 (* 1 = 0.523263 loss)
I0926 09:17:36.223554  4406 sgd_solver.cpp:105] Iteration 9500, lr = 0.01
I0926 09:17:50.923938  4406 solver.cpp:218] Iteration 9600 (6.80256 iter/s, 14.7004s/100 iters), loss = 0.411029
I0926 09:17:50.923979  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.411029 (* 1 = 0.411029 loss)
I0926 09:17:50.923985  4406 sgd_solver.cpp:105] Iteration 9600, lr = 0.01
I0926 09:18:05.689657  4406 solver.cpp:218] Iteration 9700 (6.77248 iter/s, 14.7656s/100 iters), loss = 0.488183
I0926 09:18:05.689739  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.488183 (* 1 = 0.488183 loss)
I0926 09:18:05.689757  4406 sgd_solver.cpp:105] Iteration 9700, lr = 0.01
I0926 09:18:20.400501  4406 solver.cpp:218] Iteration 9800 (6.79776 iter/s, 14.7107s/100 iters), loss = 0.59278
I0926 09:18:20.400529  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.59278 (* 1 = 0.59278 loss)
I0926 09:18:20.400544  4406 sgd_solver.cpp:105] Iteration 9800, lr = 0.01
I0926 09:18:35.128641  4406 solver.cpp:218] Iteration 9900 (6.78976 iter/s, 14.7281s/100 iters), loss = 0.545636
I0926 09:18:35.128720  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.545636 (* 1 = 0.545636 loss)
I0926 09:18:35.128731  4406 sgd_solver.cpp:105] Iteration 9900, lr = 0.01
I0926 09:18:49.414729  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:18:50.001910  4406 solver.cpp:330] Iteration 10000, Testing net (#0)
I0926 09:18:53.448340  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:18:53.591094  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7846
I0926 09:18:53.591130  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.619141 (* 1 = 0.619141 loss)
I0926 09:18:53.742144  4406 solver.cpp:218] Iteration 10000 (5.37248 iter/s, 18.6134s/100 iters), loss = 0.550783
I0926 09:18:53.742179  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.550783 (* 1 = 0.550783 loss)
I0926 09:18:53.742187  4406 sgd_solver.cpp:105] Iteration 10000, lr = 0.01
I0926 09:19:08.691390  4406 solver.cpp:218] Iteration 10100 (6.68933 iter/s, 14.9492s/100 iters), loss = 0.422136
I0926 09:19:08.691431  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.422136 (* 1 = 0.422136 loss)
I0926 09:19:08.691438  4406 sgd_solver.cpp:105] Iteration 10100, lr = 0.01
I0926 09:19:23.333878  4406 solver.cpp:218] Iteration 10200 (6.82948 iter/s, 14.6424s/100 iters), loss = 0.471081
I0926 09:19:23.333986  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.471081 (* 1 = 0.471081 loss)
I0926 09:19:23.333993  4406 sgd_solver.cpp:105] Iteration 10200, lr = 0.01
I0926 09:19:38.029068  4406 solver.cpp:218] Iteration 10300 (6.80501 iter/s, 14.695s/100 iters), loss = 0.544822
I0926 09:19:38.029103  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.544822 (* 1 = 0.544822 loss)
I0926 09:19:38.029110  4406 sgd_solver.cpp:105] Iteration 10300, lr = 0.01
I0926 09:19:52.734974  4406 solver.cpp:218] Iteration 10400 (6.80002 iter/s, 14.7058s/100 iters), loss = 0.531874
I0926 09:19:52.735008  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.531874 (* 1 = 0.531874 loss)
I0926 09:19:52.735015  4406 sgd_solver.cpp:105] Iteration 10400, lr = 0.01
I0926 09:20:06.801718  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:20:07.394456  4406 solver.cpp:330] Iteration 10500, Testing net (#0)
I0926 09:20:10.872251  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:20:11.015439  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7734
I0926 09:20:11.015475  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.647363 (* 1 = 0.647363 loss)
I0926 09:20:11.160878  4406 solver.cpp:218] Iteration 10500 (5.42716 iter/s, 18.4258s/100 iters), loss = 0.44925
I0926 09:20:11.160908  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44925 (* 1 = 0.44925 loss)
I0926 09:20:11.160915  4406 sgd_solver.cpp:105] Iteration 10500, lr = 0.01
I0926 09:20:25.944154  4406 solver.cpp:218] Iteration 10600 (6.76443 iter/s, 14.7832s/100 iters), loss = 0.403409
I0926 09:20:25.944188  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.403409 (* 1 = 0.403409 loss)
I0926 09:20:25.944196  4406 sgd_solver.cpp:105] Iteration 10600, lr = 0.01
I0926 09:20:40.710618  4406 solver.cpp:218] Iteration 10700 (6.77213 iter/s, 14.7664s/100 iters), loss = 0.507985
I0926 09:20:40.710752  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.507985 (* 1 = 0.507985 loss)
I0926 09:20:40.710759  4406 sgd_solver.cpp:105] Iteration 10700, lr = 0.01
I0926 09:20:55.559849  4406 solver.cpp:218] Iteration 10800 (6.73443 iter/s, 14.8491s/100 iters), loss = 0.474914
I0926 09:20:55.559881  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.474914 (* 1 = 0.474914 loss)
I0926 09:20:55.559887  4406 sgd_solver.cpp:105] Iteration 10800, lr = 0.01
I0926 09:21:10.390120  4406 solver.cpp:218] Iteration 10900 (6.743 iter/s, 14.8302s/100 iters), loss = 0.464638
I0926 09:21:10.390151  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.464638 (* 1 = 0.464638 loss)
I0926 09:21:10.390157  4406 sgd_solver.cpp:105] Iteration 10900, lr = 0.01
I0926 09:21:24.476114  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:21:25.062463  4406 solver.cpp:330] Iteration 11000, Testing net (#0)
I0926 09:21:28.557265  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:21:28.700374  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.778
I0926 09:21:28.700412  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.627528 (* 1 = 0.627528 loss)
I0926 09:21:28.846732  4406 solver.cpp:218] Iteration 11000 (5.41813 iter/s, 18.4565s/100 iters), loss = 0.404212
I0926 09:21:28.846768  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.404212 (* 1 = 0.404212 loss)
I0926 09:21:28.846776  4406 sgd_solver.cpp:105] Iteration 11000, lr = 0.01
I0926 09:21:43.680562  4406 solver.cpp:218] Iteration 11100 (6.74145 iter/s, 14.8336s/100 iters), loss = 0.461218
I0926 09:21:43.680593  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.461218 (* 1 = 0.461218 loss)
I0926 09:21:43.680600  4406 sgd_solver.cpp:105] Iteration 11100, lr = 0.01
I0926 09:21:58.434877  4406 solver.cpp:218] Iteration 11200 (6.77771 iter/s, 14.7542s/100 iters), loss = 0.381972
I0926 09:21:58.434990  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.381972 (* 1 = 0.381972 loss)
I0926 09:21:58.434998  4406 sgd_solver.cpp:105] Iteration 11200, lr = 0.01
I0926 09:22:13.234292  4406 solver.cpp:218] Iteration 11300 (6.75709 iter/s, 14.7993s/100 iters), loss = 0.579601
I0926 09:22:13.234326  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.579601 (* 1 = 0.579601 loss)
I0926 09:22:13.234333  4406 sgd_solver.cpp:105] Iteration 11300, lr = 0.01
I0926 09:22:28.045296  4406 solver.cpp:218] Iteration 11400 (6.75177 iter/s, 14.8109s/100 iters), loss = 0.475519
I0926 09:22:28.045331  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.475519 (* 1 = 0.475519 loss)
I0926 09:22:28.045337  4406 sgd_solver.cpp:105] Iteration 11400, lr = 0.01
I0926 09:22:42.133736  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:22:42.719172  4406 solver.cpp:330] Iteration 11500, Testing net (#0)
I0926 09:22:46.213290  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:22:46.360215  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8052
I0926 09:22:46.360252  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.569515 (* 1 = 0.569515 loss)
I0926 09:22:46.506911  4406 solver.cpp:218] Iteration 11500 (5.41667 iter/s, 18.4615s/100 iters), loss = 0.3899
I0926 09:22:46.506942  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.3899 (* 1 = 0.3899 loss)
I0926 09:22:46.506949  4406 sgd_solver.cpp:105] Iteration 11500, lr = 0.01
I0926 09:23:01.344079  4406 solver.cpp:218] Iteration 11600 (6.73986 iter/s, 14.8371s/100 iters), loss = 0.366129
I0926 09:23:01.344110  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.366129 (* 1 = 0.366129 loss)
I0926 09:23:01.344126  4406 sgd_solver.cpp:105] Iteration 11600, lr = 0.01
I0926 09:23:16.142058  4406 solver.cpp:218] Iteration 11700 (6.75771 iter/s, 14.7979s/100 iters), loss = 0.429931
I0926 09:23:16.142197  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.429931 (* 1 = 0.429931 loss)
I0926 09:23:16.142215  4406 sgd_solver.cpp:105] Iteration 11700, lr = 0.01
I0926 09:23:30.949883  4406 solver.cpp:218] Iteration 11800 (6.75326 iter/s, 14.8077s/100 iters), loss = 0.516854
I0926 09:23:30.949916  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.516854 (* 1 = 0.516854 loss)
I0926 09:23:30.949923  4406 sgd_solver.cpp:105] Iteration 11800, lr = 0.01
I0926 09:23:45.794205  4406 solver.cpp:218] Iteration 11900 (6.73661 iter/s, 14.8443s/100 iters), loss = 0.436403
I0926 09:23:45.794235  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.436403 (* 1 = 0.436403 loss)
I0926 09:23:45.794241  4406 sgd_solver.cpp:105] Iteration 11900, lr = 0.01
I0926 09:23:59.892756  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:24:00.492050  4406 solver.cpp:330] Iteration 12000, Testing net (#0)
I0926 09:24:03.970402  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:24:04.112336  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7863
I0926 09:24:04.112361  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.618537 (* 1 = 0.618537 loss)
I0926 09:24:04.257048  4406 solver.cpp:218] Iteration 12000 (5.41631 iter/s, 18.4628s/100 iters), loss = 0.405778
I0926 09:24:04.257082  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.405778 (* 1 = 0.405778 loss)
I0926 09:24:04.257088  4406 sgd_solver.cpp:105] Iteration 12000, lr = 0.01
I0926 09:24:18.998049  4406 solver.cpp:218] Iteration 12100 (6.78383 iter/s, 14.7409s/100 iters), loss = 0.385801
I0926 09:24:18.998077  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.385801 (* 1 = 0.385801 loss)
I0926 09:24:18.998083  4406 sgd_solver.cpp:105] Iteration 12100, lr = 0.01
I0926 09:24:33.756430  4406 solver.cpp:218] Iteration 12200 (6.77584 iter/s, 14.7583s/100 iters), loss = 0.427719
I0926 09:24:33.756520  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.427719 (* 1 = 0.427719 loss)
I0926 09:24:33.756536  4406 sgd_solver.cpp:105] Iteration 12200, lr = 0.01
I0926 09:24:48.569545  4406 solver.cpp:218] Iteration 12300 (6.75083 iter/s, 14.813s/100 iters), loss = 0.480818
I0926 09:24:48.569579  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.480818 (* 1 = 0.480818 loss)
I0926 09:24:48.569586  4406 sgd_solver.cpp:105] Iteration 12300, lr = 0.01
I0926 09:25:03.321988  4406 solver.cpp:218] Iteration 12400 (6.77859 iter/s, 14.7523s/100 iters), loss = 0.4425
I0926 09:25:03.322037  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.4425 (* 1 = 0.4425 loss)
I0926 09:25:03.322044  4406 sgd_solver.cpp:105] Iteration 12400, lr = 0.01
I0926 09:25:17.441187  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:25:18.033198  4406 solver.cpp:330] Iteration 12500, Testing net (#0)
I0926 09:25:21.509088  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:25:21.654094  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8011
I0926 09:25:21.654120  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.578174 (* 1 = 0.578174 loss)
I0926 09:25:21.800015  4406 solver.cpp:218] Iteration 12500 (5.41186 iter/s, 18.4779s/100 iters), loss = 0.477192
I0926 09:25:21.800062  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.477192 (* 1 = 0.477192 loss)
I0926 09:25:21.800071  4406 sgd_solver.cpp:105] Iteration 12500, lr = 0.01
I0926 09:25:36.603214  4406 solver.cpp:218] Iteration 12600 (6.75533 iter/s, 14.8031s/100 iters), loss = 0.368073
I0926 09:25:36.603242  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.368073 (* 1 = 0.368073 loss)
I0926 09:25:36.603248  4406 sgd_solver.cpp:105] Iteration 12600, lr = 0.01
I0926 09:25:51.394256  4406 solver.cpp:218] Iteration 12700 (6.76088 iter/s, 14.791s/100 iters), loss = 0.32801
I0926 09:25:51.394336  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.32801 (* 1 = 0.32801 loss)
I0926 09:25:51.394353  4406 sgd_solver.cpp:105] Iteration 12700, lr = 0.01
I0926 09:26:06.077069  4406 solver.cpp:218] Iteration 12800 (6.81074 iter/s, 14.6827s/100 iters), loss = 0.586482
I0926 09:26:06.077098  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.586482 (* 1 = 0.586482 loss)
I0926 09:26:06.077105  4406 sgd_solver.cpp:105] Iteration 12800, lr = 0.01
I0926 09:26:20.798843  4406 solver.cpp:218] Iteration 12900 (6.79269 iter/s, 14.7217s/100 iters), loss = 0.423534
I0926 09:26:20.798883  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.423534 (* 1 = 0.423534 loss)
I0926 09:26:20.798889  4406 sgd_solver.cpp:105] Iteration 12900, lr = 0.01
I0926 09:26:34.748492  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:26:35.333840  4406 solver.cpp:330] Iteration 13000, Testing net (#0)
I0926 09:26:38.764562  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:26:38.907526  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.804
I0926 09:26:38.907562  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.564578 (* 1 = 0.564578 loss)
I0926 09:26:39.053444  4406 solver.cpp:218] Iteration 13000 (5.4781 iter/s, 18.2545s/100 iters), loss = 0.427092
I0926 09:26:39.053477  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.427092 (* 1 = 0.427092 loss)
I0926 09:26:39.053483  4406 sgd_solver.cpp:105] Iteration 13000, lr = 0.01
I0926 09:26:53.757187  4406 solver.cpp:218] Iteration 13100 (6.80102 iter/s, 14.7037s/100 iters), loss = 0.368163
I0926 09:26:53.757217  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.368163 (* 1 = 0.368163 loss)
I0926 09:26:53.757225  4406 sgd_solver.cpp:105] Iteration 13100, lr = 0.01
I0926 09:27:08.449065  4406 solver.cpp:218] Iteration 13200 (6.80651 iter/s, 14.6918s/100 iters), loss = 0.429822
I0926 09:27:08.449162  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.429822 (* 1 = 0.429822 loss)
I0926 09:27:08.449168  4406 sgd_solver.cpp:105] Iteration 13200, lr = 0.01
I0926 09:27:23.115093  4406 solver.cpp:218] Iteration 13300 (6.81854 iter/s, 14.6659s/100 iters), loss = 0.432848
I0926 09:27:23.115125  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.432848 (* 1 = 0.432848 loss)
I0926 09:27:23.115134  4406 sgd_solver.cpp:105] Iteration 13300, lr = 0.01
I0926 09:27:37.829144  4406 solver.cpp:218] Iteration 13400 (6.79625 iter/s, 14.714s/100 iters), loss = 0.335798
I0926 09:27:37.829174  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.335798 (* 1 = 0.335798 loss)
I0926 09:27:37.829180  4406 sgd_solver.cpp:105] Iteration 13400, lr = 0.01
I0926 09:27:51.796658  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:27:52.382021  4406 solver.cpp:330] Iteration 13500, Testing net (#0)
I0926 09:27:55.825505  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:27:55.970564  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8047
I0926 09:27:55.970602  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.565913 (* 1 = 0.565913 loss)
I0926 09:27:56.115612  4406 solver.cpp:218] Iteration 13500 (5.46855 iter/s, 18.2864s/100 iters), loss = 0.339044
I0926 09:27:56.115643  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339044 (* 1 = 0.339044 loss)
I0926 09:27:56.115651  4406 sgd_solver.cpp:105] Iteration 13500, lr = 0.01
I0926 09:28:10.872403  4406 solver.cpp:218] Iteration 13600 (6.77659 iter/s, 14.7567s/100 iters), loss = 0.356309
I0926 09:28:10.872432  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.356309 (* 1 = 0.356309 loss)
I0926 09:28:10.872438  4406 sgd_solver.cpp:105] Iteration 13600, lr = 0.01
I0926 09:28:25.519362  4406 solver.cpp:218] Iteration 13700 (6.82739 iter/s, 14.6469s/100 iters), loss = 0.423461
I0926 09:28:25.519495  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.423461 (* 1 = 0.423461 loss)
I0926 09:28:25.519512  4406 sgd_solver.cpp:105] Iteration 13700, lr = 0.01
I0926 09:28:40.219864  4406 solver.cpp:218] Iteration 13800 (6.80256 iter/s, 14.7003s/100 iters), loss = 0.545393
I0926 09:28:40.219893  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.545393 (* 1 = 0.545393 loss)
I0926 09:28:40.219900  4406 sgd_solver.cpp:105] Iteration 13800, lr = 0.01
I0926 09:28:54.902400  4406 solver.cpp:218] Iteration 13900 (6.81084 iter/s, 14.6825s/100 iters), loss = 0.389882
I0926 09:28:54.902436  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.389882 (* 1 = 0.389882 loss)
I0926 09:28:54.902443  4406 sgd_solver.cpp:105] Iteration 13900, lr = 0.01
I0926 09:29:08.891109  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:29:09.497263  4406 solver.cpp:330] Iteration 14000, Testing net (#0)
I0926 09:29:12.961885  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:29:13.104151  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7996
I0926 09:29:13.104176  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.590569 (* 1 = 0.590569 loss)
I0926 09:29:13.248628  4406 solver.cpp:218] Iteration 14000 (5.45079 iter/s, 18.346s/100 iters), loss = 0.332769
I0926 09:29:13.248661  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332769 (* 1 = 0.332769 loss)
I0926 09:29:13.248667  4406 sgd_solver.cpp:105] Iteration 14000, lr = 0.01
I0926 09:29:27.967864  4406 solver.cpp:218] Iteration 14100 (6.79386 iter/s, 14.7192s/100 iters), loss = 0.372696
I0926 09:29:27.967895  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372695 (* 1 = 0.372695 loss)
I0926 09:29:27.967900  4406 sgd_solver.cpp:105] Iteration 14100, lr = 0.01
I0926 09:29:42.756326  4406 solver.cpp:218] Iteration 14200 (6.76206 iter/s, 14.7884s/100 iters), loss = 0.377064
I0926 09:29:42.756443  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.377064 (* 1 = 0.377064 loss)
I0926 09:29:42.756460  4406 sgd_solver.cpp:105] Iteration 14200, lr = 0.01
I0926 09:29:57.474889  4406 solver.cpp:218] Iteration 14300 (6.79424 iter/s, 14.7183s/100 iters), loss = 0.44339
I0926 09:29:57.474923  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44339 (* 1 = 0.44339 loss)
I0926 09:29:57.474930  4406 sgd_solver.cpp:105] Iteration 14300, lr = 0.01
I0926 09:30:12.160346  4406 solver.cpp:218] Iteration 14400 (6.80949 iter/s, 14.6854s/100 iters), loss = 0.397819
I0926 09:30:12.160377  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.397819 (* 1 = 0.397819 loss)
I0926 09:30:12.160383  4406 sgd_solver.cpp:105] Iteration 14400, lr = 0.01
I0926 09:30:26.132817  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:30:26.719826  4406 solver.cpp:330] Iteration 14500, Testing net (#0)
I0926 09:30:30.160924  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:30:30.304456  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8103
I0926 09:30:30.304492  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.560124 (* 1 = 0.560124 loss)
I0926 09:30:30.450078  4406 solver.cpp:218] Iteration 14500 (5.46757 iter/s, 18.2897s/100 iters), loss = 0.454518
I0926 09:30:30.450109  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.454518 (* 1 = 0.454518 loss)
I0926 09:30:30.450116  4406 sgd_solver.cpp:105] Iteration 14500, lr = 0.01
I0926 09:30:45.156358  4406 solver.cpp:218] Iteration 14600 (6.79985 iter/s, 14.7062s/100 iters), loss = 0.346139
I0926 09:30:45.156397  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.346139 (* 1 = 0.346139 loss)
I0926 09:30:45.156404  4406 sgd_solver.cpp:105] Iteration 14600, lr = 0.01
I0926 09:30:59.817106  4406 solver.cpp:218] Iteration 14700 (6.82097 iter/s, 14.6607s/100 iters), loss = 0.395384
I0926 09:30:59.817291  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.395384 (* 1 = 0.395384 loss)
I0926 09:30:59.817309  4406 sgd_solver.cpp:105] Iteration 14700, lr = 0.01
I0926 09:31:14.516352  4406 solver.cpp:218] Iteration 14800 (6.80316 iter/s, 14.699s/100 iters), loss = 0.419111
I0926 09:31:14.516384  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419111 (* 1 = 0.419111 loss)
I0926 09:31:14.516391  4406 sgd_solver.cpp:105] Iteration 14800, lr = 0.01
I0926 09:31:29.209782  4406 solver.cpp:218] Iteration 14900 (6.8058 iter/s, 14.6934s/100 iters), loss = 0.358964
I0926 09:31:29.209815  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.358964 (* 1 = 0.358964 loss)
I0926 09:31:29.209820  4406 sgd_solver.cpp:105] Iteration 14900, lr = 0.01
I0926 09:31:43.157991  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:31:43.744436  4406 solver.cpp:330] Iteration 15000, Testing net (#0)
I0926 09:31:47.177479  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:31:47.321902  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8028
I0926 09:31:47.321938  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.58127 (* 1 = 0.58127 loss)
I0926 09:31:47.467747  4406 solver.cpp:218] Iteration 15000 (5.47708 iter/s, 18.2579s/100 iters), loss = 0.419136
I0926 09:31:47.467790  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419136 (* 1 = 0.419136 loss)
I0926 09:31:47.467797  4406 sgd_solver.cpp:105] Iteration 15000, lr = 0.01
I0926 09:32:02.183032  4406 solver.cpp:218] Iteration 15100 (6.79569 iter/s, 14.7152s/100 iters), loss = 0.332668
I0926 09:32:02.183060  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332668 (* 1 = 0.332668 loss)
I0926 09:32:02.183066  4406 sgd_solver.cpp:105] Iteration 15100, lr = 0.01
I0926 09:32:16.896081  4406 solver.cpp:218] Iteration 15200 (6.79672 iter/s, 14.713s/100 iters), loss = 0.31524
I0926 09:32:16.896194  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.31524 (* 1 = 0.31524 loss)
I0926 09:32:16.896203  4406 sgd_solver.cpp:105] Iteration 15200, lr = 0.01
I0926 09:32:31.582212  4406 solver.cpp:218] Iteration 15300 (6.80921 iter/s, 14.686s/100 iters), loss = 0.424725
I0926 09:32:31.582242  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.424724 (* 1 = 0.424724 loss)
I0926 09:32:31.582247  4406 sgd_solver.cpp:105] Iteration 15300, lr = 0.01
I0926 09:32:46.304303  4406 solver.cpp:218] Iteration 15400 (6.79254 iter/s, 14.722s/100 iters), loss = 0.356519
I0926 09:32:46.304335  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.356519 (* 1 = 0.356519 loss)
I0926 09:32:46.304344  4406 sgd_solver.cpp:105] Iteration 15400, lr = 0.01
I0926 09:33:00.291942  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:33:00.877703  4406 solver.cpp:330] Iteration 15500, Testing net (#0)
I0926 09:33:04.320634  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:33:04.464812  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8109
I0926 09:33:04.464838  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.553897 (* 1 = 0.553897 loss)
I0926 09:33:04.609308  4406 solver.cpp:218] Iteration 15500 (5.46301 iter/s, 18.3049s/100 iters), loss = 0.407435
I0926 09:33:04.609342  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.407435 (* 1 = 0.407435 loss)
I0926 09:33:04.609351  4406 sgd_solver.cpp:105] Iteration 15500, lr = 0.01
I0926 09:33:19.316603  4406 solver.cpp:218] Iteration 15600 (6.79938 iter/s, 14.7072s/100 iters), loss = 0.345513
I0926 09:33:19.316638  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.345513 (* 1 = 0.345513 loss)
I0926 09:33:19.316648  4406 sgd_solver.cpp:105] Iteration 15600, lr = 0.01
I0926 09:33:34.044838  4406 solver.cpp:218] Iteration 15700 (6.78971 iter/s, 14.7282s/100 iters), loss = 0.413211
I0926 09:33:34.044966  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.413211 (* 1 = 0.413211 loss)
I0926 09:33:34.044975  4406 sgd_solver.cpp:105] Iteration 15700, lr = 0.01
I0926 09:33:48.729110  4406 solver.cpp:218] Iteration 15800 (6.81008 iter/s, 14.6841s/100 iters), loss = 0.486177
I0926 09:33:48.729148  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.486176 (* 1 = 0.486176 loss)
I0926 09:33:48.729156  4406 sgd_solver.cpp:105] Iteration 15800, lr = 0.01
I0926 09:34:03.422834  4406 solver.cpp:218] Iteration 15900 (6.80566 iter/s, 14.6936s/100 iters), loss = 0.309107
I0926 09:34:03.422863  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309107 (* 1 = 0.309107 loss)
I0926 09:34:03.422869  4406 sgd_solver.cpp:105] Iteration 15900, lr = 0.01
I0926 09:34:17.405654  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:34:17.991120  4406 solver.cpp:330] Iteration 16000, Testing net (#0)
I0926 09:34:21.434594  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:34:21.578467  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8125
I0926 09:34:21.578502  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.549981 (* 1 = 0.549981 loss)
I0926 09:34:21.723275  4406 solver.cpp:218] Iteration 16000 (5.46437 iter/s, 18.3004s/100 iters), loss = 0.354363
I0926 09:34:21.723304  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.354362 (* 1 = 0.354362 loss)
I0926 09:34:21.723310  4406 sgd_solver.cpp:105] Iteration 16000, lr = 0.01
I0926 09:34:36.388818  4406 solver.cpp:218] Iteration 16100 (6.81874 iter/s, 14.6655s/100 iters), loss = 0.380961
I0926 09:34:36.388851  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.380961 (* 1 = 0.380961 loss)
I0926 09:34:36.388859  4406 sgd_solver.cpp:105] Iteration 16100, lr = 0.01
I0926 09:34:51.102826  4406 solver.cpp:218] Iteration 16200 (6.79628 iter/s, 14.7139s/100 iters), loss = 0.408605
I0926 09:34:51.102952  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.408605 (* 1 = 0.408605 loss)
I0926 09:34:51.102972  4406 sgd_solver.cpp:105] Iteration 16200, lr = 0.01
I0926 09:35:05.812670  4406 solver.cpp:218] Iteration 16300 (6.79827 iter/s, 14.7096s/100 iters), loss = 0.488543
I0926 09:35:05.812701  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.488543 (* 1 = 0.488543 loss)
I0926 09:35:05.812708  4406 sgd_solver.cpp:105] Iteration 16300, lr = 0.01
I0926 09:35:20.479739  4406 solver.cpp:218] Iteration 16400 (6.81803 iter/s, 14.667s/100 iters), loss = 0.406126
I0926 09:35:20.479773  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.406126 (* 1 = 0.406126 loss)
I0926 09:35:20.479779  4406 sgd_solver.cpp:105] Iteration 16400, lr = 0.01
I0926 09:35:34.478368  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:35:35.063540  4406 solver.cpp:330] Iteration 16500, Testing net (#0)
I0926 09:35:38.498697  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:35:38.641181  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.809
I0926 09:35:38.641217  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.55985 (* 1 = 0.55985 loss)
I0926 09:35:38.786300  4406 solver.cpp:218] Iteration 16500 (5.46255 iter/s, 18.3065s/100 iters), loss = 0.380777
I0926 09:35:38.786330  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.380777 (* 1 = 0.380777 loss)
I0926 09:35:38.786336  4406 sgd_solver.cpp:105] Iteration 16500, lr = 0.01
I0926 09:35:53.498126  4406 solver.cpp:218] Iteration 16600 (6.79728 iter/s, 14.7118s/100 iters), loss = 0.293889
I0926 09:35:53.498159  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293889 (* 1 = 0.293889 loss)
I0926 09:35:53.498165  4406 sgd_solver.cpp:105] Iteration 16600, lr = 0.01
I0926 09:36:08.181689  4406 solver.cpp:218] Iteration 16700 (6.81037 iter/s, 14.6835s/100 iters), loss = 0.365853
I0926 09:36:08.181816  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.365853 (* 1 = 0.365853 loss)
I0926 09:36:08.181825  4406 sgd_solver.cpp:105] Iteration 16700, lr = 0.01
I0926 09:36:22.898216  4406 solver.cpp:218] Iteration 16800 (6.79516 iter/s, 14.7164s/100 iters), loss = 0.378391
I0926 09:36:22.898247  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.378391 (* 1 = 0.378391 loss)
I0926 09:36:22.898262  4406 sgd_solver.cpp:105] Iteration 16800, lr = 0.01
I0926 09:36:37.588363  4406 solver.cpp:218] Iteration 16900 (6.80732 iter/s, 14.6901s/100 iters), loss = 0.297554
I0926 09:36:37.588397  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.297554 (* 1 = 0.297554 loss)
I0926 09:36:37.588402  4406 sgd_solver.cpp:105] Iteration 16900, lr = 0.01
I0926 09:36:51.544224  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:36:52.130844  4406 solver.cpp:330] Iteration 17000, Testing net (#0)
I0926 09:36:55.579092  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:36:55.721624  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8135
I0926 09:36:55.721650  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.544359 (* 1 = 0.544359 loss)
I0926 09:36:55.866796  4406 solver.cpp:218] Iteration 17000 (5.47095 iter/s, 18.2784s/100 iters), loss = 0.322272
I0926 09:36:55.866827  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.322272 (* 1 = 0.322272 loss)
I0926 09:36:55.866832  4406 sgd_solver.cpp:105] Iteration 17000, lr = 0.01
I0926 09:37:10.571542  4406 solver.cpp:218] Iteration 17100 (6.80056 iter/s, 14.7047s/100 iters), loss = 0.292604
I0926 09:37:10.571573  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292604 (* 1 = 0.292604 loss)
I0926 09:37:10.571581  4406 sgd_solver.cpp:105] Iteration 17100, lr = 0.01
I0926 09:37:25.293964  4406 solver.cpp:218] Iteration 17200 (6.79239 iter/s, 14.7224s/100 iters), loss = 0.325548
I0926 09:37:25.294075  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.325548 (* 1 = 0.325548 loss)
I0926 09:37:25.294081  4406 sgd_solver.cpp:105] Iteration 17200, lr = 0.01
I0926 09:37:39.971400  4406 solver.cpp:218] Iteration 17300 (6.81325 iter/s, 14.6773s/100 iters), loss = 0.394361
I0926 09:37:39.971433  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394361 (* 1 = 0.394361 loss)
I0926 09:37:39.971439  4406 sgd_solver.cpp:105] Iteration 17300, lr = 0.01
I0926 09:37:54.677134  4406 solver.cpp:218] Iteration 17400 (6.8001 iter/s, 14.7057s/100 iters), loss = 0.364065
I0926 09:37:54.677165  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364065 (* 1 = 0.364065 loss)
I0926 09:37:54.677171  4406 sgd_solver.cpp:105] Iteration 17400, lr = 0.01
I0926 09:38:08.656353  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:38:09.240694  4406 solver.cpp:330] Iteration 17500, Testing net (#0)
I0926 09:38:12.684139  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:38:12.827648  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8217
I0926 09:38:12.827683  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.520136 (* 1 = 0.520136 loss)
I0926 09:38:12.973111  4406 solver.cpp:218] Iteration 17500 (5.46571 iter/s, 18.2959s/100 iters), loss = 0.454511
I0926 09:38:12.973145  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.454511 (* 1 = 0.454511 loss)
I0926 09:38:12.973151  4406 sgd_solver.cpp:105] Iteration 17500, lr = 0.01
I0926 09:38:27.643566  4406 solver.cpp:218] Iteration 17600 (6.81645 iter/s, 14.6704s/100 iters), loss = 0.286975
I0926 09:38:27.643606  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286975 (* 1 = 0.286975 loss)
I0926 09:38:27.643612  4406 sgd_solver.cpp:105] Iteration 17600, lr = 0.01
I0926 09:38:42.367560  4406 solver.cpp:218] Iteration 17700 (6.79167 iter/s, 14.7239s/100 iters), loss = 0.289164
I0926 09:38:42.367702  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289164 (* 1 = 0.289164 loss)
I0926 09:38:42.367710  4406 sgd_solver.cpp:105] Iteration 17700, lr = 0.01
I0926 09:38:57.086465  4406 solver.cpp:218] Iteration 17800 (6.79406 iter/s, 14.7187s/100 iters), loss = 0.346085
I0926 09:38:57.086496  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.346085 (* 1 = 0.346085 loss)
I0926 09:38:57.086503  4406 sgd_solver.cpp:105] Iteration 17800, lr = 0.01
I0926 09:39:11.765395  4406 solver.cpp:218] Iteration 17900 (6.81252 iter/s, 14.6789s/100 iters), loss = 0.339314
I0926 09:39:11.765427  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339314 (* 1 = 0.339314 loss)
I0926 09:39:11.765444  4406 sgd_solver.cpp:105] Iteration 17900, lr = 0.01
I0926 09:39:25.751551  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:39:26.340512  4406 solver.cpp:330] Iteration 18000, Testing net (#0)
I0926 09:39:29.776449  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:39:29.919762  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7998
I0926 09:39:29.919791  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.587988 (* 1 = 0.587988 loss)
I0926 09:39:30.070406  4406 solver.cpp:218] Iteration 18000 (5.46301 iter/s, 18.3049s/100 iters), loss = 0.348445
I0926 09:39:30.070439  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348445 (* 1 = 0.348445 loss)
I0926 09:39:30.070446  4406 sgd_solver.cpp:105] Iteration 18000, lr = 0.01
I0926 09:39:44.806480  4406 solver.cpp:218] Iteration 18100 (6.7861 iter/s, 14.736s/100 iters), loss = 0.2483
I0926 09:39:44.806511  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2483 (* 1 = 0.2483 loss)
I0926 09:39:44.806517  4406 sgd_solver.cpp:105] Iteration 18100, lr = 0.01
I0926 09:39:59.486147  4406 solver.cpp:218] Iteration 18200 (6.81218 iter/s, 14.6796s/100 iters), loss = 0.257996
I0926 09:39:59.486265  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257996 (* 1 = 0.257996 loss)
I0926 09:39:59.486274  4406 sgd_solver.cpp:105] Iteration 18200, lr = 0.01
I0926 09:40:14.195816  4406 solver.cpp:218] Iteration 18300 (6.79832 iter/s, 14.7095s/100 iters), loss = 0.409604
I0926 09:40:14.195849  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.409604 (* 1 = 0.409604 loss)
I0926 09:40:14.195857  4406 sgd_solver.cpp:105] Iteration 18300, lr = 0.01
I0926 09:40:28.914242  4406 solver.cpp:218] Iteration 18400 (6.79424 iter/s, 14.7184s/100 iters), loss = 0.32149
I0926 09:40:28.914274  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.32149 (* 1 = 0.32149 loss)
I0926 09:40:28.914280  4406 sgd_solver.cpp:105] Iteration 18400, lr = 0.01
I0926 09:40:42.872351  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:40:43.459185  4406 solver.cpp:330] Iteration 18500, Testing net (#0)
I0926 09:40:46.898129  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:40:47.041501  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8118
I0926 09:40:47.041537  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.543884 (* 1 = 0.543884 loss)
I0926 09:40:47.186977  4406 solver.cpp:218] Iteration 18500 (5.47266 iter/s, 18.2727s/100 iters), loss = 0.34486
I0926 09:40:47.187011  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.34486 (* 1 = 0.34486 loss)
I0926 09:40:47.187016  4406 sgd_solver.cpp:105] Iteration 18500, lr = 0.01
I0926 09:41:01.915187  4406 solver.cpp:218] Iteration 18600 (6.78972 iter/s, 14.7281s/100 iters), loss = 0.316638
I0926 09:41:01.915217  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316638 (* 1 = 0.316638 loss)
I0926 09:41:01.915225  4406 sgd_solver.cpp:105] Iteration 18600, lr = 0.01
I0926 09:41:16.625041  4406 solver.cpp:218] Iteration 18700 (6.7982 iter/s, 14.7098s/100 iters), loss = 0.319863
I0926 09:41:16.625162  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319863 (* 1 = 0.319863 loss)
I0926 09:41:16.625180  4406 sgd_solver.cpp:105] Iteration 18700, lr = 0.01
I0926 09:41:31.311066  4406 solver.cpp:218] Iteration 18800 (6.8093 iter/s, 14.6858s/100 iters), loss = 0.487938
I0926 09:41:31.311096  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.487938 (* 1 = 0.487938 loss)
I0926 09:41:31.311103  4406 sgd_solver.cpp:105] Iteration 18800, lr = 0.01
I0926 09:41:46.033668  4406 solver.cpp:218] Iteration 18900 (6.79231 iter/s, 14.7225s/100 iters), loss = 0.375725
I0926 09:41:46.033709  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.375725 (* 1 = 0.375725 loss)
I0926 09:41:46.033715  4406 sgd_solver.cpp:105] Iteration 18900, lr = 0.01
I0926 09:42:00.015933  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:42:00.610994  4406 solver.cpp:330] Iteration 19000, Testing net (#0)
I0926 09:42:04.046093  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:42:04.190752  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8161
I0926 09:42:04.190780  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.533337 (* 1 = 0.533337 loss)
I0926 09:42:04.337424  4406 solver.cpp:218] Iteration 19000 (5.46338 iter/s, 18.3037s/100 iters), loss = 0.293111
I0926 09:42:04.337456  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293111 (* 1 = 0.293111 loss)
I0926 09:42:04.337463  4406 sgd_solver.cpp:105] Iteration 19000, lr = 0.01
I0926 09:42:19.015483  4406 solver.cpp:218] Iteration 19100 (6.81292 iter/s, 14.678s/100 iters), loss = 0.274589
I0926 09:42:19.015523  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.274589 (* 1 = 0.274589 loss)
I0926 09:42:19.015529  4406 sgd_solver.cpp:105] Iteration 19100, lr = 0.01
I0926 09:42:33.740365  4406 solver.cpp:218] Iteration 19200 (6.79126 iter/s, 14.7248s/100 iters), loss = 0.353785
I0926 09:42:33.740463  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353785 (* 1 = 0.353785 loss)
I0926 09:42:33.740479  4406 sgd_solver.cpp:105] Iteration 19200, lr = 0.01
I0926 09:42:48.457239  4406 solver.cpp:218] Iteration 19300 (6.79498 iter/s, 14.7167s/100 iters), loss = 0.410675
I0926 09:42:48.457269  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.410675 (* 1 = 0.410675 loss)
I0926 09:42:48.457275  4406 sgd_solver.cpp:105] Iteration 19300, lr = 0.01
I0926 09:43:03.143709  4406 solver.cpp:218] Iteration 19400 (6.80902 iter/s, 14.6864s/100 iters), loss = 0.325806
I0926 09:43:03.143751  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.325806 (* 1 = 0.325806 loss)
I0926 09:43:03.143759  4406 sgd_solver.cpp:105] Iteration 19400, lr = 0.01
I0926 09:43:17.099035  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:43:17.688632  4406 solver.cpp:330] Iteration 19500, Testing net (#0)
I0926 09:43:21.132588  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:43:21.275210  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8158
I0926 09:43:21.275246  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.540744 (* 1 = 0.540744 loss)
I0926 09:43:21.420692  4406 solver.cpp:218] Iteration 19500 (5.47139 iter/s, 18.2769s/100 iters), loss = 0.293671
I0926 09:43:21.420722  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293671 (* 1 = 0.293671 loss)
I0926 09:43:21.420729  4406 sgd_solver.cpp:105] Iteration 19500, lr = 0.01
I0926 09:43:36.121228  4406 solver.cpp:218] Iteration 19600 (6.8025 iter/s, 14.7005s/100 iters), loss = 0.31841
I0926 09:43:36.121261  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.31841 (* 1 = 0.31841 loss)
I0926 09:43:36.121268  4406 sgd_solver.cpp:105] Iteration 19600, lr = 0.01
I0926 09:43:50.837105  4406 solver.cpp:218] Iteration 19700 (6.79541 iter/s, 14.7158s/100 iters), loss = 0.334305
I0926 09:43:50.837262  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334305 (* 1 = 0.334305 loss)
I0926 09:43:50.837271  4406 sgd_solver.cpp:105] Iteration 19700, lr = 0.01
I0926 09:44:05.553612  4406 solver.cpp:218] Iteration 19800 (6.79518 iter/s, 14.7163s/100 iters), loss = 0.405303
I0926 09:44:05.553647  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.405303 (* 1 = 0.405303 loss)
I0926 09:44:05.553653  4406 sgd_solver.cpp:105] Iteration 19800, lr = 0.01
I0926 09:44:20.198544  4406 solver.cpp:218] Iteration 19900 (6.82833 iter/s, 14.6449s/100 iters), loss = 0.266893
I0926 09:44:20.198573  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266893 (* 1 = 0.266893 loss)
I0926 09:44:20.198580  4406 sgd_solver.cpp:105] Iteration 19900, lr = 0.01
I0926 09:44:34.132776  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:44:34.721212  4406 solver.cpp:330] Iteration 20000, Testing net (#0)
I0926 09:44:38.165736  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:44:38.308636  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8124
I0926 09:44:38.308661  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.541649 (* 1 = 0.541649 loss)
I0926 09:44:38.453104  4406 solver.cpp:218] Iteration 20000 (5.47811 iter/s, 18.2545s/100 iters), loss = 0.278763
I0926 09:44:38.453133  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278763 (* 1 = 0.278763 loss)
I0926 09:44:38.453140  4406 sgd_solver.cpp:105] Iteration 20000, lr = 0.01
I0926 09:44:53.095340  4406 solver.cpp:218] Iteration 20100 (6.82959 iter/s, 14.6422s/100 iters), loss = 0.254636
I0926 09:44:53.095371  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254636 (* 1 = 0.254636 loss)
I0926 09:44:53.095376  4406 sgd_solver.cpp:105] Iteration 20100, lr = 0.01
I0926 09:45:07.817811  4406 solver.cpp:218] Iteration 20200 (6.79237 iter/s, 14.7224s/100 iters), loss = 0.364403
I0926 09:45:07.817930  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364403 (* 1 = 0.364403 loss)
I0926 09:45:07.817937  4406 sgd_solver.cpp:105] Iteration 20200, lr = 0.01
I0926 09:45:22.478173  4406 solver.cpp:218] Iteration 20300 (6.82119 iter/s, 14.6602s/100 iters), loss = 0.382402
I0926 09:45:22.478214  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.382402 (* 1 = 0.382402 loss)
I0926 09:45:22.478220  4406 sgd_solver.cpp:105] Iteration 20300, lr = 0.01
I0926 09:45:37.159039  4406 solver.cpp:218] Iteration 20400 (6.81163 iter/s, 14.6808s/100 iters), loss = 0.413754
I0926 09:45:37.159072  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.413754 (* 1 = 0.413754 loss)
I0926 09:45:37.159080  4406 sgd_solver.cpp:105] Iteration 20400, lr = 0.01
I0926 09:45:51.141069  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:45:51.730968  4406 solver.cpp:330] Iteration 20500, Testing net (#0)
I0926 09:45:55.195380  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:45:55.343241  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8189
I0926 09:45:55.343266  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.537229 (* 1 = 0.537229 loss)
I0926 09:45:55.487831  4406 solver.cpp:218] Iteration 20500 (5.45592 iter/s, 18.3287s/100 iters), loss = 0.308131
I0926 09:45:55.487862  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.308131 (* 1 = 0.308131 loss)
I0926 09:45:55.487869  4406 sgd_solver.cpp:105] Iteration 20500, lr = 0.01
I0926 09:46:10.162340  4406 solver.cpp:218] Iteration 20600 (6.81457 iter/s, 14.6744s/100 iters), loss = 0.231969
I0926 09:46:10.162370  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231969 (* 1 = 0.231969 loss)
I0926 09:46:10.162376  4406 sgd_solver.cpp:105] Iteration 20600, lr = 0.01
I0926 09:46:24.862330  4406 solver.cpp:218] Iteration 20700 (6.80276 iter/s, 14.6999s/100 iters), loss = 0.283863
I0926 09:46:24.862447  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283863 (* 1 = 0.283863 loss)
I0926 09:46:24.862464  4406 sgd_solver.cpp:105] Iteration 20700, lr = 0.01
I0926 09:46:39.565488  4406 solver.cpp:218] Iteration 20800 (6.80133 iter/s, 14.703s/100 iters), loss = 0.306836
I0926 09:46:39.565522  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306836 (* 1 = 0.306836 loss)
I0926 09:46:39.565529  4406 sgd_solver.cpp:105] Iteration 20800, lr = 0.01
I0926 09:46:54.255923  4406 solver.cpp:218] Iteration 20900 (6.80718 iter/s, 14.6904s/100 iters), loss = 0.301136
I0926 09:46:54.255955  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301136 (* 1 = 0.301136 loss)
I0926 09:46:54.255961  4406 sgd_solver.cpp:105] Iteration 20900, lr = 0.01
I0926 09:47:08.259182  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:47:08.846901  4406 solver.cpp:330] Iteration 21000, Testing net (#0)
I0926 09:47:12.282171  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:47:12.426939  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8238
I0926 09:47:12.426964  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.509206 (* 1 = 0.509206 loss)
I0926 09:47:12.572026  4406 solver.cpp:218] Iteration 21000 (5.4597 iter/s, 18.316s/100 iters), loss = 0.255679
I0926 09:47:12.572062  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255679 (* 1 = 0.255679 loss)
I0926 09:47:12.572067  4406 sgd_solver.cpp:105] Iteration 21000, lr = 0.01
I0926 09:47:27.270951  4406 solver.cpp:218] Iteration 21100 (6.80325 iter/s, 14.6989s/100 iters), loss = 0.334432
I0926 09:47:27.270983  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334432 (* 1 = 0.334432 loss)
I0926 09:47:27.270989  4406 sgd_solver.cpp:105] Iteration 21100, lr = 0.01
I0926 09:47:41.943724  4406 solver.cpp:218] Iteration 21200 (6.81538 iter/s, 14.6727s/100 iters), loss = 0.265966
I0926 09:47:41.943852  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265966 (* 1 = 0.265966 loss)
I0926 09:47:41.943861  4406 sgd_solver.cpp:105] Iteration 21200, lr = 0.01
I0926 09:47:56.658780  4406 solver.cpp:218] Iteration 21300 (6.79584 iter/s, 14.7149s/100 iters), loss = 0.283925
I0926 09:47:56.658812  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283925 (* 1 = 0.283925 loss)
I0926 09:47:56.658818  4406 sgd_solver.cpp:105] Iteration 21300, lr = 0.01
I0926 09:48:11.376900  4406 solver.cpp:218] Iteration 21400 (6.79438 iter/s, 14.718s/100 iters), loss = 0.339303
I0926 09:48:11.376933  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.339303 (* 1 = 0.339303 loss)
I0926 09:48:11.376940  4406 sgd_solver.cpp:105] Iteration 21400, lr = 0.01
I0926 09:48:25.310947  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:48:25.901854  4406 solver.cpp:330] Iteration 21500, Testing net (#0)
I0926 09:48:29.361706  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:48:29.504081  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8244
I0926 09:48:29.504119  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.516039 (* 1 = 0.516039 loss)
I0926 09:48:29.650243  4406 solver.cpp:218] Iteration 21500 (5.47248 iter/s, 18.2733s/100 iters), loss = 0.249773
I0926 09:48:29.650276  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249773 (* 1 = 0.249773 loss)
I0926 09:48:29.650282  4406 sgd_solver.cpp:105] Iteration 21500, lr = 0.01
I0926 09:48:44.371493  4406 solver.cpp:218] Iteration 21600 (6.79293 iter/s, 14.7212s/100 iters), loss = 0.332206
I0926 09:48:44.371528  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332206 (* 1 = 0.332206 loss)
I0926 09:48:44.371536  4406 sgd_solver.cpp:105] Iteration 21600, lr = 0.01
I0926 09:48:59.062449  4406 solver.cpp:218] Iteration 21700 (6.80694 iter/s, 14.6909s/100 iters), loss = 0.302445
I0926 09:48:59.062554  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.302445 (* 1 = 0.302445 loss)
I0926 09:48:59.062574  4406 sgd_solver.cpp:105] Iteration 21700, lr = 0.01
I0926 09:49:13.779083  4406 solver.cpp:218] Iteration 21800 (6.79509 iter/s, 14.7165s/100 iters), loss = 0.347537
I0926 09:49:13.779122  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.347537 (* 1 = 0.347537 loss)
I0926 09:49:13.779129  4406 sgd_solver.cpp:105] Iteration 21800, lr = 0.01
I0926 09:49:28.503975  4406 solver.cpp:218] Iteration 21900 (6.79126 iter/s, 14.7248s/100 iters), loss = 0.301924
I0926 09:49:28.504015  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301924 (* 1 = 0.301924 loss)
I0926 09:49:28.504021  4406 sgd_solver.cpp:105] Iteration 21900, lr = 0.01
I0926 09:49:42.456647  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:49:43.058991  4406 solver.cpp:330] Iteration 22000, Testing net (#0)
I0926 09:49:46.515748  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:49:46.658799  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8112
I0926 09:49:46.658835  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.564244 (* 1 = 0.564244 loss)
I0926 09:49:46.804800  4406 solver.cpp:218] Iteration 22000 (5.46426 iter/s, 18.3007s/100 iters), loss = 0.432919
I0926 09:49:46.804831  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.432919 (* 1 = 0.432919 loss)
I0926 09:49:46.804838  4406 sgd_solver.cpp:105] Iteration 22000, lr = 0.01
I0926 09:50:01.529126  4406 solver.cpp:218] Iteration 22100 (6.79151 iter/s, 14.7243s/100 iters), loss = 0.218717
I0926 09:50:01.529155  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218717 (* 1 = 0.218717 loss)
I0926 09:50:01.529161  4406 sgd_solver.cpp:105] Iteration 22100, lr = 0.01
I0926 09:50:16.208405  4406 solver.cpp:218] Iteration 22200 (6.81236 iter/s, 14.6792s/100 iters), loss = 0.265558
I0926 09:50:16.208534  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265557 (* 1 = 0.265557 loss)
I0926 09:50:16.208554  4406 sgd_solver.cpp:105] Iteration 22200, lr = 0.01
I0926 09:50:30.903645  4406 solver.cpp:218] Iteration 22300 (6.805 iter/s, 14.6951s/100 iters), loss = 0.378139
I0926 09:50:30.903676  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.378139 (* 1 = 0.378139 loss)
I0926 09:50:30.903681  4406 sgd_solver.cpp:105] Iteration 22300, lr = 0.01
I0926 09:50:45.629042  4406 solver.cpp:218] Iteration 22400 (6.79102 iter/s, 14.7253s/100 iters), loss = 0.238582
I0926 09:50:45.629073  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238582 (* 1 = 0.238582 loss)
I0926 09:50:45.629079  4406 sgd_solver.cpp:105] Iteration 22400, lr = 0.01
I0926 09:50:59.551931  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:51:00.137420  4406 solver.cpp:330] Iteration 22500, Testing net (#0)
I0926 09:51:03.564100  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:51:03.706904  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8247
I0926 09:51:03.706939  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.519499 (* 1 = 0.519499 loss)
I0926 09:51:03.851230  4406 solver.cpp:218] Iteration 22500 (5.48784 iter/s, 18.2221s/100 iters), loss = 0.260688
I0926 09:51:03.851258  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260688 (* 1 = 0.260688 loss)
I0926 09:51:03.851265  4406 sgd_solver.cpp:105] Iteration 22500, lr = 0.01
I0926 09:51:18.470201  4406 solver.cpp:218] Iteration 22600 (6.84046 iter/s, 14.6189s/100 iters), loss = 0.362718
I0926 09:51:18.470240  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.362718 (* 1 = 0.362718 loss)
I0926 09:51:18.470247  4406 sgd_solver.cpp:105] Iteration 22600, lr = 0.01
I0926 09:51:33.091305  4406 solver.cpp:218] Iteration 22700 (6.83946 iter/s, 14.621s/100 iters), loss = 0.279706
I0926 09:51:33.091409  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279706 (* 1 = 0.279706 loss)
I0926 09:51:33.091416  4406 sgd_solver.cpp:105] Iteration 22700, lr = 0.01
I0926 09:51:47.712837  4406 solver.cpp:218] Iteration 22800 (6.83929 iter/s, 14.6214s/100 iters), loss = 0.30105
I0926 09:51:47.712869  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.30105 (* 1 = 0.30105 loss)
I0926 09:51:47.712875  4406 sgd_solver.cpp:105] Iteration 22800, lr = 0.01
I0926 09:52:02.331081  4406 solver.cpp:218] Iteration 22900 (6.8408 iter/s, 14.6182s/100 iters), loss = 0.30662
I0926 09:52:02.331123  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.30662 (* 1 = 0.30662 loss)
I0926 09:52:02.331130  4406 sgd_solver.cpp:105] Iteration 22900, lr = 0.01
I0926 09:52:16.221688  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:52:16.804267  4406 solver.cpp:330] Iteration 23000, Testing net (#0)
I0926 09:52:20.229763  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:52:20.372344  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8056
I0926 09:52:20.372378  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.566331 (* 1 = 0.566331 loss)
I0926 09:52:20.517527  4406 solver.cpp:218] Iteration 23000 (5.49863 iter/s, 18.1864s/100 iters), loss = 0.256988
I0926 09:52:20.517556  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256988 (* 1 = 0.256988 loss)
I0926 09:52:20.517562  4406 sgd_solver.cpp:105] Iteration 23000, lr = 0.01
I0926 09:52:35.147054  4406 solver.cpp:218] Iteration 23100 (6.83552 iter/s, 14.6295s/100 iters), loss = 0.263661
I0926 09:52:35.147094  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263661 (* 1 = 0.263661 loss)
I0926 09:52:35.147101  4406 sgd_solver.cpp:105] Iteration 23100, lr = 0.01
I0926 09:52:49.773135  4406 solver.cpp:218] Iteration 23200 (6.83714 iter/s, 14.626s/100 iters), loss = 0.342198
I0926 09:52:49.773237  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.342197 (* 1 = 0.342197 loss)
I0926 09:52:49.773244  4406 sgd_solver.cpp:105] Iteration 23200, lr = 0.01
I0926 09:53:04.402467  4406 solver.cpp:218] Iteration 23300 (6.83565 iter/s, 14.6292s/100 iters), loss = 0.363966
I0926 09:53:04.402506  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.363966 (* 1 = 0.363966 loss)
I0926 09:53:04.402513  4406 sgd_solver.cpp:105] Iteration 23300, lr = 0.01
I0926 09:53:19.035359  4406 solver.cpp:218] Iteration 23400 (6.83395 iter/s, 14.6328s/100 iters), loss = 0.383011
I0926 09:53:19.035388  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.383011 (* 1 = 0.383011 loss)
I0926 09:53:19.035394  4406 sgd_solver.cpp:105] Iteration 23400, lr = 0.01
I0926 09:53:32.932152  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:53:33.517169  4406 solver.cpp:330] Iteration 23500, Testing net (#0)
I0926 09:53:36.939427  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:53:37.081624  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8234
I0926 09:53:37.081648  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.517825 (* 1 = 0.517825 loss)
I0926 09:53:37.225852  4406 solver.cpp:218] Iteration 23500 (5.4974 iter/s, 18.1904s/100 iters), loss = 0.273393
I0926 09:53:37.225880  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273393 (* 1 = 0.273393 loss)
I0926 09:53:37.225886  4406 sgd_solver.cpp:105] Iteration 23500, lr = 0.01
I0926 09:53:51.851685  4406 solver.cpp:218] Iteration 23600 (6.83725 iter/s, 14.6258s/100 iters), loss = 0.238366
I0926 09:53:51.851723  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238365 (* 1 = 0.238365 loss)
I0926 09:53:51.851729  4406 sgd_solver.cpp:105] Iteration 23600, lr = 0.01
I0926 09:54:06.479581  4406 solver.cpp:218] Iteration 23700 (6.83629 iter/s, 14.6278s/100 iters), loss = 0.312519
I0926 09:54:06.479729  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312519 (* 1 = 0.312519 loss)
I0926 09:54:06.479738  4406 sgd_solver.cpp:105] Iteration 23700, lr = 0.01
I0926 09:54:21.098150  4406 solver.cpp:218] Iteration 23800 (6.84069 iter/s, 14.6184s/100 iters), loss = 0.385777
I0926 09:54:21.098189  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.385777 (* 1 = 0.385777 loss)
I0926 09:54:21.098196  4406 sgd_solver.cpp:105] Iteration 23800, lr = 0.01
I0926 09:54:35.725700  4406 solver.cpp:218] Iteration 23900 (6.83645 iter/s, 14.6275s/100 iters), loss = 0.645311
I0926 09:54:35.725739  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.645311 (* 1 = 0.645311 loss)
I0926 09:54:35.725746  4406 sgd_solver.cpp:105] Iteration 23900, lr = 0.01
I0926 09:54:49.622905  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:54:50.206954  4406 solver.cpp:330] Iteration 24000, Testing net (#0)
I0926 09:54:53.630332  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:54:53.772694  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6776
I0926 09:54:53.772718  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.02616 (* 1 = 1.02616 loss)
I0926 09:54:53.916766  4406 solver.cpp:218] Iteration 24000 (5.49723 iter/s, 18.191s/100 iters), loss = 0.507492
I0926 09:54:53.916795  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.507492 (* 1 = 0.507492 loss)
I0926 09:54:53.916800  4406 sgd_solver.cpp:105] Iteration 24000, lr = 0.01
I0926 09:55:08.542914  4406 solver.cpp:218] Iteration 24100 (6.8371 iter/s, 14.6261s/100 iters), loss = 0.322481
I0926 09:55:08.542945  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.32248 (* 1 = 0.32248 loss)
I0926 09:55:08.542950  4406 sgd_solver.cpp:105] Iteration 24100, lr = 0.01
I0926 09:55:23.165364  4406 solver.cpp:218] Iteration 24200 (6.83883 iter/s, 14.6224s/100 iters), loss = 0.404636
I0926 09:55:23.165443  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.404636 (* 1 = 0.404636 loss)
I0926 09:55:23.165459  4406 sgd_solver.cpp:105] Iteration 24200, lr = 0.01
I0926 09:55:37.792750  4406 solver.cpp:218] Iteration 24300 (6.83655 iter/s, 14.6273s/100 iters), loss = 0.42413
I0926 09:55:37.792780  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.424129 (* 1 = 0.424129 loss)
I0926 09:55:37.792786  4406 sgd_solver.cpp:105] Iteration 24300, lr = 0.01
I0926 09:55:52.413014  4406 solver.cpp:218] Iteration 24400 (6.83985 iter/s, 14.6202s/100 iters), loss = 0.437096
I0926 09:55:52.413044  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.437096 (* 1 = 0.437096 loss)
I0926 09:55:52.413050  4406 sgd_solver.cpp:105] Iteration 24400, lr = 0.01
I0926 09:56:06.313819  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:56:06.899346  4406 solver.cpp:330] Iteration 24500, Testing net (#0)
I0926 09:56:10.324163  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:56:10.466785  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8018
I0926 09:56:10.466820  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.606007 (* 1 = 0.606007 loss)
I0926 09:56:10.611793  4406 solver.cpp:218] Iteration 24500 (5.4949 iter/s, 18.1987s/100 iters), loss = 0.344622
I0926 09:56:10.611821  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.344622 (* 1 = 0.344622 loss)
I0926 09:56:10.611827  4406 sgd_solver.cpp:105] Iteration 24500, lr = 0.01
I0926 09:56:25.227968  4406 solver.cpp:218] Iteration 24600 (6.84177 iter/s, 14.6161s/100 iters), loss = 0.269753
I0926 09:56:25.227998  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269753 (* 1 = 0.269753 loss)
I0926 09:56:25.228004  4406 sgd_solver.cpp:105] Iteration 24600, lr = 0.01
I0926 09:56:39.837688  4406 solver.cpp:218] Iteration 24700 (6.84479 iter/s, 14.6097s/100 iters), loss = 0.243751
I0926 09:56:39.837796  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243751 (* 1 = 0.243751 loss)
I0926 09:56:39.837803  4406 sgd_solver.cpp:105] Iteration 24700, lr = 0.01
I0926 09:56:54.453801  4406 solver.cpp:218] Iteration 24800 (6.84183 iter/s, 14.616s/100 iters), loss = 0.372727
I0926 09:56:54.453830  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372726 (* 1 = 0.372726 loss)
I0926 09:56:54.453836  4406 sgd_solver.cpp:105] Iteration 24800, lr = 0.01
I0926 09:57:09.071666  4406 solver.cpp:218] Iteration 24900 (6.84098 iter/s, 14.6178s/100 iters), loss = 0.436816
I0926 09:57:09.071694  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.436816 (* 1 = 0.436816 loss)
I0926 09:57:09.071701  4406 sgd_solver.cpp:105] Iteration 24900, lr = 0.01
I0926 09:57:22.957099  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:57:23.543589  4406 solver.cpp:330] Iteration 25000, Testing net (#0)
I0926 09:57:26.967864  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:57:27.110718  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8267
I0926 09:57:27.110752  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.502334 (* 1 = 0.502334 loss)
I0926 09:57:27.254920  4406 solver.cpp:218] Iteration 25000 (5.49959 iter/s, 18.1832s/100 iters), loss = 0.329097
I0926 09:57:27.254951  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329096 (* 1 = 0.329096 loss)
I0926 09:57:27.254956  4406 sgd_solver.cpp:105] Iteration 25000, lr = 0.01
I0926 09:57:41.887864  4406 solver.cpp:218] Iteration 25100 (6.83393 iter/s, 14.6329s/100 iters), loss = 0.224371
I0926 09:57:41.887895  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22437 (* 1 = 0.22437 loss)
I0926 09:57:41.887902  4406 sgd_solver.cpp:105] Iteration 25100, lr = 0.01
I0926 09:57:56.520633  4406 solver.cpp:218] Iteration 25200 (6.83401 iter/s, 14.6327s/100 iters), loss = 0.289745
I0926 09:57:56.520752  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289745 (* 1 = 0.289745 loss)
I0926 09:57:56.520769  4406 sgd_solver.cpp:105] Iteration 25200, lr = 0.01
I0926 09:58:11.151268  4406 solver.cpp:218] Iteration 25300 (6.83504 iter/s, 14.6305s/100 iters), loss = 0.379878
I0926 09:58:11.151298  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.379877 (* 1 = 0.379877 loss)
I0926 09:58:11.151304  4406 sgd_solver.cpp:105] Iteration 25300, lr = 0.01
I0926 09:58:25.772265  4406 solver.cpp:218] Iteration 25400 (6.83951 iter/s, 14.6209s/100 iters), loss = 0.289773
I0926 09:58:25.772306  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289773 (* 1 = 0.289773 loss)
I0926 09:58:25.772312  4406 sgd_solver.cpp:105] Iteration 25400, lr = 0.01
I0926 09:58:39.676627  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:58:40.260988  4406 solver.cpp:330] Iteration 25500, Testing net (#0)
I0926 09:58:43.687005  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:58:43.829847  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8204
I0926 09:58:43.829882  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535933 (* 1 = 0.535933 loss)
I0926 09:58:43.974087  4406 solver.cpp:218] Iteration 25500 (5.49398 iter/s, 18.2017s/100 iters), loss = 0.324724
I0926 09:58:43.974118  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.324724 (* 1 = 0.324724 loss)
I0926 09:58:43.974124  4406 sgd_solver.cpp:105] Iteration 25500, lr = 0.01
I0926 09:58:58.600392  4406 solver.cpp:218] Iteration 25600 (6.83703 iter/s, 14.6262s/100 iters), loss = 0.26819
I0926 09:58:58.600421  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26819 (* 1 = 0.26819 loss)
I0926 09:58:58.600427  4406 sgd_solver.cpp:105] Iteration 25600, lr = 0.01
I0926 09:59:13.222753  4406 solver.cpp:218] Iteration 25700 (6.83887 iter/s, 14.6223s/100 iters), loss = 0.264021
I0926 09:59:13.222863  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26402 (* 1 = 0.26402 loss)
I0926 09:59:13.222870  4406 sgd_solver.cpp:105] Iteration 25700, lr = 0.01
I0926 09:59:27.847113  4406 solver.cpp:218] Iteration 25800 (6.83797 iter/s, 14.6242s/100 iters), loss = 0.236969
I0926 09:59:27.847153  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236969 (* 1 = 0.236969 loss)
I0926 09:59:27.847159  4406 sgd_solver.cpp:105] Iteration 25800, lr = 0.01
I0926 09:59:42.470432  4406 solver.cpp:218] Iteration 25900 (6.83843 iter/s, 14.6232s/100 iters), loss = 0.27902
I0926 09:59:42.470472  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27902 (* 1 = 0.27902 loss)
I0926 09:59:42.470479  4406 sgd_solver.cpp:105] Iteration 25900, lr = 0.01
I0926 09:59:56.367902  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 09:59:56.953248  4406 solver.cpp:330] Iteration 26000, Testing net (#0)
I0926 10:00:00.380012  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:00:00.523066  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8078
I0926 10:00:00.523100  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.569549 (* 1 = 0.569549 loss)
I0926 10:00:00.667548  4406 solver.cpp:218] Iteration 26000 (5.4954 iter/s, 18.197s/100 iters), loss = 0.326899
I0926 10:00:00.667577  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.326898 (* 1 = 0.326898 loss)
I0926 10:00:00.667583  4406 sgd_solver.cpp:105] Iteration 26000, lr = 0.01
I0926 10:00:15.279347  4406 solver.cpp:218] Iteration 26100 (6.84381 iter/s, 14.6117s/100 iters), loss = 0.245623
I0926 10:00:15.279386  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245623 (* 1 = 0.245623 loss)
I0926 10:00:15.279392  4406 sgd_solver.cpp:105] Iteration 26100, lr = 0.01
I0926 10:00:29.891011  4406 solver.cpp:218] Iteration 26200 (6.84388 iter/s, 14.6116s/100 iters), loss = 0.223298
I0926 10:00:29.891181  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223298 (* 1 = 0.223298 loss)
I0926 10:00:29.891191  4406 sgd_solver.cpp:105] Iteration 26200, lr = 0.01
I0926 10:00:44.508270  4406 solver.cpp:218] Iteration 26300 (6.84132 iter/s, 14.6171s/100 iters), loss = 0.582267
I0926 10:00:44.508311  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.582267 (* 1 = 0.582267 loss)
I0926 10:00:44.508316  4406 sgd_solver.cpp:105] Iteration 26300, lr = 0.01
I0926 10:00:59.125644  4406 solver.cpp:218] Iteration 26400 (6.84121 iter/s, 14.6173s/100 iters), loss = 0.387359
I0926 10:00:59.125684  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.387358 (* 1 = 0.387358 loss)
I0926 10:00:59.125690  4406 sgd_solver.cpp:105] Iteration 26400, lr = 0.01
I0926 10:01:13.020370  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:01:13.606892  4406 solver.cpp:330] Iteration 26500, Testing net (#0)
I0926 10:01:17.031458  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:01:17.174746  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8057
I0926 10:01:17.174782  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.56482 (* 1 = 0.56482 loss)
I0926 10:01:17.319769  4406 solver.cpp:218] Iteration 26500 (5.49631 iter/s, 18.194s/100 iters), loss = 0.326089
I0926 10:01:17.319798  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.326088 (* 1 = 0.326088 loss)
I0926 10:01:17.319804  4406 sgd_solver.cpp:105] Iteration 26500, lr = 0.01
I0926 10:01:31.938422  4406 solver.cpp:218] Iteration 26600 (6.84061 iter/s, 14.6186s/100 iters), loss = 0.252885
I0926 10:01:31.938450  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.252884 (* 1 = 0.252884 loss)
I0926 10:01:31.938457  4406 sgd_solver.cpp:105] Iteration 26600, lr = 0.01
I0926 10:01:46.557463  4406 solver.cpp:218] Iteration 26700 (6.84042 iter/s, 14.619s/100 iters), loss = 0.270095
I0926 10:01:46.557600  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270095 (* 1 = 0.270095 loss)
I0926 10:01:46.557606  4406 sgd_solver.cpp:105] Iteration 26700, lr = 0.01
I0926 10:02:01.178323  4406 solver.cpp:218] Iteration 26800 (6.83962 iter/s, 14.6207s/100 iters), loss = 0.390532
I0926 10:02:01.178354  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.390532 (* 1 = 0.390532 loss)
I0926 10:02:01.178359  4406 sgd_solver.cpp:105] Iteration 26800, lr = 0.01
I0926 10:02:15.800628  4406 solver.cpp:218] Iteration 26900 (6.8389 iter/s, 14.6222s/100 iters), loss = 0.194304
I0926 10:02:15.800668  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194304 (* 1 = 0.194304 loss)
I0926 10:02:15.800674  4406 sgd_solver.cpp:105] Iteration 26900, lr = 0.01
I0926 10:02:29.696228  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:02:30.279870  4406 solver.cpp:330] Iteration 27000, Testing net (#0)
I0926 10:02:33.704638  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:02:33.847306  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8368
I0926 10:02:33.847340  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.47568 (* 1 = 0.47568 loss)
I0926 10:02:33.992684  4406 solver.cpp:218] Iteration 27000 (5.49693 iter/s, 18.192s/100 iters), loss = 0.2391
I0926 10:02:33.992712  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2391 (* 1 = 0.2391 loss)
I0926 10:02:33.992718  4406 sgd_solver.cpp:105] Iteration 27000, lr = 0.01
I0926 10:02:48.606848  4406 solver.cpp:218] Iteration 27100 (6.84271 iter/s, 14.6141s/100 iters), loss = 0.255602
I0926 10:02:48.606878  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255602 (* 1 = 0.255602 loss)
I0926 10:02:48.606884  4406 sgd_solver.cpp:105] Iteration 27100, lr = 0.01
I0926 10:03:03.221621  4406 solver.cpp:218] Iteration 27200 (6.84244 iter/s, 14.6147s/100 iters), loss = 0.230791
I0926 10:03:03.221755  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23079 (* 1 = 0.23079 loss)
I0926 10:03:03.221763  4406 sgd_solver.cpp:105] Iteration 27200, lr = 0.01
I0926 10:03:17.839856  4406 solver.cpp:218] Iteration 27300 (6.84085 iter/s, 14.6181s/100 iters), loss = 0.283557
I0926 10:03:17.839887  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283557 (* 1 = 0.283557 loss)
I0926 10:03:17.839893  4406 sgd_solver.cpp:105] Iteration 27300, lr = 0.01
I0926 10:03:32.463538  4406 solver.cpp:218] Iteration 27400 (6.83826 iter/s, 14.6236s/100 iters), loss = 0.315665
I0926 10:03:32.463578  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.315665 (* 1 = 0.315665 loss)
I0926 10:03:32.463584  4406 sgd_solver.cpp:105] Iteration 27400, lr = 0.01
I0926 10:03:46.355301  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:03:46.940521  4406 solver.cpp:330] Iteration 27500, Testing net (#0)
I0926 10:03:50.364796  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:03:50.507320  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.823
I0926 10:03:50.507355  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.516886 (* 1 = 0.516886 loss)
I0926 10:03:50.651212  4406 solver.cpp:218] Iteration 27500 (5.49825 iter/s, 18.1876s/100 iters), loss = 0.377317
I0926 10:03:50.651240  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.377317 (* 1 = 0.377317 loss)
I0926 10:03:50.651247  4406 sgd_solver.cpp:105] Iteration 27500, lr = 0.01
I0926 10:04:05.269814  4406 solver.cpp:218] Iteration 27600 (6.84063 iter/s, 14.6185s/100 iters), loss = 0.195558
I0926 10:04:05.269855  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195557 (* 1 = 0.195557 loss)
I0926 10:04:05.269860  4406 sgd_solver.cpp:105] Iteration 27600, lr = 0.01
I0926 10:04:19.893946  4406 solver.cpp:218] Iteration 27700 (6.83805 iter/s, 14.6241s/100 iters), loss = 0.241862
I0926 10:04:19.894052  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241862 (* 1 = 0.241862 loss)
I0926 10:04:19.894058  4406 sgd_solver.cpp:105] Iteration 27700, lr = 0.01
I0926 10:04:34.510107  4406 solver.cpp:218] Iteration 27800 (6.84181 iter/s, 14.616s/100 iters), loss = 0.320328
I0926 10:04:34.510135  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320328 (* 1 = 0.320328 loss)
I0926 10:04:34.510140  4406 sgd_solver.cpp:105] Iteration 27800, lr = 0.01
I0926 10:04:49.130205  4406 solver.cpp:218] Iteration 27900 (6.83993 iter/s, 14.62s/100 iters), loss = 0.248548
I0926 10:04:49.130246  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.248548 (* 1 = 0.248548 loss)
I0926 10:04:49.130252  4406 sgd_solver.cpp:105] Iteration 27900, lr = 0.01
I0926 10:05:03.022866  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:05:03.607622  4406 solver.cpp:330] Iteration 28000, Testing net (#0)
I0926 10:05:07.034283  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:05:07.176889  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.835
I0926 10:05:07.176915  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.48153 (* 1 = 0.48153 loss)
I0926 10:05:07.320824  4406 solver.cpp:218] Iteration 28000 (5.49736 iter/s, 18.1905s/100 iters), loss = 0.285563
I0926 10:05:07.320852  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285563 (* 1 = 0.285563 loss)
I0926 10:05:07.320858  4406 sgd_solver.cpp:105] Iteration 28000, lr = 0.01
I0926 10:05:21.940086  4406 solver.cpp:218] Iteration 28100 (6.84032 iter/s, 14.6192s/100 iters), loss = 0.286471
I0926 10:05:21.940117  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28647 (* 1 = 0.28647 loss)
I0926 10:05:21.940124  4406 sgd_solver.cpp:105] Iteration 28100, lr = 0.01
I0926 10:05:36.561617  4406 solver.cpp:218] Iteration 28200 (6.83926 iter/s, 14.6215s/100 iters), loss = 0.340349
I0926 10:05:36.561772  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340348 (* 1 = 0.340348 loss)
I0926 10:05:36.561782  4406 sgd_solver.cpp:105] Iteration 28200, lr = 0.01
I0926 10:05:51.187160  4406 solver.cpp:218] Iteration 28300 (6.83744 iter/s, 14.6254s/100 iters), loss = 0.294374
I0926 10:05:51.187191  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.294373 (* 1 = 0.294373 loss)
I0926 10:05:51.187196  4406 sgd_solver.cpp:105] Iteration 28300, lr = 0.01
I0926 10:06:05.812834  4406 solver.cpp:218] Iteration 28400 (6.83732 iter/s, 14.6256s/100 iters), loss = 0.283907
I0926 10:06:05.812863  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283906 (* 1 = 0.283906 loss)
I0926 10:06:05.812870  4406 sgd_solver.cpp:105] Iteration 28400, lr = 0.01
I0926 10:06:19.712847  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:06:20.298406  4406 solver.cpp:330] Iteration 28500, Testing net (#0)
I0926 10:06:23.720554  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:06:23.864240  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8277
I0926 10:06:23.864265  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.515029 (* 1 = 0.515029 loss)
I0926 10:06:24.010855  4406 solver.cpp:218] Iteration 28500 (5.49513 iter/s, 18.1979s/100 iters), loss = 0.273001
I0926 10:06:24.010886  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.273 (* 1 = 0.273 loss)
I0926 10:06:24.010892  4406 sgd_solver.cpp:105] Iteration 28500, lr = 0.01
I0926 10:06:38.634770  4406 solver.cpp:218] Iteration 28600 (6.83815 iter/s, 14.6238s/100 iters), loss = 0.253216
I0926 10:06:38.634810  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253216 (* 1 = 0.253216 loss)
I0926 10:06:38.634817  4406 sgd_solver.cpp:105] Iteration 28600, lr = 0.01
I0926 10:06:53.260310  4406 solver.cpp:218] Iteration 28700 (6.83739 iter/s, 14.6255s/100 iters), loss = 0.197059
I0926 10:06:53.260444  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197059 (* 1 = 0.197059 loss)
I0926 10:06:53.260452  4406 sgd_solver.cpp:105] Iteration 28700, lr = 0.01
I0926 10:07:07.888430  4406 solver.cpp:218] Iteration 28800 (6.83623 iter/s, 14.628s/100 iters), loss = 0.340613
I0926 10:07:07.888461  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340612 (* 1 = 0.340612 loss)
I0926 10:07:07.888478  4406 sgd_solver.cpp:105] Iteration 28800, lr = 0.01
I0926 10:07:22.518268  4406 solver.cpp:218] Iteration 28900 (6.83538 iter/s, 14.6298s/100 iters), loss = 0.303051
I0926 10:07:22.518299  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.30305 (* 1 = 0.30305 loss)
I0926 10:07:22.518306  4406 sgd_solver.cpp:105] Iteration 28900, lr = 0.01
I0926 10:07:36.414907  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:07:36.999428  4406 solver.cpp:330] Iteration 29000, Testing net (#0)
I0926 10:07:40.425596  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:07:40.567927  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8345
I0926 10:07:40.567952  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.479614 (* 1 = 0.479614 loss)
I0926 10:07:40.712954  4406 solver.cpp:218] Iteration 29000 (5.49613 iter/s, 18.1946s/100 iters), loss = 0.28193
I0926 10:07:40.712982  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28193 (* 1 = 0.28193 loss)
I0926 10:07:40.712990  4406 sgd_solver.cpp:105] Iteration 29000, lr = 0.01
I0926 10:07:55.342968  4406 solver.cpp:218] Iteration 29100 (6.83529 iter/s, 14.6299s/100 iters), loss = 0.254875
I0926 10:07:55.343008  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254874 (* 1 = 0.254874 loss)
I0926 10:07:55.343014  4406 sgd_solver.cpp:105] Iteration 29100, lr = 0.01
I0926 10:08:09.972968  4406 solver.cpp:218] Iteration 29200 (6.83531 iter/s, 14.6299s/100 iters), loss = 0.296518
I0926 10:08:09.973140  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296517 (* 1 = 0.296517 loss)
I0926 10:08:09.973160  4406 sgd_solver.cpp:105] Iteration 29200, lr = 0.01
I0926 10:08:24.606024  4406 solver.cpp:218] Iteration 29300 (6.83394 iter/s, 14.6328s/100 iters), loss = 0.346232
I0926 10:08:24.606053  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.346232 (* 1 = 0.346232 loss)
I0926 10:08:24.606060  4406 sgd_solver.cpp:105] Iteration 29300, lr = 0.01
I0926 10:08:39.235306  4406 solver.cpp:218] Iteration 29400 (6.83564 iter/s, 14.6292s/100 iters), loss = 0.236259
I0926 10:08:39.235338  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.236259 (* 1 = 0.236259 loss)
I0926 10:08:39.235344  4406 sgd_solver.cpp:105] Iteration 29400, lr = 0.01
I0926 10:08:53.132625  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:08:53.718888  4406 solver.cpp:330] Iteration 29500, Testing net (#0)
I0926 10:08:57.143045  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:08:57.286330  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8427
I0926 10:08:57.286366  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.463397 (* 1 = 0.463397 loss)
I0926 10:08:57.430691  4406 solver.cpp:218] Iteration 29500 (5.49592 iter/s, 18.1953s/100 iters), loss = 0.313673
I0926 10:08:57.430719  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313672 (* 1 = 0.313672 loss)
I0926 10:08:57.430727  4406 sgd_solver.cpp:105] Iteration 29500, lr = 0.01
I0926 10:09:12.052597  4406 solver.cpp:218] Iteration 29600 (6.83909 iter/s, 14.6218s/100 iters), loss = 0.228241
I0926 10:09:12.052626  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.228241 (* 1 = 0.228241 loss)
I0926 10:09:12.052633  4406 sgd_solver.cpp:105] Iteration 29600, lr = 0.01
I0926 10:09:26.673033  4406 solver.cpp:218] Iteration 29700 (6.83977 iter/s, 14.6204s/100 iters), loss = 0.270537
I0926 10:09:26.673116  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270536 (* 1 = 0.270536 loss)
I0926 10:09:26.673125  4406 sgd_solver.cpp:105] Iteration 29700, lr = 0.01
I0926 10:09:41.298198  4406 solver.cpp:218] Iteration 29800 (6.83759 iter/s, 14.625s/100 iters), loss = 0.353915
I0926 10:09:41.298238  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353915 (* 1 = 0.353915 loss)
I0926 10:09:41.298244  4406 sgd_solver.cpp:105] Iteration 29800, lr = 0.01
I0926 10:09:55.921763  4406 solver.cpp:218] Iteration 29900 (6.83831 iter/s, 14.6235s/100 iters), loss = 0.17865
I0926 10:09:55.921803  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17865 (* 1 = 0.17865 loss)
I0926 10:09:55.921809  4406 sgd_solver.cpp:105] Iteration 29900, lr = 0.01
I0926 10:10:09.815801  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:10:10.402381  4406 solver.cpp:330] Iteration 30000, Testing net (#0)
I0926 10:10:13.825496  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:10:13.968267  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8409
I0926 10:10:13.968303  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.468268 (* 1 = 0.468268 loss)
I0926 10:10:14.111999  4406 solver.cpp:218] Iteration 30000 (5.49748 iter/s, 18.1902s/100 iters), loss = 0.247846
I0926 10:10:14.112027  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247846 (* 1 = 0.247846 loss)
I0926 10:10:14.112035  4406 sgd_solver.cpp:105] Iteration 30000, lr = 0.01
I0926 10:10:28.743927  4406 solver.cpp:218] Iteration 30100 (6.8344 iter/s, 14.6319s/100 iters), loss = 0.237209
I0926 10:10:28.743968  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237209 (* 1 = 0.237209 loss)
I0926 10:10:28.743973  4406 sgd_solver.cpp:105] Iteration 30100, lr = 0.01
I0926 10:10:43.372992  4406 solver.cpp:218] Iteration 30200 (6.83574 iter/s, 14.629s/100 iters), loss = 0.183556
I0926 10:10:43.373133  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183556 (* 1 = 0.183556 loss)
I0926 10:10:43.373152  4406 sgd_solver.cpp:105] Iteration 30200, lr = 0.01
I0926 10:10:58.001163  4406 solver.cpp:218] Iteration 30300 (6.83621 iter/s, 14.628s/100 iters), loss = 0.364379
I0926 10:10:58.001202  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364379 (* 1 = 0.364379 loss)
I0926 10:10:58.001209  4406 sgd_solver.cpp:105] Iteration 30300, lr = 0.01
I0926 10:11:12.624924  4406 solver.cpp:218] Iteration 30400 (6.83822 iter/s, 14.6237s/100 iters), loss = 0.225104
I0926 10:11:12.624963  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.225103 (* 1 = 0.225103 loss)
I0926 10:11:12.624969  4406 sgd_solver.cpp:105] Iteration 30400, lr = 0.01
I0926 10:11:26.525578  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:11:27.110502  4406 solver.cpp:330] Iteration 30500, Testing net (#0)
I0926 10:11:30.537293  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:11:30.680140  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8435
I0926 10:11:30.680176  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.459683 (* 1 = 0.459683 loss)
I0926 10:11:30.824275  4406 solver.cpp:218] Iteration 30500 (5.49473 iter/s, 18.1993s/100 iters), loss = 0.240306
I0926 10:11:30.824306  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240305 (* 1 = 0.240305 loss)
I0926 10:11:30.824311  4406 sgd_solver.cpp:105] Iteration 30500, lr = 0.01
I0926 10:11:45.442832  4406 solver.cpp:218] Iteration 30600 (6.84065 iter/s, 14.6185s/100 iters), loss = 0.296597
I0926 10:11:45.442873  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.296597 (* 1 = 0.296597 loss)
I0926 10:11:45.442878  4406 sgd_solver.cpp:105] Iteration 30600, lr = 0.01
I0926 10:12:00.067117  4406 solver.cpp:218] Iteration 30700 (6.83798 iter/s, 14.6242s/100 iters), loss = 0.22106
I0926 10:12:00.067205  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22106 (* 1 = 0.22106 loss)
I0926 10:12:00.067224  4406 sgd_solver.cpp:105] Iteration 30700, lr = 0.01
I0926 10:12:14.689738  4406 solver.cpp:218] Iteration 30800 (6.83878 iter/s, 14.6225s/100 iters), loss = 0.271721
I0926 10:12:14.689779  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.271721 (* 1 = 0.271721 loss)
I0926 10:12:14.689785  4406 sgd_solver.cpp:105] Iteration 30800, lr = 0.01
I0926 10:12:29.306824  4406 solver.cpp:218] Iteration 30900 (6.84135 iter/s, 14.617s/100 iters), loss = 0.192088
I0926 10:12:29.306865  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192088 (* 1 = 0.192088 loss)
I0926 10:12:29.306871  4406 sgd_solver.cpp:105] Iteration 30900, lr = 0.01
I0926 10:12:43.205847  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:12:43.791036  4406 solver.cpp:330] Iteration 31000, Testing net (#0)
I0926 10:12:47.213260  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:12:47.355989  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8369
I0926 10:12:47.356025  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.473454 (* 1 = 0.473454 loss)
I0926 10:12:47.500545  4406 solver.cpp:218] Iteration 31000 (5.49643 iter/s, 18.1936s/100 iters), loss = 0.204623
I0926 10:12:47.500574  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204623 (* 1 = 0.204623 loss)
I0926 10:12:47.500581  4406 sgd_solver.cpp:105] Iteration 31000, lr = 0.01
I0926 10:13:02.121927  4406 solver.cpp:218] Iteration 31100 (6.83933 iter/s, 14.6213s/100 iters), loss = 0.261974
I0926 10:13:02.121968  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261974 (* 1 = 0.261974 loss)
I0926 10:13:02.121974  4406 sgd_solver.cpp:105] Iteration 31100, lr = 0.01
I0926 10:13:16.740802  4406 solver.cpp:218] Iteration 31200 (6.84051 iter/s, 14.6188s/100 iters), loss = 0.177731
I0926 10:13:16.740965  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177731 (* 1 = 0.177731 loss)
I0926 10:13:16.740975  4406 sgd_solver.cpp:105] Iteration 31200, lr = 0.01
I0926 10:13:31.359020  4406 solver.cpp:218] Iteration 31300 (6.84087 iter/s, 14.618s/100 iters), loss = 0.310334
I0926 10:13:31.359061  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310334 (* 1 = 0.310334 loss)
I0926 10:13:31.359066  4406 sgd_solver.cpp:105] Iteration 31300, lr = 0.01
I0926 10:13:45.979238  4406 solver.cpp:218] Iteration 31400 (6.83988 iter/s, 14.6201s/100 iters), loss = 0.175051
I0926 10:13:45.979279  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175051 (* 1 = 0.175051 loss)
I0926 10:13:45.979285  4406 sgd_solver.cpp:105] Iteration 31400, lr = 0.01
I0926 10:13:59.873538  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:14:00.459666  4406 solver.cpp:330] Iteration 31500, Testing net (#0)
I0926 10:14:03.883913  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:14:04.026908  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8393
I0926 10:14:04.026944  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.468276 (* 1 = 0.468276 loss)
I0926 10:14:04.171322  4406 solver.cpp:218] Iteration 31500 (5.49692 iter/s, 18.192s/100 iters), loss = 0.283015
I0926 10:14:04.171350  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.283014 (* 1 = 0.283014 loss)
I0926 10:14:04.171355  4406 sgd_solver.cpp:105] Iteration 31500, lr = 0.01
I0926 10:14:18.792075  4406 solver.cpp:218] Iteration 31600 (6.83962 iter/s, 14.6207s/100 iters), loss = 0.210566
I0926 10:14:18.792116  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210566 (* 1 = 0.210566 loss)
I0926 10:14:18.792122  4406 sgd_solver.cpp:105] Iteration 31600, lr = 0.01
I0926 10:14:33.420169  4406 solver.cpp:218] Iteration 31700 (6.8362 iter/s, 14.628s/100 iters), loss = 0.237502
I0926 10:14:33.420241  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237502 (* 1 = 0.237502 loss)
I0926 10:14:33.420249  4406 sgd_solver.cpp:105] Iteration 31700, lr = 0.01
I0926 10:14:48.038329  4406 solver.cpp:218] Iteration 31800 (6.84086 iter/s, 14.6181s/100 iters), loss = 0.250992
I0926 10:14:48.038379  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250991 (* 1 = 0.250991 loss)
I0926 10:14:48.038385  4406 sgd_solver.cpp:105] Iteration 31800, lr = 0.01
I0926 10:15:02.666440  4406 solver.cpp:218] Iteration 31900 (6.83619 iter/s, 14.628s/100 iters), loss = 0.193311
I0926 10:15:02.666481  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193311 (* 1 = 0.193311 loss)
I0926 10:15:02.666496  4406 sgd_solver.cpp:105] Iteration 31900, lr = 0.01
I0926 10:15:16.569294  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:15:17.154516  4406 solver.cpp:330] Iteration 32000, Testing net (#0)
I0926 10:15:20.581346  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:15:20.724638  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8429
I0926 10:15:20.724674  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.469374 (* 1 = 0.469374 loss)
I0926 10:15:20.869467  4406 solver.cpp:218] Iteration 32000 (5.49362 iter/s, 18.2029s/100 iters), loss = 0.244135
I0926 10:15:20.869494  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244135 (* 1 = 0.244135 loss)
I0926 10:15:20.869500  4406 sgd_solver.cpp:105] Iteration 32000, lr = 0.01
I0926 10:15:35.498314  4406 solver.cpp:218] Iteration 32100 (6.83584 iter/s, 14.6288s/100 iters), loss = 0.214282
I0926 10:15:35.498344  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214282 (* 1 = 0.214282 loss)
I0926 10:15:35.498350  4406 sgd_solver.cpp:105] Iteration 32100, lr = 0.01
I0926 10:15:50.124893  4406 solver.cpp:218] Iteration 32200 (6.8369 iter/s, 14.6265s/100 iters), loss = 0.215411
I0926 10:15:50.125028  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215411 (* 1 = 0.215411 loss)
I0926 10:15:50.125036  4406 sgd_solver.cpp:105] Iteration 32200, lr = 0.01
I0926 10:16:04.751919  4406 solver.cpp:218] Iteration 32300 (6.83674 iter/s, 14.6269s/100 iters), loss = 0.324468
I0926 10:16:04.751958  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.324468 (* 1 = 0.324468 loss)
I0926 10:16:04.751965  4406 sgd_solver.cpp:105] Iteration 32300, lr = 0.01
I0926 10:16:19.379195  4406 solver.cpp:218] Iteration 32400 (6.83658 iter/s, 14.6272s/100 iters), loss = 0.213627
I0926 10:16:19.379225  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213627 (* 1 = 0.213627 loss)
I0926 10:16:19.379231  4406 sgd_solver.cpp:105] Iteration 32400, lr = 0.01
I0926 10:16:33.271419  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:16:33.856427  4406 solver.cpp:330] Iteration 32500, Testing net (#0)
I0926 10:16:37.281893  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:16:37.424737  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8359
I0926 10:16:37.424773  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.477857 (* 1 = 0.477857 loss)
I0926 10:16:37.569188  4406 solver.cpp:218] Iteration 32500 (5.49755 iter/s, 18.1899s/100 iters), loss = 0.194949
I0926 10:16:37.569216  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194948 (* 1 = 0.194948 loss)
I0926 10:16:37.569222  4406 sgd_solver.cpp:105] Iteration 32500, lr = 0.01
I0926 10:16:52.199959  4406 solver.cpp:218] Iteration 32600 (6.83494 iter/s, 14.6307s/100 iters), loss = 0.1878
I0926 10:16:52.199998  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.187799 (* 1 = 0.187799 loss)
I0926 10:16:52.200006  4406 sgd_solver.cpp:105] Iteration 32600, lr = 0.01
I0926 10:17:06.826066  4406 solver.cpp:218] Iteration 32700 (6.83713 iter/s, 14.626s/100 iters), loss = 0.218994
I0926 10:17:06.826174  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218994 (* 1 = 0.218994 loss)
I0926 10:17:06.826191  4406 sgd_solver.cpp:105] Iteration 32700, lr = 0.01
I0926 10:17:21.452024  4406 solver.cpp:218] Iteration 32800 (6.83722 iter/s, 14.6258s/100 iters), loss = 0.281243
I0926 10:17:21.452064  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281243 (* 1 = 0.281243 loss)
I0926 10:17:21.452070  4406 sgd_solver.cpp:105] Iteration 32800, lr = 0.01
I0926 10:17:36.077203  4406 solver.cpp:218] Iteration 32900 (6.83756 iter/s, 14.6251s/100 iters), loss = 0.247798
I0926 10:17:36.077244  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247798 (* 1 = 0.247798 loss)
I0926 10:17:36.077250  4406 sgd_solver.cpp:105] Iteration 32900, lr = 0.01
I0926 10:17:49.972968  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:17:50.561808  4406 solver.cpp:330] Iteration 33000, Testing net (#0)
I0926 10:17:53.985965  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:17:54.128921  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8402
I0926 10:17:54.128957  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.475222 (* 1 = 0.475222 loss)
I0926 10:17:54.273463  4406 solver.cpp:218] Iteration 33000 (5.49566 iter/s, 18.1962s/100 iters), loss = 0.239144
I0926 10:17:54.273488  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239144 (* 1 = 0.239144 loss)
I0926 10:17:54.273494  4406 sgd_solver.cpp:105] Iteration 33000, lr = 0.01
I0926 10:18:08.901468  4406 solver.cpp:218] Iteration 33100 (6.83624 iter/s, 14.6279s/100 iters), loss = 0.221426
I0926 10:18:08.901505  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221426 (* 1 = 0.221426 loss)
I0926 10:18:08.901521  4406 sgd_solver.cpp:105] Iteration 33100, lr = 0.01
I0926 10:18:23.519461  4406 solver.cpp:218] Iteration 33200 (6.84092 iter/s, 14.6179s/100 iters), loss = 0.210231
I0926 10:18:23.519632  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21023 (* 1 = 0.21023 loss)
I0926 10:18:23.519642  4406 sgd_solver.cpp:105] Iteration 33200, lr = 0.01
I0926 10:18:38.142405  4406 solver.cpp:218] Iteration 33300 (6.83866 iter/s, 14.6227s/100 iters), loss = 0.184715
I0926 10:18:38.142434  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184715 (* 1 = 0.184715 loss)
I0926 10:18:38.142439  4406 sgd_solver.cpp:105] Iteration 33300, lr = 0.01
I0926 10:18:52.759363  4406 solver.cpp:218] Iteration 33400 (6.8414 iter/s, 14.6169s/100 iters), loss = 0.315651
I0926 10:18:52.759393  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.315651 (* 1 = 0.315651 loss)
I0926 10:18:52.759399  4406 sgd_solver.cpp:105] Iteration 33400, lr = 0.01
I0926 10:19:06.651062  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:19:07.237411  4406 solver.cpp:330] Iteration 33500, Testing net (#0)
I0926 10:19:10.662868  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:19:10.805582  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8351
I0926 10:19:10.805619  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.494252 (* 1 = 0.494252 loss)
I0926 10:19:10.950165  4406 solver.cpp:218] Iteration 33500 (5.49731 iter/s, 18.1907s/100 iters), loss = 0.240021
I0926 10:19:10.950192  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240021 (* 1 = 0.240021 loss)
I0926 10:19:10.950199  4406 sgd_solver.cpp:105] Iteration 33500, lr = 0.01
I0926 10:19:25.568478  4406 solver.cpp:218] Iteration 33600 (6.84076 iter/s, 14.6182s/100 iters), loss = 0.235
I0926 10:19:25.568509  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235 (* 1 = 0.235 loss)
I0926 10:19:25.568514  4406 sgd_solver.cpp:105] Iteration 33600, lr = 0.01
I0926 10:19:40.179064  4406 solver.cpp:218] Iteration 33700 (6.84438 iter/s, 14.6105s/100 iters), loss = 0.205482
I0926 10:19:40.179190  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205481 (* 1 = 0.205481 loss)
I0926 10:19:40.179198  4406 sgd_solver.cpp:105] Iteration 33700, lr = 0.01
I0926 10:19:54.794471  4406 solver.cpp:218] Iteration 33800 (6.84227 iter/s, 14.615s/100 iters), loss = 0.253159
I0926 10:19:54.794502  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253158 (* 1 = 0.253158 loss)
I0926 10:19:54.794509  4406 sgd_solver.cpp:105] Iteration 33800, lr = 0.01
I0926 10:20:09.412686  4406 solver.cpp:218] Iteration 33900 (6.84081 iter/s, 14.6181s/100 iters), loss = 0.189163
I0926 10:20:09.412715  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.189163 (* 1 = 0.189163 loss)
I0926 10:20:09.412722  4406 sgd_solver.cpp:105] Iteration 33900, lr = 0.01
I0926 10:20:23.305559  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:20:23.889880  4406 solver.cpp:330] Iteration 34000, Testing net (#0)
I0926 10:20:27.314692  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:20:27.457340  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8375
I0926 10:20:27.457366  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.481883 (* 1 = 0.481883 loss)
I0926 10:20:27.602324  4406 solver.cpp:218] Iteration 34000 (5.49766 iter/s, 18.1896s/100 iters), loss = 0.221315
I0926 10:20:27.602354  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221315 (* 1 = 0.221315 loss)
I0926 10:20:27.602360  4406 sgd_solver.cpp:105] Iteration 34000, lr = 0.01
I0926 10:20:42.222111  4406 solver.cpp:218] Iteration 34100 (6.84008 iter/s, 14.6197s/100 iters), loss = 0.260126
I0926 10:20:42.222151  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260126 (* 1 = 0.260126 loss)
I0926 10:20:42.222157  4406 sgd_solver.cpp:105] Iteration 34100, lr = 0.01
I0926 10:20:56.848067  4406 solver.cpp:218] Iteration 34200 (6.83719 iter/s, 14.6259s/100 iters), loss = 0.287406
I0926 10:20:56.848193  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.287406 (* 1 = 0.287406 loss)
I0926 10:20:56.848212  4406 sgd_solver.cpp:105] Iteration 34200, lr = 0.01
I0926 10:21:11.480252  4406 solver.cpp:218] Iteration 34300 (6.83432 iter/s, 14.632s/100 iters), loss = 0.172239
I0926 10:21:11.480293  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.172239 (* 1 = 0.172239 loss)
I0926 10:21:11.480298  4406 sgd_solver.cpp:105] Iteration 34300, lr = 0.01
I0926 10:21:26.098084  4406 solver.cpp:218] Iteration 34400 (6.841 iter/s, 14.6178s/100 iters), loss = 0.216832
I0926 10:21:26.098115  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216832 (* 1 = 0.216832 loss)
I0926 10:21:26.098121  4406 sgd_solver.cpp:105] Iteration 34400, lr = 0.01
I0926 10:21:39.990959  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:21:40.577443  4406 solver.cpp:330] Iteration 34500, Testing net (#0)
I0926 10:21:44.003311  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:21:44.146476  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8444
I0926 10:21:44.146512  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.465 (* 1 = 0.465 loss)
I0926 10:21:44.290873  4406 solver.cpp:218] Iteration 34500 (5.49671 iter/s, 18.1927s/100 iters), loss = 0.247371
I0926 10:21:44.290902  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247371 (* 1 = 0.247371 loss)
I0926 10:21:44.290908  4406 sgd_solver.cpp:105] Iteration 34500, lr = 0.01
I0926 10:21:58.896558  4406 solver.cpp:218] Iteration 34600 (6.84668 iter/s, 14.6056s/100 iters), loss = 0.254927
I0926 10:21:58.896597  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254927 (* 1 = 0.254927 loss)
I0926 10:21:58.896603  4406 sgd_solver.cpp:105] Iteration 34600, lr = 0.01
I0926 10:22:13.514986  4406 solver.cpp:218] Iteration 34700 (6.84072 iter/s, 14.6184s/100 iters), loss = 0.221254
I0926 10:22:13.515089  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221253 (* 1 = 0.221253 loss)
I0926 10:22:13.515106  4406 sgd_solver.cpp:105] Iteration 34700, lr = 0.01
I0926 10:22:28.132218  4406 solver.cpp:218] Iteration 34800 (6.8413 iter/s, 14.6171s/100 iters), loss = 0.212788
I0926 10:22:28.132259  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212788 (* 1 = 0.212788 loss)
I0926 10:22:28.132266  4406 sgd_solver.cpp:105] Iteration 34800, lr = 0.01
I0926 10:22:42.745090  4406 solver.cpp:218] Iteration 34900 (6.84332 iter/s, 14.6128s/100 iters), loss = 0.269926
I0926 10:22:42.745118  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269925 (* 1 = 0.269925 loss)
I0926 10:22:42.745124  4406 sgd_solver.cpp:105] Iteration 34900, lr = 0.01
I0926 10:22:56.630097  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:22:57.214845  4406 solver.cpp:330] Iteration 35000, Testing net (#0)
I0926 10:23:00.641533  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:23:00.784034  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.842
I0926 10:23:00.784070  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.480341 (* 1 = 0.480341 loss)
I0926 10:23:00.928292  4406 solver.cpp:218] Iteration 35000 (5.4996 iter/s, 18.1831s/100 iters), loss = 0.23399
I0926 10:23:00.928319  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23399 (* 1 = 0.23399 loss)
I0926 10:23:00.928325  4406 sgd_solver.cpp:105] Iteration 35000, lr = 0.01
I0926 10:23:15.545127  4406 solver.cpp:218] Iteration 35100 (6.84146 iter/s, 14.6168s/100 iters), loss = 0.249839
I0926 10:23:15.545157  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249838 (* 1 = 0.249838 loss)
I0926 10:23:15.545163  4406 sgd_solver.cpp:105] Iteration 35100, lr = 0.01
I0926 10:23:30.161629  4406 solver.cpp:218] Iteration 35200 (6.84161 iter/s, 14.6164s/100 iters), loss = 0.247482
I0926 10:23:30.161772  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247482 (* 1 = 0.247482 loss)
I0926 10:23:30.161789  4406 sgd_solver.cpp:105] Iteration 35200, lr = 0.01
I0926 10:23:44.771662  4406 solver.cpp:218] Iteration 35300 (6.84469 iter/s, 14.6099s/100 iters), loss = 0.212431
I0926 10:23:44.771692  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21243 (* 1 = 0.21243 loss)
I0926 10:23:44.771698  4406 sgd_solver.cpp:105] Iteration 35300, lr = 0.01
I0926 10:23:59.393436  4406 solver.cpp:218] Iteration 35400 (6.83915 iter/s, 14.6217s/100 iters), loss = 0.247804
I0926 10:23:59.393476  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.247804 (* 1 = 0.247804 loss)
I0926 10:23:59.393481  4406 sgd_solver.cpp:105] Iteration 35400, lr = 0.01
I0926 10:24:13.287356  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:24:13.872627  4406 solver.cpp:330] Iteration 35500, Testing net (#0)
I0926 10:24:17.297206  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:24:17.440202  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8397
I0926 10:24:17.440237  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.491854 (* 1 = 0.491854 loss)
I0926 10:24:17.584686  4406 solver.cpp:218] Iteration 35500 (5.49717 iter/s, 18.1912s/100 iters), loss = 0.278203
I0926 10:24:17.584715  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278203 (* 1 = 0.278203 loss)
I0926 10:24:17.584722  4406 sgd_solver.cpp:105] Iteration 35500, lr = 0.01
I0926 10:24:32.201211  4406 solver.cpp:218] Iteration 35600 (6.8416 iter/s, 14.6165s/100 iters), loss = 0.20989
I0926 10:24:32.201251  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20989 (* 1 = 0.20989 loss)
I0926 10:24:32.201257  4406 sgd_solver.cpp:105] Iteration 35600, lr = 0.01
I0926 10:24:46.821843  4406 solver.cpp:218] Iteration 35700 (6.83968 iter/s, 14.6206s/100 iters), loss = 0.270694
I0926 10:24:46.821985  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270694 (* 1 = 0.270694 loss)
I0926 10:24:46.821992  4406 sgd_solver.cpp:105] Iteration 35700, lr = 0.01
I0926 10:25:01.443382  4406 solver.cpp:218] Iteration 35800 (6.8393 iter/s, 14.6214s/100 iters), loss = 0.319101
I0926 10:25:01.443409  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319101 (* 1 = 0.319101 loss)
I0926 10:25:01.443414  4406 sgd_solver.cpp:105] Iteration 35800, lr = 0.01
I0926 10:25:16.067684  4406 solver.cpp:218] Iteration 35900 (6.83796 iter/s, 14.6242s/100 iters), loss = 0.167433
I0926 10:25:16.067716  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167433 (* 1 = 0.167433 loss)
I0926 10:25:16.067723  4406 sgd_solver.cpp:105] Iteration 35900, lr = 0.01
I0926 10:25:29.957569  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:25:30.542444  4406 solver.cpp:330] Iteration 36000, Testing net (#0)
I0926 10:25:33.967217  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:25:34.110134  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8416
I0926 10:25:34.110169  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.471015 (* 1 = 0.471015 loss)
I0926 10:25:34.254879  4406 solver.cpp:218] Iteration 36000 (5.4984 iter/s, 18.1871s/100 iters), loss = 0.216077
I0926 10:25:34.254911  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216077 (* 1 = 0.216077 loss)
I0926 10:25:34.254918  4406 sgd_solver.cpp:105] Iteration 36000, lr = 0.01
I0926 10:25:48.871376  4406 solver.cpp:218] Iteration 36100 (6.84162 iter/s, 14.6164s/100 iters), loss = 0.269956
I0926 10:25:48.871407  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.269955 (* 1 = 0.269955 loss)
I0926 10:25:48.871412  4406 sgd_solver.cpp:105] Iteration 36100, lr = 0.01
I0926 10:26:03.485386  4406 solver.cpp:218] Iteration 36200 (6.84278 iter/s, 14.6139s/100 iters), loss = 0.318989
I0926 10:26:03.485496  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.318989 (* 1 = 0.318989 loss)
I0926 10:26:03.485503  4406 sgd_solver.cpp:105] Iteration 36200, lr = 0.01
I0926 10:26:18.093272  4406 solver.cpp:218] Iteration 36300 (6.84568 iter/s, 14.6078s/100 iters), loss = 0.178971
I0926 10:26:18.093302  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178971 (* 1 = 0.178971 loss)
I0926 10:26:18.093308  4406 sgd_solver.cpp:105] Iteration 36300, lr = 0.01
I0926 10:26:32.707635  4406 solver.cpp:218] Iteration 36400 (6.84261 iter/s, 14.6143s/100 iters), loss = 0.308443
I0926 10:26:32.707665  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.308443 (* 1 = 0.308443 loss)
I0926 10:26:32.707681  4406 sgd_solver.cpp:105] Iteration 36400, lr = 0.01
I0926 10:26:46.591243  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:26:47.177382  4406 solver.cpp:330] Iteration 36500, Testing net (#0)
I0926 10:26:50.600756  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:26:50.743712  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8255
I0926 10:26:50.743748  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535439 (* 1 = 0.535439 loss)
I0926 10:26:50.889606  4406 solver.cpp:218] Iteration 36500 (5.49998 iter/s, 18.1819s/100 iters), loss = 0.180389
I0926 10:26:50.889636  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180388 (* 1 = 0.180388 loss)
I0926 10:26:50.889643  4406 sgd_solver.cpp:105] Iteration 36500, lr = 0.01
I0926 10:27:05.504921  4406 solver.cpp:218] Iteration 36600 (6.84217 iter/s, 14.6153s/100 iters), loss = 0.198458
I0926 10:27:05.504951  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198458 (* 1 = 0.198458 loss)
I0926 10:27:05.504956  4406 sgd_solver.cpp:105] Iteration 36600, lr = 0.01
I0926 10:27:20.121246  4406 solver.cpp:218] Iteration 36700 (6.8417 iter/s, 14.6163s/100 iters), loss = 0.218186
I0926 10:27:20.121361  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218186 (* 1 = 0.218186 loss)
I0926 10:27:20.121377  4406 sgd_solver.cpp:105] Iteration 36700, lr = 0.01
I0926 10:27:34.734469  4406 solver.cpp:218] Iteration 36800 (6.84318 iter/s, 14.6131s/100 iters), loss = 0.191115
I0926 10:27:34.734499  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191115 (* 1 = 0.191115 loss)
I0926 10:27:34.734505  4406 sgd_solver.cpp:105] Iteration 36800, lr = 0.01
I0926 10:27:49.354275  4406 solver.cpp:218] Iteration 36900 (6.84007 iter/s, 14.6197s/100 iters), loss = 0.300752
I0926 10:27:49.354303  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.300752 (* 1 = 0.300752 loss)
I0926 10:27:49.354308  4406 sgd_solver.cpp:105] Iteration 36900, lr = 0.01
I0926 10:28:03.251268  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:28:03.835153  4406 solver.cpp:330] Iteration 37000, Testing net (#0)
I0926 10:28:07.259234  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:28:07.401914  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8412
I0926 10:28:07.401949  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.47283 (* 1 = 0.47283 loss)
I0926 10:28:07.546381  4406 solver.cpp:218] Iteration 37000 (5.49691 iter/s, 18.192s/100 iters), loss = 0.245592
I0926 10:28:07.546411  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245592 (* 1 = 0.245592 loss)
I0926 10:28:07.546417  4406 sgd_solver.cpp:105] Iteration 37000, lr = 0.01
I0926 10:28:22.163995  4406 solver.cpp:218] Iteration 37100 (6.84109 iter/s, 14.6175s/100 iters), loss = 0.267617
I0926 10:28:22.164036  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267617 (* 1 = 0.267617 loss)
I0926 10:28:22.164041  4406 sgd_solver.cpp:105] Iteration 37100, lr = 0.01
I0926 10:28:36.773444  4406 solver.cpp:218] Iteration 37200 (6.84492 iter/s, 14.6094s/100 iters), loss = 0.218338
I0926 10:28:36.773557  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.218338 (* 1 = 0.218338 loss)
I0926 10:28:36.773577  4406 sgd_solver.cpp:105] Iteration 37200, lr = 0.01
I0926 10:28:51.392009  4406 solver.cpp:218] Iteration 37300 (6.84068 iter/s, 14.6184s/100 iters), loss = 0.19425
I0926 10:28:51.392038  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19425 (* 1 = 0.19425 loss)
I0926 10:28:51.392043  4406 sgd_solver.cpp:105] Iteration 37300, lr = 0.01
I0926 10:29:06.005679  4406 solver.cpp:218] Iteration 37400 (6.84294 iter/s, 14.6136s/100 iters), loss = 0.213597
I0926 10:29:06.005709  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213597 (* 1 = 0.213597 loss)
I0926 10:29:06.005715  4406 sgd_solver.cpp:105] Iteration 37400, lr = 0.01
I0926 10:29:19.893654  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:29:20.478504  4406 solver.cpp:330] Iteration 37500, Testing net (#0)
I0926 10:29:23.904291  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:29:24.047463  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8263
I0926 10:29:24.047499  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.519983 (* 1 = 0.519983 loss)
I0926 10:29:24.192036  4406 solver.cpp:218] Iteration 37500 (5.49865 iter/s, 18.1863s/100 iters), loss = 0.183181
I0926 10:29:24.192066  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183181 (* 1 = 0.183181 loss)
I0926 10:29:24.192073  4406 sgd_solver.cpp:105] Iteration 37500, lr = 0.01
I0926 10:29:38.811220  4406 solver.cpp:218] Iteration 37600 (6.84036 iter/s, 14.6191s/100 iters), loss = 0.250096
I0926 10:29:38.811260  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250096 (* 1 = 0.250096 loss)
I0926 10:29:38.811266  4406 sgd_solver.cpp:105] Iteration 37600, lr = 0.01
I0926 10:29:53.428390  4406 solver.cpp:218] Iteration 37700 (6.84131 iter/s, 14.6171s/100 iters), loss = 0.18178
I0926 10:29:53.428491  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18178 (* 1 = 0.18178 loss)
I0926 10:29:53.428503  4406 sgd_solver.cpp:105] Iteration 37700, lr = 0.01
I0926 10:30:08.049187  4406 solver.cpp:218] Iteration 37800 (6.83964 iter/s, 14.6207s/100 iters), loss = 0.231794
I0926 10:30:08.049219  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231794 (* 1 = 0.231794 loss)
I0926 10:30:08.049226  4406 sgd_solver.cpp:105] Iteration 37800, lr = 0.01
I0926 10:30:22.660686  4406 solver.cpp:218] Iteration 37900 (6.84396 iter/s, 14.6114s/100 iters), loss = 0.192525
I0926 10:30:22.660715  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192525 (* 1 = 0.192525 loss)
I0926 10:30:22.660720  4406 sgd_solver.cpp:105] Iteration 37900, lr = 0.01
I0926 10:30:36.557230  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:30:37.141764  4406 solver.cpp:330] Iteration 38000, Testing net (#0)
I0926 10:30:40.565193  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:30:40.707940  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8234
I0926 10:30:40.707975  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.541455 (* 1 = 0.541455 loss)
I0926 10:30:40.852442  4406 solver.cpp:218] Iteration 38000 (5.49702 iter/s, 18.1917s/100 iters), loss = 0.216999
I0926 10:30:40.852471  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216999 (* 1 = 0.216999 loss)
I0926 10:30:40.852478  4406 sgd_solver.cpp:105] Iteration 38000, lr = 0.01
I0926 10:30:55.470976  4406 solver.cpp:218] Iteration 38100 (6.84066 iter/s, 14.6185s/100 iters), loss = 0.235296
I0926 10:30:55.471005  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235296 (* 1 = 0.235296 loss)
I0926 10:30:55.471011  4406 sgd_solver.cpp:105] Iteration 38100, lr = 0.01
I0926 10:31:10.095336  4406 solver.cpp:218] Iteration 38200 (6.83794 iter/s, 14.6243s/100 iters), loss = 0.191952
I0926 10:31:10.095438  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191952 (* 1 = 0.191952 loss)
I0926 10:31:10.095446  4406 sgd_solver.cpp:105] Iteration 38200, lr = 0.01
I0926 10:31:24.713969  4406 solver.cpp:218] Iteration 38300 (6.84064 iter/s, 14.6185s/100 iters), loss = 0.279028
I0926 10:31:24.714010  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.279028 (* 1 = 0.279028 loss)
I0926 10:31:24.714016  4406 sgd_solver.cpp:105] Iteration 38300, lr = 0.01
I0926 10:31:39.334545  4406 solver.cpp:218] Iteration 38400 (6.83971 iter/s, 14.6205s/100 iters), loss = 0.191151
I0926 10:31:39.334586  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191151 (* 1 = 0.191151 loss)
I0926 10:31:39.334592  4406 sgd_solver.cpp:105] Iteration 38400, lr = 0.01
I0926 10:31:53.226073  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:31:53.812322  4406 solver.cpp:330] Iteration 38500, Testing net (#0)
I0926 10:31:57.237674  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:31:57.380822  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8315
I0926 10:31:57.380858  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.523696 (* 1 = 0.523696 loss)
I0926 10:31:57.525133  4406 solver.cpp:218] Iteration 38500 (5.49737 iter/s, 18.1905s/100 iters), loss = 0.220871
I0926 10:31:57.525163  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220871 (* 1 = 0.220871 loss)
I0926 10:31:57.525169  4406 sgd_solver.cpp:105] Iteration 38500, lr = 0.01
I0926 10:32:12.138146  4406 solver.cpp:218] Iteration 38600 (6.84325 iter/s, 14.6129s/100 iters), loss = 0.222191
I0926 10:32:12.138186  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222191 (* 1 = 0.222191 loss)
I0926 10:32:12.138191  4406 sgd_solver.cpp:105] Iteration 38600, lr = 0.01
I0926 10:32:26.759191  4406 solver.cpp:218] Iteration 38700 (6.8395 iter/s, 14.621s/100 iters), loss = 0.23957
I0926 10:32:26.759289  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23957 (* 1 = 0.23957 loss)
I0926 10:32:26.759295  4406 sgd_solver.cpp:105] Iteration 38700, lr = 0.01
I0926 10:32:41.373486  4406 solver.cpp:218] Iteration 38800 (6.84268 iter/s, 14.6142s/100 iters), loss = 0.241293
I0926 10:32:41.373517  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241293 (* 1 = 0.241293 loss)
I0926 10:32:41.373522  4406 sgd_solver.cpp:105] Iteration 38800, lr = 0.01
I0926 10:32:55.993638  4406 solver.cpp:218] Iteration 38900 (6.83991 iter/s, 14.6201s/100 iters), loss = 0.196702
I0926 10:32:55.993676  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196702 (* 1 = 0.196702 loss)
I0926 10:32:55.993682  4406 sgd_solver.cpp:105] Iteration 38900, lr = 0.01
I0926 10:33:09.887058  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:33:10.471400  4406 solver.cpp:330] Iteration 39000, Testing net (#0)
I0926 10:33:13.895720  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:33:14.037933  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8361
I0926 10:33:14.037967  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.521293 (* 1 = 0.521293 loss)
I0926 10:33:14.182574  4406 solver.cpp:218] Iteration 39000 (5.49787 iter/s, 18.1889s/100 iters), loss = 0.168572
I0926 10:33:14.182601  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168572 (* 1 = 0.168572 loss)
I0926 10:33:14.182608  4406 sgd_solver.cpp:105] Iteration 39000, lr = 0.01
I0926 10:33:28.804003  4406 solver.cpp:218] Iteration 39100 (6.83931 iter/s, 14.6214s/100 iters), loss = 0.26314
I0926 10:33:28.804033  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26314 (* 1 = 0.26314 loss)
I0926 10:33:28.804039  4406 sgd_solver.cpp:105] Iteration 39100, lr = 0.01
I0926 10:33:43.422948  4406 solver.cpp:218] Iteration 39200 (6.84047 iter/s, 14.6189s/100 iters), loss = 0.205833
I0926 10:33:43.423053  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205833 (* 1 = 0.205833 loss)
I0926 10:33:43.423059  4406 sgd_solver.cpp:105] Iteration 39200, lr = 0.01
I0926 10:33:58.044201  4406 solver.cpp:218] Iteration 39300 (6.83942 iter/s, 14.6211s/100 iters), loss = 0.181417
I0926 10:33:58.044241  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181417 (* 1 = 0.181417 loss)
I0926 10:33:58.044247  4406 sgd_solver.cpp:105] Iteration 39300, lr = 0.01
I0926 10:34:12.663328  4406 solver.cpp:218] Iteration 39400 (6.84039 iter/s, 14.6191s/100 iters), loss = 0.199263
I0926 10:34:12.663369  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199263 (* 1 = 0.199263 loss)
I0926 10:34:12.663375  4406 sgd_solver.cpp:105] Iteration 39400, lr = 0.01
I0926 10:34:26.558434  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:34:27.141614  4406 solver.cpp:330] Iteration 39500, Testing net (#0)
I0926 10:34:30.564215  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:34:30.707168  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8375
I0926 10:34:30.707204  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.490795 (* 1 = 0.490795 loss)
I0926 10:34:30.851668  4406 solver.cpp:218] Iteration 39500 (5.49805 iter/s, 18.1883s/100 iters), loss = 0.19123
I0926 10:34:30.851697  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19123 (* 1 = 0.19123 loss)
I0926 10:34:30.851704  4406 sgd_solver.cpp:105] Iteration 39500, lr = 0.01
I0926 10:34:45.465286  4406 solver.cpp:218] Iteration 39600 (6.84296 iter/s, 14.6136s/100 iters), loss = 0.151595
I0926 10:34:45.465315  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151595 (* 1 = 0.151595 loss)
I0926 10:34:45.465322  4406 sgd_solver.cpp:105] Iteration 39600, lr = 0.01
I0926 10:35:00.078454  4406 solver.cpp:218] Iteration 39700 (6.84317 iter/s, 14.6131s/100 iters), loss = 0.212789
I0926 10:35:00.078580  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212789 (* 1 = 0.212789 loss)
I0926 10:35:00.078588  4406 sgd_solver.cpp:105] Iteration 39700, lr = 0.01
I0926 10:35:14.693162  4406 solver.cpp:218] Iteration 39800 (6.8425 iter/s, 14.6145s/100 iters), loss = 0.189138
I0926 10:35:14.693202  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.189138 (* 1 = 0.189138 loss)
I0926 10:35:14.693208  4406 sgd_solver.cpp:105] Iteration 39800, lr = 0.01
I0926 10:35:29.308997  4406 solver.cpp:218] Iteration 39900 (6.84193 iter/s, 14.6158s/100 iters), loss = 0.217833
I0926 10:35:29.309039  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.217833 (* 1 = 0.217833 loss)
I0926 10:35:29.309046  4406 sgd_solver.cpp:105] Iteration 39900, lr = 0.01
I0926 10:35:43.194000  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:35:43.778019  4406 solver.cpp:330] Iteration 40000, Testing net (#0)
I0926 10:35:47.203727  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:35:47.346804  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8412
I0926 10:35:47.346840  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.481988 (* 1 = 0.481988 loss)
I0926 10:35:47.491689  4406 solver.cpp:218] Iteration 40000 (5.49976 iter/s, 18.1826s/100 iters), loss = 0.19223
I0926 10:35:47.491719  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19223 (* 1 = 0.19223 loss)
I0926 10:35:47.491724  4406 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I0926 10:36:02.122838  4406 solver.cpp:218] Iteration 40100 (6.83476 iter/s, 14.6311s/100 iters), loss = 0.196024
I0926 10:36:02.122877  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196024 (* 1 = 0.196024 loss)
I0926 10:36:02.122884  4406 sgd_solver.cpp:105] Iteration 40100, lr = 0.01
I0926 10:36:16.740792  4406 solver.cpp:218] Iteration 40200 (6.84094 iter/s, 14.6179s/100 iters), loss = 0.198909
I0926 10:36:16.740932  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198909 (* 1 = 0.198909 loss)
I0926 10:36:16.740942  4406 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I0926 10:36:31.362136  4406 solver.cpp:218] Iteration 40300 (6.83939 iter/s, 14.6212s/100 iters), loss = 0.224981
I0926 10:36:31.362176  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224981 (* 1 = 0.224981 loss)
I0926 10:36:31.362182  4406 sgd_solver.cpp:105] Iteration 40300, lr = 0.01
I0926 10:36:45.988164  4406 solver.cpp:218] Iteration 40400 (6.83716 iter/s, 14.626s/100 iters), loss = 0.176533
I0926 10:36:45.988193  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176533 (* 1 = 0.176533 loss)
I0926 10:36:45.988198  4406 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I0926 10:36:59.882107  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:37:00.468870  4406 solver.cpp:330] Iteration 40500, Testing net (#0)
I0926 10:37:03.894583  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:37:04.037345  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.843
I0926 10:37:04.037380  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.48829 (* 1 = 0.48829 loss)
I0926 10:37:04.181388  4406 solver.cpp:218] Iteration 40500 (5.49657 iter/s, 18.1932s/100 iters), loss = 0.194269
I0926 10:37:04.181416  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194269 (* 1 = 0.194269 loss)
I0926 10:37:04.181422  4406 sgd_solver.cpp:105] Iteration 40500, lr = 0.01
I0926 10:37:18.804539  4406 solver.cpp:218] Iteration 40600 (6.8385 iter/s, 14.6231s/100 iters), loss = 0.183007
I0926 10:37:18.804570  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183007 (* 1 = 0.183007 loss)
I0926 10:37:18.804577  4406 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I0926 10:37:33.420655  4406 solver.cpp:218] Iteration 40700 (6.84179 iter/s, 14.616s/100 iters), loss = 0.210091
I0926 10:37:33.420744  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210091 (* 1 = 0.210091 loss)
I0926 10:37:33.420761  4406 sgd_solver.cpp:105] Iteration 40700, lr = 0.01
I0926 10:37:48.039492  4406 solver.cpp:218] Iteration 40800 (6.84055 iter/s, 14.6187s/100 iters), loss = 0.206262
I0926 10:37:48.039521  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206262 (* 1 = 0.206262 loss)
I0926 10:37:48.039527  4406 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I0926 10:38:02.660015  4406 solver.cpp:218] Iteration 40900 (6.83973 iter/s, 14.6205s/100 iters), loss = 0.219091
I0926 10:38:02.660045  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219091 (* 1 = 0.219091 loss)
I0926 10:38:02.660051  4406 sgd_solver.cpp:105] Iteration 40900, lr = 0.01
I0926 10:38:16.555244  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:38:17.141950  4406 solver.cpp:330] Iteration 41000, Testing net (#0)
I0926 10:38:20.563432  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:38:20.706194  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8544
I0926 10:38:20.706229  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.445285 (* 1 = 0.445285 loss)
I0926 10:38:20.851155  4406 solver.cpp:218] Iteration 41000 (5.4972 iter/s, 18.1911s/100 iters), loss = 0.207887
I0926 10:38:20.851187  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207887 (* 1 = 0.207887 loss)
I0926 10:38:20.851194  4406 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I0926 10:38:35.463604  4406 solver.cpp:218] Iteration 41100 (6.84351 iter/s, 14.6124s/100 iters), loss = 0.24787
I0926 10:38:35.463644  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24787 (* 1 = 0.24787 loss)
I0926 10:38:35.463660  4406 sgd_solver.cpp:105] Iteration 41100, lr = 0.01
I0926 10:38:50.079358  4406 solver.cpp:218] Iteration 41200 (6.84197 iter/s, 14.6157s/100 iters), loss = 0.182199
I0926 10:38:50.079457  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.182199 (* 1 = 0.182199 loss)
I0926 10:38:50.079473  4406 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I0926 10:39:04.695384  4406 solver.cpp:218] Iteration 41300 (6.84187 iter/s, 14.6159s/100 iters), loss = 0.318915
I0926 10:39:04.695413  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.318915 (* 1 = 0.318915 loss)
I0926 10:39:04.695420  4406 sgd_solver.cpp:105] Iteration 41300, lr = 0.01
I0926 10:39:19.307296  4406 solver.cpp:218] Iteration 41400 (6.84376 iter/s, 14.6118s/100 iters), loss = 0.209981
I0926 10:39:19.307327  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209981 (* 1 = 0.209981 loss)
I0926 10:39:19.307333  4406 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I0926 10:39:33.194398  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:39:33.779350  4406 solver.cpp:330] Iteration 41500, Testing net (#0)
I0926 10:39:37.204468  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:39:37.346905  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8517
I0926 10:39:37.346941  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.44411 (* 1 = 0.44411 loss)
I0926 10:39:37.491619  4406 solver.cpp:218] Iteration 41500 (5.49927 iter/s, 18.1842s/100 iters), loss = 0.207239
I0926 10:39:37.491647  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207239 (* 1 = 0.207239 loss)
I0926 10:39:37.491652  4406 sgd_solver.cpp:105] Iteration 41500, lr = 0.01
I0926 10:39:52.105468  4406 solver.cpp:218] Iteration 41600 (6.84285 iter/s, 14.6138s/100 iters), loss = 0.173446
I0926 10:39:52.105499  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173446 (* 1 = 0.173446 loss)
I0926 10:39:52.105505  4406 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I0926 10:40:06.727360  4406 solver.cpp:218] Iteration 41700 (6.83909 iter/s, 14.6218s/100 iters), loss = 0.25645
I0926 10:40:06.727484  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25645 (* 1 = 0.25645 loss)
I0926 10:40:06.727501  4406 sgd_solver.cpp:105] Iteration 41700, lr = 0.01
I0926 10:40:21.355453  4406 solver.cpp:218] Iteration 41800 (6.83624 iter/s, 14.6279s/100 iters), loss = 0.179436
I0926 10:40:21.355494  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179435 (* 1 = 0.179435 loss)
I0926 10:40:21.355499  4406 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I0926 10:40:35.976492  4406 solver.cpp:218] Iteration 41900 (6.8395 iter/s, 14.621s/100 iters), loss = 0.202721
I0926 10:40:35.976524  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202721 (* 1 = 0.202721 loss)
I0926 10:40:35.976531  4406 sgd_solver.cpp:105] Iteration 41900, lr = 0.01
I0926 10:40:49.871595  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:40:50.456307  4406 solver.cpp:330] Iteration 42000, Testing net (#0)
I0926 10:40:53.880028  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:40:54.023205  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8342
I0926 10:40:54.023242  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.512289 (* 1 = 0.512289 loss)
I0926 10:40:54.168988  4406 solver.cpp:218] Iteration 42000 (5.49679 iter/s, 18.1924s/100 iters), loss = 0.215131
I0926 10:40:54.169015  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215131 (* 1 = 0.215131 loss)
I0926 10:40:54.169021  4406 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I0926 10:41:08.800387  4406 solver.cpp:218] Iteration 42100 (6.83465 iter/s, 14.6313s/100 iters), loss = 0.202353
I0926 10:41:08.800418  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202353 (* 1 = 0.202353 loss)
I0926 10:41:08.800424  4406 sgd_solver.cpp:105] Iteration 42100, lr = 0.01
I0926 10:41:23.436460  4406 solver.cpp:218] Iteration 42200 (6.83247 iter/s, 14.636s/100 iters), loss = 0.183596
I0926 10:41:23.436568  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183596 (* 1 = 0.183596 loss)
I0926 10:41:23.436575  4406 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I0926 10:41:38.069861  4406 solver.cpp:218] Iteration 42300 (6.83375 iter/s, 14.6333s/100 iters), loss = 0.176257
I0926 10:41:38.069900  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176257 (* 1 = 0.176257 loss)
I0926 10:41:38.069906  4406 sgd_solver.cpp:105] Iteration 42300, lr = 0.01
I0926 10:41:52.696887  4406 solver.cpp:218] Iteration 42400 (6.8367 iter/s, 14.6269s/100 iters), loss = 0.112069
I0926 10:41:52.696916  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.112069 (* 1 = 0.112069 loss)
I0926 10:41:52.696923  4406 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I0926 10:42:06.602778  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:42:07.187042  4406 solver.cpp:330] Iteration 42500, Testing net (#0)
I0926 10:42:10.614233  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:42:10.757130  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8469
I0926 10:42:10.757165  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.466029 (* 1 = 0.466029 loss)
I0926 10:42:10.902278  4406 solver.cpp:218] Iteration 42500 (5.4929 iter/s, 18.2053s/100 iters), loss = 0.194132
I0926 10:42:10.902307  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194132 (* 1 = 0.194132 loss)
I0926 10:42:10.902312  4406 sgd_solver.cpp:105] Iteration 42500, lr = 0.01
I0926 10:42:25.529042  4406 solver.cpp:218] Iteration 42600 (6.83681 iter/s, 14.6267s/100 iters), loss = 0.164032
I0926 10:42:25.529083  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164032 (* 1 = 0.164032 loss)
I0926 10:42:25.529088  4406 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I0926 10:42:40.154528  4406 solver.cpp:218] Iteration 42700 (6.83742 iter/s, 14.6254s/100 iters), loss = 0.14988
I0926 10:42:40.154649  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149879 (* 1 = 0.149879 loss)
I0926 10:42:40.154657  4406 sgd_solver.cpp:105] Iteration 42700, lr = 0.01
I0926 10:42:54.781137  4406 solver.cpp:218] Iteration 42800 (6.83693 iter/s, 14.6265s/100 iters), loss = 0.212998
I0926 10:42:54.781167  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212998 (* 1 = 0.212998 loss)
I0926 10:42:54.781173  4406 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I0926 10:43:09.404497  4406 solver.cpp:218] Iteration 42900 (6.83841 iter/s, 14.6233s/100 iters), loss = 0.159424
I0926 10:43:09.404536  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159424 (* 1 = 0.159424 loss)
I0926 10:43:09.404542  4406 sgd_solver.cpp:105] Iteration 42900, lr = 0.01
I0926 10:43:23.300973  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:43:23.885301  4406 solver.cpp:330] Iteration 43000, Testing net (#0)
I0926 10:43:27.309623  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:43:27.453567  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8231
I0926 10:43:27.453593  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.558813 (* 1 = 0.558813 loss)
I0926 10:43:27.597987  4406 solver.cpp:218] Iteration 43000 (5.4965 iter/s, 18.1934s/100 iters), loss = 0.244214
I0926 10:43:27.598016  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244213 (* 1 = 0.244213 loss)
I0926 10:43:27.598021  4406 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I0926 10:43:42.218091  4406 solver.cpp:218] Iteration 43100 (6.83993 iter/s, 14.62s/100 iters), loss = 0.2731
I0926 10:43:42.218119  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2731 (* 1 = 0.2731 loss)
I0926 10:43:42.218125  4406 sgd_solver.cpp:105] Iteration 43100, lr = 0.01
I0926 10:43:56.834422  4406 solver.cpp:218] Iteration 43200 (6.84169 iter/s, 14.6163s/100 iters), loss = 0.212381
I0926 10:43:56.834534  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212381 (* 1 = 0.212381 loss)
I0926 10:43:56.834542  4406 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I0926 10:44:11.449936  4406 solver.cpp:218] Iteration 43300 (6.84211 iter/s, 14.6154s/100 iters), loss = 0.205973
I0926 10:44:11.449965  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205973 (* 1 = 0.205973 loss)
I0926 10:44:11.449970  4406 sgd_solver.cpp:105] Iteration 43300, lr = 0.01
I0926 10:44:26.065871  4406 solver.cpp:218] Iteration 43400 (6.84188 iter/s, 14.6159s/100 iters), loss = 0.209044
I0926 10:44:26.065910  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209044 (* 1 = 0.209044 loss)
I0926 10:44:26.065917  4406 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I0926 10:44:39.955693  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:44:40.539698  4406 solver.cpp:330] Iteration 43500, Testing net (#0)
I0926 10:44:43.964792  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:44:44.107950  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8482
I0926 10:44:44.107985  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.469922 (* 1 = 0.469922 loss)
I0926 10:44:44.252552  4406 solver.cpp:218] Iteration 43500 (5.49855 iter/s, 18.1866s/100 iters), loss = 0.178778
I0926 10:44:44.252580  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178778 (* 1 = 0.178778 loss)
I0926 10:44:44.252586  4406 sgd_solver.cpp:105] Iteration 43500, lr = 0.01
I0926 10:44:58.862563  4406 solver.cpp:218] Iteration 43600 (6.84465 iter/s, 14.6099s/100 iters), loss = 0.226682
I0926 10:44:58.862593  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226682 (* 1 = 0.226682 loss)
I0926 10:44:58.862599  4406 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I0926 10:45:13.484242  4406 solver.cpp:218] Iteration 43700 (6.83919 iter/s, 14.6216s/100 iters), loss = 0.235915
I0926 10:45:13.484372  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235915 (* 1 = 0.235915 loss)
I0926 10:45:13.484390  4406 sgd_solver.cpp:105] Iteration 43700, lr = 0.01
I0926 10:45:28.098855  4406 solver.cpp:218] Iteration 43800 (6.84254 iter/s, 14.6145s/100 iters), loss = 0.271423
I0926 10:45:28.098883  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.271423 (* 1 = 0.271423 loss)
I0926 10:45:28.098889  4406 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I0926 10:45:42.712016  4406 solver.cpp:218] Iteration 43900 (6.84318 iter/s, 14.6131s/100 iters), loss = 0.19687
I0926 10:45:42.712056  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196869 (* 1 = 0.196869 loss)
I0926 10:45:42.712064  4406 sgd_solver.cpp:105] Iteration 43900, lr = 0.01
I0926 10:45:56.600759  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:45:57.186275  4406 solver.cpp:330] Iteration 44000, Testing net (#0)
I0926 10:46:00.611636  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:46:00.754371  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8394
I0926 10:46:00.754406  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.501471 (* 1 = 0.501471 loss)
I0926 10:46:00.899152  4406 solver.cpp:218] Iteration 44000 (5.49842 iter/s, 18.187s/100 iters), loss = 0.148843
I0926 10:46:00.899194  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148843 (* 1 = 0.148843 loss)
I0926 10:46:00.899200  4406 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I0926 10:46:15.522317  4406 solver.cpp:218] Iteration 44100 (6.8385 iter/s, 14.6231s/100 iters), loss = 0.157839
I0926 10:46:15.522347  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.157839 (* 1 = 0.157839 loss)
I0926 10:46:15.522353  4406 sgd_solver.cpp:105] Iteration 44100, lr = 0.01
I0926 10:46:30.142045  4406 solver.cpp:218] Iteration 44200 (6.8401 iter/s, 14.6197s/100 iters), loss = 0.223812
I0926 10:46:30.142138  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223811 (* 1 = 0.223811 loss)
I0926 10:46:30.142144  4406 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I0926 10:46:44.762540  4406 solver.cpp:218] Iteration 44300 (6.83977 iter/s, 14.6204s/100 iters), loss = 0.225037
I0926 10:46:44.762583  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.225036 (* 1 = 0.225036 loss)
I0926 10:46:44.762588  4406 sgd_solver.cpp:105] Iteration 44300, lr = 0.01
I0926 10:46:59.384336  4406 solver.cpp:218] Iteration 44400 (6.83914 iter/s, 14.6217s/100 iters), loss = 0.219703
I0926 10:46:59.384377  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219703 (* 1 = 0.219703 loss)
I0926 10:46:59.384382  4406 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I0926 10:47:13.272022  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:47:13.856374  4406 solver.cpp:330] Iteration 44500, Testing net (#0)
I0926 10:47:17.278971  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:47:17.422163  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8121
I0926 10:47:17.422188  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.620907 (* 1 = 0.620907 loss)
I0926 10:47:17.566238  4406 solver.cpp:218] Iteration 44500 (5.5 iter/s, 18.1818s/100 iters), loss = 0.171816
I0926 10:47:17.566270  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171816 (* 1 = 0.171816 loss)
I0926 10:47:17.566277  4406 sgd_solver.cpp:105] Iteration 44500, lr = 0.01
I0926 10:47:32.192050  4406 solver.cpp:218] Iteration 44600 (6.83726 iter/s, 14.6257s/100 iters), loss = 0.191466
I0926 10:47:32.192090  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.191466 (* 1 = 0.191466 loss)
I0926 10:47:32.192096  4406 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I0926 10:47:46.820616  4406 solver.cpp:218] Iteration 44700 (6.83598 iter/s, 14.6285s/100 iters), loss = 0.176288
I0926 10:47:46.820742  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176287 (* 1 = 0.176287 loss)
I0926 10:47:46.820760  4406 sgd_solver.cpp:105] Iteration 44700, lr = 0.01
I0926 10:48:01.454316  4406 solver.cpp:218] Iteration 44800 (6.83362 iter/s, 14.6335s/100 iters), loss = 0.176195
I0926 10:48:01.454355  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176195 (* 1 = 0.176195 loss)
I0926 10:48:01.454362  4406 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I0926 10:48:16.082924  4406 solver.cpp:218] Iteration 44900 (6.83595 iter/s, 14.6285s/100 iters), loss = 0.145271
I0926 10:48:16.082964  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145271 (* 1 = 0.145271 loss)
I0926 10:48:16.082970  4406 sgd_solver.cpp:105] Iteration 44900, lr = 0.01
I0926 10:48:29.986488  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:48:30.571475  4406 solver.cpp:330] Iteration 45000, Testing net (#0)
I0926 10:48:33.997031  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:48:34.140228  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8369
I0926 10:48:34.140254  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.507938 (* 1 = 0.507938 loss)
I0926 10:48:34.284996  4406 solver.cpp:218] Iteration 45000 (5.49391 iter/s, 18.202s/100 iters), loss = 0.21832
I0926 10:48:34.285022  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21832 (* 1 = 0.21832 loss)
I0926 10:48:34.285028  4406 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I0926 10:48:48.902967  4406 solver.cpp:218] Iteration 45100 (6.84092 iter/s, 14.6179s/100 iters), loss = 0.198537
I0926 10:48:48.902997  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198537 (* 1 = 0.198537 loss)
I0926 10:48:48.903004  4406 sgd_solver.cpp:105] Iteration 45100, lr = 0.01
I0926 10:49:03.525331  4406 solver.cpp:218] Iteration 45200 (6.83887 iter/s, 14.6223s/100 iters), loss = 0.254476
I0926 10:49:03.525475  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254476 (* 1 = 0.254476 loss)
I0926 10:49:03.525483  4406 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I0926 10:49:18.144955  4406 solver.cpp:218] Iteration 45300 (6.8402 iter/s, 14.6194s/100 iters), loss = 0.148732
I0926 10:49:18.144994  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148732 (* 1 = 0.148732 loss)
I0926 10:49:18.145001  4406 sgd_solver.cpp:105] Iteration 45300, lr = 0.01
I0926 10:49:32.766374  4406 solver.cpp:218] Iteration 45400 (6.83932 iter/s, 14.6213s/100 iters), loss = 0.173072
I0926 10:49:32.766414  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173072 (* 1 = 0.173072 loss)
I0926 10:49:32.766420  4406 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I0926 10:49:46.660591  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:49:47.246800  4406 solver.cpp:330] Iteration 45500, Testing net (#0)
I0926 10:49:50.670657  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:49:50.813354  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8498
I0926 10:49:50.813390  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.463472 (* 1 = 0.463472 loss)
I0926 10:49:50.958199  4406 solver.cpp:218] Iteration 45500 (5.497 iter/s, 18.1917s/100 iters), loss = 0.116185
I0926 10:49:50.958230  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.116185 (* 1 = 0.116185 loss)
I0926 10:49:50.958237  4406 sgd_solver.cpp:105] Iteration 45500, lr = 0.01
I0926 10:50:05.580365  4406 solver.cpp:218] Iteration 45600 (6.83896 iter/s, 14.6221s/100 iters), loss = 0.18512
I0926 10:50:05.580394  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18512 (* 1 = 0.18512 loss)
I0926 10:50:05.580400  4406 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I0926 10:50:20.201604  4406 solver.cpp:218] Iteration 45700 (6.8394 iter/s, 14.6212s/100 iters), loss = 0.266172
I0926 10:50:20.201742  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266172 (* 1 = 0.266172 loss)
I0926 10:50:20.201759  4406 sgd_solver.cpp:105] Iteration 45700, lr = 0.01
I0926 10:50:34.820489  4406 solver.cpp:218] Iteration 45800 (6.84055 iter/s, 14.6187s/100 iters), loss = 0.243627
I0926 10:50:34.820519  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243627 (* 1 = 0.243627 loss)
I0926 10:50:34.820535  4406 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I0926 10:50:49.437934  4406 solver.cpp:218] Iteration 45900 (6.84117 iter/s, 14.6174s/100 iters), loss = 0.433199
I0926 10:50:49.437963  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.433198 (* 1 = 0.433198 loss)
I0926 10:50:49.437968  4406 sgd_solver.cpp:105] Iteration 45900, lr = 0.01
I0926 10:51:03.327292  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:51:03.912358  4406 solver.cpp:330] Iteration 46000, Testing net (#0)
I0926 10:51:07.337949  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:51:07.481034  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8108
I0926 10:51:07.481070  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.579886 (* 1 = 0.579886 loss)
I0926 10:51:07.625373  4406 solver.cpp:218] Iteration 46000 (5.49832 iter/s, 18.1874s/100 iters), loss = 0.210677
I0926 10:51:07.625407  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210676 (* 1 = 0.210676 loss)
I0926 10:51:07.625414  4406 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I0926 10:51:22.247229  4406 solver.cpp:218] Iteration 46100 (6.83911 iter/s, 14.6218s/100 iters), loss = 0.179923
I0926 10:51:22.247269  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.179922 (* 1 = 0.179922 loss)
I0926 10:51:22.247275  4406 sgd_solver.cpp:105] Iteration 46100, lr = 0.01
I0926 10:51:36.863250  4406 solver.cpp:218] Iteration 46200 (6.84184 iter/s, 14.6159s/100 iters), loss = 0.178437
I0926 10:51:36.863390  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178437 (* 1 = 0.178437 loss)
I0926 10:51:36.863399  4406 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I0926 10:51:51.486507  4406 solver.cpp:218] Iteration 46300 (6.8385 iter/s, 14.6231s/100 iters), loss = 0.222364
I0926 10:51:51.486536  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222363 (* 1 = 0.222363 loss)
I0926 10:51:51.486542  4406 sgd_solver.cpp:105] Iteration 46300, lr = 0.01
I0926 10:52:06.108726  4406 solver.cpp:218] Iteration 46400 (6.83894 iter/s, 14.6222s/100 iters), loss = 0.164193
I0926 10:52:06.108767  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164193 (* 1 = 0.164193 loss)
I0926 10:52:06.108773  4406 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I0926 10:52:20.004787  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:52:20.590611  4406 solver.cpp:330] Iteration 46500, Testing net (#0)
I0926 10:52:24.015605  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:52:24.158602  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8416
I0926 10:52:24.158638  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.4788 (* 1 = 0.4788 loss)
I0926 10:52:24.303449  4406 solver.cpp:218] Iteration 46500 (5.49612 iter/s, 18.1946s/100 iters), loss = 0.195747
I0926 10:52:24.303477  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195747 (* 1 = 0.195747 loss)
I0926 10:52:24.303483  4406 sgd_solver.cpp:105] Iteration 46500, lr = 0.01
I0926 10:52:38.927098  4406 solver.cpp:218] Iteration 46600 (6.83827 iter/s, 14.6236s/100 iters), loss = 0.223619
I0926 10:52:38.927127  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223619 (* 1 = 0.223619 loss)
I0926 10:52:38.927134  4406 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I0926 10:52:53.550742  4406 solver.cpp:218] Iteration 46700 (6.83827 iter/s, 14.6236s/100 iters), loss = 0.201509
I0926 10:52:53.550902  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.201508 (* 1 = 0.201508 loss)
I0926 10:52:53.550912  4406 sgd_solver.cpp:105] Iteration 46700, lr = 0.01
I0926 10:53:08.175102  4406 solver.cpp:218] Iteration 46800 (6.83799 iter/s, 14.6242s/100 iters), loss = 0.226842
I0926 10:53:08.175143  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226841 (* 1 = 0.226841 loss)
I0926 10:53:08.175148  4406 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I0926 10:53:22.799942  4406 solver.cpp:218] Iteration 46900 (6.83772 iter/s, 14.6248s/100 iters), loss = 0.132023
I0926 10:53:22.799981  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132022 (* 1 = 0.132022 loss)
I0926 10:53:22.799988  4406 sgd_solver.cpp:105] Iteration 46900, lr = 0.01
I0926 10:53:36.700274  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:53:37.283807  4406 solver.cpp:330] Iteration 47000, Testing net (#0)
I0926 10:53:40.708266  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:53:40.850478  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8331
I0926 10:53:40.850515  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.526976 (* 1 = 0.526976 loss)
I0926 10:53:40.995252  4406 solver.cpp:218] Iteration 47000 (5.49595 iter/s, 18.1952s/100 iters), loss = 0.214098
I0926 10:53:40.995282  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214097 (* 1 = 0.214097 loss)
I0926 10:53:40.995290  4406 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I0926 10:53:55.626951  4406 solver.cpp:218] Iteration 47100 (6.83451 iter/s, 14.6316s/100 iters), loss = 0.292321
I0926 10:53:55.626981  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.29232 (* 1 = 0.29232 loss)
I0926 10:53:55.626987  4406 sgd_solver.cpp:105] Iteration 47100, lr = 0.01
I0926 10:54:10.253228  4406 solver.cpp:218] Iteration 47200 (6.83704 iter/s, 14.6262s/100 iters), loss = 0.22659
I0926 10:54:10.253334  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22659 (* 1 = 0.22659 loss)
I0926 10:54:10.253341  4406 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I0926 10:54:24.877365  4406 solver.cpp:218] Iteration 47300 (6.83808 iter/s, 14.624s/100 iters), loss = 0.206249
I0926 10:54:24.877396  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206248 (* 1 = 0.206248 loss)
I0926 10:54:24.877403  4406 sgd_solver.cpp:105] Iteration 47300, lr = 0.01
I0926 10:54:39.507086  4406 solver.cpp:218] Iteration 47400 (6.83543 iter/s, 14.6297s/100 iters), loss = 0.152662
I0926 10:54:39.507125  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.152662 (* 1 = 0.152662 loss)
I0926 10:54:39.507131  4406 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I0926 10:54:53.411962  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:54:53.997035  4406 solver.cpp:330] Iteration 47500, Testing net (#0)
I0926 10:54:57.420388  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:54:57.563268  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.812
I0926 10:54:57.563304  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.604696 (* 1 = 0.604696 loss)
I0926 10:54:57.707862  4406 solver.cpp:218] Iteration 47500 (5.4943 iter/s, 18.2007s/100 iters), loss = 0.215195
I0926 10:54:57.707890  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215195 (* 1 = 0.215195 loss)
I0926 10:54:57.707896  4406 sgd_solver.cpp:105] Iteration 47500, lr = 0.01
I0926 10:55:12.325516  4406 solver.cpp:218] Iteration 47600 (6.84107 iter/s, 14.6176s/100 iters), loss = 0.247201
I0926 10:55:12.325546  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2472 (* 1 = 0.2472 loss)
I0926 10:55:12.325552  4406 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I0926 10:55:26.944175  4406 solver.cpp:218] Iteration 47700 (6.84061 iter/s, 14.6186s/100 iters), loss = 0.16708
I0926 10:55:26.944335  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167079 (* 1 = 0.167079 loss)
I0926 10:55:26.944345  4406 sgd_solver.cpp:105] Iteration 47700, lr = 0.01
I0926 10:55:41.567046  4406 solver.cpp:218] Iteration 47800 (6.83869 iter/s, 14.6227s/100 iters), loss = 0.285164
I0926 10:55:41.567087  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.285164 (* 1 = 0.285164 loss)
I0926 10:55:41.567093  4406 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I0926 10:55:56.195483  4406 solver.cpp:218] Iteration 47900 (6.83604 iter/s, 14.6284s/100 iters), loss = 0.148177
I0926 10:55:56.195523  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148177 (* 1 = 0.148177 loss)
I0926 10:55:56.195530  4406 sgd_solver.cpp:105] Iteration 47900, lr = 0.01
I0926 10:56:10.090620  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:56:10.676934  4406 solver.cpp:330] Iteration 48000, Testing net (#0)
I0926 10:56:14.101302  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:56:14.244040  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8395
I0926 10:56:14.244066  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.487174 (* 1 = 0.487174 loss)
I0926 10:56:14.388552  4406 solver.cpp:218] Iteration 48000 (5.49662 iter/s, 18.193s/100 iters), loss = 0.185547
I0926 10:56:14.388578  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185547 (* 1 = 0.185547 loss)
I0926 10:56:14.388586  4406 sgd_solver.cpp:105] Iteration 48000, lr = 0.01
I0926 10:56:29.001698  4406 solver.cpp:218] Iteration 48100 (6.84318 iter/s, 14.6131s/100 iters), loss = 0.203813
I0926 10:56:29.001729  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203813 (* 1 = 0.203813 loss)
I0926 10:56:29.001734  4406 sgd_solver.cpp:105] Iteration 48100, lr = 0.01
I0926 10:56:43.622437  4406 solver.cpp:218] Iteration 48200 (6.83963 iter/s, 14.6207s/100 iters), loss = 0.200863
I0926 10:56:43.622577  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200862 (* 1 = 0.200862 loss)
I0926 10:56:43.622586  4406 sgd_solver.cpp:105] Iteration 48200, lr = 0.01
I0926 10:56:58.239655  4406 solver.cpp:218] Iteration 48300 (6.84133 iter/s, 14.617s/100 iters), loss = 0.215082
I0926 10:56:58.239696  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215081 (* 1 = 0.215081 loss)
I0926 10:56:58.239702  4406 sgd_solver.cpp:105] Iteration 48300, lr = 0.01
I0926 10:57:12.862819  4406 solver.cpp:218] Iteration 48400 (6.8385 iter/s, 14.6231s/100 iters), loss = 0.216945
I0926 10:57:12.862849  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216945 (* 1 = 0.216945 loss)
I0926 10:57:12.862856  4406 sgd_solver.cpp:105] Iteration 48400, lr = 0.01
I0926 10:57:26.757714  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:57:27.343752  4406 solver.cpp:330] Iteration 48500, Testing net (#0)
I0926 10:57:30.769147  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:57:30.912050  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8458
I0926 10:57:30.912084  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.475825 (* 1 = 0.475825 loss)
I0926 10:57:31.057070  4406 solver.cpp:218] Iteration 48500 (5.49626 iter/s, 18.1942s/100 iters), loss = 0.0963947
I0926 10:57:31.057096  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0963944 (* 1 = 0.0963944 loss)
I0926 10:57:31.057103  4406 sgd_solver.cpp:105] Iteration 48500, lr = 0.01
I0926 10:57:45.669373  4406 solver.cpp:218] Iteration 48600 (6.84358 iter/s, 14.6122s/100 iters), loss = 0.210808
I0926 10:57:45.669412  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210807 (* 1 = 0.210807 loss)
I0926 10:57:45.669417  4406 sgd_solver.cpp:105] Iteration 48600, lr = 0.01
I0926 10:58:00.285799  4406 solver.cpp:218] Iteration 48700 (6.84165 iter/s, 14.6163s/100 iters), loss = 0.176815
I0926 10:58:00.285961  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176814 (* 1 = 0.176814 loss)
I0926 10:58:00.285971  4406 sgd_solver.cpp:105] Iteration 48700, lr = 0.01
I0926 10:58:14.904475  4406 solver.cpp:218] Iteration 48800 (6.84065 iter/s, 14.6185s/100 iters), loss = 0.16819
I0926 10:58:14.904520  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168189 (* 1 = 0.168189 loss)
I0926 10:58:14.904525  4406 sgd_solver.cpp:105] Iteration 48800, lr = 0.01
I0926 10:58:29.513587  4406 solver.cpp:218] Iteration 48900 (6.84508 iter/s, 14.609s/100 iters), loss = 0.149795
I0926 10:58:29.513617  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149795 (* 1 = 0.149795 loss)
I0926 10:58:29.513622  4406 sgd_solver.cpp:105] Iteration 48900, lr = 0.01
I0926 10:58:43.398798  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:58:43.984236  4406 solver.cpp:330] Iteration 49000, Testing net (#0)
I0926 10:58:47.408598  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 10:58:47.551167  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.838
I0926 10:58:47.551203  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.505779 (* 1 = 0.505779 loss)
I0926 10:58:47.695922  4406 solver.cpp:218] Iteration 49000 (5.49987 iter/s, 18.1823s/100 iters), loss = 0.123069
I0926 10:58:47.695956  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.123069 (* 1 = 0.123069 loss)
I0926 10:58:47.695962  4406 sgd_solver.cpp:105] Iteration 49000, lr = 0.01
I0926 10:59:02.308861  4406 solver.cpp:218] Iteration 49100 (6.84328 iter/s, 14.6129s/100 iters), loss = 0.180705
I0926 10:59:02.308902  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180705 (* 1 = 0.180705 loss)
I0926 10:59:02.308907  4406 sgd_solver.cpp:105] Iteration 49100, lr = 0.01
I0926 10:59:16.921872  4406 solver.cpp:218] Iteration 49200 (6.84325 iter/s, 14.6129s/100 iters), loss = 0.235389
I0926 10:59:16.921955  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235388 (* 1 = 0.235388 loss)
I0926 10:59:16.921963  4406 sgd_solver.cpp:105] Iteration 49200, lr = 0.01
I0926 10:59:31.537907  4406 solver.cpp:218] Iteration 49300 (6.84186 iter/s, 14.6159s/100 iters), loss = 0.173058
I0926 10:59:31.537936  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173057 (* 1 = 0.173057 loss)
I0926 10:59:31.537941  4406 sgd_solver.cpp:105] Iteration 49300, lr = 0.01
I0926 10:59:46.147842  4406 solver.cpp:218] Iteration 49400 (6.84469 iter/s, 14.6099s/100 iters), loss = 0.155224
I0926 10:59:46.147882  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.155224 (* 1 = 0.155224 loss)
I0926 10:59:46.147887  4406 sgd_solver.cpp:105] Iteration 49400, lr = 0.01
I0926 11:00:00.038460  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:00:00.623167  4406 solver.cpp:330] Iteration 49500, Testing net (#0)
I0926 11:00:04.049634  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:00:04.192497  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8045
I0926 11:00:04.192533  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.665228 (* 1 = 0.665228 loss)
I0926 11:00:04.337501  4406 solver.cpp:218] Iteration 49500 (5.49765 iter/s, 18.1896s/100 iters), loss = 0.145172
I0926 11:00:04.337529  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145172 (* 1 = 0.145172 loss)
I0926 11:00:04.337535  4406 sgd_solver.cpp:105] Iteration 49500, lr = 0.01
I0926 11:00:18.963366  4406 solver.cpp:218] Iteration 49600 (6.83723 iter/s, 14.6258s/100 iters), loss = 0.282421
I0926 11:00:18.963397  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.282421 (* 1 = 0.282421 loss)
I0926 11:00:18.963403  4406 sgd_solver.cpp:105] Iteration 49600, lr = 0.01
I0926 11:00:33.591061  4406 solver.cpp:218] Iteration 49700 (6.83638 iter/s, 14.6276s/100 iters), loss = 0.199536
I0926 11:00:33.591158  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199536 (* 1 = 0.199536 loss)
I0926 11:00:33.591166  4406 sgd_solver.cpp:105] Iteration 49700, lr = 0.01
I0926 11:00:48.215065  4406 solver.cpp:218] Iteration 49800 (6.83814 iter/s, 14.6239s/100 iters), loss = 0.199027
I0926 11:00:48.215095  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199027 (* 1 = 0.199027 loss)
I0926 11:00:48.215101  4406 sgd_solver.cpp:105] Iteration 49800, lr = 0.01
I0926 11:01:02.833153  4406 solver.cpp:218] Iteration 49900 (6.84087 iter/s, 14.618s/100 iters), loss = 0.134663
I0926 11:01:02.833184  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134663 (* 1 = 0.134663 loss)
I0926 11:01:02.833190  4406 sgd_solver.cpp:105] Iteration 49900, lr = 0.01
I0926 11:01:16.727181  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:01:17.311939  4406 solver.cpp:330] Iteration 50000, Testing net (#0)
I0926 11:01:20.737479  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:01:20.880338  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.825
I0926 11:01:20.880365  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.566408 (* 1 = 0.566408 loss)
I0926 11:01:21.024629  4406 solver.cpp:218] Iteration 50000 (5.4971 iter/s, 18.1914s/100 iters), loss = 0.227611
I0926 11:01:21.024660  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.22761 (* 1 = 0.22761 loss)
I0926 11:01:21.024667  4406 sgd_solver.cpp:105] Iteration 50000, lr = 0.01
I0926 11:01:35.637425  4406 solver.cpp:218] Iteration 50100 (6.84335 iter/s, 14.6127s/100 iters), loss = 0.321062
I0926 11:01:35.637454  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.321061 (* 1 = 0.321061 loss)
I0926 11:01:35.637460  4406 sgd_solver.cpp:105] Iteration 50100, lr = 0.01
I0926 11:01:50.259204  4406 solver.cpp:218] Iteration 50200 (6.83914 iter/s, 14.6217s/100 iters), loss = 0.140077
I0926 11:01:50.259328  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140076 (* 1 = 0.140076 loss)
I0926 11:01:50.259335  4406 sgd_solver.cpp:105] Iteration 50200, lr = 0.01
I0926 11:02:04.882387  4406 solver.cpp:218] Iteration 50300 (6.83853 iter/s, 14.623s/100 iters), loss = 0.30184
I0926 11:02:04.882417  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.30184 (* 1 = 0.30184 loss)
I0926 11:02:04.882422  4406 sgd_solver.cpp:105] Iteration 50300, lr = 0.01
I0926 11:02:19.505760  4406 solver.cpp:218] Iteration 50400 (6.8384 iter/s, 14.6233s/100 iters), loss = 0.13368
I0926 11:02:19.505789  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.13368 (* 1 = 0.13368 loss)
I0926 11:02:19.505795  4406 sgd_solver.cpp:105] Iteration 50400, lr = 0.01
I0926 11:02:33.398855  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:02:33.984397  4406 solver.cpp:330] Iteration 50500, Testing net (#0)
I0926 11:02:37.410174  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:02:37.553072  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8492
I0926 11:02:37.553100  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.462055 (* 1 = 0.462055 loss)
I0926 11:02:37.697908  4406 solver.cpp:218] Iteration 50500 (5.4969 iter/s, 18.1921s/100 iters), loss = 0.120918
I0926 11:02:37.697939  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120918 (* 1 = 0.120918 loss)
I0926 11:02:37.697947  4406 sgd_solver.cpp:105] Iteration 50500, lr = 0.01
I0926 11:02:52.319408  4406 solver.cpp:218] Iteration 50600 (6.83928 iter/s, 14.6214s/100 iters), loss = 0.215386
I0926 11:02:52.319447  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215386 (* 1 = 0.215386 loss)
I0926 11:02:52.319453  4406 sgd_solver.cpp:105] Iteration 50600, lr = 0.01
I0926 11:03:06.943588  4406 solver.cpp:218] Iteration 50700 (6.83803 iter/s, 14.6241s/100 iters), loss = 0.24287
I0926 11:03:06.943713  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24287 (* 1 = 0.24287 loss)
I0926 11:03:06.943730  4406 sgd_solver.cpp:105] Iteration 50700, lr = 0.01
I0926 11:03:21.575428  4406 solver.cpp:218] Iteration 50800 (6.83449 iter/s, 14.6317s/100 iters), loss = 0.124994
I0926 11:03:21.575458  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124994 (* 1 = 0.124994 loss)
I0926 11:03:21.575462  4406 sgd_solver.cpp:105] Iteration 50800, lr = 0.01
I0926 11:03:36.199033  4406 solver.cpp:218] Iteration 50900 (6.83829 iter/s, 14.6235s/100 iters), loss = 0.14805
I0926 11:03:36.199064  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14805 (* 1 = 0.14805 loss)
I0926 11:03:36.199069  4406 sgd_solver.cpp:105] Iteration 50900, lr = 0.01
I0926 11:03:50.092190  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:03:50.677181  4406 solver.cpp:330] Iteration 51000, Testing net (#0)
I0926 11:03:54.100450  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:03:54.244669  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8185
I0926 11:03:54.244705  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.621924 (* 1 = 0.621924 loss)
I0926 11:03:54.388345  4406 solver.cpp:218] Iteration 51000 (5.49776 iter/s, 18.1892s/100 iters), loss = 0.226295
I0926 11:03:54.388373  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226295 (* 1 = 0.226295 loss)
I0926 11:03:54.388380  4406 sgd_solver.cpp:105] Iteration 51000, lr = 0.01
I0926 11:04:09.017206  4406 solver.cpp:218] Iteration 51100 (6.83583 iter/s, 14.6288s/100 iters), loss = 0.151256
I0926 11:04:09.017247  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151256 (* 1 = 0.151256 loss)
I0926 11:04:09.017253  4406 sgd_solver.cpp:105] Iteration 51100, lr = 0.01
I0926 11:04:23.644508  4406 solver.cpp:218] Iteration 51200 (6.83657 iter/s, 14.6272s/100 iters), loss = 0.118568
I0926 11:04:23.644603  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118567 (* 1 = 0.118567 loss)
I0926 11:04:23.644610  4406 sgd_solver.cpp:105] Iteration 51200, lr = 0.01
I0926 11:04:38.264545  4406 solver.cpp:218] Iteration 51300 (6.83999 iter/s, 14.6199s/100 iters), loss = 0.229214
I0926 11:04:38.264576  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229213 (* 1 = 0.229213 loss)
I0926 11:04:38.264582  4406 sgd_solver.cpp:105] Iteration 51300, lr = 0.01
I0926 11:04:52.889434  4406 solver.cpp:218] Iteration 51400 (6.83769 iter/s, 14.6248s/100 iters), loss = 0.1983
I0926 11:04:52.889466  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1983 (* 1 = 0.1983 loss)
I0926 11:04:52.889470  4406 sgd_solver.cpp:105] Iteration 51400, lr = 0.01
I0926 11:05:06.787958  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:05:07.373600  4406 solver.cpp:330] Iteration 51500, Testing net (#0)
I0926 11:05:10.798523  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:05:10.940989  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8377
I0926 11:05:10.941025  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.50982 (* 1 = 0.50982 loss)
I0926 11:05:11.086710  4406 solver.cpp:218] Iteration 51500 (5.49535 iter/s, 18.1972s/100 iters), loss = 0.1744
I0926 11:05:11.086740  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.174399 (* 1 = 0.174399 loss)
I0926 11:05:11.086746  4406 sgd_solver.cpp:105] Iteration 51500, lr = 0.01
I0926 11:05:25.715958  4406 solver.cpp:218] Iteration 51600 (6.83565 iter/s, 14.6292s/100 iters), loss = 0.175658
I0926 11:05:25.715999  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175658 (* 1 = 0.175658 loss)
I0926 11:05:25.716006  4406 sgd_solver.cpp:105] Iteration 51600, lr = 0.01
I0926 11:05:40.339499  4406 solver.cpp:218] Iteration 51700 (6.83833 iter/s, 14.6235s/100 iters), loss = 0.186246
I0926 11:05:40.339633  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186246 (* 1 = 0.186246 loss)
I0926 11:05:40.339651  4406 sgd_solver.cpp:105] Iteration 51700, lr = 0.01
I0926 11:05:54.960101  4406 solver.cpp:218] Iteration 51800 (6.83974 iter/s, 14.6204s/100 iters), loss = 0.205126
I0926 11:05:54.960141  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205125 (* 1 = 0.205125 loss)
I0926 11:05:54.960147  4406 sgd_solver.cpp:105] Iteration 51800, lr = 0.01
I0926 11:06:09.591692  4406 solver.cpp:218] Iteration 51900 (6.83456 iter/s, 14.6315s/100 iters), loss = 0.14059
I0926 11:06:09.591733  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14059 (* 1 = 0.14059 loss)
I0926 11:06:09.591740  4406 sgd_solver.cpp:105] Iteration 51900, lr = 0.01
I0926 11:06:23.485661  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:06:24.070816  4406 solver.cpp:330] Iteration 52000, Testing net (#0)
I0926 11:06:27.496654  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:06:27.639614  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8311
I0926 11:06:27.639652  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.542708 (* 1 = 0.542708 loss)
I0926 11:06:27.784484  4406 solver.cpp:218] Iteration 52000 (5.49671 iter/s, 18.1927s/100 iters), loss = 0.224666
I0926 11:06:27.784518  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224665 (* 1 = 0.224665 loss)
I0926 11:06:27.784525  4406 sgd_solver.cpp:105] Iteration 52000, lr = 0.01
I0926 11:06:42.411584  4406 solver.cpp:218] Iteration 52100 (6.83666 iter/s, 14.627s/100 iters), loss = 0.156945
I0926 11:06:42.411625  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156945 (* 1 = 0.156945 loss)
I0926 11:06:42.411631  4406 sgd_solver.cpp:105] Iteration 52100, lr = 0.01
I0926 11:06:57.040496  4406 solver.cpp:218] Iteration 52200 (6.83582 iter/s, 14.6288s/100 iters), loss = 0.159847
I0926 11:06:57.040599  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159847 (* 1 = 0.159847 loss)
I0926 11:06:57.040617  4406 sgd_solver.cpp:105] Iteration 52200, lr = 0.01
I0926 11:07:11.672273  4406 solver.cpp:218] Iteration 52300 (6.8345 iter/s, 14.6316s/100 iters), loss = 0.120218
I0926 11:07:11.672314  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120218 (* 1 = 0.120218 loss)
I0926 11:07:11.672322  4406 sgd_solver.cpp:105] Iteration 52300, lr = 0.01
I0926 11:07:26.297869  4406 solver.cpp:218] Iteration 52400 (6.83736 iter/s, 14.6255s/100 iters), loss = 0.169205
I0926 11:07:26.297909  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169204 (* 1 = 0.169204 loss)
I0926 11:07:26.297915  4406 sgd_solver.cpp:105] Iteration 52400, lr = 0.01
I0926 11:07:40.199882  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:07:40.784638  4406 solver.cpp:330] Iteration 52500, Testing net (#0)
I0926 11:07:44.207523  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:07:44.350337  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8401
I0926 11:07:44.350373  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.494262 (* 1 = 0.494262 loss)
I0926 11:07:44.495091  4406 solver.cpp:218] Iteration 52500 (5.49537 iter/s, 18.1971s/100 iters), loss = 0.165334
I0926 11:07:44.495121  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165333 (* 1 = 0.165333 loss)
I0926 11:07:44.495127  4406 sgd_solver.cpp:105] Iteration 52500, lr = 0.01
I0926 11:07:59.124873  4406 solver.cpp:218] Iteration 52600 (6.8354 iter/s, 14.6297s/100 iters), loss = 0.0930245
I0926 11:07:59.124903  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.093024 (* 1 = 0.093024 loss)
I0926 11:07:59.124909  4406 sgd_solver.cpp:105] Iteration 52600, lr = 0.01
I0926 11:08:13.748472  4406 solver.cpp:218] Iteration 52700 (6.83829 iter/s, 14.6235s/100 iters), loss = 0.171486
I0926 11:08:13.748606  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171485 (* 1 = 0.171485 loss)
I0926 11:08:13.748625  4406 sgd_solver.cpp:105] Iteration 52700, lr = 0.01
I0926 11:08:28.368343  4406 solver.cpp:218] Iteration 52800 (6.84008 iter/s, 14.6197s/100 iters), loss = 0.123408
I0926 11:08:28.368373  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.123407 (* 1 = 0.123407 loss)
I0926 11:08:28.368378  4406 sgd_solver.cpp:105] Iteration 52800, lr = 0.01
I0926 11:08:42.990810  4406 solver.cpp:218] Iteration 52900 (6.83882 iter/s, 14.6224s/100 iters), loss = 0.0866972
I0926 11:08:42.990850  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0866967 (* 1 = 0.0866967 loss)
I0926 11:08:42.990856  4406 sgd_solver.cpp:105] Iteration 52900, lr = 0.01
I0926 11:08:56.884912  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:08:57.470656  4406 solver.cpp:330] Iteration 53000, Testing net (#0)
I0926 11:09:00.894815  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:09:01.037345  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8345
I0926 11:09:01.037380  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.524234 (* 1 = 0.524234 loss)
I0926 11:09:01.182140  4406 solver.cpp:218] Iteration 53000 (5.49715 iter/s, 18.1912s/100 iters), loss = 0.190417
I0926 11:09:01.182170  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190416 (* 1 = 0.190416 loss)
I0926 11:09:01.182176  4406 sgd_solver.cpp:105] Iteration 53000, lr = 0.01
I0926 11:09:15.798133  4406 solver.cpp:218] Iteration 53100 (6.84185 iter/s, 14.6159s/100 iters), loss = 0.186382
I0926 11:09:15.798164  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186381 (* 1 = 0.186381 loss)
I0926 11:09:15.798171  4406 sgd_solver.cpp:105] Iteration 53100, lr = 0.01
I0926 11:09:30.417285  4406 solver.cpp:218] Iteration 53200 (6.84037 iter/s, 14.6191s/100 iters), loss = 0.192696
I0926 11:09:30.417392  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192695 (* 1 = 0.192695 loss)
I0926 11:09:30.417408  4406 sgd_solver.cpp:105] Iteration 53200, lr = 0.01
I0926 11:09:45.043268  4406 solver.cpp:218] Iteration 53300 (6.83721 iter/s, 14.6258s/100 iters), loss = 0.131402
I0926 11:09:45.043298  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131401 (* 1 = 0.131401 loss)
I0926 11:09:45.043303  4406 sgd_solver.cpp:105] Iteration 53300, lr = 0.01
I0926 11:09:59.661348  4406 solver.cpp:218] Iteration 53400 (6.84087 iter/s, 14.618s/100 iters), loss = 0.118807
I0926 11:09:59.661388  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118807 (* 1 = 0.118807 loss)
I0926 11:09:59.661394  4406 sgd_solver.cpp:105] Iteration 53400, lr = 0.01
I0926 11:10:13.554322  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:10:14.138715  4406 solver.cpp:330] Iteration 53500, Testing net (#0)
I0926 11:10:17.564062  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:10:17.706791  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8536
I0926 11:10:17.706828  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.476089 (* 1 = 0.476089 loss)
I0926 11:10:17.851585  4406 solver.cpp:218] Iteration 53500 (5.49748 iter/s, 18.1902s/100 iters), loss = 0.163252
I0926 11:10:17.851617  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163252 (* 1 = 0.163252 loss)
I0926 11:10:17.851624  4406 sgd_solver.cpp:105] Iteration 53500, lr = 0.01
I0926 11:10:32.471333  4406 solver.cpp:218] Iteration 53600 (6.8401 iter/s, 14.6197s/100 iters), loss = 0.222998
I0926 11:10:32.471364  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222998 (* 1 = 0.222998 loss)
I0926 11:10:32.471369  4406 sgd_solver.cpp:105] Iteration 53600, lr = 0.01
I0926 11:10:47.095896  4406 solver.cpp:218] Iteration 53700 (6.83784 iter/s, 14.6245s/100 iters), loss = 0.159232
I0926 11:10:47.096027  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159232 (* 1 = 0.159232 loss)
I0926 11:10:47.096045  4406 sgd_solver.cpp:105] Iteration 53700, lr = 0.01
I0926 11:11:01.713205  4406 solver.cpp:218] Iteration 53800 (6.84128 iter/s, 14.6171s/100 iters), loss = 0.111862
I0926 11:11:01.713248  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111861 (* 1 = 0.111861 loss)
I0926 11:11:01.713253  4406 sgd_solver.cpp:105] Iteration 53800, lr = 0.01
I0926 11:11:16.326472  4406 solver.cpp:218] Iteration 53900 (6.84313 iter/s, 14.6132s/100 iters), loss = 0.13157
I0926 11:11:16.326510  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131569 (* 1 = 0.131569 loss)
I0926 11:11:16.326516  4406 sgd_solver.cpp:105] Iteration 53900, lr = 0.01
I0926 11:11:30.218803  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:11:30.803112  4406 solver.cpp:330] Iteration 54000, Testing net (#0)
I0926 11:11:34.225934  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:11:34.369217  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.841
I0926 11:11:34.369253  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.522084 (* 1 = 0.522084 loss)
I0926 11:11:34.513411  4406 solver.cpp:218] Iteration 54000 (5.49848 iter/s, 18.1869s/100 iters), loss = 0.163589
I0926 11:11:34.513440  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163589 (* 1 = 0.163589 loss)
I0926 11:11:34.513447  4406 sgd_solver.cpp:105] Iteration 54000, lr = 0.01
I0926 11:11:49.126983  4406 solver.cpp:218] Iteration 54100 (6.84298 iter/s, 14.6135s/100 iters), loss = 0.254713
I0926 11:11:49.127024  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254713 (* 1 = 0.254713 loss)
I0926 11:11:49.127030  4406 sgd_solver.cpp:105] Iteration 54100, lr = 0.01
I0926 11:12:03.743810  4406 solver.cpp:218] Iteration 54200 (6.84147 iter/s, 14.6167s/100 iters), loss = 0.181022
I0926 11:12:03.743923  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181022 (* 1 = 0.181022 loss)
I0926 11:12:03.743930  4406 sgd_solver.cpp:105] Iteration 54200, lr = 0.01
I0926 11:12:18.355963  4406 solver.cpp:218] Iteration 54300 (6.84368 iter/s, 14.612s/100 iters), loss = 0.192799
I0926 11:12:18.355993  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192799 (* 1 = 0.192799 loss)
I0926 11:12:18.355998  4406 sgd_solver.cpp:105] Iteration 54300, lr = 0.01
I0926 11:12:32.969146  4406 solver.cpp:218] Iteration 54400 (6.84317 iter/s, 14.6131s/100 iters), loss = 0.168047
I0926 11:12:32.969175  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.168046 (* 1 = 0.168046 loss)
I0926 11:12:32.969182  4406 sgd_solver.cpp:105] Iteration 54400, lr = 0.01
I0926 11:12:46.868127  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:12:47.453675  4406 solver.cpp:330] Iteration 54500, Testing net (#0)
I0926 11:12:50.877809  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:12:51.020570  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8467
I0926 11:12:51.020596  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.492881 (* 1 = 0.492881 loss)
I0926 11:12:51.164839  4406 solver.cpp:218] Iteration 54500 (5.49583 iter/s, 18.1956s/100 iters), loss = 0.106836
I0926 11:12:51.164870  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106836 (* 1 = 0.106836 loss)
I0926 11:12:51.164875  4406 sgd_solver.cpp:105] Iteration 54500, lr = 0.01
I0926 11:13:05.771615  4406 solver.cpp:218] Iteration 54600 (6.84617 iter/s, 14.6067s/100 iters), loss = 0.208174
I0926 11:13:05.771646  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208174 (* 1 = 0.208174 loss)
I0926 11:13:05.771652  4406 sgd_solver.cpp:105] Iteration 54600, lr = 0.01
I0926 11:13:20.381278  4406 solver.cpp:218] Iteration 54700 (6.84482 iter/s, 14.6096s/100 iters), loss = 0.111439
I0926 11:13:20.381417  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111439 (* 1 = 0.111439 loss)
I0926 11:13:20.381423  4406 sgd_solver.cpp:105] Iteration 54700, lr = 0.01
I0926 11:13:34.988183  4406 solver.cpp:218] Iteration 54800 (6.84615 iter/s, 14.6067s/100 iters), loss = 0.158134
I0926 11:13:34.988212  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158133 (* 1 = 0.158133 loss)
I0926 11:13:34.988219  4406 sgd_solver.cpp:105] Iteration 54800, lr = 0.01
I0926 11:13:49.597764  4406 solver.cpp:218] Iteration 54900 (6.84485 iter/s, 14.6095s/100 iters), loss = 0.166647
I0926 11:13:49.597793  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166646 (* 1 = 0.166646 loss)
I0926 11:13:49.597800  4406 sgd_solver.cpp:105] Iteration 54900, lr = 0.01
I0926 11:14:03.477444  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:14:04.062034  4406 solver.cpp:330] Iteration 55000, Testing net (#0)
I0926 11:14:07.485642  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:14:07.628676  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8262
I0926 11:14:07.628702  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.56039 (* 1 = 0.56039 loss)
I0926 11:14:07.774286  4406 solver.cpp:218] Iteration 55000 (5.50163 iter/s, 18.1764s/100 iters), loss = 0.181769
I0926 11:14:07.774319  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181769 (* 1 = 0.181769 loss)
I0926 11:14:07.774327  4406 sgd_solver.cpp:105] Iteration 55000, lr = 0.01
I0926 11:14:22.404660  4406 solver.cpp:218] Iteration 55100 (6.83513 iter/s, 14.6303s/100 iters), loss = 0.127465
I0926 11:14:22.404701  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127464 (* 1 = 0.127464 loss)
I0926 11:14:22.404707  4406 sgd_solver.cpp:105] Iteration 55100, lr = 0.01
I0926 11:14:37.024960  4406 solver.cpp:218] Iteration 55200 (6.83984 iter/s, 14.6202s/100 iters), loss = 0.108471
I0926 11:14:37.025096  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108471 (* 1 = 0.108471 loss)
I0926 11:14:37.025104  4406 sgd_solver.cpp:105] Iteration 55200, lr = 0.01
I0926 11:14:51.648466  4406 solver.cpp:218] Iteration 55300 (6.83838 iter/s, 14.6233s/100 iters), loss = 0.195245
I0926 11:14:51.648501  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195245 (* 1 = 0.195245 loss)
I0926 11:14:51.648510  4406 sgd_solver.cpp:105] Iteration 55300, lr = 0.01
I0926 11:15:06.271749  4406 solver.cpp:218] Iteration 55400 (6.83844 iter/s, 14.6232s/100 iters), loss = 0.145047
I0926 11:15:06.271780  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145047 (* 1 = 0.145047 loss)
I0926 11:15:06.271788  4406 sgd_solver.cpp:105] Iteration 55400, lr = 0.01
I0926 11:15:20.163728  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:15:20.751232  4406 solver.cpp:330] Iteration 55500, Testing net (#0)
I0926 11:15:24.175078  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:15:24.317889  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8329
I0926 11:15:24.317916  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.565068 (* 1 = 0.565068 loss)
I0926 11:15:24.462980  4406 solver.cpp:218] Iteration 55500 (5.49718 iter/s, 18.1912s/100 iters), loss = 0.187437
I0926 11:15:24.463014  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.187437 (* 1 = 0.187437 loss)
I0926 11:15:24.463023  4406 sgd_solver.cpp:105] Iteration 55500, lr = 0.01
I0926 11:15:39.083696  4406 solver.cpp:218] Iteration 55600 (6.83964 iter/s, 14.6206s/100 iters), loss = 0.134541
I0926 11:15:39.083725  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134541 (* 1 = 0.134541 loss)
I0926 11:15:39.083731  4406 sgd_solver.cpp:105] Iteration 55600, lr = 0.01
I0926 11:15:53.701598  4406 solver.cpp:218] Iteration 55700 (6.84096 iter/s, 14.6178s/100 iters), loss = 0.169874
I0926 11:15:53.701674  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169874 (* 1 = 0.169874 loss)
I0926 11:15:53.701691  4406 sgd_solver.cpp:105] Iteration 55700, lr = 0.01
I0926 11:16:08.318919  4406 solver.cpp:218] Iteration 55800 (6.84125 iter/s, 14.6172s/100 iters), loss = 0.206746
I0926 11:16:08.318948  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206745 (* 1 = 0.206745 loss)
I0926 11:16:08.318953  4406 sgd_solver.cpp:105] Iteration 55800, lr = 0.01
I0926 11:16:22.941465  4406 solver.cpp:218] Iteration 55900 (6.83879 iter/s, 14.6225s/100 iters), loss = 0.165396
I0926 11:16:22.941514  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165395 (* 1 = 0.165395 loss)
I0926 11:16:22.941522  4406 sgd_solver.cpp:105] Iteration 55900, lr = 0.01
I0926 11:16:36.836020  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:16:37.420488  4406 solver.cpp:330] Iteration 56000, Testing net (#0)
I0926 11:16:40.844408  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:16:40.987645  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8146
I0926 11:16:40.987673  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.614607 (* 1 = 0.614607 loss)
I0926 11:16:41.132460  4406 solver.cpp:218] Iteration 56000 (5.49725 iter/s, 18.1909s/100 iters), loss = 0.0903687
I0926 11:16:41.132494  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0903682 (* 1 = 0.0903682 loss)
I0926 11:16:41.132503  4406 sgd_solver.cpp:105] Iteration 56000, lr = 0.01
I0926 11:16:55.752209  4406 solver.cpp:218] Iteration 56100 (6.8401 iter/s, 14.6197s/100 iters), loss = 0.1985
I0926 11:16:55.752240  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198499 (* 1 = 0.198499 loss)
I0926 11:16:55.752246  4406 sgd_solver.cpp:105] Iteration 56100, lr = 0.01
I0926 11:17:10.360944  4406 solver.cpp:218] Iteration 56200 (6.84525 iter/s, 14.6087s/100 iters), loss = 0.135488
I0926 11:17:10.361081  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135488 (* 1 = 0.135488 loss)
I0926 11:17:10.361088  4406 sgd_solver.cpp:105] Iteration 56200, lr = 0.01
I0926 11:17:24.982756  4406 solver.cpp:218] Iteration 56300 (6.83917 iter/s, 14.6216s/100 iters), loss = 0.132684
I0926 11:17:24.982796  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132684 (* 1 = 0.132684 loss)
I0926 11:17:24.982802  4406 sgd_solver.cpp:105] Iteration 56300, lr = 0.01
I0926 11:17:39.600565  4406 solver.cpp:218] Iteration 56400 (6.84101 iter/s, 14.6177s/100 iters), loss = 0.185449
I0926 11:17:39.600595  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185448 (* 1 = 0.185448 loss)
I0926 11:17:39.600601  4406 sgd_solver.cpp:105] Iteration 56400, lr = 0.01
I0926 11:17:53.493937  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:17:54.077818  4406 solver.cpp:330] Iteration 56500, Testing net (#0)
I0926 11:17:57.503279  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:17:57.646136  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8307
I0926 11:17:57.646162  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.573142 (* 1 = 0.573142 loss)
I0926 11:17:57.790479  4406 solver.cpp:218] Iteration 56500 (5.49757 iter/s, 18.1898s/100 iters), loss = 0.180242
I0926 11:17:57.790511  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180242 (* 1 = 0.180242 loss)
I0926 11:17:57.790518  4406 sgd_solver.cpp:105] Iteration 56500, lr = 0.01
I0926 11:18:12.409212  4406 solver.cpp:218] Iteration 56600 (6.84057 iter/s, 14.6187s/100 iters), loss = 0.124076
I0926 11:18:12.409240  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124076 (* 1 = 0.124076 loss)
I0926 11:18:12.409246  4406 sgd_solver.cpp:105] Iteration 56600, lr = 0.01
I0926 11:18:27.035545  4406 solver.cpp:218] Iteration 56700 (6.83701 iter/s, 14.6263s/100 iters), loss = 0.199351
I0926 11:18:27.035662  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199351 (* 1 = 0.199351 loss)
I0926 11:18:27.035679  4406 sgd_solver.cpp:105] Iteration 56700, lr = 0.01
I0926 11:18:41.659605  4406 solver.cpp:218] Iteration 56800 (6.83811 iter/s, 14.6239s/100 iters), loss = 0.146846
I0926 11:18:41.659634  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.146845 (* 1 = 0.146845 loss)
I0926 11:18:41.659641  4406 sgd_solver.cpp:105] Iteration 56800, lr = 0.01
I0926 11:18:56.281879  4406 solver.cpp:218] Iteration 56900 (6.83891 iter/s, 14.6222s/100 iters), loss = 0.128194
I0926 11:18:56.281909  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.128193 (* 1 = 0.128193 loss)
I0926 11:18:56.281914  4406 sgd_solver.cpp:105] Iteration 56900, lr = 0.01
I0926 11:19:10.172509  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:19:10.759536  4406 solver.cpp:330] Iteration 57000, Testing net (#0)
I0926 11:19:14.182741  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:19:14.325947  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8443
I0926 11:19:14.325983  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.501556 (* 1 = 0.501556 loss)
I0926 11:19:14.470976  4406 solver.cpp:218] Iteration 57000 (5.49782 iter/s, 18.189s/100 iters), loss = 0.113705
I0926 11:19:14.471005  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113705 (* 1 = 0.113705 loss)
I0926 11:19:14.471012  4406 sgd_solver.cpp:105] Iteration 57000, lr = 0.01
I0926 11:19:29.081987  4406 solver.cpp:218] Iteration 57100 (6.84418 iter/s, 14.6109s/100 iters), loss = 0.118106
I0926 11:19:29.082027  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118106 (* 1 = 0.118106 loss)
I0926 11:19:29.082033  4406 sgd_solver.cpp:105] Iteration 57100, lr = 0.01
I0926 11:19:43.687141  4406 solver.cpp:218] Iteration 57200 (6.84693 iter/s, 14.6051s/100 iters), loss = 0.186971
I0926 11:19:43.687211  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186971 (* 1 = 0.186971 loss)
I0926 11:19:43.687217  4406 sgd_solver.cpp:105] Iteration 57200, lr = 0.01
I0926 11:19:58.299480  4406 solver.cpp:218] Iteration 57300 (6.84358 iter/s, 14.6122s/100 iters), loss = 0.19225
I0926 11:19:58.299520  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192249 (* 1 = 0.192249 loss)
I0926 11:19:58.299526  4406 sgd_solver.cpp:105] Iteration 57300, lr = 0.01
I0926 11:20:12.908576  4406 solver.cpp:218] Iteration 57400 (6.84509 iter/s, 14.609s/100 iters), loss = 0.152351
I0926 11:20:12.908617  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15235 (* 1 = 0.15235 loss)
I0926 11:20:12.908623  4406 sgd_solver.cpp:105] Iteration 57400, lr = 0.01
I0926 11:20:26.788648  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:20:27.373401  4406 solver.cpp:330] Iteration 57500, Testing net (#0)
I0926 11:20:30.797894  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:20:30.939998  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8371
I0926 11:20:30.940023  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.501361 (* 1 = 0.501361 loss)
I0926 11:20:31.083978  4406 solver.cpp:218] Iteration 57500 (5.50197 iter/s, 18.1753s/100 iters), loss = 0.151829
I0926 11:20:31.084007  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151828 (* 1 = 0.151828 loss)
I0926 11:20:31.084012  4406 sgd_solver.cpp:105] Iteration 57500, lr = 0.01
I0926 11:20:45.704669  4406 solver.cpp:218] Iteration 57600 (6.83965 iter/s, 14.6206s/100 iters), loss = 0.106067
I0926 11:20:45.704699  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106066 (* 1 = 0.106066 loss)
I0926 11:20:45.704704  4406 sgd_solver.cpp:105] Iteration 57600, lr = 0.01
I0926 11:21:00.329913  4406 solver.cpp:218] Iteration 57700 (6.83752 iter/s, 14.6252s/100 iters), loss = 0.216375
I0926 11:21:00.330018  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216374 (* 1 = 0.216374 loss)
I0926 11:21:00.330025  4406 sgd_solver.cpp:105] Iteration 57700, lr = 0.01
I0926 11:21:14.953763  4406 solver.cpp:218] Iteration 57800 (6.83821 iter/s, 14.6237s/100 iters), loss = 0.165937
I0926 11:21:14.953794  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165936 (* 1 = 0.165936 loss)
I0926 11:21:14.953810  4406 sgd_solver.cpp:105] Iteration 57800, lr = 0.01
I0926 11:21:29.582885  4406 solver.cpp:218] Iteration 57900 (6.83571 iter/s, 14.6291s/100 iters), loss = 0.135095
I0926 11:21:29.582916  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135094 (* 1 = 0.135094 loss)
I0926 11:21:29.582921  4406 sgd_solver.cpp:105] Iteration 57900, lr = 0.01
I0926 11:21:43.481637  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:21:44.067270  4406 solver.cpp:330] Iteration 58000, Testing net (#0)
I0926 11:21:47.492643  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:21:47.635694  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8196
I0926 11:21:47.635730  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.583444 (* 1 = 0.583444 loss)
I0926 11:21:47.780288  4406 solver.cpp:218] Iteration 58000 (5.49531 iter/s, 18.1973s/100 iters), loss = 0.148379
I0926 11:21:47.780318  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148378 (* 1 = 0.148378 loss)
I0926 11:21:47.780325  4406 sgd_solver.cpp:105] Iteration 58000, lr = 0.01
I0926 11:22:02.399111  4406 solver.cpp:218] Iteration 58100 (6.84053 iter/s, 14.6188s/100 iters), loss = 0.146629
I0926 11:22:02.399142  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.146629 (* 1 = 0.146629 loss)
I0926 11:22:02.399147  4406 sgd_solver.cpp:105] Iteration 58100, lr = 0.01
I0926 11:22:17.017395  4406 solver.cpp:218] Iteration 58200 (6.84078 iter/s, 14.6182s/100 iters), loss = 0.177159
I0926 11:22:17.017506  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177158 (* 1 = 0.177158 loss)
I0926 11:22:17.017524  4406 sgd_solver.cpp:105] Iteration 58200, lr = 0.01
I0926 11:22:31.632464  4406 solver.cpp:218] Iteration 58300 (6.84232 iter/s, 14.6149s/100 iters), loss = 0.118447
I0926 11:22:31.632498  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.118446 (* 1 = 0.118446 loss)
I0926 11:22:31.632504  4406 sgd_solver.cpp:105] Iteration 58300, lr = 0.01
I0926 11:22:46.246960  4406 solver.cpp:218] Iteration 58400 (6.84255 iter/s, 14.6144s/100 iters), loss = 0.0897036
I0926 11:22:46.247000  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0897031 (* 1 = 0.0897031 loss)
I0926 11:22:46.247007  4406 sgd_solver.cpp:105] Iteration 58400, lr = 0.01
I0926 11:23:00.140847  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:23:00.726253  4406 solver.cpp:330] Iteration 58500, Testing net (#0)
I0926 11:23:04.151046  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:23:04.294107  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8044
I0926 11:23:04.294142  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.639326 (* 1 = 0.639326 loss)
I0926 11:23:04.438758  4406 solver.cpp:218] Iteration 58500 (5.49701 iter/s, 18.1917s/100 iters), loss = 0.277091
I0926 11:23:04.438791  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27709 (* 1 = 0.27709 loss)
I0926 11:23:04.438798  4406 sgd_solver.cpp:105] Iteration 58500, lr = 0.01
I0926 11:23:19.056020  4406 solver.cpp:218] Iteration 58600 (6.84126 iter/s, 14.6172s/100 iters), loss = 0.167293
I0926 11:23:19.056061  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167293 (* 1 = 0.167293 loss)
I0926 11:23:19.056066  4406 sgd_solver.cpp:105] Iteration 58600, lr = 0.01
I0926 11:23:33.675338  4406 solver.cpp:218] Iteration 58700 (6.8403 iter/s, 14.6192s/100 iters), loss = 0.22304
I0926 11:23:33.675451  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223039 (* 1 = 0.223039 loss)
I0926 11:23:33.675457  4406 sgd_solver.cpp:105] Iteration 58700, lr = 0.01
I0926 11:23:48.300824  4406 solver.cpp:218] Iteration 58800 (6.83744 iter/s, 14.6253s/100 iters), loss = 0.15681
I0926 11:23:48.300864  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15681 (* 1 = 0.15681 loss)
I0926 11:23:48.300869  4406 sgd_solver.cpp:105] Iteration 58800, lr = 0.01
I0926 11:24:02.926899  4406 solver.cpp:218] Iteration 58900 (6.83714 iter/s, 14.626s/100 iters), loss = 0.126837
I0926 11:24:02.926930  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126837 (* 1 = 0.126837 loss)
I0926 11:24:02.926936  4406 sgd_solver.cpp:105] Iteration 58900, lr = 0.01
I0926 11:24:16.824997  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:24:17.408951  4406 solver.cpp:330] Iteration 59000, Testing net (#0)
I0926 11:24:20.834194  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:24:20.976124  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8428
I0926 11:24:20.976160  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.49582 (* 1 = 0.49582 loss)
I0926 11:24:21.121270  4406 solver.cpp:218] Iteration 59000 (5.49623 iter/s, 18.1943s/100 iters), loss = 0.140878
I0926 11:24:21.121299  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140878 (* 1 = 0.140878 loss)
I0926 11:24:21.121305  4406 sgd_solver.cpp:105] Iteration 59000, lr = 0.01
I0926 11:24:35.742486  4406 solver.cpp:218] Iteration 59100 (6.83941 iter/s, 14.6212s/100 iters), loss = 0.106872
I0926 11:24:35.742528  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106871 (* 1 = 0.106871 loss)
I0926 11:24:35.742534  4406 sgd_solver.cpp:105] Iteration 59100, lr = 0.01
I0926 11:24:50.368995  4406 solver.cpp:218] Iteration 59200 (6.83694 iter/s, 14.6264s/100 iters), loss = 0.209842
I0926 11:24:50.369096  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209842 (* 1 = 0.209842 loss)
I0926 11:24:50.369113  4406 sgd_solver.cpp:105] Iteration 59200, lr = 0.01
I0926 11:25:04.996794  4406 solver.cpp:218] Iteration 59300 (6.83636 iter/s, 14.6277s/100 iters), loss = 0.215397
I0926 11:25:04.996835  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215397 (* 1 = 0.215397 loss)
I0926 11:25:04.996840  4406 sgd_solver.cpp:105] Iteration 59300, lr = 0.01
I0926 11:25:19.621433  4406 solver.cpp:218] Iteration 59400 (6.83781 iter/s, 14.6246s/100 iters), loss = 0.173056
I0926 11:25:19.621462  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173055 (* 1 = 0.173055 loss)
I0926 11:25:19.621469  4406 sgd_solver.cpp:105] Iteration 59400, lr = 0.01
I0926 11:25:33.520753  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:25:34.105744  4406 solver.cpp:330] Iteration 59500, Testing net (#0)
I0926 11:25:37.532025  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:25:37.675112  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8328
I0926 11:25:37.675139  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.572686 (* 1 = 0.572686 loss)
I0926 11:25:37.819981  4406 solver.cpp:218] Iteration 59500 (5.49497 iter/s, 18.1985s/100 iters), loss = 0.162292
I0926 11:25:37.820014  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.162291 (* 1 = 0.162291 loss)
I0926 11:25:37.820020  4406 sgd_solver.cpp:105] Iteration 59500, lr = 0.01
I0926 11:25:52.426246  4406 solver.cpp:218] Iteration 59600 (6.84641 iter/s, 14.6062s/100 iters), loss = 0.27064
I0926 11:25:52.426275  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270639 (* 1 = 0.270639 loss)
I0926 11:25:52.426281  4406 sgd_solver.cpp:105] Iteration 59600, lr = 0.01
I0926 11:26:07.033776  4406 solver.cpp:218] Iteration 59700 (6.84582 iter/s, 14.6075s/100 iters), loss = 0.190534
I0926 11:26:07.033916  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190534 (* 1 = 0.190534 loss)
I0926 11:26:07.033923  4406 sgd_solver.cpp:105] Iteration 59700, lr = 0.01
I0926 11:26:21.639716  4406 solver.cpp:218] Iteration 59800 (6.84661 iter/s, 14.6058s/100 iters), loss = 0.207263
I0926 11:26:21.639745  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207263 (* 1 = 0.207263 loss)
I0926 11:26:21.639751  4406 sgd_solver.cpp:105] Iteration 59800, lr = 0.01
I0926 11:26:36.241176  4406 solver.cpp:218] Iteration 59900 (6.84866 iter/s, 14.6014s/100 iters), loss = 0.176799
I0926 11:26:36.241206  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176799 (* 1 = 0.176799 loss)
I0926 11:26:36.241212  4406 sgd_solver.cpp:105] Iteration 59900, lr = 0.01
I0926 11:26:50.119788  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:26:50.703788  4406 solver.cpp:330] Iteration 60000, Testing net (#0)
I0926 11:26:54.128911  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:26:54.271595  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8381
I0926 11:26:54.271630  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.519149 (* 1 = 0.519149 loss)
I0926 11:26:54.415702  4406 solver.cpp:218] Iteration 60000 (5.50223 iter/s, 18.1745s/100 iters), loss = 0.187698
I0926 11:26:54.415735  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.187697 (* 1 = 0.187697 loss)
I0926 11:26:54.415740  4406 sgd_solver.cpp:105] Iteration 60000, lr = 0.01
I0926 11:27:09.037042  4406 solver.cpp:218] Iteration 60100 (6.83935 iter/s, 14.6213s/100 iters), loss = 0.151366
I0926 11:27:09.037070  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151365 (* 1 = 0.151365 loss)
I0926 11:27:09.037076  4406 sgd_solver.cpp:105] Iteration 60100, lr = 0.01
I0926 11:27:23.662019  4406 solver.cpp:218] Iteration 60200 (6.83765 iter/s, 14.6249s/100 iters), loss = 0.0951718
I0926 11:27:23.662173  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0951713 (* 1 = 0.0951713 loss)
I0926 11:27:23.662180  4406 sgd_solver.cpp:105] Iteration 60200, lr = 0.01
I0926 11:27:38.284627  4406 solver.cpp:218] Iteration 60300 (6.83881 iter/s, 14.6224s/100 iters), loss = 0.148282
I0926 11:27:38.284657  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148282 (* 1 = 0.148282 loss)
I0926 11:27:38.284662  4406 sgd_solver.cpp:105] Iteration 60300, lr = 0.01
I0926 11:27:52.910595  4406 solver.cpp:218] Iteration 60400 (6.83719 iter/s, 14.6259s/100 iters), loss = 0.0757619
I0926 11:27:52.910645  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0757615 (* 1 = 0.0757615 loss)
I0926 11:27:52.910650  4406 sgd_solver.cpp:105] Iteration 60400, lr = 0.01
I0926 11:28:06.807889  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:28:07.393945  4406 solver.cpp:330] Iteration 60500, Testing net (#0)
I0926 11:28:10.817538  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:28:10.960602  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.855
I0926 11:28:10.960636  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.485667 (* 1 = 0.485667 loss)
I0926 11:28:11.105494  4406 solver.cpp:218] Iteration 60500 (5.49607 iter/s, 18.1948s/100 iters), loss = 0.13977
I0926 11:28:11.105521  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139769 (* 1 = 0.139769 loss)
I0926 11:28:11.105527  4406 sgd_solver.cpp:105] Iteration 60500, lr = 0.01
I0926 11:28:25.718643  4406 solver.cpp:218] Iteration 60600 (6.84318 iter/s, 14.6131s/100 iters), loss = 0.252669
I0926 11:28:25.718672  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.252668 (* 1 = 0.252668 loss)
I0926 11:28:25.718677  4406 sgd_solver.cpp:105] Iteration 60600, lr = 0.01
I0926 11:28:40.338407  4406 solver.cpp:218] Iteration 60700 (6.84009 iter/s, 14.6197s/100 iters), loss = 0.16911
I0926 11:28:40.338510  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169109 (* 1 = 0.169109 loss)
I0926 11:28:40.338526  4406 sgd_solver.cpp:105] Iteration 60700, lr = 0.01
I0926 11:28:54.951459  4406 solver.cpp:218] Iteration 60800 (6.84326 iter/s, 14.6129s/100 iters), loss = 0.219708
I0926 11:28:54.951489  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219707 (* 1 = 0.219707 loss)
I0926 11:28:54.951505  4406 sgd_solver.cpp:105] Iteration 60800, lr = 0.01
I0926 11:29:09.563349  4406 solver.cpp:218] Iteration 60900 (6.84377 iter/s, 14.6118s/100 iters), loss = 0.195385
I0926 11:29:09.563380  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195385 (* 1 = 0.195385 loss)
I0926 11:29:09.563386  4406 sgd_solver.cpp:105] Iteration 60900, lr = 0.01
I0926 11:29:23.455667  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:29:24.039547  4406 solver.cpp:330] Iteration 61000, Testing net (#0)
I0926 11:29:27.465606  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:29:27.608448  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8446
I0926 11:29:27.608474  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.512001 (* 1 = 0.512001 loss)
I0926 11:29:27.752712  4406 solver.cpp:218] Iteration 61000 (5.49774 iter/s, 18.1893s/100 iters), loss = 0.171825
I0926 11:29:27.752745  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171825 (* 1 = 0.171825 loss)
I0926 11:29:27.752751  4406 sgd_solver.cpp:105] Iteration 61000, lr = 0.01
I0926 11:29:42.368607  4406 solver.cpp:218] Iteration 61100 (6.8419 iter/s, 14.6158s/100 iters), loss = 0.139945
I0926 11:29:42.368638  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139944 (* 1 = 0.139944 loss)
I0926 11:29:42.368643  4406 sgd_solver.cpp:105] Iteration 61100, lr = 0.01
I0926 11:29:56.981878  4406 solver.cpp:218] Iteration 61200 (6.84313 iter/s, 14.6132s/100 iters), loss = 0.160758
I0926 11:29:56.981986  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160758 (* 1 = 0.160758 loss)
I0926 11:29:56.981992  4406 sgd_solver.cpp:105] Iteration 61200, lr = 0.01
I0926 11:30:11.596284  4406 solver.cpp:218] Iteration 61300 (6.84263 iter/s, 14.6143s/100 iters), loss = 0.197729
I0926 11:30:11.596324  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.197728 (* 1 = 0.197728 loss)
I0926 11:30:11.596330  4406 sgd_solver.cpp:105] Iteration 61300, lr = 0.01
I0926 11:30:26.206804  4406 solver.cpp:218] Iteration 61400 (6.84442 iter/s, 14.6104s/100 iters), loss = 0.132403
I0926 11:30:26.206845  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132402 (* 1 = 0.132402 loss)
I0926 11:30:26.206849  4406 sgd_solver.cpp:105] Iteration 61400, lr = 0.01
I0926 11:30:40.093694  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:30:40.678211  4406 solver.cpp:330] Iteration 61500, Testing net (#0)
I0926 11:30:44.102988  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:30:44.246057  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8226
I0926 11:30:44.246080  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.577737 (* 1 = 0.577737 loss)
I0926 11:30:44.390244  4406 solver.cpp:218] Iteration 61500 (5.49953 iter/s, 18.1834s/100 iters), loss = 0.133444
I0926 11:30:44.390272  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133444 (* 1 = 0.133444 loss)
I0926 11:30:44.390280  4406 sgd_solver.cpp:105] Iteration 61500, lr = 0.01
I0926 11:30:59.010792  4406 solver.cpp:218] Iteration 61600 (6.83972 iter/s, 14.6205s/100 iters), loss = 0.159285
I0926 11:30:59.010820  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159284 (* 1 = 0.159284 loss)
I0926 11:30:59.010825  4406 sgd_solver.cpp:105] Iteration 61600, lr = 0.01
I0926 11:31:13.630781  4406 solver.cpp:218] Iteration 61700 (6.83998 iter/s, 14.6199s/100 iters), loss = 0.0737438
I0926 11:31:13.630872  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0737434 (* 1 = 0.0737434 loss)
I0926 11:31:13.630888  4406 sgd_solver.cpp:105] Iteration 61700, lr = 0.01
I0926 11:31:28.253885  4406 solver.cpp:218] Iteration 61800 (6.83855 iter/s, 14.623s/100 iters), loss = 0.137178
I0926 11:31:28.253924  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.137178 (* 1 = 0.137178 loss)
I0926 11:31:28.253931  4406 sgd_solver.cpp:105] Iteration 61800, lr = 0.01
I0926 11:31:42.873900  4406 solver.cpp:218] Iteration 61900 (6.83997 iter/s, 14.6199s/100 iters), loss = 0.108714
I0926 11:31:42.873931  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108713 (* 1 = 0.108713 loss)
I0926 11:31:42.873937  4406 sgd_solver.cpp:105] Iteration 61900, lr = 0.01
I0926 11:31:56.769809  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:31:57.355192  4406 solver.cpp:330] Iteration 62000, Testing net (#0)
I0926 11:32:00.779284  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:32:00.922513  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8374
I0926 11:32:00.922539  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.552848 (* 1 = 0.552848 loss)
I0926 11:32:01.066494  4406 solver.cpp:218] Iteration 62000 (5.49676 iter/s, 18.1925s/100 iters), loss = 0.164935
I0926 11:32:01.066520  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164934 (* 1 = 0.164934 loss)
I0926 11:32:01.066527  4406 sgd_solver.cpp:105] Iteration 62000, lr = 0.01
I0926 11:32:15.673523  4406 solver.cpp:218] Iteration 62100 (6.84605 iter/s, 14.607s/100 iters), loss = 0.154159
I0926 11:32:15.673552  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154159 (* 1 = 0.154159 loss)
I0926 11:32:15.673558  4406 sgd_solver.cpp:105] Iteration 62100, lr = 0.01
I0926 11:32:30.276253  4406 solver.cpp:218] Iteration 62200 (6.84806 iter/s, 14.6027s/100 iters), loss = 0.130664
I0926 11:32:30.276355  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130663 (* 1 = 0.130663 loss)
I0926 11:32:30.276363  4406 sgd_solver.cpp:105] Iteration 62200, lr = 0.01
I0926 11:32:44.877985  4406 solver.cpp:218] Iteration 62300 (6.84856 iter/s, 14.6016s/100 iters), loss = 0.171892
I0926 11:32:44.878013  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171892 (* 1 = 0.171892 loss)
I0926 11:32:44.878020  4406 sgd_solver.cpp:105] Iteration 62300, lr = 0.01
I0926 11:32:59.487725  4406 solver.cpp:218] Iteration 62400 (6.84478 iter/s, 14.6097s/100 iters), loss = 0.149964
I0926 11:32:59.487766  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149963 (* 1 = 0.149963 loss)
I0926 11:32:59.487771  4406 sgd_solver.cpp:105] Iteration 62400, lr = 0.01
I0926 11:33:13.371436  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:33:13.955080  4406 solver.cpp:330] Iteration 62500, Testing net (#0)
I0926 11:33:17.380041  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:33:17.522806  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8491
I0926 11:33:17.522841  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.497978 (* 1 = 0.497978 loss)
I0926 11:33:17.667361  4406 solver.cpp:218] Iteration 62500 (5.50069 iter/s, 18.1796s/100 iters), loss = 0.134792
I0926 11:33:17.667389  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134792 (* 1 = 0.134792 loss)
I0926 11:33:17.667395  4406 sgd_solver.cpp:105] Iteration 62500, lr = 0.01
I0926 11:33:32.285090  4406 solver.cpp:218] Iteration 62600 (6.84104 iter/s, 14.6177s/100 iters), loss = 0.169615
I0926 11:33:32.285122  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169615 (* 1 = 0.169615 loss)
I0926 11:33:32.285130  4406 sgd_solver.cpp:105] Iteration 62600, lr = 0.01
I0926 11:33:46.893813  4406 solver.cpp:218] Iteration 62700 (6.84526 iter/s, 14.6087s/100 iters), loss = 0.186505
I0926 11:33:46.893919  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186504 (* 1 = 0.186504 loss)
I0926 11:33:46.893934  4406 sgd_solver.cpp:105] Iteration 62700, lr = 0.01
I0926 11:34:01.508204  4406 solver.cpp:218] Iteration 62800 (6.84264 iter/s, 14.6143s/100 iters), loss = 0.12451
I0926 11:34:01.508234  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.12451 (* 1 = 0.12451 loss)
I0926 11:34:01.508239  4406 sgd_solver.cpp:105] Iteration 62800, lr = 0.01
I0926 11:34:16.128422  4406 solver.cpp:218] Iteration 62900 (6.83987 iter/s, 14.6202s/100 iters), loss = 0.134012
I0926 11:34:16.128461  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134011 (* 1 = 0.134011 loss)
I0926 11:34:16.128468  4406 sgd_solver.cpp:105] Iteration 62900, lr = 0.01
I0926 11:34:30.014503  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:34:30.600436  4406 solver.cpp:330] Iteration 63000, Testing net (#0)
I0926 11:34:34.025665  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:34:34.168627  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.809
I0926 11:34:34.168661  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.657906 (* 1 = 0.657906 loss)
I0926 11:34:34.313729  4406 solver.cpp:218] Iteration 63000 (5.49897 iter/s, 18.1852s/100 iters), loss = 0.10714
I0926 11:34:34.313756  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107139 (* 1 = 0.107139 loss)
I0926 11:34:34.313762  4406 sgd_solver.cpp:105] Iteration 63000, lr = 0.01
I0926 11:34:48.933931  4406 solver.cpp:218] Iteration 63100 (6.83988 iter/s, 14.6201s/100 iters), loss = 0.148323
I0926 11:34:48.933960  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148323 (* 1 = 0.148323 loss)
I0926 11:34:48.933966  4406 sgd_solver.cpp:105] Iteration 63100, lr = 0.01
I0926 11:35:03.552904  4406 solver.cpp:218] Iteration 63200 (6.84046 iter/s, 14.6189s/100 iters), loss = 0.183325
I0926 11:35:03.553052  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183325 (* 1 = 0.183325 loss)
I0926 11:35:03.553061  4406 sgd_solver.cpp:105] Iteration 63200, lr = 0.01
I0926 11:35:18.173681  4406 solver.cpp:218] Iteration 63300 (6.83967 iter/s, 14.6206s/100 iters), loss = 0.173495
I0926 11:35:18.173722  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173495 (* 1 = 0.173495 loss)
I0926 11:35:18.173727  4406 sgd_solver.cpp:105] Iteration 63300, lr = 0.01
I0926 11:35:32.797040  4406 solver.cpp:218] Iteration 63400 (6.83841 iter/s, 14.6233s/100 iters), loss = 0.0971418
I0926 11:35:32.797081  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0971413 (* 1 = 0.0971413 loss)
I0926 11:35:32.797087  4406 sgd_solver.cpp:105] Iteration 63400, lr = 0.01
I0926 11:35:46.689303  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:35:47.275631  4406 solver.cpp:330] Iteration 63500, Testing net (#0)
I0926 11:35:50.699509  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:35:50.841691  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8481
I0926 11:35:50.841727  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.492095 (* 1 = 0.492095 loss)
I0926 11:35:50.986568  4406 solver.cpp:218] Iteration 63500 (5.49769 iter/s, 18.1894s/100 iters), loss = 0.149369
I0926 11:35:50.986596  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.149369 (* 1 = 0.149369 loss)
I0926 11:35:50.986603  4406 sgd_solver.cpp:105] Iteration 63500, lr = 0.01
I0926 11:36:05.609791  4406 solver.cpp:218] Iteration 63600 (6.83847 iter/s, 14.6232s/100 iters), loss = 0.195628
I0926 11:36:05.609818  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195627 (* 1 = 0.195627 loss)
I0926 11:36:05.609824  4406 sgd_solver.cpp:105] Iteration 63600, lr = 0.01
I0926 11:36:20.234793  4406 solver.cpp:218] Iteration 63700 (6.83764 iter/s, 14.6249s/100 iters), loss = 0.127596
I0926 11:36:20.234899  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127596 (* 1 = 0.127596 loss)
I0926 11:36:20.234917  4406 sgd_solver.cpp:105] Iteration 63700, lr = 0.01
I0926 11:36:34.866441  4406 solver.cpp:218] Iteration 63800 (6.83457 iter/s, 14.6315s/100 iters), loss = 0.155663
I0926 11:36:34.866483  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.155662 (* 1 = 0.155662 loss)
I0926 11:36:34.866488  4406 sgd_solver.cpp:105] Iteration 63800, lr = 0.01
I0926 11:36:49.491506  4406 solver.cpp:218] Iteration 63900 (6.83761 iter/s, 14.625s/100 iters), loss = 0.160665
I0926 11:36:49.491547  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160665 (* 1 = 0.160665 loss)
I0926 11:36:49.491554  4406 sgd_solver.cpp:105] Iteration 63900, lr = 0.01
I0926 11:37:03.396286  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:37:03.982214  4406 solver.cpp:330] Iteration 64000, Testing net (#0)
I0926 11:37:07.406841  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:37:07.549348  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8476
I0926 11:37:07.549373  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.490014 (* 1 = 0.490014 loss)
I0926 11:37:07.693795  4406 solver.cpp:218] Iteration 64000 (5.49384 iter/s, 18.2022s/100 iters), loss = 0.154079
I0926 11:37:07.693825  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154079 (* 1 = 0.154079 loss)
I0926 11:37:07.693831  4406 sgd_solver.cpp:105] Iteration 64000, lr = 0.01
I0926 11:37:22.328779  4406 solver.cpp:218] Iteration 64100 (6.83297 iter/s, 14.6349s/100 iters), loss = 0.259121
I0926 11:37:22.328811  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25912 (* 1 = 0.25912 loss)
I0926 11:37:22.328819  4406 sgd_solver.cpp:105] Iteration 64100, lr = 0.01
I0926 11:37:36.957370  4406 solver.cpp:218] Iteration 64200 (6.83596 iter/s, 14.6285s/100 iters), loss = 0.130059
I0926 11:37:36.957502  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130059 (* 1 = 0.130059 loss)
I0926 11:37:36.957520  4406 sgd_solver.cpp:105] Iteration 64200, lr = 0.01
I0926 11:37:51.587283  4406 solver.cpp:218] Iteration 64300 (6.83539 iter/s, 14.6297s/100 iters), loss = 0.124214
I0926 11:37:51.587324  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124214 (* 1 = 0.124214 loss)
I0926 11:37:51.587330  4406 sgd_solver.cpp:105] Iteration 64300, lr = 0.01
I0926 11:38:06.221271  4406 solver.cpp:218] Iteration 64400 (6.83344 iter/s, 14.6339s/100 iters), loss = 0.126297
I0926 11:38:06.221312  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126296 (* 1 = 0.126296 loss)
I0926 11:38:06.221318  4406 sgd_solver.cpp:105] Iteration 64400, lr = 0.01
I0926 11:38:20.125232  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:38:20.710073  4406 solver.cpp:330] Iteration 64500, Testing net (#0)
I0926 11:38:24.133095  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:38:24.276242  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8523
I0926 11:38:24.276276  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.487943 (* 1 = 0.487943 loss)
I0926 11:38:24.420262  4406 solver.cpp:218] Iteration 64500 (5.49484 iter/s, 18.1989s/100 iters), loss = 0.120424
I0926 11:38:24.420289  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120424 (* 1 = 0.120424 loss)
I0926 11:38:24.420295  4406 sgd_solver.cpp:105] Iteration 64500, lr = 0.01
I0926 11:38:39.039368  4406 solver.cpp:218] Iteration 64600 (6.84039 iter/s, 14.619s/100 iters), loss = 0.12068
I0926 11:38:39.039410  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.12068 (* 1 = 0.12068 loss)
I0926 11:38:39.039417  4406 sgd_solver.cpp:105] Iteration 64600, lr = 0.01
I0926 11:38:53.651089  4406 solver.cpp:218] Iteration 64700 (6.84386 iter/s, 14.6116s/100 iters), loss = 0.121149
I0926 11:38:53.651190  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121149 (* 1 = 0.121149 loss)
I0926 11:38:53.651207  4406 sgd_solver.cpp:105] Iteration 64700, lr = 0.01
I0926 11:39:08.265897  4406 solver.cpp:218] Iteration 64800 (6.84244 iter/s, 14.6147s/100 iters), loss = 0.141311
I0926 11:39:08.265938  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14131 (* 1 = 0.14131 loss)
I0926 11:39:08.265944  4406 sgd_solver.cpp:105] Iteration 64800, lr = 0.01
I0926 11:39:22.877104  4406 solver.cpp:218] Iteration 64900 (6.8441 iter/s, 14.6111s/100 iters), loss = 0.155481
I0926 11:39:22.877146  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15548 (* 1 = 0.15548 loss)
I0926 11:39:22.877152  4406 sgd_solver.cpp:105] Iteration 64900, lr = 0.01
I0926 11:39:36.767124  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:39:37.351461  4406 solver.cpp:330] Iteration 65000, Testing net (#0)
I0926 11:39:40.776891  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:39:40.919770  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8325
I0926 11:39:40.919806  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.568642 (* 1 = 0.568642 loss)
I0926 11:39:41.064544  4406 solver.cpp:218] Iteration 65000 (5.49833 iter/s, 18.1874s/100 iters), loss = 0.132071
I0926 11:39:41.064571  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132071 (* 1 = 0.132071 loss)
I0926 11:39:41.064577  4406 sgd_solver.cpp:105] Iteration 65000, lr = 0.01
I0926 11:39:55.692147  4406 solver.cpp:218] Iteration 65100 (6.83642 iter/s, 14.6275s/100 iters), loss = 0.16929
I0926 11:39:55.692188  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.16929 (* 1 = 0.16929 loss)
I0926 11:39:55.692193  4406 sgd_solver.cpp:105] Iteration 65100, lr = 0.01
I0926 11:40:10.316195  4406 solver.cpp:218] Iteration 65200 (6.83809 iter/s, 14.624s/100 iters), loss = 0.210555
I0926 11:40:10.316320  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210554 (* 1 = 0.210554 loss)
I0926 11:40:10.316329  4406 sgd_solver.cpp:105] Iteration 65200, lr = 0.01
I0926 11:40:24.936992  4406 solver.cpp:218] Iteration 65300 (6.83965 iter/s, 14.6206s/100 iters), loss = 0.125367
I0926 11:40:24.937034  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125367 (* 1 = 0.125367 loss)
I0926 11:40:24.937041  4406 sgd_solver.cpp:105] Iteration 65300, lr = 0.01
I0926 11:40:39.561440  4406 solver.cpp:218] Iteration 65400 (6.8379 iter/s, 14.6244s/100 iters), loss = 0.151273
I0926 11:40:39.561481  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151273 (* 1 = 0.151273 loss)
I0926 11:40:39.561487  4406 sgd_solver.cpp:105] Iteration 65400, lr = 0.01
I0926 11:40:53.449339  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:40:54.035742  4406 solver.cpp:330] Iteration 65500, Testing net (#0)
I0926 11:40:57.460321  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:40:57.602996  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8238
I0926 11:40:57.603032  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.592951 (* 1 = 0.592951 loss)
I0926 11:40:57.748735  4406 solver.cpp:218] Iteration 65500 (5.49837 iter/s, 18.1872s/100 iters), loss = 0.120614
I0926 11:40:57.748764  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120613 (* 1 = 0.120613 loss)
I0926 11:40:57.748769  4406 sgd_solver.cpp:105] Iteration 65500, lr = 0.01
I0926 11:41:12.367626  4406 solver.cpp:218] Iteration 65600 (6.84049 iter/s, 14.6188s/100 iters), loss = 0.109154
I0926 11:41:12.367669  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.109154 (* 1 = 0.109154 loss)
I0926 11:41:12.367676  4406 sgd_solver.cpp:105] Iteration 65600, lr = 0.01
I0926 11:41:26.983028  4406 solver.cpp:218] Iteration 65700 (6.84213 iter/s, 14.6153s/100 iters), loss = 0.176737
I0926 11:41:26.983168  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176737 (* 1 = 0.176737 loss)
I0926 11:41:26.983176  4406 sgd_solver.cpp:105] Iteration 65700, lr = 0.01
I0926 11:41:41.597568  4406 solver.cpp:218] Iteration 65800 (6.84258 iter/s, 14.6144s/100 iters), loss = 0.175776
I0926 11:41:41.597609  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175775 (* 1 = 0.175775 loss)
I0926 11:41:41.597615  4406 sgd_solver.cpp:105] Iteration 65800, lr = 0.01
I0926 11:41:56.215631  4406 solver.cpp:218] Iteration 65900 (6.84089 iter/s, 14.618s/100 iters), loss = 0.122428
I0926 11:41:56.215672  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.122428 (* 1 = 0.122428 loss)
I0926 11:41:56.215677  4406 sgd_solver.cpp:105] Iteration 65900, lr = 0.01
I0926 11:42:10.103154  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:42:10.687602  4406 solver.cpp:330] Iteration 66000, Testing net (#0)
I0926 11:42:14.112787  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:42:14.256028  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8482
I0926 11:42:14.256065  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.505601 (* 1 = 0.505601 loss)
I0926 11:42:14.400434  4406 solver.cpp:218] Iteration 66000 (5.49912 iter/s, 18.1847s/100 iters), loss = 0.100182
I0926 11:42:14.400461  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100182 (* 1 = 0.100182 loss)
I0926 11:42:14.400467  4406 sgd_solver.cpp:105] Iteration 66000, lr = 0.01
I0926 11:42:29.016183  4406 solver.cpp:218] Iteration 66100 (6.84196 iter/s, 14.6157s/100 iters), loss = 0.0948204
I0926 11:42:29.016223  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.09482 (* 1 = 0.09482 loss)
I0926 11:42:29.016230  4406 sgd_solver.cpp:105] Iteration 66100, lr = 0.01
I0926 11:42:43.632212  4406 solver.cpp:218] Iteration 66200 (6.84184 iter/s, 14.616s/100 iters), loss = 0.173671
I0926 11:42:43.632328  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173671 (* 1 = 0.173671 loss)
I0926 11:42:43.632334  4406 sgd_solver.cpp:105] Iteration 66200, lr = 0.01
I0926 11:42:58.248303  4406 solver.cpp:218] Iteration 66300 (6.84185 iter/s, 14.6159s/100 iters), loss = 0.117085
I0926 11:42:58.248342  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117084 (* 1 = 0.117084 loss)
I0926 11:42:58.248348  4406 sgd_solver.cpp:105] Iteration 66300, lr = 0.01
I0926 11:43:12.864707  4406 solver.cpp:218] Iteration 66400 (6.84166 iter/s, 14.6163s/100 iters), loss = 0.123311
I0926 11:43:12.864735  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.123311 (* 1 = 0.123311 loss)
I0926 11:43:12.864742  4406 sgd_solver.cpp:105] Iteration 66400, lr = 0.01
I0926 11:43:26.754444  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:43:27.338594  4406 solver.cpp:330] Iteration 66500, Testing net (#0)
I0926 11:43:30.761970  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:43:30.905165  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8475
I0926 11:43:30.905201  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.50001 (* 1 = 0.50001 loss)
I0926 11:43:31.049365  4406 solver.cpp:218] Iteration 66500 (5.49916 iter/s, 18.1846s/100 iters), loss = 0.103026
I0926 11:43:31.049392  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103025 (* 1 = 0.103025 loss)
I0926 11:43:31.049399  4406 sgd_solver.cpp:105] Iteration 66500, lr = 0.01
I0926 11:43:45.676530  4406 solver.cpp:218] Iteration 66600 (6.83663 iter/s, 14.6271s/100 iters), loss = 0.239117
I0926 11:43:45.676559  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239116 (* 1 = 0.239116 loss)
I0926 11:43:45.676565  4406 sgd_solver.cpp:105] Iteration 66600, lr = 0.01
I0926 11:44:00.302439  4406 solver.cpp:218] Iteration 66700 (6.83721 iter/s, 14.6258s/100 iters), loss = 0.131626
I0926 11:44:00.302551  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131626 (* 1 = 0.131626 loss)
I0926 11:44:00.302558  4406 sgd_solver.cpp:105] Iteration 66700, lr = 0.01
I0926 11:44:14.925734  4406 solver.cpp:218] Iteration 66800 (6.83847 iter/s, 14.6231s/100 iters), loss = 0.177643
I0926 11:44:14.925763  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177643 (* 1 = 0.177643 loss)
I0926 11:44:14.925770  4406 sgd_solver.cpp:105] Iteration 66800, lr = 0.01
I0926 11:44:29.554414  4406 solver.cpp:218] Iteration 66900 (6.83592 iter/s, 14.6286s/100 iters), loss = 0.121645
I0926 11:44:29.554455  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121645 (* 1 = 0.121645 loss)
I0926 11:44:29.554461  4406 sgd_solver.cpp:105] Iteration 66900, lr = 0.01
I0926 11:44:43.450274  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:44:44.035336  4406 solver.cpp:330] Iteration 67000, Testing net (#0)
I0926 11:44:47.458058  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:44:47.600585  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8445
I0926 11:44:47.600610  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.532631 (* 1 = 0.532631 loss)
I0926 11:44:47.745889  4406 solver.cpp:218] Iteration 67000 (5.49711 iter/s, 18.1914s/100 iters), loss = 0.1134
I0926 11:44:47.745923  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1134 (* 1 = 0.1134 loss)
I0926 11:44:47.745929  4406 sgd_solver.cpp:105] Iteration 67000, lr = 0.01
I0926 11:45:02.349238  4406 solver.cpp:218] Iteration 67100 (6.84778 iter/s, 14.6033s/100 iters), loss = 0.145131
I0926 11:45:02.349277  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145131 (* 1 = 0.145131 loss)
I0926 11:45:02.349283  4406 sgd_solver.cpp:105] Iteration 67100, lr = 0.01
I0926 11:45:16.949262  4406 solver.cpp:218] Iteration 67200 (6.84934 iter/s, 14.5999s/100 iters), loss = 0.148847
I0926 11:45:16.949399  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148847 (* 1 = 0.148847 loss)
I0926 11:45:16.949406  4406 sgd_solver.cpp:105] Iteration 67200, lr = 0.01
I0926 11:45:31.555166  4406 solver.cpp:218] Iteration 67300 (6.84663 iter/s, 14.6057s/100 iters), loss = 0.261233
I0926 11:45:31.555197  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261233 (* 1 = 0.261233 loss)
I0926 11:45:31.555202  4406 sgd_solver.cpp:105] Iteration 67300, lr = 0.01
I0926 11:45:46.162360  4406 solver.cpp:218] Iteration 67400 (6.84597 iter/s, 14.6071s/100 iters), loss = 0.143162
I0926 11:45:46.162398  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143161 (* 1 = 0.143161 loss)
I0926 11:45:46.162405  4406 sgd_solver.cpp:105] Iteration 67400, lr = 0.01
I0926 11:46:00.047782  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:46:00.633297  4406 solver.cpp:330] Iteration 67500, Testing net (#0)
I0926 11:46:04.058804  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:46:04.201288  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8239
I0926 11:46:04.201323  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.587127 (* 1 = 0.587127 loss)
I0926 11:46:04.346141  4406 solver.cpp:218] Iteration 67500 (5.49943 iter/s, 18.1837s/100 iters), loss = 0.234495
I0926 11:46:04.346169  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234494 (* 1 = 0.234494 loss)
I0926 11:46:04.346175  4406 sgd_solver.cpp:105] Iteration 67500, lr = 0.01
I0926 11:46:18.963856  4406 solver.cpp:218] Iteration 67600 (6.84105 iter/s, 14.6176s/100 iters), loss = 0.157569
I0926 11:46:18.963886  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.157569 (* 1 = 0.157569 loss)
I0926 11:46:18.963891  4406 sgd_solver.cpp:105] Iteration 67600, lr = 0.01
I0926 11:46:33.582129  4406 solver.cpp:218] Iteration 67700 (6.84079 iter/s, 14.6182s/100 iters), loss = 0.140862
I0926 11:46:33.582242  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140862 (* 1 = 0.140862 loss)
I0926 11:46:33.582250  4406 sgd_solver.cpp:105] Iteration 67700, lr = 0.01
I0926 11:46:48.197058  4406 solver.cpp:218] Iteration 67800 (6.84239 iter/s, 14.6148s/100 iters), loss = 0.234233
I0926 11:46:48.197088  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234233 (* 1 = 0.234233 loss)
I0926 11:46:48.197104  4406 sgd_solver.cpp:105] Iteration 67800, lr = 0.01
I0926 11:47:02.809389  4406 solver.cpp:218] Iteration 67900 (6.84357 iter/s, 14.6123s/100 iters), loss = 0.169585
I0926 11:47:02.809419  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169585 (* 1 = 0.169585 loss)
I0926 11:47:02.809425  4406 sgd_solver.cpp:105] Iteration 67900, lr = 0.01
I0926 11:47:16.702971  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:47:17.287848  4406 solver.cpp:330] Iteration 68000, Testing net (#0)
I0926 11:47:20.713688  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:47:20.857012  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8386
I0926 11:47:20.857036  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.535237 (* 1 = 0.535237 loss)
I0926 11:47:21.002368  4406 solver.cpp:218] Iteration 68000 (5.49665 iter/s, 18.1929s/100 iters), loss = 0.158312
I0926 11:47:21.002398  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158311 (* 1 = 0.158311 loss)
I0926 11:47:21.002404  4406 sgd_solver.cpp:105] Iteration 68000, lr = 0.01
I0926 11:47:35.617168  4406 solver.cpp:218] Iteration 68100 (6.84241 iter/s, 14.6147s/100 iters), loss = 0.266402
I0926 11:47:35.617198  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266402 (* 1 = 0.266402 loss)
I0926 11:47:35.617207  4406 sgd_solver.cpp:105] Iteration 68100, lr = 0.01
I0926 11:47:50.232820  4406 solver.cpp:218] Iteration 68200 (6.84201 iter/s, 14.6156s/100 iters), loss = 0.107875
I0926 11:47:50.232959  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107874 (* 1 = 0.107874 loss)
I0926 11:47:50.232966  4406 sgd_solver.cpp:105] Iteration 68200, lr = 0.01
I0926 11:48:04.848564  4406 solver.cpp:218] Iteration 68300 (6.84202 iter/s, 14.6156s/100 iters), loss = 0.193839
I0926 11:48:04.848595  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193839 (* 1 = 0.193839 loss)
I0926 11:48:04.848603  4406 sgd_solver.cpp:105] Iteration 68300, lr = 0.01
I0926 11:48:19.456825  4406 solver.cpp:218] Iteration 68400 (6.84547 iter/s, 14.6082s/100 iters), loss = 0.175643
I0926 11:48:19.456866  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175643 (* 1 = 0.175643 loss)
I0926 11:48:19.456871  4406 sgd_solver.cpp:105] Iteration 68400, lr = 0.01
I0926 11:48:33.345643  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:48:33.930387  4406 solver.cpp:330] Iteration 68500, Testing net (#0)
I0926 11:48:37.353602  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:48:37.496179  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8468
I0926 11:48:37.496206  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.504494 (* 1 = 0.504494 loss)
I0926 11:48:37.641711  4406 solver.cpp:218] Iteration 68500 (5.4991 iter/s, 18.1848s/100 iters), loss = 0.132929
I0926 11:48:37.641738  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132929 (* 1 = 0.132929 loss)
I0926 11:48:37.641744  4406 sgd_solver.cpp:105] Iteration 68500, lr = 0.01
I0926 11:48:52.258090  4406 solver.cpp:218] Iteration 68600 (6.84167 iter/s, 14.6163s/100 iters), loss = 0.232383
I0926 11:48:52.258128  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232383 (* 1 = 0.232383 loss)
I0926 11:48:52.258134  4406 sgd_solver.cpp:105] Iteration 68600, lr = 0.01
I0926 11:49:06.876695  4406 solver.cpp:218] Iteration 68700 (6.84063 iter/s, 14.6185s/100 iters), loss = 0.212753
I0926 11:49:06.876816  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212752 (* 1 = 0.212752 loss)
I0926 11:49:06.876823  4406 sgd_solver.cpp:105] Iteration 68700, lr = 0.01
I0926 11:49:21.494412  4406 solver.cpp:218] Iteration 68800 (6.84109 iter/s, 14.6176s/100 iters), loss = 0.124573
I0926 11:49:21.494441  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124572 (* 1 = 0.124572 loss)
I0926 11:49:21.494447  4406 sgd_solver.cpp:105] Iteration 68800, lr = 0.01
I0926 11:49:36.112063  4406 solver.cpp:218] Iteration 68900 (6.84108 iter/s, 14.6176s/100 iters), loss = 0.0746767
I0926 11:49:36.112093  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0746765 (* 1 = 0.0746765 loss)
I0926 11:49:36.112099  4406 sgd_solver.cpp:105] Iteration 68900, lr = 0.01
I0926 11:49:50.003208  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:49:50.587401  4406 solver.cpp:330] Iteration 69000, Testing net (#0)
I0926 11:49:54.011265  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:49:54.153614  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8328
I0926 11:49:54.153650  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.546962 (* 1 = 0.546962 loss)
I0926 11:49:54.298413  4406 solver.cpp:218] Iteration 69000 (5.49865 iter/s, 18.1863s/100 iters), loss = 0.0970614
I0926 11:49:54.298439  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0970612 (* 1 = 0.0970612 loss)
I0926 11:49:54.298446  4406 sgd_solver.cpp:105] Iteration 69000, lr = 0.01
I0926 11:50:08.921838  4406 solver.cpp:218] Iteration 69100 (6.83837 iter/s, 14.6234s/100 iters), loss = 0.1977
I0926 11:50:08.921867  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1977 (* 1 = 0.1977 loss)
I0926 11:50:08.921874  4406 sgd_solver.cpp:105] Iteration 69100, lr = 0.01
I0926 11:50:23.542834  4406 solver.cpp:218] Iteration 69200 (6.83951 iter/s, 14.6209s/100 iters), loss = 0.174071
I0926 11:50:23.542973  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.174071 (* 1 = 0.174071 loss)
I0926 11:50:23.542997  4406 sgd_solver.cpp:105] Iteration 69200, lr = 0.01
I0926 11:50:38.167295  4406 solver.cpp:218] Iteration 69300 (6.83793 iter/s, 14.6243s/100 iters), loss = 0.204903
I0926 11:50:38.167325  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204903 (* 1 = 0.204903 loss)
I0926 11:50:38.167331  4406 sgd_solver.cpp:105] Iteration 69300, lr = 0.01
I0926 11:50:52.789366  4406 solver.cpp:218] Iteration 69400 (6.83901 iter/s, 14.622s/100 iters), loss = 0.204898
I0926 11:50:52.789394  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204898 (* 1 = 0.204898 loss)
I0926 11:50:52.789399  4406 sgd_solver.cpp:105] Iteration 69400, lr = 0.01
I0926 11:51:06.686702  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:51:07.271631  4406 solver.cpp:330] Iteration 69500, Testing net (#0)
I0926 11:51:10.696602  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:51:10.839455  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8431
I0926 11:51:10.839480  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.514713 (* 1 = 0.514713 loss)
I0926 11:51:10.983968  4406 solver.cpp:218] Iteration 69500 (5.49616 iter/s, 18.1945s/100 iters), loss = 0.16881
I0926 11:51:10.983997  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.16881 (* 1 = 0.16881 loss)
I0926 11:51:10.984004  4406 sgd_solver.cpp:105] Iteration 69500, lr = 0.01
I0926 11:51:25.588182  4406 solver.cpp:218] Iteration 69600 (6.84737 iter/s, 14.6041s/100 iters), loss = 0.0967807
I0926 11:51:25.588212  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0967805 (* 1 = 0.0967805 loss)
I0926 11:51:25.588217  4406 sgd_solver.cpp:105] Iteration 69600, lr = 0.01
I0926 11:51:40.194921  4406 solver.cpp:218] Iteration 69700 (6.84619 iter/s, 14.6067s/100 iters), loss = 0.147574
I0926 11:51:40.195060  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.147574 (* 1 = 0.147574 loss)
I0926 11:51:40.195066  4406 sgd_solver.cpp:105] Iteration 69700, lr = 0.01
I0926 11:51:54.801383  4406 solver.cpp:218] Iteration 69800 (6.84636 iter/s, 14.6063s/100 iters), loss = 0.0805531
I0926 11:51:54.801412  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.080553 (* 1 = 0.080553 loss)
I0926 11:51:54.801419  4406 sgd_solver.cpp:105] Iteration 69800, lr = 0.01
I0926 11:52:09.406358  4406 solver.cpp:218] Iteration 69900 (6.84701 iter/s, 14.6049s/100 iters), loss = 0.130347
I0926 11:52:09.406386  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130347 (* 1 = 0.130347 loss)
I0926 11:52:09.406402  4406 sgd_solver.cpp:105] Iteration 69900, lr = 0.01
I0926 11:52:23.285984  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:52:23.871233  4406 solver.cpp:330] Iteration 70000, Testing net (#0)
I0926 11:52:27.293025  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:52:27.436075  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8358
I0926 11:52:27.436111  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.547671 (* 1 = 0.547671 loss)
I0926 11:52:27.580373  4406 solver.cpp:218] Iteration 70000 (5.50239 iter/s, 18.1739s/100 iters), loss = 0.107803
I0926 11:52:27.580417  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107803 (* 1 = 0.107803 loss)
I0926 11:52:27.580425  4406 sgd_solver.cpp:105] Iteration 70000, lr = 0.01
I0926 11:52:42.194612  4406 solver.cpp:218] Iteration 70100 (6.84268 iter/s, 14.6142s/100 iters), loss = 0.0955988
I0926 11:52:42.194651  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0955986 (* 1 = 0.0955986 loss)
I0926 11:52:42.194658  4406 sgd_solver.cpp:105] Iteration 70100, lr = 0.01
I0926 11:52:56.808373  4406 solver.cpp:218] Iteration 70200 (6.8429 iter/s, 14.6137s/100 iters), loss = 0.169111
I0926 11:52:56.808533  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.169111 (* 1 = 0.169111 loss)
I0926 11:52:56.808552  4406 sgd_solver.cpp:105] Iteration 70200, lr = 0.01
I0926 11:53:11.422525  4406 solver.cpp:218] Iteration 70300 (6.84277 iter/s, 14.614s/100 iters), loss = 0.171419
I0926 11:53:11.422565  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171419 (* 1 = 0.171419 loss)
I0926 11:53:11.422571  4406 sgd_solver.cpp:105] Iteration 70300, lr = 0.01
I0926 11:53:26.036412  4406 solver.cpp:218] Iteration 70400 (6.84284 iter/s, 14.6138s/100 iters), loss = 0.171036
I0926 11:53:26.036440  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171036 (* 1 = 0.171036 loss)
I0926 11:53:26.036447  4406 sgd_solver.cpp:105] Iteration 70400, lr = 0.01
I0926 11:53:39.920711  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:53:40.505813  4406 solver.cpp:330] Iteration 70500, Testing net (#0)
I0926 11:53:43.931602  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:53:44.074630  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8371
I0926 11:53:44.074666  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.552451 (* 1 = 0.552451 loss)
I0926 11:53:44.219298  4406 solver.cpp:218] Iteration 70500 (5.4997 iter/s, 18.1828s/100 iters), loss = 0.185856
I0926 11:53:44.219328  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185856 (* 1 = 0.185856 loss)
I0926 11:53:44.219334  4406 sgd_solver.cpp:105] Iteration 70500, lr = 0.01
I0926 11:53:58.832494  4406 solver.cpp:218] Iteration 70600 (6.84316 iter/s, 14.6131s/100 iters), loss = 0.166712
I0926 11:53:58.832535  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166712 (* 1 = 0.166712 loss)
I0926 11:53:58.832540  4406 sgd_solver.cpp:105] Iteration 70600, lr = 0.01
I0926 11:54:13.447803  4406 solver.cpp:218] Iteration 70700 (6.84218 iter/s, 14.6152s/100 iters), loss = 0.0979569
I0926 11:54:13.447909  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0979566 (* 1 = 0.0979566 loss)
I0926 11:54:13.447916  4406 sgd_solver.cpp:105] Iteration 70700, lr = 0.01
I0926 11:54:28.060360  4406 solver.cpp:218] Iteration 70800 (6.8435 iter/s, 14.6124s/100 iters), loss = 0.180311
I0926 11:54:28.060401  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180311 (* 1 = 0.180311 loss)
I0926 11:54:28.060406  4406 sgd_solver.cpp:105] Iteration 70800, lr = 0.01
I0926 11:54:42.673562  4406 solver.cpp:218] Iteration 70900 (6.84316 iter/s, 14.6131s/100 iters), loss = 0.112829
I0926 11:54:42.673600  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.112829 (* 1 = 0.112829 loss)
I0926 11:54:42.673616  4406 sgd_solver.cpp:105] Iteration 70900, lr = 0.01
I0926 11:54:56.560876  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:54:57.145362  4406 solver.cpp:330] Iteration 71000, Testing net (#0)
I0926 11:55:00.569720  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:55:00.712646  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8393
I0926 11:55:00.712682  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.532748 (* 1 = 0.532748 loss)
I0926 11:55:00.857183  4406 solver.cpp:218] Iteration 71000 (5.49948 iter/s, 18.1835s/100 iters), loss = 0.143358
I0926 11:55:00.857214  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143358 (* 1 = 0.143358 loss)
I0926 11:55:00.857221  4406 sgd_solver.cpp:105] Iteration 71000, lr = 0.01
I0926 11:55:15.470042  4406 solver.cpp:218] Iteration 71100 (6.84332 iter/s, 14.6128s/100 iters), loss = 0.158743
I0926 11:55:15.470082  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158743 (* 1 = 0.158743 loss)
I0926 11:55:15.470088  4406 sgd_solver.cpp:105] Iteration 71100, lr = 0.01
I0926 11:55:30.084980  4406 solver.cpp:218] Iteration 71200 (6.84235 iter/s, 14.6149s/100 iters), loss = 0.0746602
I0926 11:55:30.085074  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.07466 (* 1 = 0.07466 loss)
I0926 11:55:30.085083  4406 sgd_solver.cpp:105] Iteration 71200, lr = 0.01
I0926 11:55:44.699630  4406 solver.cpp:218] Iteration 71300 (6.84251 iter/s, 14.6145s/100 iters), loss = 0.122213
I0926 11:55:44.699671  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.122213 (* 1 = 0.122213 loss)
I0926 11:55:44.699677  4406 sgd_solver.cpp:105] Iteration 71300, lr = 0.01
I0926 11:55:59.315584  4406 solver.cpp:218] Iteration 71400 (6.84188 iter/s, 14.6159s/100 iters), loss = 0.242306
I0926 11:55:59.315624  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.242306 (* 1 = 0.242306 loss)
I0926 11:55:59.315630  4406 sgd_solver.cpp:105] Iteration 71400, lr = 0.01
I0926 11:56:13.201794  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:56:13.786689  4406 solver.cpp:330] Iteration 71500, Testing net (#0)
I0926 11:56:17.210957  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:56:17.353809  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7966
I0926 11:56:17.353845  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.7078 (* 1 = 0.7078 loss)
I0926 11:56:17.498677  4406 solver.cpp:218] Iteration 71500 (5.49964 iter/s, 18.183s/100 iters), loss = 0.200069
I0926 11:56:17.498709  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200069 (* 1 = 0.200069 loss)
I0926 11:56:17.498716  4406 sgd_solver.cpp:105] Iteration 71500, lr = 0.01
I0926 11:56:32.120738  4406 solver.cpp:218] Iteration 71600 (6.83901 iter/s, 14.622s/100 iters), loss = 0.16199
I0926 11:56:32.120777  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.161989 (* 1 = 0.161989 loss)
I0926 11:56:32.120782  4406 sgd_solver.cpp:105] Iteration 71600, lr = 0.01
I0926 11:56:46.744913  4406 solver.cpp:218] Iteration 71700 (6.83803 iter/s, 14.6241s/100 iters), loss = 0.0740972
I0926 11:56:46.745048  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.074097 (* 1 = 0.074097 loss)
I0926 11:56:46.745055  4406 sgd_solver.cpp:105] Iteration 71700, lr = 0.01
I0926 11:57:01.372143  4406 solver.cpp:218] Iteration 71800 (6.83664 iter/s, 14.6271s/100 iters), loss = 0.12502
I0926 11:57:01.372184  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.12502 (* 1 = 0.12502 loss)
I0926 11:57:01.372189  4406 sgd_solver.cpp:105] Iteration 71800, lr = 0.01
I0926 11:57:15.997850  4406 solver.cpp:218] Iteration 71900 (6.83731 iter/s, 14.6256s/100 iters), loss = 0.108635
I0926 11:57:15.997889  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108635 (* 1 = 0.108635 loss)
I0926 11:57:15.997895  4406 sgd_solver.cpp:105] Iteration 71900, lr = 0.01
I0926 11:57:29.897091  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:57:30.482600  4406 solver.cpp:330] Iteration 72000, Testing net (#0)
I0926 11:57:33.905798  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:57:34.049242  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8339
I0926 11:57:34.049278  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.556832 (* 1 = 0.556832 loss)
I0926 11:57:34.193593  4406 solver.cpp:218] Iteration 72000 (5.49582 iter/s, 18.1957s/100 iters), loss = 0.125897
I0926 11:57:34.193639  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125897 (* 1 = 0.125897 loss)
I0926 11:57:34.193660  4406 sgd_solver.cpp:105] Iteration 72000, lr = 0.01
I0926 11:57:48.799609  4406 solver.cpp:218] Iteration 72100 (6.84656 iter/s, 14.6059s/100 iters), loss = 0.117827
I0926 11:57:48.799649  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117827 (* 1 = 0.117827 loss)
I0926 11:57:48.799655  4406 sgd_solver.cpp:105] Iteration 72100, lr = 0.01
I0926 11:58:03.407513  4406 solver.cpp:218] Iteration 72200 (6.84565 iter/s, 14.6078s/100 iters), loss = 0.184983
I0926 11:58:03.407634  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184983 (* 1 = 0.184983 loss)
I0926 11:58:03.407641  4406 sgd_solver.cpp:105] Iteration 72200, lr = 0.01
I0926 11:58:18.011487  4406 solver.cpp:218] Iteration 72300 (6.84752 iter/s, 14.6038s/100 iters), loss = 0.141048
I0926 11:58:18.011528  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141048 (* 1 = 0.141048 loss)
I0926 11:58:18.011533  4406 sgd_solver.cpp:105] Iteration 72300, lr = 0.01
I0926 11:58:32.618679  4406 solver.cpp:218] Iteration 72400 (6.84598 iter/s, 14.6071s/100 iters), loss = 0.0706908
I0926 11:58:32.618719  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0706906 (* 1 = 0.0706906 loss)
I0926 11:58:32.618726  4406 sgd_solver.cpp:105] Iteration 72400, lr = 0.01
I0926 11:58:46.496183  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:58:47.080482  4406 solver.cpp:330] Iteration 72500, Testing net (#0)
I0926 11:58:50.506364  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 11:58:50.649278  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8077
I0926 11:58:50.649314  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.735516 (* 1 = 0.735516 loss)
I0926 11:58:50.793823  4406 solver.cpp:218] Iteration 72500 (5.50205 iter/s, 18.1751s/100 iters), loss = 0.1602
I0926 11:58:50.793854  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.1602 (* 1 = 0.1602 loss)
I0926 11:58:50.793861  4406 sgd_solver.cpp:105] Iteration 72500, lr = 0.01
I0926 11:59:05.416929  4406 solver.cpp:218] Iteration 72600 (6.83852 iter/s, 14.623s/100 iters), loss = 0.171215
I0926 11:59:05.416970  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.171215 (* 1 = 0.171215 loss)
I0926 11:59:05.416975  4406 sgd_solver.cpp:105] Iteration 72600, lr = 0.01
I0926 11:59:20.033831  4406 solver.cpp:218] Iteration 72700 (6.84143 iter/s, 14.6168s/100 iters), loss = 0.204009
I0926 11:59:20.033972  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204009 (* 1 = 0.204009 loss)
I0926 11:59:20.033980  4406 sgd_solver.cpp:105] Iteration 72700, lr = 0.01
I0926 11:59:34.651340  4406 solver.cpp:218] Iteration 72800 (6.84119 iter/s, 14.6173s/100 iters), loss = 0.0876116
I0926 11:59:34.651381  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0876113 (* 1 = 0.0876113 loss)
I0926 11:59:34.651387  4406 sgd_solver.cpp:105] Iteration 72800, lr = 0.01
I0926 11:59:49.266219  4406 solver.cpp:218] Iteration 72900 (6.84238 iter/s, 14.6148s/100 iters), loss = 0.13946
I0926 11:59:49.266259  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139459 (* 1 = 0.139459 loss)
I0926 11:59:49.266265  4406 sgd_solver.cpp:105] Iteration 72900, lr = 0.01
I0926 12:00:03.159562  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:00:03.745298  4406 solver.cpp:330] Iteration 73000, Testing net (#0)
I0926 12:00:07.169859  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:00:07.312829  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8354
I0926 12:00:07.312865  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.571268 (* 1 = 0.571268 loss)
I0926 12:00:07.457913  4406 solver.cpp:218] Iteration 73000 (5.49704 iter/s, 18.1916s/100 iters), loss = 0.134918
I0926 12:00:07.457947  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134918 (* 1 = 0.134918 loss)
I0926 12:00:07.457952  4406 sgd_solver.cpp:105] Iteration 73000, lr = 0.01
I0926 12:00:22.073256  4406 solver.cpp:218] Iteration 73100 (6.84216 iter/s, 14.6153s/100 iters), loss = 0.101876
I0926 12:00:22.073297  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.101876 (* 1 = 0.101876 loss)
I0926 12:00:22.073302  4406 sgd_solver.cpp:105] Iteration 73100, lr = 0.01
I0926 12:00:36.692363  4406 solver.cpp:218] Iteration 73200 (6.8404 iter/s, 14.619s/100 iters), loss = 0.129706
I0926 12:00:36.692538  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.129706 (* 1 = 0.129706 loss)
I0926 12:00:36.692556  4406 sgd_solver.cpp:105] Iteration 73200, lr = 0.01
I0926 12:00:51.308712  4406 solver.cpp:218] Iteration 73300 (6.84175 iter/s, 14.6161s/100 iters), loss = 0.0675855
I0926 12:00:51.308751  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0675853 (* 1 = 0.0675853 loss)
I0926 12:00:51.308756  4406 sgd_solver.cpp:105] Iteration 73300, lr = 0.01
I0926 12:01:05.922243  4406 solver.cpp:218] Iteration 73400 (6.84301 iter/s, 14.6135s/100 iters), loss = 0.329904
I0926 12:01:05.922284  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329904 (* 1 = 0.329904 loss)
I0926 12:01:05.922291  4406 sgd_solver.cpp:105] Iteration 73400, lr = 0.01
I0926 12:01:19.812680  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:01:20.398288  4406 solver.cpp:330] Iteration 73500, Testing net (#0)
I0926 12:01:23.820801  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:01:23.963804  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.565
I0926 12:01:23.963827  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.04388 (* 1 = 2.04388 loss)
I0926 12:01:24.108256  4406 solver.cpp:218] Iteration 73500 (5.49876 iter/s, 18.1859s/100 iters), loss = 0.288817
I0926 12:01:24.108284  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.288816 (* 1 = 0.288816 loss)
I0926 12:01:24.108291  4406 sgd_solver.cpp:105] Iteration 73500, lr = 0.01
I0926 12:01:38.727658  4406 solver.cpp:218] Iteration 73600 (6.84026 iter/s, 14.6193s/100 iters), loss = 0.164344
I0926 12:01:38.727699  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164344 (* 1 = 0.164344 loss)
I0926 12:01:38.727705  4406 sgd_solver.cpp:105] Iteration 73600, lr = 0.01
I0926 12:01:53.349750  4406 solver.cpp:218] Iteration 73700 (6.839 iter/s, 14.622s/100 iters), loss = 0.175416
I0926 12:01:53.349833  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.175416 (* 1 = 0.175416 loss)
I0926 12:01:53.349849  4406 sgd_solver.cpp:105] Iteration 73700, lr = 0.01
I0926 12:02:07.968118  4406 solver.cpp:218] Iteration 73800 (6.84076 iter/s, 14.6182s/100 iters), loss = 0.15771
I0926 12:02:07.968159  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15771 (* 1 = 0.15771 loss)
I0926 12:02:07.968164  4406 sgd_solver.cpp:105] Iteration 73800, lr = 0.01
I0926 12:02:22.584858  4406 solver.cpp:218] Iteration 73900 (6.84151 iter/s, 14.6167s/100 iters), loss = 0.163867
I0926 12:02:22.584890  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163866 (* 1 = 0.163866 loss)
I0926 12:02:22.584897  4406 sgd_solver.cpp:105] Iteration 73900, lr = 0.01
I0926 12:02:36.478955  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:02:37.064251  4406 solver.cpp:330] Iteration 74000, Testing net (#0)
I0926 12:02:40.489943  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:02:40.632508  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8143
I0926 12:02:40.632532  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.623398 (* 1 = 0.623398 loss)
I0926 12:02:40.777879  4406 solver.cpp:218] Iteration 74000 (5.49664 iter/s, 18.1929s/100 iters), loss = 0.133482
I0926 12:02:40.777909  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.133482 (* 1 = 0.133482 loss)
I0926 12:02:40.777915  4406 sgd_solver.cpp:105] Iteration 74000, lr = 0.01
I0926 12:02:55.407513  4406 solver.cpp:218] Iteration 74100 (6.83547 iter/s, 14.6296s/100 iters), loss = 0.226879
I0926 12:02:55.407552  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226878 (* 1 = 0.226878 loss)
I0926 12:02:55.407557  4406 sgd_solver.cpp:105] Iteration 74100, lr = 0.01
I0926 12:03:10.028694  4406 solver.cpp:218] Iteration 74200 (6.83943 iter/s, 14.6211s/100 iters), loss = 0.176652
I0926 12:03:10.028829  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.176652 (* 1 = 0.176652 loss)
I0926 12:03:10.028837  4406 sgd_solver.cpp:105] Iteration 74200, lr = 0.01
I0926 12:03:24.650789  4406 solver.cpp:218] Iteration 74300 (6.83904 iter/s, 14.6219s/100 iters), loss = 0.178955
I0926 12:03:24.650818  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178955 (* 1 = 0.178955 loss)
I0926 12:03:24.650825  4406 sgd_solver.cpp:105] Iteration 74300, lr = 0.01
I0926 12:03:39.281158  4406 solver.cpp:218] Iteration 74400 (6.83513 iter/s, 14.6303s/100 iters), loss = 0.198589
I0926 12:03:39.281198  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198589 (* 1 = 0.198589 loss)
I0926 12:03:39.281203  4406 sgd_solver.cpp:105] Iteration 74400, lr = 0.01
I0926 12:03:53.177855  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:03:53.762485  4406 solver.cpp:330] Iteration 74500, Testing net (#0)
I0926 12:03:57.187232  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:03:57.329905  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8443
I0926 12:03:57.329941  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.505834 (* 1 = 0.505834 loss)
I0926 12:03:57.474315  4406 solver.cpp:218] Iteration 74500 (5.4966 iter/s, 18.1931s/100 iters), loss = 0.0939604
I0926 12:03:57.474344  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0939603 (* 1 = 0.0939603 loss)
I0926 12:03:57.474350  4406 sgd_solver.cpp:105] Iteration 74500, lr = 0.01
I0926 12:04:12.079308  4406 solver.cpp:218] Iteration 74600 (6.847 iter/s, 14.6049s/100 iters), loss = 0.250523
I0926 12:04:12.079347  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250523 (* 1 = 0.250523 loss)
I0926 12:04:12.079354  4406 sgd_solver.cpp:105] Iteration 74600, lr = 0.01
I0926 12:04:26.684646  4406 solver.cpp:218] Iteration 74700 (6.84685 iter/s, 14.6053s/100 iters), loss = 0.0708138
I0926 12:04:26.684789  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0708137 (* 1 = 0.0708137 loss)
I0926 12:04:26.684798  4406 sgd_solver.cpp:105] Iteration 74700, lr = 0.01
I0926 12:04:41.292618  4406 solver.cpp:218] Iteration 74800 (6.84566 iter/s, 14.6078s/100 iters), loss = 0.246719
I0926 12:04:41.292656  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246719 (* 1 = 0.246719 loss)
I0926 12:04:41.292663  4406 sgd_solver.cpp:105] Iteration 74800, lr = 0.01
I0926 12:04:55.896044  4406 solver.cpp:218] Iteration 74900 (6.84774 iter/s, 14.6033s/100 iters), loss = 0.0959099
I0926 12:04:55.896073  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0959098 (* 1 = 0.0959098 loss)
I0926 12:04:55.896080  4406 sgd_solver.cpp:105] Iteration 74900, lr = 0.01
I0926 12:05:09.778668  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:05:10.364193  4406 solver.cpp:330] Iteration 75000, Testing net (#0)
I0926 12:05:13.788336  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:05:13.931519  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8257
I0926 12:05:13.931555  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.589008 (* 1 = 0.589008 loss)
I0926 12:05:14.076637  4406 solver.cpp:218] Iteration 75000 (5.50039 iter/s, 18.1805s/100 iters), loss = 0.139121
I0926 12:05:14.076663  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139121 (* 1 = 0.139121 loss)
I0926 12:05:14.076669  4406 sgd_solver.cpp:105] Iteration 75000, lr = 0.01
I0926 12:05:28.682472  4406 solver.cpp:218] Iteration 75100 (6.84661 iter/s, 14.6058s/100 iters), loss = 0.193167
I0926 12:05:28.682502  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193166 (* 1 = 0.193166 loss)
I0926 12:05:28.682508  4406 sgd_solver.cpp:105] Iteration 75100, lr = 0.01
I0926 12:05:43.293936  4406 solver.cpp:218] Iteration 75200 (6.84397 iter/s, 14.6114s/100 iters), loss = 0.130841
I0926 12:05:43.294028  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130841 (* 1 = 0.130841 loss)
I0926 12:05:43.294035  4406 sgd_solver.cpp:105] Iteration 75200, lr = 0.01
I0926 12:05:57.904034  4406 solver.cpp:218] Iteration 75300 (6.84464 iter/s, 14.61s/100 iters), loss = 0.148451
I0926 12:05:57.904075  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148451 (* 1 = 0.148451 loss)
I0926 12:05:57.904081  4406 sgd_solver.cpp:105] Iteration 75300, lr = 0.01
I0926 12:06:12.514991  4406 solver.cpp:218] Iteration 75400 (6.84422 iter/s, 14.6109s/100 iters), loss = 0.163469
I0926 12:06:12.515033  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163468 (* 1 = 0.163468 loss)
I0926 12:06:12.515038  4406 sgd_solver.cpp:105] Iteration 75400, lr = 0.01
I0926 12:06:26.404934  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:06:26.989637  4406 solver.cpp:330] Iteration 75500, Testing net (#0)
I0926 12:06:30.413295  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:06:30.555753  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8346
I0926 12:06:30.555788  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.551858 (* 1 = 0.551858 loss)
I0926 12:06:30.700062  4406 solver.cpp:218] Iteration 75500 (5.49904 iter/s, 18.185s/100 iters), loss = 0.0944124
I0926 12:06:30.700093  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0944123 (* 1 = 0.0944123 loss)
I0926 12:06:30.700100  4406 sgd_solver.cpp:105] Iteration 75500, lr = 0.01
I0926 12:06:45.316537  4406 solver.cpp:218] Iteration 75600 (6.84163 iter/s, 14.6164s/100 iters), loss = 0.186926
I0926 12:06:45.316566  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186926 (* 1 = 0.186926 loss)
I0926 12:06:45.316572  4406 sgd_solver.cpp:105] Iteration 75600, lr = 0.01
I0926 12:06:59.927830  4406 solver.cpp:218] Iteration 75700 (6.84405 iter/s, 14.6112s/100 iters), loss = 0.0646065
I0926 12:06:59.927973  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0646064 (* 1 = 0.0646064 loss)
I0926 12:06:59.927983  4406 sgd_solver.cpp:105] Iteration 75700, lr = 0.01
I0926 12:07:14.541977  4406 solver.cpp:218] Iteration 75800 (6.84277 iter/s, 14.614s/100 iters), loss = 0.144581
I0926 12:07:14.542007  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.144581 (* 1 = 0.144581 loss)
I0926 12:07:14.542013  4406 sgd_solver.cpp:105] Iteration 75800, lr = 0.01
I0926 12:07:29.153939  4406 solver.cpp:218] Iteration 75900 (6.84374 iter/s, 14.6119s/100 iters), loss = 0.152576
I0926 12:07:29.153969  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.152576 (* 1 = 0.152576 loss)
I0926 12:07:29.153975  4406 sgd_solver.cpp:105] Iteration 75900, lr = 0.01
I0926 12:07:43.041693  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:07:43.626484  4406 solver.cpp:330] Iteration 76000, Testing net (#0)
I0926 12:07:47.049252  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:07:47.192306  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8051
I0926 12:07:47.192330  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.702202 (* 1 = 0.702202 loss)
I0926 12:07:47.336889  4406 solver.cpp:218] Iteration 76000 (5.49968 iter/s, 18.1829s/100 iters), loss = 0.202483
I0926 12:07:47.336916  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202483 (* 1 = 0.202483 loss)
I0926 12:07:47.336923  4406 sgd_solver.cpp:105] Iteration 76000, lr = 0.01
I0926 12:08:01.948467  4406 solver.cpp:218] Iteration 76100 (6.84392 iter/s, 14.6115s/100 iters), loss = 0.135367
I0926 12:08:01.948498  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135367 (* 1 = 0.135367 loss)
I0926 12:08:01.948504  4406 sgd_solver.cpp:105] Iteration 76100, lr = 0.01
I0926 12:08:16.561208  4406 solver.cpp:218] Iteration 76200 (6.84337 iter/s, 14.6127s/100 iters), loss = 0.158986
I0926 12:08:16.561314  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158986 (* 1 = 0.158986 loss)
I0926 12:08:16.561321  4406 sgd_solver.cpp:105] Iteration 76200, lr = 0.01
I0926 12:08:31.178931  4406 solver.cpp:218] Iteration 76300 (6.84108 iter/s, 14.6176s/100 iters), loss = 0.162661
I0926 12:08:31.178972  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.162661 (* 1 = 0.162661 loss)
I0926 12:08:31.178977  4406 sgd_solver.cpp:105] Iteration 76300, lr = 0.01
I0926 12:08:45.788305  4406 solver.cpp:218] Iteration 76400 (6.84496 iter/s, 14.6093s/100 iters), loss = 0.0941311
I0926 12:08:45.788334  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.094131 (* 1 = 0.094131 loss)
I0926 12:08:45.788341  4406 sgd_solver.cpp:105] Iteration 76400, lr = 0.01
I0926 12:08:59.672654  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:09:00.258038  4406 solver.cpp:330] Iteration 76500, Testing net (#0)
I0926 12:09:03.680517  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:09:03.823228  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8437
I0926 12:09:03.823264  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.536951 (* 1 = 0.536951 loss)
I0926 12:09:03.967674  4406 solver.cpp:218] Iteration 76500 (5.50076 iter/s, 18.1793s/100 iters), loss = 0.213082
I0926 12:09:03.967701  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213082 (* 1 = 0.213082 loss)
I0926 12:09:03.967708  4406 sgd_solver.cpp:105] Iteration 76500, lr = 0.01
I0926 12:09:18.586628  4406 solver.cpp:218] Iteration 76600 (6.84046 iter/s, 14.6189s/100 iters), loss = 0.260712
I0926 12:09:18.586669  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260712 (* 1 = 0.260712 loss)
I0926 12:09:18.586674  4406 sgd_solver.cpp:105] Iteration 76600, lr = 0.01
I0926 12:09:33.207139  4406 solver.cpp:218] Iteration 76700 (6.83974 iter/s, 14.6204s/100 iters), loss = 0.198087
I0926 12:09:33.207250  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.198087 (* 1 = 0.198087 loss)
I0926 12:09:33.207267  4406 sgd_solver.cpp:105] Iteration 76700, lr = 0.01
I0926 12:09:47.829743  4406 solver.cpp:218] Iteration 76800 (6.83879 iter/s, 14.6225s/100 iters), loss = 0.213367
I0926 12:09:47.829774  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213366 (* 1 = 0.213366 loss)
I0926 12:09:47.829780  4406 sgd_solver.cpp:105] Iteration 76800, lr = 0.01
I0926 12:10:02.452971  4406 solver.cpp:218] Iteration 76900 (6.83847 iter/s, 14.6232s/100 iters), loss = 0.165164
I0926 12:10:02.453011  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165164 (* 1 = 0.165164 loss)
I0926 12:10:02.453016  4406 sgd_solver.cpp:105] Iteration 76900, lr = 0.01
I0926 12:10:16.345670  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:10:16.931424  4406 solver.cpp:330] Iteration 77000, Testing net (#0)
I0926 12:10:20.354401  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:10:20.499553  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8455
I0926 12:10:20.499588  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.498978 (* 1 = 0.498978 loss)
I0926 12:10:20.643676  4406 solver.cpp:218] Iteration 77000 (5.49734 iter/s, 18.1906s/100 iters), loss = 0.148566
I0926 12:10:20.643707  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148566 (* 1 = 0.148566 loss)
I0926 12:10:20.643713  4406 sgd_solver.cpp:105] Iteration 77000, lr = 0.01
I0926 12:10:35.241123  4406 solver.cpp:218] Iteration 77100 (6.85054 iter/s, 14.5974s/100 iters), loss = 0.138913
I0926 12:10:35.241165  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138912 (* 1 = 0.138912 loss)
I0926 12:10:35.241171  4406 sgd_solver.cpp:105] Iteration 77100, lr = 0.01
I0926 12:10:49.847947  4406 solver.cpp:218] Iteration 77200 (6.84615 iter/s, 14.6067s/100 iters), loss = 0.136516
I0926 12:10:49.848089  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.136516 (* 1 = 0.136516 loss)
I0926 12:10:49.848098  4406 sgd_solver.cpp:105] Iteration 77200, lr = 0.01
I0926 12:11:04.451453  4406 solver.cpp:218] Iteration 77300 (6.84775 iter/s, 14.6033s/100 iters), loss = 0.121166
I0926 12:11:04.451483  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121166 (* 1 = 0.121166 loss)
I0926 12:11:04.451488  4406 sgd_solver.cpp:105] Iteration 77300, lr = 0.01
I0926 12:11:19.060088  4406 solver.cpp:218] Iteration 77400 (6.8453 iter/s, 14.6086s/100 iters), loss = 0.254071
I0926 12:11:19.060118  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.254071 (* 1 = 0.254071 loss)
I0926 12:11:19.060133  4406 sgd_solver.cpp:105] Iteration 77400, lr = 0.01
I0926 12:11:32.945770  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:11:33.530489  4406 solver.cpp:330] Iteration 77500, Testing net (#0)
I0926 12:11:36.954135  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:11:37.097311  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8308
I0926 12:11:37.097337  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.575426 (* 1 = 0.575426 loss)
I0926 12:11:37.241968  4406 solver.cpp:218] Iteration 77500 (5.5 iter/s, 18.1818s/100 iters), loss = 0.124677
I0926 12:11:37.241997  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124677 (* 1 = 0.124677 loss)
I0926 12:11:37.242004  4406 sgd_solver.cpp:105] Iteration 77500, lr = 0.01
I0926 12:11:51.863127  4406 solver.cpp:218] Iteration 77600 (6.83943 iter/s, 14.6211s/100 iters), loss = 0.126837
I0926 12:11:51.863157  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126837 (* 1 = 0.126837 loss)
I0926 12:11:51.863162  4406 sgd_solver.cpp:105] Iteration 77600, lr = 0.01
I0926 12:12:06.482425  4406 solver.cpp:218] Iteration 77700 (6.8403 iter/s, 14.6192s/100 iters), loss = 0.143356
I0926 12:12:06.482537  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143356 (* 1 = 0.143356 loss)
I0926 12:12:06.482543  4406 sgd_solver.cpp:105] Iteration 77700, lr = 0.01
I0926 12:12:21.103945  4406 solver.cpp:218] Iteration 77800 (6.8393 iter/s, 14.6214s/100 iters), loss = 0.0719794
I0926 12:12:21.103974  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0719793 (* 1 = 0.0719793 loss)
I0926 12:12:21.103981  4406 sgd_solver.cpp:105] Iteration 77800, lr = 0.01
I0926 12:12:35.725215  4406 solver.cpp:218] Iteration 77900 (6.83938 iter/s, 14.6212s/100 iters), loss = 0.07863
I0926 12:12:35.725247  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0786299 (* 1 = 0.0786299 loss)
I0926 12:12:35.725255  4406 sgd_solver.cpp:105] Iteration 77900, lr = 0.01
I0926 12:12:49.616003  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:12:50.200786  4406 solver.cpp:330] Iteration 78000, Testing net (#0)
I0926 12:12:53.626623  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:12:53.769811  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8517
I0926 12:12:53.769846  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.477196 (* 1 = 0.477196 loss)
I0926 12:12:53.914794  4406 solver.cpp:218] Iteration 78000 (5.49768 iter/s, 18.1895s/100 iters), loss = 0.167385
I0926 12:12:53.914822  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167385 (* 1 = 0.167385 loss)
I0926 12:12:53.914829  4406 sgd_solver.cpp:105] Iteration 78000, lr = 0.01
I0926 12:13:08.541100  4406 solver.cpp:218] Iteration 78100 (6.83703 iter/s, 14.6262s/100 iters), loss = 0.157294
I0926 12:13:08.541141  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.157294 (* 1 = 0.157294 loss)
I0926 12:13:08.541146  4406 sgd_solver.cpp:105] Iteration 78100, lr = 0.01
I0926 12:13:23.158288  4406 solver.cpp:218] Iteration 78200 (6.8413 iter/s, 14.6171s/100 iters), loss = 0.131224
I0926 12:13:23.158376  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131224 (* 1 = 0.131224 loss)
I0926 12:13:23.158385  4406 sgd_solver.cpp:105] Iteration 78200, lr = 0.01
I0926 12:13:37.777053  4406 solver.cpp:218] Iteration 78300 (6.84058 iter/s, 14.6186s/100 iters), loss = 0.137908
I0926 12:13:37.777086  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.137908 (* 1 = 0.137908 loss)
I0926 12:13:37.777092  4406 sgd_solver.cpp:105] Iteration 78300, lr = 0.01
I0926 12:13:52.394374  4406 solver.cpp:218] Iteration 78400 (6.84123 iter/s, 14.6173s/100 iters), loss = 0.148913
I0926 12:13:52.394403  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148912 (* 1 = 0.148912 loss)
I0926 12:13:52.394409  4406 sgd_solver.cpp:105] Iteration 78400, lr = 0.01
I0926 12:14:06.285208  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:14:06.869820  4406 solver.cpp:330] Iteration 78500, Testing net (#0)
I0926 12:14:10.294215  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:14:10.436738  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8176
I0926 12:14:10.436774  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.66637 (* 1 = 0.66637 loss)
I0926 12:14:10.581274  4406 solver.cpp:218] Iteration 78500 (5.49849 iter/s, 18.1868s/100 iters), loss = 0.207269
I0926 12:14:10.581303  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207269 (* 1 = 0.207269 loss)
I0926 12:14:10.581310  4406 sgd_solver.cpp:105] Iteration 78500, lr = 0.01
I0926 12:14:25.201930  4406 solver.cpp:218] Iteration 78600 (6.83967 iter/s, 14.6206s/100 iters), loss = 0.140354
I0926 12:14:25.201972  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140354 (* 1 = 0.140354 loss)
I0926 12:14:25.201977  4406 sgd_solver.cpp:105] Iteration 78600, lr = 0.01
I0926 12:14:39.817823  4406 solver.cpp:218] Iteration 78700 (6.8419 iter/s, 14.6158s/100 iters), loss = 0.120304
I0926 12:14:39.817924  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120303 (* 1 = 0.120303 loss)
I0926 12:14:39.817931  4406 sgd_solver.cpp:105] Iteration 78700, lr = 0.01
I0926 12:14:54.435535  4406 solver.cpp:218] Iteration 78800 (6.84108 iter/s, 14.6176s/100 iters), loss = 0.277089
I0926 12:14:54.435575  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277089 (* 1 = 0.277089 loss)
I0926 12:14:54.435581  4406 sgd_solver.cpp:105] Iteration 78800, lr = 0.01
I0926 12:15:09.055873  4406 solver.cpp:218] Iteration 78900 (6.83982 iter/s, 14.6203s/100 iters), loss = 0.158928
I0926 12:15:09.055913  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158928 (* 1 = 0.158928 loss)
I0926 12:15:09.055918  4406 sgd_solver.cpp:105] Iteration 78900, lr = 0.01
I0926 12:15:22.945905  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:15:23.530656  4406 solver.cpp:330] Iteration 79000, Testing net (#0)
I0926 12:15:26.956249  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:15:27.098927  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7979
I0926 12:15:27.098963  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.713272 (* 1 = 0.713272 loss)
I0926 12:15:27.243924  4406 solver.cpp:218] Iteration 79000 (5.49814 iter/s, 18.188s/100 iters), loss = 0.150672
I0926 12:15:27.243952  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.150672 (* 1 = 0.150672 loss)
I0926 12:15:27.243959  4406 sgd_solver.cpp:105] Iteration 79000, lr = 0.01
I0926 12:15:41.870242  4406 solver.cpp:218] Iteration 79100 (6.83702 iter/s, 14.6263s/100 iters), loss = 0.182214
I0926 12:15:41.870272  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.182214 (* 1 = 0.182214 loss)
I0926 12:15:41.870278  4406 sgd_solver.cpp:105] Iteration 79100, lr = 0.01
I0926 12:15:56.488867  4406 solver.cpp:218] Iteration 79200 (6.84062 iter/s, 14.6186s/100 iters), loss = 0.185047
I0926 12:15:56.488968  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.185047 (* 1 = 0.185047 loss)
I0926 12:15:56.488986  4406 sgd_solver.cpp:105] Iteration 79200, lr = 0.01
I0926 12:16:11.114094  4406 solver.cpp:218] Iteration 79300 (6.83757 iter/s, 14.6251s/100 iters), loss = 0.113179
I0926 12:16:11.114133  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113179 (* 1 = 0.113179 loss)
I0926 12:16:11.114140  4406 sgd_solver.cpp:105] Iteration 79300, lr = 0.01
I0926 12:16:25.736351  4406 solver.cpp:218] Iteration 79400 (6.83892 iter/s, 14.6222s/100 iters), loss = 0.09023
I0926 12:16:25.736380  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0902299 (* 1 = 0.0902299 loss)
I0926 12:16:25.736387  4406 sgd_solver.cpp:105] Iteration 79400, lr = 0.01
I0926 12:16:39.634037  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:16:40.219890  4406 solver.cpp:330] Iteration 79500, Testing net (#0)
I0926 12:16:43.645622  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:16:43.788545  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8401
I0926 12:16:43.788570  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.531219 (* 1 = 0.531219 loss)
I0926 12:16:43.932937  4406 solver.cpp:218] Iteration 79500 (5.49556 iter/s, 18.1965s/100 iters), loss = 0.185081
I0926 12:16:43.932966  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18508 (* 1 = 0.18508 loss)
I0926 12:16:43.932972  4406 sgd_solver.cpp:105] Iteration 79500, lr = 0.01
I0926 12:16:58.534437  4406 solver.cpp:218] Iteration 79600 (6.84864 iter/s, 14.6014s/100 iters), loss = 0.2383
I0926 12:16:58.534466  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2383 (* 1 = 0.2383 loss)
I0926 12:16:58.534471  4406 sgd_solver.cpp:105] Iteration 79600, lr = 0.01
I0926 12:17:13.141207  4406 solver.cpp:218] Iteration 79700 (6.84617 iter/s, 14.6067s/100 iters), loss = 0.193149
I0926 12:17:13.141345  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193149 (* 1 = 0.193149 loss)
I0926 12:17:13.141351  4406 sgd_solver.cpp:105] Iteration 79700, lr = 0.01
I0926 12:17:27.742411  4406 solver.cpp:218] Iteration 79800 (6.84883 iter/s, 14.601s/100 iters), loss = 0.103358
I0926 12:17:27.742441  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103358 (* 1 = 0.103358 loss)
I0926 12:17:27.742449  4406 sgd_solver.cpp:105] Iteration 79800, lr = 0.01
I0926 12:17:42.349333  4406 solver.cpp:218] Iteration 79900 (6.8461 iter/s, 14.6069s/100 iters), loss = 0.0856503
I0926 12:17:42.349362  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0856503 (* 1 = 0.0856503 loss)
I0926 12:17:42.349369  4406 sgd_solver.cpp:105] Iteration 79900, lr = 0.01
I0926 12:17:56.226272  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:17:56.811072  4406 solver.cpp:330] Iteration 80000, Testing net (#0)
I0926 12:18:00.236011  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:18:00.378638  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8481
I0926 12:18:00.378664  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.5147 (* 1 = 0.5147 loss)
I0926 12:18:00.523094  4406 solver.cpp:218] Iteration 80000 (5.50246 iter/s, 18.1737s/100 iters), loss = 0.0349381
I0926 12:18:00.523125  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0349382 (* 1 = 0.0349382 loss)
I0926 12:18:00.523130  4406 sgd_solver.cpp:46] MultiStep Status: Iteration 80000, step = 2
I0926 12:18:00.523133  4406 sgd_solver.cpp:105] Iteration 80000, lr = 0.001
I0926 12:18:15.132691  4406 solver.cpp:218] Iteration 80100 (6.84485 iter/s, 14.6095s/100 iters), loss = 0.125407
I0926 12:18:15.132720  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125408 (* 1 = 0.125408 loss)
I0926 12:18:15.132726  4406 sgd_solver.cpp:105] Iteration 80100, lr = 0.001
I0926 12:18:29.742815  4406 solver.cpp:218] Iteration 80200 (6.8446 iter/s, 14.6101s/100 iters), loss = 0.131602
I0926 12:18:29.742914  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131602 (* 1 = 0.131602 loss)
I0926 12:18:29.742931  4406 sgd_solver.cpp:105] Iteration 80200, lr = 0.001
I0926 12:18:44.354693  4406 solver.cpp:218] Iteration 80300 (6.84381 iter/s, 14.6117s/100 iters), loss = 0.0670788
I0926 12:18:44.354722  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0670789 (* 1 = 0.0670789 loss)
I0926 12:18:44.354728  4406 sgd_solver.cpp:105] Iteration 80300, lr = 0.001
I0926 12:18:58.961277  4406 solver.cpp:218] Iteration 80400 (6.84626 iter/s, 14.6065s/100 iters), loss = 0.0801415
I0926 12:18:58.961308  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0801417 (* 1 = 0.0801417 loss)
I0926 12:18:58.961313  4406 sgd_solver.cpp:105] Iteration 80400, lr = 0.001
I0926 12:19:12.847187  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:19:13.431951  4406 solver.cpp:330] Iteration 80500, Testing net (#0)
I0926 12:19:16.856493  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:19:16.999382  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8712
I0926 12:19:16.999418  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.421747 (* 1 = 0.421747 loss)
I0926 12:19:17.144055  4406 solver.cpp:218] Iteration 80500 (5.49973 iter/s, 18.1827s/100 iters), loss = 0.0620702
I0926 12:19:17.144083  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0620703 (* 1 = 0.0620703 loss)
I0926 12:19:17.144088  4406 sgd_solver.cpp:105] Iteration 80500, lr = 0.001
I0926 12:19:31.755035  4406 solver.cpp:218] Iteration 80600 (6.8442 iter/s, 14.6109s/100 iters), loss = 0.0578273
I0926 12:19:31.755074  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0578274 (* 1 = 0.0578274 loss)
I0926 12:19:31.755080  4406 sgd_solver.cpp:105] Iteration 80600, lr = 0.001
I0926 12:19:46.362819  4406 solver.cpp:218] Iteration 80700 (6.8457 iter/s, 14.6077s/100 iters), loss = 0.032375
I0926 12:19:46.362932  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.032375 (* 1 = 0.032375 loss)
I0926 12:19:46.362951  4406 sgd_solver.cpp:105] Iteration 80700, lr = 0.001
I0926 12:20:00.972977  4406 solver.cpp:218] Iteration 80800 (6.84462 iter/s, 14.61s/100 iters), loss = 0.117054
I0926 12:20:00.973006  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117054 (* 1 = 0.117054 loss)
I0926 12:20:00.973012  4406 sgd_solver.cpp:105] Iteration 80800, lr = 0.001
I0926 12:20:15.584159  4406 solver.cpp:218] Iteration 80900 (6.8441 iter/s, 14.6111s/100 iters), loss = 0.0363068
I0926 12:20:15.584199  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0363069 (* 1 = 0.0363069 loss)
I0926 12:20:15.584206  4406 sgd_solver.cpp:105] Iteration 80900, lr = 0.001
I0926 12:20:29.470849  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:20:30.054999  4406 solver.cpp:330] Iteration 81000, Testing net (#0)
I0926 12:20:33.478812  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:20:33.621767  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8762
I0926 12:20:33.621803  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.400197 (* 1 = 0.400197 loss)
I0926 12:20:33.766707  4406 solver.cpp:218] Iteration 81000 (5.4998 iter/s, 18.1825s/100 iters), loss = 0.0874136
I0926 12:20:33.766736  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0874136 (* 1 = 0.0874136 loss)
I0926 12:20:33.766742  4406 sgd_solver.cpp:105] Iteration 81000, lr = 0.001
I0926 12:20:48.378518  4406 solver.cpp:218] Iteration 81100 (6.84381 iter/s, 14.6117s/100 iters), loss = 0.0941415
I0926 12:20:48.378546  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0941415 (* 1 = 0.0941415 loss)
I0926 12:20:48.378552  4406 sgd_solver.cpp:105] Iteration 81100, lr = 0.001
I0926 12:21:02.988533  4406 solver.cpp:218] Iteration 81200 (6.84465 iter/s, 14.6099s/100 iters), loss = 0.0690526
I0926 12:21:02.988672  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0690526 (* 1 = 0.0690526 loss)
I0926 12:21:02.988680  4406 sgd_solver.cpp:105] Iteration 81200, lr = 0.001
I0926 12:21:17.597914  4406 solver.cpp:218] Iteration 81300 (6.84499 iter/s, 14.6092s/100 iters), loss = 0.0691467
I0926 12:21:17.597944  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0691467 (* 1 = 0.0691467 loss)
I0926 12:21:17.597950  4406 sgd_solver.cpp:105] Iteration 81300, lr = 0.001
I0926 12:21:32.213274  4406 solver.cpp:218] Iteration 81400 (6.84215 iter/s, 14.6153s/100 iters), loss = 0.0380494
I0926 12:21:32.213302  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380495 (* 1 = 0.0380495 loss)
I0926 12:21:32.213309  4406 sgd_solver.cpp:105] Iteration 81400, lr = 0.001
I0926 12:21:46.097901  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:21:46.683490  4406 solver.cpp:330] Iteration 81500, Testing net (#0)
I0926 12:21:50.098167  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:21:50.240222  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8819
I0926 12:21:50.240247  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.395859 (* 1 = 0.395859 loss)
I0926 12:21:50.384930  4406 solver.cpp:218] Iteration 81500 (5.5031 iter/s, 18.1716s/100 iters), loss = 0.0503021
I0926 12:21:50.384963  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0503021 (* 1 = 0.0503021 loss)
I0926 12:21:50.384970  4406 sgd_solver.cpp:105] Iteration 81500, lr = 0.001
I0926 12:22:05.014780  4406 solver.cpp:218] Iteration 81600 (6.83537 iter/s, 14.6298s/100 iters), loss = 0.0667946
I0926 12:22:05.014809  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0667947 (* 1 = 0.0667947 loss)
I0926 12:22:05.014817  4406 sgd_solver.cpp:105] Iteration 81600, lr = 0.001
I0926 12:22:19.639425  4406 solver.cpp:218] Iteration 81700 (6.8378 iter/s, 14.6246s/100 iters), loss = 0.0322612
I0926 12:22:19.639520  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0322613 (* 1 = 0.0322613 loss)
I0926 12:22:19.639538  4406 sgd_solver.cpp:105] Iteration 81700, lr = 0.001
I0926 12:22:34.269333  4406 solver.cpp:218] Iteration 81800 (6.83537 iter/s, 14.6298s/100 iters), loss = 0.0490429
I0926 12:22:34.269364  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0490431 (* 1 = 0.0490431 loss)
I0926 12:22:34.269381  4406 sgd_solver.cpp:105] Iteration 81800, lr = 0.001
I0926 12:22:48.895386  4406 solver.cpp:218] Iteration 81900 (6.83715 iter/s, 14.626s/100 iters), loss = 0.0522153
I0926 12:22:48.895417  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0522155 (* 1 = 0.0522155 loss)
I0926 12:22:48.895433  4406 sgd_solver.cpp:105] Iteration 81900, lr = 0.001
I0926 12:23:02.791479  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:23:03.377876  4406 solver.cpp:330] Iteration 82000, Testing net (#0)
I0926 12:23:06.800196  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:23:06.943243  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8812
I0926 12:23:06.943279  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.398406 (* 1 = 0.398406 loss)
I0926 12:23:07.087337  4406 solver.cpp:218] Iteration 82000 (5.49696 iter/s, 18.1919s/100 iters), loss = 0.0684105
I0926 12:23:07.087368  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0684106 (* 1 = 0.0684106 loss)
I0926 12:23:07.087374  4406 sgd_solver.cpp:105] Iteration 82000, lr = 0.001
I0926 12:23:21.691730  4406 solver.cpp:218] Iteration 82100 (6.84729 iter/s, 14.6043s/100 iters), loss = 0.0645611
I0926 12:23:21.691763  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0645612 (* 1 = 0.0645612 loss)
I0926 12:23:21.691769  4406 sgd_solver.cpp:105] Iteration 82100, lr = 0.001
I0926 12:23:36.297575  4406 solver.cpp:218] Iteration 82200 (6.84661 iter/s, 14.6058s/100 iters), loss = 0.047731
I0926 12:23:36.297684  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0477311 (* 1 = 0.0477311 loss)
I0926 12:23:36.297691  4406 sgd_solver.cpp:105] Iteration 82200, lr = 0.001
I0926 12:23:50.903702  4406 solver.cpp:218] Iteration 82300 (6.84651 iter/s, 14.606s/100 iters), loss = 0.0859326
I0926 12:23:50.903730  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0859328 (* 1 = 0.0859328 loss)
I0926 12:23:50.903736  4406 sgd_solver.cpp:105] Iteration 82300, lr = 0.001
I0926 12:24:05.512338  4406 solver.cpp:218] Iteration 82400 (6.8453 iter/s, 14.6086s/100 iters), loss = 0.0608201
I0926 12:24:05.512368  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0608202 (* 1 = 0.0608202 loss)
I0926 12:24:05.512374  4406 sgd_solver.cpp:105] Iteration 82400, lr = 0.001
I0926 12:24:19.391167  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:24:19.975785  4406 solver.cpp:330] Iteration 82500, Testing net (#0)
I0926 12:24:23.400931  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:24:23.543870  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8813
I0926 12:24:23.543907  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.397752 (* 1 = 0.397752 loss)
I0926 12:24:23.688155  4406 solver.cpp:218] Iteration 82500 (5.50184 iter/s, 18.1757s/100 iters), loss = 0.0531678
I0926 12:24:23.688186  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0531679 (* 1 = 0.0531679 loss)
I0926 12:24:23.688192  4406 sgd_solver.cpp:105] Iteration 82500, lr = 0.001
I0926 12:24:38.306809  4406 solver.cpp:218] Iteration 82600 (6.84061 iter/s, 14.6186s/100 iters), loss = 0.0782344
I0926 12:24:38.306839  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0782346 (* 1 = 0.0782346 loss)
I0926 12:24:38.306844  4406 sgd_solver.cpp:105] Iteration 82600, lr = 0.001
I0926 12:24:52.919630  4406 solver.cpp:218] Iteration 82700 (6.84334 iter/s, 14.6128s/100 iters), loss = 0.0539599
I0926 12:24:52.919728  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0539601 (* 1 = 0.0539601 loss)
I0926 12:24:52.919735  4406 sgd_solver.cpp:105] Iteration 82700, lr = 0.001
I0926 12:25:07.539335  4406 solver.cpp:218] Iteration 82800 (6.84015 iter/s, 14.6196s/100 iters), loss = 0.0590604
I0926 12:25:07.539366  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0590605 (* 1 = 0.0590605 loss)
I0926 12:25:07.539371  4406 sgd_solver.cpp:105] Iteration 82800, lr = 0.001
I0926 12:25:22.157666  4406 solver.cpp:218] Iteration 82900 (6.84076 iter/s, 14.6183s/100 iters), loss = 0.0245829
I0926 12:25:22.157707  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.024583 (* 1 = 0.024583 loss)
I0926 12:25:22.157713  4406 sgd_solver.cpp:105] Iteration 82900, lr = 0.001
I0926 12:25:36.050508  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:25:36.636548  4406 solver.cpp:330] Iteration 83000, Testing net (#0)
I0926 12:25:40.062609  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:25:40.205762  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8833
I0926 12:25:40.205797  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.405734 (* 1 = 0.405734 loss)
I0926 12:25:40.350859  4406 solver.cpp:218] Iteration 83000 (5.49659 iter/s, 18.1931s/100 iters), loss = 0.0544906
I0926 12:25:40.350888  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0544908 (* 1 = 0.0544908 loss)
I0926 12:25:40.350894  4406 sgd_solver.cpp:105] Iteration 83000, lr = 0.001
I0926 12:25:54.964972  4406 solver.cpp:218] Iteration 83100 (6.84273 iter/s, 14.614s/100 iters), loss = 0.142254
I0926 12:25:54.965000  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.142254 (* 1 = 0.142254 loss)
I0926 12:25:54.965006  4406 sgd_solver.cpp:105] Iteration 83100, lr = 0.001
I0926 12:26:09.582736  4406 solver.cpp:218] Iteration 83200 (6.84102 iter/s, 14.6177s/100 iters), loss = 0.0394384
I0926 12:26:09.582845  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0394385 (* 1 = 0.0394385 loss)
I0926 12:26:09.582852  4406 sgd_solver.cpp:105] Iteration 83200, lr = 0.001
I0926 12:26:24.202528  4406 solver.cpp:218] Iteration 83300 (6.84011 iter/s, 14.6197s/100 iters), loss = 0.0499592
I0926 12:26:24.202569  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0499593 (* 1 = 0.0499593 loss)
I0926 12:26:24.202574  4406 sgd_solver.cpp:105] Iteration 83300, lr = 0.001
I0926 12:26:38.820339  4406 solver.cpp:218] Iteration 83400 (6.84101 iter/s, 14.6177s/100 iters), loss = 0.040314
I0926 12:26:38.820380  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0403142 (* 1 = 0.0403142 loss)
I0926 12:26:38.820386  4406 sgd_solver.cpp:105] Iteration 83400, lr = 0.001
I0926 12:26:52.710592  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:26:53.295861  4406 solver.cpp:330] Iteration 83500, Testing net (#0)
I0926 12:26:56.719218  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:26:56.861986  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8832
I0926 12:26:56.862022  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.408045 (* 1 = 0.408045 loss)
I0926 12:26:57.007385  4406 solver.cpp:218] Iteration 83500 (5.49845 iter/s, 18.187s/100 iters), loss = 0.0637693
I0926 12:26:57.007413  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0637694 (* 1 = 0.0637694 loss)
I0926 12:26:57.007421  4406 sgd_solver.cpp:105] Iteration 83500, lr = 0.001
I0926 12:27:11.622433  4406 solver.cpp:218] Iteration 83600 (6.84229 iter/s, 14.615s/100 iters), loss = 0.0701602
I0926 12:27:11.622463  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0701603 (* 1 = 0.0701603 loss)
I0926 12:27:11.622468  4406 sgd_solver.cpp:105] Iteration 83600, lr = 0.001
I0926 12:27:26.238904  4406 solver.cpp:218] Iteration 83700 (6.84163 iter/s, 14.6164s/100 iters), loss = 0.0313202
I0926 12:27:26.238983  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313203 (* 1 = 0.0313203 loss)
I0926 12:27:26.238991  4406 sgd_solver.cpp:105] Iteration 83700, lr = 0.001
I0926 12:27:40.853415  4406 solver.cpp:218] Iteration 83800 (6.84257 iter/s, 14.6144s/100 iters), loss = 0.102012
I0926 12:27:40.853453  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.102012 (* 1 = 0.102012 loss)
I0926 12:27:40.853459  4406 sgd_solver.cpp:105] Iteration 83800, lr = 0.001
I0926 12:27:55.468842  4406 solver.cpp:218] Iteration 83900 (6.84212 iter/s, 14.6154s/100 iters), loss = 0.0513729
I0926 12:27:55.468873  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0513731 (* 1 = 0.0513731 loss)
I0926 12:27:55.468878  4406 sgd_solver.cpp:105] Iteration 83900, lr = 0.001
I0926 12:28:09.365562  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:28:09.951309  4406 solver.cpp:330] Iteration 84000, Testing net (#0)
I0926 12:28:13.377315  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:28:13.520501  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8843
I0926 12:28:13.520527  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.400513 (* 1 = 0.400513 loss)
I0926 12:28:13.665153  4406 solver.cpp:218] Iteration 84000 (5.49564 iter/s, 18.1962s/100 iters), loss = 0.0512882
I0926 12:28:13.665182  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0512882 (* 1 = 0.0512882 loss)
I0926 12:28:13.665189  4406 sgd_solver.cpp:105] Iteration 84000, lr = 0.001
I0926 12:28:28.290349  4406 solver.cpp:218] Iteration 84100 (6.83755 iter/s, 14.6251s/100 iters), loss = 0.0232617
I0926 12:28:28.290388  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0232618 (* 1 = 0.0232618 loss)
I0926 12:28:28.290395  4406 sgd_solver.cpp:105] Iteration 84100, lr = 0.001
I0926 12:28:42.912220  4406 solver.cpp:218] Iteration 84200 (6.83911 iter/s, 14.6218s/100 iters), loss = 0.0289216
I0926 12:28:42.912293  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0289217 (* 1 = 0.0289217 loss)
I0926 12:28:42.912300  4406 sgd_solver.cpp:105] Iteration 84200, lr = 0.001
I0926 12:28:57.527593  4406 solver.cpp:218] Iteration 84300 (6.84216 iter/s, 14.6153s/100 iters), loss = 0.104014
I0926 12:28:57.527632  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.104014 (* 1 = 0.104014 loss)
I0926 12:28:57.527638  4406 sgd_solver.cpp:105] Iteration 84300, lr = 0.001
I0926 12:29:12.151036  4406 solver.cpp:218] Iteration 84400 (6.83837 iter/s, 14.6234s/100 iters), loss = 0.0321532
I0926 12:29:12.151065  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0321533 (* 1 = 0.0321533 loss)
I0926 12:29:12.151072  4406 sgd_solver.cpp:105] Iteration 84400, lr = 0.001
I0926 12:29:26.048965  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:29:26.633674  4406 solver.cpp:330] Iteration 84500, Testing net (#0)
I0926 12:29:30.056962  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:29:30.199846  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8818
I0926 12:29:30.199882  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.413612 (* 1 = 0.413612 loss)
I0926 12:29:30.343796  4406 solver.cpp:218] Iteration 84500 (5.49671 iter/s, 18.1927s/100 iters), loss = 0.0543079
I0926 12:29:30.343823  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.054308 (* 1 = 0.054308 loss)
I0926 12:29:30.343830  4406 sgd_solver.cpp:105] Iteration 84500, lr = 0.001
I0926 12:29:44.946873  4406 solver.cpp:218] Iteration 84600 (6.8479 iter/s, 14.603s/100 iters), loss = 0.143659
I0926 12:29:44.946913  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143659 (* 1 = 0.143659 loss)
I0926 12:29:44.946918  4406 sgd_solver.cpp:105] Iteration 84600, lr = 0.001
I0926 12:29:59.550318  4406 solver.cpp:218] Iteration 84700 (6.84774 iter/s, 14.6034s/100 iters), loss = 0.0611828
I0926 12:29:59.550406  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0611829 (* 1 = 0.0611829 loss)
I0926 12:29:59.550422  4406 sgd_solver.cpp:105] Iteration 84700, lr = 0.001
I0926 12:30:14.159123  4406 solver.cpp:218] Iteration 84800 (6.84524 iter/s, 14.6087s/100 iters), loss = 0.0785964
I0926 12:30:14.159162  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0785965 (* 1 = 0.0785965 loss)
I0926 12:30:14.159168  4406 sgd_solver.cpp:105] Iteration 84800, lr = 0.001
I0926 12:30:28.827769  4406 solver.cpp:218] Iteration 84900 (6.8173 iter/s, 14.6686s/100 iters), loss = 0.0697865
I0926 12:30:28.827797  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0697866 (* 1 = 0.0697866 loss)
I0926 12:30:28.827803  4406 sgd_solver.cpp:105] Iteration 84900, lr = 0.001
I0926 12:30:42.723770  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:30:43.308574  4406 solver.cpp:330] Iteration 85000, Testing net (#0)
I0926 12:30:46.735638  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:30:46.878968  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8827
I0926 12:30:46.878993  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.41703 (* 1 = 0.41703 loss)
I0926 12:30:47.023913  4406 solver.cpp:218] Iteration 85000 (5.49569 iter/s, 18.1961s/100 iters), loss = 0.0301503
I0926 12:30:47.023941  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0301504 (* 1 = 0.0301504 loss)
I0926 12:30:47.023948  4406 sgd_solver.cpp:105] Iteration 85000, lr = 0.001
I0926 12:31:01.648422  4406 solver.cpp:218] Iteration 85100 (6.83787 iter/s, 14.6244s/100 iters), loss = 0.0795897
I0926 12:31:01.648454  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0795898 (* 1 = 0.0795898 loss)
I0926 12:31:01.648460  4406 sgd_solver.cpp:105] Iteration 85100, lr = 0.001
I0926 12:31:16.270630  4406 solver.cpp:218] Iteration 85200 (6.83895 iter/s, 14.6221s/100 iters), loss = 0.0697296
I0926 12:31:16.270735  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0697297 (* 1 = 0.0697297 loss)
I0926 12:31:16.270741  4406 sgd_solver.cpp:105] Iteration 85200, lr = 0.001
I0926 12:31:30.892386  4406 solver.cpp:218] Iteration 85300 (6.83919 iter/s, 14.6216s/100 iters), loss = 0.0844725
I0926 12:31:30.892416  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0844726 (* 1 = 0.0844726 loss)
I0926 12:31:30.892422  4406 sgd_solver.cpp:105] Iteration 85300, lr = 0.001
I0926 12:31:45.516361  4406 solver.cpp:218] Iteration 85400 (6.83812 iter/s, 14.6239s/100 iters), loss = 0.047272
I0926 12:31:45.516392  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0472722 (* 1 = 0.0472722 loss)
I0926 12:31:45.516398  4406 sgd_solver.cpp:105] Iteration 85400, lr = 0.001
I0926 12:31:59.414285  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:31:59.998553  4406 solver.cpp:330] Iteration 85500, Testing net (#0)
I0926 12:32:03.429797  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:32:03.572585  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8813
I0926 12:32:03.572620  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.42032 (* 1 = 0.42032 loss)
I0926 12:32:03.717715  4406 solver.cpp:218] Iteration 85500 (5.49412 iter/s, 18.2013s/100 iters), loss = 0.0511333
I0926 12:32:03.717746  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0511335 (* 1 = 0.0511335 loss)
I0926 12:32:03.717752  4406 sgd_solver.cpp:105] Iteration 85500, lr = 0.001
I0926 12:32:18.328742  4406 solver.cpp:218] Iteration 85600 (6.84418 iter/s, 14.611s/100 iters), loss = 0.0487693
I0926 12:32:18.328783  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0487694 (* 1 = 0.0487694 loss)
I0926 12:32:18.328789  4406 sgd_solver.cpp:105] Iteration 85600, lr = 0.001
I0926 12:32:32.948019  4406 solver.cpp:218] Iteration 85700 (6.84032 iter/s, 14.6192s/100 iters), loss = 0.041799
I0926 12:32:32.948175  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0417991 (* 1 = 0.0417991 loss)
I0926 12:32:32.948184  4406 sgd_solver.cpp:105] Iteration 85700, lr = 0.001
I0926 12:32:47.568692  4406 solver.cpp:218] Iteration 85800 (6.83972 iter/s, 14.6205s/100 iters), loss = 0.0648459
I0926 12:32:47.568723  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.064846 (* 1 = 0.064846 loss)
I0926 12:32:47.568730  4406 sgd_solver.cpp:105] Iteration 85800, lr = 0.001
I0926 12:33:02.191620  4406 solver.cpp:218] Iteration 85900 (6.83861 iter/s, 14.6229s/100 iters), loss = 0.0225074
I0926 12:33:02.191649  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0225075 (* 1 = 0.0225075 loss)
I0926 12:33:02.191655  4406 sgd_solver.cpp:105] Iteration 85900, lr = 0.001
I0926 12:33:16.082556  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:33:16.667637  4406 solver.cpp:330] Iteration 86000, Testing net (#0)
I0926 12:33:20.095068  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:33:20.238123  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8852
I0926 12:33:20.238160  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.419138 (* 1 = 0.419138 loss)
I0926 12:33:20.382601  4406 solver.cpp:218] Iteration 86000 (5.49725 iter/s, 18.1909s/100 iters), loss = 0.0652404
I0926 12:33:20.382628  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0652405 (* 1 = 0.0652405 loss)
I0926 12:33:20.382635  4406 sgd_solver.cpp:105] Iteration 86000, lr = 0.001
I0926 12:33:34.999544  4406 solver.cpp:218] Iteration 86100 (6.84141 iter/s, 14.6169s/100 iters), loss = 0.0564582
I0926 12:33:34.999573  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0564583 (* 1 = 0.0564583 loss)
I0926 12:33:34.999579  4406 sgd_solver.cpp:105] Iteration 86100, lr = 0.001
I0926 12:33:49.627451  4406 solver.cpp:218] Iteration 86200 (6.83628 iter/s, 14.6278s/100 iters), loss = 0.0404159
I0926 12:33:49.627547  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.040416 (* 1 = 0.040416 loss)
I0926 12:33:49.627564  4406 sgd_solver.cpp:105] Iteration 86200, lr = 0.001
I0926 12:34:04.254796  4406 solver.cpp:218] Iteration 86300 (6.83657 iter/s, 14.6272s/100 iters), loss = 0.066296
I0926 12:34:04.254835  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0662961 (* 1 = 0.0662961 loss)
I0926 12:34:04.254842  4406 sgd_solver.cpp:105] Iteration 86300, lr = 0.001
I0926 12:34:18.885813  4406 solver.cpp:218] Iteration 86400 (6.83483 iter/s, 14.6309s/100 iters), loss = 0.0346064
I0926 12:34:18.885854  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0346066 (* 1 = 0.0346066 loss)
I0926 12:34:18.885860  4406 sgd_solver.cpp:105] Iteration 86400, lr = 0.001
I0926 12:34:32.788638  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:34:33.374332  4406 solver.cpp:330] Iteration 86500, Testing net (#0)
I0926 12:34:36.799525  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:34:36.942364  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8837
I0926 12:34:36.942389  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.425387 (* 1 = 0.425387 loss)
I0926 12:34:37.087352  4406 solver.cpp:218] Iteration 86500 (5.49407 iter/s, 18.2015s/100 iters), loss = 0.0285928
I0926 12:34:37.087380  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.028593 (* 1 = 0.028593 loss)
I0926 12:34:37.087388  4406 sgd_solver.cpp:105] Iteration 86500, lr = 0.001
I0926 12:34:51.721356  4406 solver.cpp:218] Iteration 86600 (6.83343 iter/s, 14.6339s/100 iters), loss = 0.107302
I0926 12:34:51.721387  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107302 (* 1 = 0.107302 loss)
I0926 12:34:51.721393  4406 sgd_solver.cpp:105] Iteration 86600, lr = 0.001
I0926 12:35:06.387303  4406 solver.cpp:218] Iteration 86700 (6.81855 iter/s, 14.6659s/100 iters), loss = 0.0529792
I0926 12:35:06.387480  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0529793 (* 1 = 0.0529793 loss)
I0926 12:35:06.387490  4406 sgd_solver.cpp:105] Iteration 86700, lr = 0.001
I0926 12:35:21.100517  4406 solver.cpp:218] Iteration 86800 (6.79671 iter/s, 14.713s/100 iters), loss = 0.080136
I0926 12:35:21.100564  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0801361 (* 1 = 0.0801361 loss)
I0926 12:35:21.100570  4406 sgd_solver.cpp:105] Iteration 86800, lr = 0.001
I0926 12:35:35.727874  4406 solver.cpp:218] Iteration 86900 (6.83654 iter/s, 14.6273s/100 iters), loss = 0.0920716
I0926 12:35:35.727915  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0920717 (* 1 = 0.0920717 loss)
I0926 12:35:35.727921  4406 sgd_solver.cpp:105] Iteration 86900, lr = 0.001
I0926 12:35:49.632822  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:35:50.219020  4406 solver.cpp:330] Iteration 87000, Testing net (#0)
I0926 12:35:53.647841  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:35:53.790675  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8867
I0926 12:35:53.790700  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.422424 (* 1 = 0.422424 loss)
I0926 12:35:53.935508  4406 solver.cpp:218] Iteration 87000 (5.49223 iter/s, 18.2075s/100 iters), loss = 0.0300936
I0926 12:35:53.935535  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0300937 (* 1 = 0.0300937 loss)
I0926 12:35:53.935542  4406 sgd_solver.cpp:105] Iteration 87000, lr = 0.001
I0926 12:36:08.557924  4406 solver.cpp:218] Iteration 87100 (6.83885 iter/s, 14.6224s/100 iters), loss = 0.100905
I0926 12:36:08.557966  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100905 (* 1 = 0.100905 loss)
I0926 12:36:08.557972  4406 sgd_solver.cpp:105] Iteration 87100, lr = 0.001
I0926 12:36:23.177712  4406 solver.cpp:218] Iteration 87200 (6.84008 iter/s, 14.6197s/100 iters), loss = 0.0489421
I0926 12:36:23.177821  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0489422 (* 1 = 0.0489422 loss)
I0926 12:36:23.177837  4406 sgd_solver.cpp:105] Iteration 87200, lr = 0.001
I0926 12:36:37.801513  4406 solver.cpp:218] Iteration 87300 (6.83823 iter/s, 14.6237s/100 iters), loss = 0.0295408
I0926 12:36:37.801544  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0295409 (* 1 = 0.0295409 loss)
I0926 12:36:37.801550  4406 sgd_solver.cpp:105] Iteration 87300, lr = 0.001
I0926 12:36:52.420677  4406 solver.cpp:218] Iteration 87400 (6.84037 iter/s, 14.6191s/100 iters), loss = 0.0316359
I0926 12:36:52.420708  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.031636 (* 1 = 0.031636 loss)
I0926 12:36:52.420714  4406 sgd_solver.cpp:105] Iteration 87400, lr = 0.001
I0926 12:37:06.314390  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:37:06.898764  4406 solver.cpp:330] Iteration 87500, Testing net (#0)
I0926 12:37:10.325830  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:37:10.469239  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8827
I0926 12:37:10.469264  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.441745 (* 1 = 0.441745 loss)
I0926 12:37:10.613248  4406 solver.cpp:218] Iteration 87500 (5.49677 iter/s, 18.1925s/100 iters), loss = 0.0523378
I0926 12:37:10.613292  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0523379 (* 1 = 0.0523379 loss)
I0926 12:37:10.613298  4406 sgd_solver.cpp:105] Iteration 87500, lr = 0.001
I0926 12:37:25.242812  4406 solver.cpp:218] Iteration 87600 (6.83551 iter/s, 14.6295s/100 iters), loss = 0.0793623
I0926 12:37:25.242852  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0793624 (* 1 = 0.0793624 loss)
I0926 12:37:25.242857  4406 sgd_solver.cpp:105] Iteration 87600, lr = 0.001
I0926 12:37:39.875725  4406 solver.cpp:218] Iteration 87700 (6.83395 iter/s, 14.6328s/100 iters), loss = 0.0584395
I0926 12:37:39.875833  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0584395 (* 1 = 0.0584395 loss)
I0926 12:37:39.875850  4406 sgd_solver.cpp:105] Iteration 87700, lr = 0.001
I0926 12:37:54.500509  4406 solver.cpp:218] Iteration 87800 (6.83778 iter/s, 14.6246s/100 iters), loss = 0.027044
I0926 12:37:54.500548  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0270441 (* 1 = 0.0270441 loss)
I0926 12:37:54.500555  4406 sgd_solver.cpp:105] Iteration 87800, lr = 0.001
I0926 12:38:09.136234  4406 solver.cpp:218] Iteration 87900 (6.83263 iter/s, 14.6356s/100 iters), loss = 0.0931548
I0926 12:38:09.136273  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0931549 (* 1 = 0.0931549 loss)
I0926 12:38:09.136278  4406 sgd_solver.cpp:105] Iteration 87900, lr = 0.001
I0926 12:38:23.043709  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:38:23.628674  4406 solver.cpp:330] Iteration 88000, Testing net (#0)
I0926 12:38:27.057452  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:38:27.200572  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8839
I0926 12:38:27.200606  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.431449 (* 1 = 0.431449 loss)
I0926 12:38:27.345969  4406 solver.cpp:218] Iteration 88000 (5.49159 iter/s, 18.2097s/100 iters), loss = 0.106969
I0926 12:38:27.346004  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106969 (* 1 = 0.106969 loss)
I0926 12:38:27.346010  4406 sgd_solver.cpp:105] Iteration 88000, lr = 0.001
I0926 12:38:41.973314  4406 solver.cpp:218] Iteration 88100 (6.83654 iter/s, 14.6273s/100 iters), loss = 0.0368764
I0926 12:38:41.973353  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0368764 (* 1 = 0.0368764 loss)
I0926 12:38:41.973359  4406 sgd_solver.cpp:105] Iteration 88100, lr = 0.001
I0926 12:38:56.602108  4406 solver.cpp:218] Iteration 88200 (6.83587 iter/s, 14.6287s/100 iters), loss = 0.0374382
I0926 12:38:56.602203  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0374383 (* 1 = 0.0374383 loss)
I0926 12:38:56.602211  4406 sgd_solver.cpp:105] Iteration 88200, lr = 0.001
I0926 12:39:11.233163  4406 solver.cpp:218] Iteration 88300 (6.83484 iter/s, 14.6309s/100 iters), loss = 0.040467
I0926 12:39:11.233203  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.040467 (* 1 = 0.040467 loss)
I0926 12:39:11.233209  4406 sgd_solver.cpp:105] Iteration 88300, lr = 0.001
I0926 12:39:25.866065  4406 solver.cpp:218] Iteration 88400 (6.83395 iter/s, 14.6328s/100 iters), loss = 0.00834946
I0926 12:39:25.866104  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00834954 (* 1 = 0.00834954 loss)
I0926 12:39:25.866111  4406 sgd_solver.cpp:105] Iteration 88400, lr = 0.001
I0926 12:39:39.768592  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:39:40.353989  4406 solver.cpp:330] Iteration 88500, Testing net (#0)
I0926 12:39:43.785214  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:39:43.928059  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8835
I0926 12:39:43.928094  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.430653 (* 1 = 0.430653 loss)
I0926 12:39:44.073307  4406 solver.cpp:218] Iteration 88500 (5.49234 iter/s, 18.2072s/100 iters), loss = 0.0705969
I0926 12:39:44.073335  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.070597 (* 1 = 0.070597 loss)
I0926 12:39:44.073341  4406 sgd_solver.cpp:105] Iteration 88500, lr = 0.001
I0926 12:39:58.703563  4406 solver.cpp:218] Iteration 88600 (6.83518 iter/s, 14.6302s/100 iters), loss = 0.113961
I0926 12:39:58.703595  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113961 (* 1 = 0.113961 loss)
I0926 12:39:58.703601  4406 sgd_solver.cpp:105] Iteration 88600, lr = 0.001
I0926 12:40:13.339426  4406 solver.cpp:218] Iteration 88700 (6.83256 iter/s, 14.6358s/100 iters), loss = 0.0542424
I0926 12:40:13.339543  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0542425 (* 1 = 0.0542425 loss)
I0926 12:40:13.339551  4406 sgd_solver.cpp:105] Iteration 88700, lr = 0.001
I0926 12:40:27.965751  4406 solver.cpp:218] Iteration 88800 (6.83706 iter/s, 14.6262s/100 iters), loss = 0.0156925
I0926 12:40:27.965791  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0156926 (* 1 = 0.0156926 loss)
I0926 12:40:27.965796  4406 sgd_solver.cpp:105] Iteration 88800, lr = 0.001
I0926 12:40:42.592741  4406 solver.cpp:218] Iteration 88900 (6.83671 iter/s, 14.6269s/100 iters), loss = 0.0167917
I0926 12:40:42.592779  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0167918 (* 1 = 0.0167918 loss)
I0926 12:40:42.592785  4406 sgd_solver.cpp:105] Iteration 88900, lr = 0.001
I0926 12:40:56.488364  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:40:57.074002  4406 solver.cpp:330] Iteration 89000, Testing net (#0)
I0926 12:41:00.503567  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:41:00.646555  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8853
I0926 12:41:00.646591  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.432445 (* 1 = 0.432445 loss)
I0926 12:41:00.791615  4406 solver.cpp:218] Iteration 89000 (5.49487 iter/s, 18.1988s/100 iters), loss = 0.0175744
I0926 12:41:00.791649  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175744 (* 1 = 0.0175744 loss)
I0926 12:41:00.791656  4406 sgd_solver.cpp:105] Iteration 89000, lr = 0.001
I0926 12:41:15.417778  4406 solver.cpp:218] Iteration 89100 (6.8371 iter/s, 14.6261s/100 iters), loss = 0.0708965
I0926 12:41:15.417807  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0708965 (* 1 = 0.0708965 loss)
I0926 12:41:15.417814  4406 sgd_solver.cpp:105] Iteration 89100, lr = 0.001
I0926 12:41:30.051651  4406 solver.cpp:218] Iteration 89200 (6.83349 iter/s, 14.6338s/100 iters), loss = 0.0188513
I0926 12:41:30.051789  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0188514 (* 1 = 0.0188514 loss)
I0926 12:41:30.051795  4406 sgd_solver.cpp:105] Iteration 89200, lr = 0.001
I0926 12:41:44.683099  4406 solver.cpp:218] Iteration 89300 (6.83467 iter/s, 14.6313s/100 iters), loss = 0.0327261
I0926 12:41:44.683130  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0327262 (* 1 = 0.0327262 loss)
I0926 12:41:44.683135  4406 sgd_solver.cpp:105] Iteration 89300, lr = 0.001
I0926 12:41:59.312896  4406 solver.cpp:218] Iteration 89400 (6.8354 iter/s, 14.6297s/100 iters), loss = 0.0184467
I0926 12:41:59.312935  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0184468 (* 1 = 0.0184468 loss)
I0926 12:41:59.312942  4406 sgd_solver.cpp:105] Iteration 89400, lr = 0.001
I0926 12:42:13.214960  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:42:13.800797  4406 solver.cpp:330] Iteration 89500, Testing net (#0)
I0926 12:42:17.227514  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:42:17.370806  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8849
I0926 12:42:17.370832  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.435483 (* 1 = 0.435483 loss)
I0926 12:42:17.516223  4406 solver.cpp:218] Iteration 89500 (5.49353 iter/s, 18.2032s/100 iters), loss = 0.0570011
I0926 12:42:17.516254  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0570012 (* 1 = 0.0570012 loss)
I0926 12:42:17.516261  4406 sgd_solver.cpp:105] Iteration 89500, lr = 0.001
I0926 12:42:32.128862  4406 solver.cpp:218] Iteration 89600 (6.84342 iter/s, 14.6126s/100 iters), loss = 0.0451185
I0926 12:42:32.128901  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0451186 (* 1 = 0.0451186 loss)
I0926 12:42:32.128907  4406 sgd_solver.cpp:105] Iteration 89600, lr = 0.001
I0926 12:42:46.746676  4406 solver.cpp:218] Iteration 89700 (6.841 iter/s, 14.6177s/100 iters), loss = 0.0220558
I0926 12:42:46.746793  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0220559 (* 1 = 0.0220559 loss)
I0926 12:42:46.746801  4406 sgd_solver.cpp:105] Iteration 89700, lr = 0.001
I0926 12:43:01.365468  4406 solver.cpp:218] Iteration 89800 (6.84058 iter/s, 14.6186s/100 iters), loss = 0.0474732
I0926 12:43:01.365497  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0474733 (* 1 = 0.0474733 loss)
I0926 12:43:01.365502  4406 sgd_solver.cpp:105] Iteration 89800, lr = 0.001
I0926 12:43:15.988895  4406 solver.cpp:218] Iteration 89900 (6.83837 iter/s, 14.6234s/100 iters), loss = 0.0275275
I0926 12:43:15.988924  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275276 (* 1 = 0.0275276 loss)
I0926 12:43:15.988930  4406 sgd_solver.cpp:105] Iteration 89900, lr = 0.001
I0926 12:43:29.880537  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:43:30.464715  4406 solver.cpp:330] Iteration 90000, Testing net (#0)
I0926 12:43:33.892112  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:43:34.035173  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8835
I0926 12:43:34.035209  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.44364 (* 1 = 0.44364 loss)
I0926 12:43:34.180318  4406 solver.cpp:218] Iteration 90000 (5.49712 iter/s, 18.1913s/100 iters), loss = 0.0617621
I0926 12:43:34.180347  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0617622 (* 1 = 0.0617622 loss)
I0926 12:43:34.180353  4406 sgd_solver.cpp:105] Iteration 90000, lr = 0.001
I0926 12:43:48.799067  4406 solver.cpp:218] Iteration 90100 (6.84056 iter/s, 14.6187s/100 iters), loss = 0.034639
I0926 12:43:48.799098  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0346391 (* 1 = 0.0346391 loss)
I0926 12:43:48.799103  4406 sgd_solver.cpp:105] Iteration 90100, lr = 0.001
I0926 12:44:03.428488  4406 solver.cpp:218] Iteration 90200 (6.83557 iter/s, 14.6294s/100 iters), loss = 0.0449308
I0926 12:44:03.428575  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0449309 (* 1 = 0.0449309 loss)
I0926 12:44:03.428591  4406 sgd_solver.cpp:105] Iteration 90200, lr = 0.001
I0926 12:44:18.053781  4406 solver.cpp:218] Iteration 90300 (6.83753 iter/s, 14.6252s/100 iters), loss = 0.0642176
I0926 12:44:18.053819  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0642177 (* 1 = 0.0642177 loss)
I0926 12:44:18.053825  4406 sgd_solver.cpp:105] Iteration 90300, lr = 0.001
I0926 12:44:32.689414  4406 solver.cpp:218] Iteration 90400 (6.83268 iter/s, 14.6356s/100 iters), loss = 0.0543935
I0926 12:44:32.689445  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0543936 (* 1 = 0.0543936 loss)
I0926 12:44:32.689451  4406 sgd_solver.cpp:105] Iteration 90400, lr = 0.001
I0926 12:44:46.589119  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:44:47.173629  4406 solver.cpp:330] Iteration 90500, Testing net (#0)
I0926 12:44:50.602080  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:44:50.744868  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8848
I0926 12:44:50.744894  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.444748 (* 1 = 0.444748 loss)
I0926 12:44:50.890422  4406 solver.cpp:218] Iteration 90500 (5.49422 iter/s, 18.2009s/100 iters), loss = 0.0254822
I0926 12:44:50.890453  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0254823 (* 1 = 0.0254823 loss)
I0926 12:44:50.890460  4406 sgd_solver.cpp:105] Iteration 90500, lr = 0.001
I0926 12:45:05.510946  4406 solver.cpp:218] Iteration 90600 (6.83973 iter/s, 14.6205s/100 iters), loss = 0.062657
I0926 12:45:05.510975  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0626571 (* 1 = 0.0626571 loss)
I0926 12:45:05.510980  4406 sgd_solver.cpp:105] Iteration 90600, lr = 0.001
I0926 12:45:20.124553  4406 solver.cpp:218] Iteration 90700 (6.84297 iter/s, 14.6135s/100 iters), loss = 0.0143047
I0926 12:45:20.124687  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0143048 (* 1 = 0.0143048 loss)
I0926 12:45:20.124694  4406 sgd_solver.cpp:105] Iteration 90700, lr = 0.001
I0926 12:45:34.743340  4406 solver.cpp:218] Iteration 90800 (6.84059 iter/s, 14.6186s/100 iters), loss = 0.104232
I0926 12:45:34.743381  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.104232 (* 1 = 0.104232 loss)
I0926 12:45:34.743386  4406 sgd_solver.cpp:105] Iteration 90800, lr = 0.001
I0926 12:45:49.373193  4406 solver.cpp:218] Iteration 90900 (6.83537 iter/s, 14.6298s/100 iters), loss = 0.0493335
I0926 12:45:49.373234  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0493336 (* 1 = 0.0493336 loss)
I0926 12:45:49.373239  4406 sgd_solver.cpp:105] Iteration 90900, lr = 0.001
I0926 12:46:03.273151  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:46:03.859843  4406 solver.cpp:330] Iteration 91000, Testing net (#0)
I0926 12:46:07.285905  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:46:07.428714  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8838
I0926 12:46:07.428741  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.447442 (* 1 = 0.447442 loss)
I0926 12:46:07.573873  4406 solver.cpp:218] Iteration 91000 (5.49433 iter/s, 18.2006s/100 iters), loss = 0.0291705
I0926 12:46:07.573906  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0291706 (* 1 = 0.0291706 loss)
I0926 12:46:07.573915  4406 sgd_solver.cpp:105] Iteration 91000, lr = 0.001
I0926 12:46:22.275753  4406 solver.cpp:218] Iteration 91100 (6.80188 iter/s, 14.7018s/100 iters), loss = 0.0400098
I0926 12:46:22.275782  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0400099 (* 1 = 0.0400099 loss)
I0926 12:46:22.275789  4406 sgd_solver.cpp:105] Iteration 91100, lr = 0.001
I0926 12:46:36.976126  4406 solver.cpp:218] Iteration 91200 (6.80258 iter/s, 14.7003s/100 iters), loss = 0.0264406
I0926 12:46:36.976274  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0264407 (* 1 = 0.0264407 loss)
I0926 12:46:36.976284  4406 sgd_solver.cpp:105] Iteration 91200, lr = 0.001
I0926 12:46:51.684129  4406 solver.cpp:218] Iteration 91300 (6.7991 iter/s, 14.7078s/100 iters), loss = 0.0632985
I0926 12:46:51.684161  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0632986 (* 1 = 0.0632986 loss)
I0926 12:46:51.684167  4406 sgd_solver.cpp:105] Iteration 91300, lr = 0.001
I0926 12:47:06.373158  4406 solver.cpp:218] Iteration 91400 (6.80783 iter/s, 14.689s/100 iters), loss = 0.0241178
I0926 12:47:06.373188  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0241179 (* 1 = 0.0241179 loss)
I0926 12:47:06.373204  4406 sgd_solver.cpp:105] Iteration 91400, lr = 0.001
I0926 12:47:20.270486  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:47:20.856194  4406 solver.cpp:330] Iteration 91500, Testing net (#0)
I0926 12:47:24.284976  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:47:24.427191  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8843
I0926 12:47:24.427227  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.447405 (* 1 = 0.447405 loss)
I0926 12:47:24.572028  4406 solver.cpp:218] Iteration 91500 (5.49487 iter/s, 18.1988s/100 iters), loss = 0.0285875
I0926 12:47:24.572057  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0285875 (* 1 = 0.0285875 loss)
I0926 12:47:24.572063  4406 sgd_solver.cpp:105] Iteration 91500, lr = 0.001
I0926 12:47:39.195297  4406 solver.cpp:218] Iteration 91600 (6.83845 iter/s, 14.6232s/100 iters), loss = 0.0471875
I0926 12:47:39.195335  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0471875 (* 1 = 0.0471875 loss)
I0926 12:47:39.195341  4406 sgd_solver.cpp:105] Iteration 91600, lr = 0.001
I0926 12:47:53.830632  4406 solver.cpp:218] Iteration 91700 (6.83281 iter/s, 14.6353s/100 iters), loss = 0.0406365
I0926 12:47:53.830807  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0406366 (* 1 = 0.0406366 loss)
I0926 12:47:53.830816  4406 sgd_solver.cpp:105] Iteration 91700, lr = 0.001
I0926 12:48:08.498404  4406 solver.cpp:218] Iteration 91800 (6.81776 iter/s, 14.6676s/100 iters), loss = 0.0650265
I0926 12:48:08.498445  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0650266 (* 1 = 0.0650266 loss)
I0926 12:48:08.498450  4406 sgd_solver.cpp:105] Iteration 91800, lr = 0.001
I0926 12:48:23.138941  4406 solver.cpp:218] Iteration 91900 (6.83039 iter/s, 14.6405s/100 iters), loss = 0.0162973
I0926 12:48:23.138980  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0162974 (* 1 = 0.0162974 loss)
I0926 12:48:23.138986  4406 sgd_solver.cpp:105] Iteration 91900, lr = 0.001
I0926 12:48:37.073284  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:48:37.660293  4406 solver.cpp:330] Iteration 92000, Testing net (#0)
I0926 12:48:41.088547  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:48:41.231927  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8863
I0926 12:48:41.231962  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.452571 (* 1 = 0.452571 loss)
I0926 12:48:41.375947  4406 solver.cpp:218] Iteration 92000 (5.48338 iter/s, 18.2369s/100 iters), loss = 0.0298781
I0926 12:48:41.375977  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0298782 (* 1 = 0.0298782 loss)
I0926 12:48:41.375983  4406 sgd_solver.cpp:105] Iteration 92000, lr = 0.001
I0926 12:48:55.993232  4406 solver.cpp:218] Iteration 92100 (6.84125 iter/s, 14.6172s/100 iters), loss = 0.0579622
I0926 12:48:55.993261  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0579622 (* 1 = 0.0579622 loss)
I0926 12:48:55.993266  4406 sgd_solver.cpp:105] Iteration 92100, lr = 0.001
I0926 12:49:10.625849  4406 solver.cpp:218] Iteration 92200 (6.83408 iter/s, 14.6325s/100 iters), loss = 0.0133509
I0926 12:49:10.625993  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.013351 (* 1 = 0.013351 loss)
I0926 12:49:10.626001  4406 sgd_solver.cpp:105] Iteration 92200, lr = 0.001
I0926 12:49:25.240697  4406 solver.cpp:218] Iteration 92300 (6.84244 iter/s, 14.6147s/100 iters), loss = 0.0593117
I0926 12:49:25.240727  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0593117 (* 1 = 0.0593117 loss)
I0926 12:49:25.240743  4406 sgd_solver.cpp:105] Iteration 92300, lr = 0.001
I0926 12:49:39.852466  4406 solver.cpp:218] Iteration 92400 (6.84383 iter/s, 14.6117s/100 iters), loss = 0.00784411
I0926 12:49:39.852497  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00784419 (* 1 = 0.00784419 loss)
I0926 12:49:39.852504  4406 sgd_solver.cpp:105] Iteration 92400, lr = 0.001
I0926 12:49:53.740638  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:49:54.326918  4406 solver.cpp:330] Iteration 92500, Testing net (#0)
I0926 12:49:57.754387  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:49:57.897514  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8864
I0926 12:49:57.897549  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.452346 (* 1 = 0.452346 loss)
I0926 12:49:58.042253  4406 solver.cpp:218] Iteration 92500 (5.49761 iter/s, 18.1897s/100 iters), loss = 0.0247101
I0926 12:49:58.042284  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0247101 (* 1 = 0.0247101 loss)
I0926 12:49:58.042290  4406 sgd_solver.cpp:105] Iteration 92500, lr = 0.001
I0926 12:50:12.665618  4406 solver.cpp:218] Iteration 92600 (6.8384 iter/s, 14.6233s/100 iters), loss = 0.0771271
I0926 12:50:12.665650  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0771272 (* 1 = 0.0771272 loss)
I0926 12:50:12.665657  4406 sgd_solver.cpp:105] Iteration 92600, lr = 0.001
I0926 12:50:27.289979  4406 solver.cpp:218] Iteration 92700 (6.83794 iter/s, 14.6243s/100 iters), loss = 0.0243896
I0926 12:50:27.290094  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0243897 (* 1 = 0.0243897 loss)
I0926 12:50:27.290102  4406 sgd_solver.cpp:105] Iteration 92700, lr = 0.001
I0926 12:50:41.916687  4406 solver.cpp:218] Iteration 92800 (6.83688 iter/s, 14.6266s/100 iters), loss = 0.0237505
I0926 12:50:41.916713  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0237506 (* 1 = 0.0237506 loss)
I0926 12:50:41.916719  4406 sgd_solver.cpp:105] Iteration 92800, lr = 0.001
I0926 12:50:56.541013  4406 solver.cpp:218] Iteration 92900 (6.83795 iter/s, 14.6243s/100 iters), loss = 0.0113931
I0926 12:50:56.541044  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0113932 (* 1 = 0.0113932 loss)
I0926 12:50:56.541060  4406 sgd_solver.cpp:105] Iteration 92900, lr = 0.001
I0926 12:51:10.442260  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:51:11.027319  4406 solver.cpp:330] Iteration 93000, Testing net (#0)
I0926 12:51:14.456876  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:51:14.599738  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8862
I0926 12:51:14.599773  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.450408 (* 1 = 0.450408 loss)
I0926 12:51:14.745725  4406 solver.cpp:218] Iteration 93000 (5.49311 iter/s, 18.2046s/100 iters), loss = 0.0291706
I0926 12:51:14.745754  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0291706 (* 1 = 0.0291706 loss)
I0926 12:51:14.745761  4406 sgd_solver.cpp:105] Iteration 93000, lr = 0.001
I0926 12:51:29.367841  4406 solver.cpp:218] Iteration 93100 (6.83899 iter/s, 14.622s/100 iters), loss = 0.0374773
I0926 12:51:29.367882  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0374774 (* 1 = 0.0374774 loss)
I0926 12:51:29.367888  4406 sgd_solver.cpp:105] Iteration 93100, lr = 0.001
I0926 12:51:43.993901  4406 solver.cpp:218] Iteration 93200 (6.83715 iter/s, 14.626s/100 iters), loss = 0.0125834
I0926 12:51:43.994009  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125835 (* 1 = 0.0125835 loss)
I0926 12:51:43.994015  4406 sgd_solver.cpp:105] Iteration 93200, lr = 0.001
I0926 12:51:58.624413  4406 solver.cpp:218] Iteration 93300 (6.8351 iter/s, 14.6304s/100 iters), loss = 0.0705018
I0926 12:51:58.624454  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0705019 (* 1 = 0.0705019 loss)
I0926 12:51:58.624459  4406 sgd_solver.cpp:105] Iteration 93300, lr = 0.001
I0926 12:52:13.251845  4406 solver.cpp:218] Iteration 93400 (6.83651 iter/s, 14.6274s/100 iters), loss = 0.0126204
I0926 12:52:13.251885  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126205 (* 1 = 0.0126205 loss)
I0926 12:52:13.251891  4406 sgd_solver.cpp:105] Iteration 93400, lr = 0.001
I0926 12:52:27.152633  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:52:27.738095  4406 solver.cpp:330] Iteration 93500, Testing net (#0)
I0926 12:52:31.167738  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:52:31.311154  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8847
I0926 12:52:31.311189  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.462063 (* 1 = 0.462063 loss)
I0926 12:52:31.456542  4406 solver.cpp:218] Iteration 93500 (5.49311 iter/s, 18.2046s/100 iters), loss = 0.0327738
I0926 12:52:31.456569  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0327739 (* 1 = 0.0327739 loss)
I0926 12:52:31.456576  4406 sgd_solver.cpp:105] Iteration 93500, lr = 0.001
I0926 12:52:46.083405  4406 solver.cpp:218] Iteration 93600 (6.83677 iter/s, 14.6268s/100 iters), loss = 0.0339411
I0926 12:52:46.083433  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0339412 (* 1 = 0.0339412 loss)
I0926 12:52:46.083439  4406 sgd_solver.cpp:105] Iteration 93600, lr = 0.001
I0926 12:53:00.713111  4406 solver.cpp:218] Iteration 93700 (6.83544 iter/s, 14.6296s/100 iters), loss = 0.0197482
I0926 12:53:00.713219  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0197483 (* 1 = 0.0197483 loss)
I0926 12:53:00.713235  4406 sgd_solver.cpp:105] Iteration 93700, lr = 0.001
I0926 12:53:15.344435  4406 solver.cpp:218] Iteration 93800 (6.83472 iter/s, 14.6312s/100 iters), loss = 0.0489261
I0926 12:53:15.344465  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0489262 (* 1 = 0.0489262 loss)
I0926 12:53:15.344470  4406 sgd_solver.cpp:105] Iteration 93800, lr = 0.001
I0926 12:53:29.971380  4406 solver.cpp:218] Iteration 93900 (6.83673 iter/s, 14.6269s/100 iters), loss = 0.0146602
I0926 12:53:29.971411  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0146603 (* 1 = 0.0146603 loss)
I0926 12:53:29.971416  4406 sgd_solver.cpp:105] Iteration 93900, lr = 0.001
I0926 12:53:43.876960  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:53:44.463475  4406 solver.cpp:330] Iteration 94000, Testing net (#0)
I0926 12:53:47.890007  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:53:48.032894  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8862
I0926 12:53:48.032920  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.462328 (* 1 = 0.462328 loss)
I0926 12:53:48.178073  4406 solver.cpp:218] Iteration 94000 (5.49251 iter/s, 18.2066s/100 iters), loss = 0.0188873
I0926 12:53:48.178102  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0188874 (* 1 = 0.0188874 loss)
I0926 12:53:48.178109  4406 sgd_solver.cpp:105] Iteration 94000, lr = 0.001
I0926 12:54:02.802309  4406 solver.cpp:218] Iteration 94100 (6.838 iter/s, 14.6242s/100 iters), loss = 0.0171586
I0926 12:54:02.802338  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171587 (* 1 = 0.0171587 loss)
I0926 12:54:02.802345  4406 sgd_solver.cpp:105] Iteration 94100, lr = 0.001
I0926 12:54:17.436103  4406 solver.cpp:218] Iteration 94200 (6.83353 iter/s, 14.6337s/100 iters), loss = 0.037115
I0926 12:54:17.436214  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0371151 (* 1 = 0.0371151 loss)
I0926 12:54:17.436221  4406 sgd_solver.cpp:105] Iteration 94200, lr = 0.001
I0926 12:54:32.067572  4406 solver.cpp:218] Iteration 94300 (6.83465 iter/s, 14.6313s/100 iters), loss = 0.0168044
I0926 12:54:32.067613  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168045 (* 1 = 0.0168045 loss)
I0926 12:54:32.067620  4406 sgd_solver.cpp:105] Iteration 94300, lr = 0.001
I0926 12:54:46.699528  4406 solver.cpp:218] Iteration 94400 (6.83439 iter/s, 14.6319s/100 iters), loss = 0.0593685
I0926 12:54:46.699558  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0593686 (* 1 = 0.0593686 loss)
I0926 12:54:46.699563  4406 sgd_solver.cpp:105] Iteration 94400, lr = 0.001
I0926 12:55:00.605844  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:55:01.190313  4406 solver.cpp:330] Iteration 94500, Testing net (#0)
I0926 12:55:04.617779  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:55:04.760977  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8843
I0926 12:55:04.761003  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.471753 (* 1 = 0.471753 loss)
I0926 12:55:04.906195  4406 solver.cpp:218] Iteration 94500 (5.49252 iter/s, 18.2066s/100 iters), loss = 0.021018
I0926 12:55:04.906225  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0210181 (* 1 = 0.0210181 loss)
I0926 12:55:04.906231  4406 sgd_solver.cpp:105] Iteration 94500, lr = 0.001
I0926 12:55:19.510823  4406 solver.cpp:218] Iteration 94600 (6.84718 iter/s, 14.6046s/100 iters), loss = 0.100451
I0926 12:55:19.510852  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100451 (* 1 = 0.100451 loss)
I0926 12:55:19.510857  4406 sgd_solver.cpp:105] Iteration 94600, lr = 0.001
I0926 12:55:34.125869  4406 solver.cpp:218] Iteration 94700 (6.84229 iter/s, 14.615s/100 iters), loss = 0.0254197
I0926 12:55:34.126019  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0254198 (* 1 = 0.0254198 loss)
I0926 12:55:34.126029  4406 sgd_solver.cpp:105] Iteration 94700, lr = 0.001
I0926 12:55:48.747297  4406 solver.cpp:218] Iteration 94800 (6.83936 iter/s, 14.6212s/100 iters), loss = 0.0769148
I0926 12:55:48.747326  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0769149 (* 1 = 0.0769149 loss)
I0926 12:55:48.747333  4406 sgd_solver.cpp:105] Iteration 94800, lr = 0.001
I0926 12:56:03.364783  4406 solver.cpp:218] Iteration 94900 (6.84115 iter/s, 14.6174s/100 iters), loss = 0.021703
I0926 12:56:03.364821  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0217031 (* 1 = 0.0217031 loss)
I0926 12:56:03.364827  4406 sgd_solver.cpp:105] Iteration 94900, lr = 0.001
I0926 12:56:17.256741  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:56:17.841346  4406 solver.cpp:330] Iteration 95000, Testing net (#0)
I0926 12:56:21.269793  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:56:21.412855  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.885
I0926 12:56:21.412880  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.477601 (* 1 = 0.477601 loss)
I0926 12:56:21.558881  4406 solver.cpp:218] Iteration 95000 (5.49631 iter/s, 18.194s/100 iters), loss = 0.0401185
I0926 12:56:21.558910  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0401185 (* 1 = 0.0401185 loss)
I0926 12:56:21.558917  4406 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I0926 12:56:36.176318  4406 solver.cpp:218] Iteration 95100 (6.84118 iter/s, 14.6174s/100 iters), loss = 0.0406204
I0926 12:56:36.176347  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0406205 (* 1 = 0.0406205 loss)
I0926 12:56:36.176354  4406 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I0926 12:56:50.799224  4406 solver.cpp:218] Iteration 95200 (6.83862 iter/s, 14.6228s/100 iters), loss = 0.04328
I0926 12:56:50.799365  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0432801 (* 1 = 0.0432801 loss)
I0926 12:56:50.799374  4406 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I0926 12:57:05.422413  4406 solver.cpp:218] Iteration 95300 (6.83854 iter/s, 14.623s/100 iters), loss = 0.0344518
I0926 12:57:05.422441  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0344518 (* 1 = 0.0344518 loss)
I0926 12:57:05.422446  4406 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I0926 12:57:20.045600  4406 solver.cpp:218] Iteration 95400 (6.83849 iter/s, 14.6231s/100 iters), loss = 0.0415953
I0926 12:57:20.045629  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0415954 (* 1 = 0.0415954 loss)
I0926 12:57:20.045634  4406 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I0926 12:57:33.938844  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:57:34.524592  4406 solver.cpp:330] Iteration 95500, Testing net (#0)
I0926 12:57:37.953766  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:57:38.096462  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8837
I0926 12:57:38.096498  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.477658 (* 1 = 0.477658 loss)
I0926 12:57:38.241786  4406 solver.cpp:218] Iteration 95500 (5.49568 iter/s, 18.1961s/100 iters), loss = 0.0129455
I0926 12:57:38.241816  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129456 (* 1 = 0.0129456 loss)
I0926 12:57:38.241822  4406 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I0926 12:57:52.858477  4406 solver.cpp:218] Iteration 95600 (6.84153 iter/s, 14.6166s/100 iters), loss = 0.0390159
I0926 12:57:52.858506  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0390161 (* 1 = 0.0390161 loss)
I0926 12:57:52.858511  4406 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I0926 12:58:07.482924  4406 solver.cpp:218] Iteration 95700 (6.8379 iter/s, 14.6244s/100 iters), loss = 0.0524153
I0926 12:58:07.483048  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0524154 (* 1 = 0.0524154 loss)
I0926 12:58:07.483067  4406 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I0926 12:58:22.104264  4406 solver.cpp:218] Iteration 95800 (6.83939 iter/s, 14.6212s/100 iters), loss = 0.0444386
I0926 12:58:22.104303  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0444386 (* 1 = 0.0444386 loss)
I0926 12:58:22.104310  4406 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I0926 12:58:36.730998  4406 solver.cpp:218] Iteration 95900 (6.83683 iter/s, 14.6267s/100 iters), loss = 0.031147
I0926 12:58:36.731027  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0311471 (* 1 = 0.0311471 loss)
I0926 12:58:36.731032  4406 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I0926 12:58:50.626102  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:58:51.212379  4406 solver.cpp:330] Iteration 96000, Testing net (#0)
I0926 12:58:54.639109  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 12:58:54.781797  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8862
I0926 12:58:54.781823  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.481372 (* 1 = 0.481372 loss)
I0926 12:58:54.928205  4406 solver.cpp:218] Iteration 96000 (5.49537 iter/s, 18.1971s/100 iters), loss = 0.00893028
I0926 12:58:54.928236  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00893035 (* 1 = 0.00893035 loss)
I0926 12:58:54.928241  4406 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I0926 12:59:09.550750  4406 solver.cpp:218] Iteration 96100 (6.83879 iter/s, 14.6225s/100 iters), loss = 0.0599059
I0926 12:59:09.550788  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.059906 (* 1 = 0.059906 loss)
I0926 12:59:09.550794  4406 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I0926 12:59:24.171228  4406 solver.cpp:218] Iteration 96200 (6.83976 iter/s, 14.6204s/100 iters), loss = 0.0141739
I0926 12:59:24.171346  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.014174 (* 1 = 0.014174 loss)
I0926 12:59:24.171362  4406 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I0926 12:59:38.795809  4406 solver.cpp:218] Iteration 96300 (6.83787 iter/s, 14.6244s/100 iters), loss = 0.0167852
I0926 12:59:38.795838  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0167852 (* 1 = 0.0167852 loss)
I0926 12:59:38.795845  4406 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I0926 12:59:53.419767  4406 solver.cpp:218] Iteration 96400 (6.83812 iter/s, 14.6239s/100 iters), loss = 0.0229436
I0926 12:59:53.419796  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0229437 (* 1 = 0.0229437 loss)
I0926 12:59:53.419801  4406 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I0926 13:00:07.313841  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:00:07.900059  4406 solver.cpp:330] Iteration 96500, Testing net (#0)
I0926 13:00:11.328691  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:00:11.471662  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8863
I0926 13:00:11.471698  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.47838 (* 1 = 0.47838 loss)
I0926 13:00:11.617380  4406 solver.cpp:218] Iteration 96500 (5.49525 iter/s, 18.1975s/100 iters), loss = 0.0274596
I0926 13:00:11.617410  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0274597 (* 1 = 0.0274597 loss)
I0926 13:00:11.617416  4406 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I0926 13:00:26.244415  4406 solver.cpp:218] Iteration 96600 (6.83669 iter/s, 14.627s/100 iters), loss = 0.0505562
I0926 13:00:26.244453  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0505563 (* 1 = 0.0505563 loss)
I0926 13:00:26.244459  4406 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I0926 13:00:40.883122  4406 solver.cpp:218] Iteration 96700 (6.83124 iter/s, 14.6386s/100 iters), loss = 0.0313511
I0926 13:00:40.883253  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313511 (* 1 = 0.0313511 loss)
I0926 13:00:40.883260  4406 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I0926 13:00:55.517827  4406 solver.cpp:218] Iteration 96800 (6.83315 iter/s, 14.6345s/100 iters), loss = 0.0291234
I0926 13:00:55.517858  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0291235 (* 1 = 0.0291235 loss)
I0926 13:00:55.517863  4406 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I0926 13:01:10.157096  4406 solver.cpp:218] Iteration 96900 (6.83097 iter/s, 14.6392s/100 iters), loss = 0.0390441
I0926 13:01:10.157127  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0390443 (* 1 = 0.0390443 loss)
I0926 13:01:10.157135  4406 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I0926 13:01:24.066833  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:01:24.651847  4406 solver.cpp:330] Iteration 97000, Testing net (#0)
I0926 13:01:28.080040  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:01:28.223245  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8866
I0926 13:01:28.223281  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.482246 (* 1 = 0.482246 loss)
I0926 13:01:28.368265  4406 solver.cpp:218] Iteration 97000 (5.49116 iter/s, 18.2111s/100 iters), loss = 0.0392556
I0926 13:01:28.368294  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0392557 (* 1 = 0.0392557 loss)
I0926 13:01:28.368301  4406 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I0926 13:01:42.983099  4406 solver.cpp:218] Iteration 97100 (6.84239 iter/s, 14.6148s/100 iters), loss = 0.0700295
I0926 13:01:42.983129  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0700296 (* 1 = 0.0700296 loss)
I0926 13:01:42.983134  4406 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I0926 13:01:57.594306  4406 solver.cpp:218] Iteration 97200 (6.84409 iter/s, 14.6111s/100 iters), loss = 0.0334162
I0926 13:01:57.594444  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0334164 (* 1 = 0.0334164 loss)
I0926 13:01:57.594451  4406 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I0926 13:02:12.212162  4406 solver.cpp:218] Iteration 97300 (6.84102 iter/s, 14.6177s/100 iters), loss = 0.0357856
I0926 13:02:12.212190  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0357857 (* 1 = 0.0357857 loss)
I0926 13:02:12.212196  4406 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I0926 13:02:26.821445  4406 solver.cpp:218] Iteration 97400 (6.84499 iter/s, 14.6092s/100 iters), loss = 0.0212928
I0926 13:02:26.821475  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.021293 (* 1 = 0.021293 loss)
I0926 13:02:26.821480  4406 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I0926 13:02:40.706862  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:02:41.290410  4406 solver.cpp:330] Iteration 97500, Testing net (#0)
I0926 13:02:44.716326  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:02:44.859622  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8868
I0926 13:02:44.859647  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.486907 (* 1 = 0.486907 loss)
I0926 13:02:45.005503  4406 solver.cpp:218] Iteration 97500 (5.49934 iter/s, 18.184s/100 iters), loss = 0.0179323
I0926 13:02:45.005533  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0179325 (* 1 = 0.0179325 loss)
I0926 13:02:45.005540  4406 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I0926 13:02:59.622722  4406 solver.cpp:218] Iteration 97600 (6.84128 iter/s, 14.6172s/100 iters), loss = 0.0243287
I0926 13:02:59.622751  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0243289 (* 1 = 0.0243289 loss)
I0926 13:02:59.622756  4406 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I0926 13:03:14.308370  4406 solver.cpp:218] Iteration 97700 (6.8094 iter/s, 14.6856s/100 iters), loss = 0.0225344
I0926 13:03:14.308490  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0225346 (* 1 = 0.0225346 loss)
I0926 13:03:14.308503  4406 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I0926 13:03:29.006620  4406 solver.cpp:218] Iteration 97800 (6.8036 iter/s, 14.6981s/100 iters), loss = 0.0233849
I0926 13:03:29.006649  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.023385 (* 1 = 0.023385 loss)
I0926 13:03:29.006654  4406 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I0926 13:03:43.746264  4406 solver.cpp:218] Iteration 97900 (6.78445 iter/s, 14.7396s/100 iters), loss = 0.0386989
I0926 13:03:43.746295  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0386991 (* 1 = 0.0386991 loss)
I0926 13:03:43.746301  4406 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I0926 13:03:57.645063  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:03:58.229408  4406 solver.cpp:330] Iteration 98000, Testing net (#0)
I0926 13:04:01.660892  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:04:01.804008  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8861
I0926 13:04:01.804044  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.489103 (* 1 = 0.489103 loss)
I0926 13:04:01.949399  4406 solver.cpp:218] Iteration 98000 (5.49358 iter/s, 18.2031s/100 iters), loss = 0.0169856
I0926 13:04:01.949426  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0169858 (* 1 = 0.0169858 loss)
I0926 13:04:01.949434  4406 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I0926 13:04:16.571164  4406 solver.cpp:218] Iteration 98100 (6.83915 iter/s, 14.6217s/100 iters), loss = 0.0142697
I0926 13:04:16.571193  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142699 (* 1 = 0.0142699 loss)
I0926 13:04:16.571199  4406 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I0926 13:04:31.190549  4406 solver.cpp:218] Iteration 98200 (6.84027 iter/s, 14.6193s/100 iters), loss = 0.0376598
I0926 13:04:31.190678  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.03766 (* 1 = 0.03766 loss)
I0926 13:04:31.190685  4406 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I0926 13:04:45.811810  4406 solver.cpp:218] Iteration 98300 (6.83943 iter/s, 14.6211s/100 iters), loss = 0.038286
I0926 13:04:45.811838  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0382861 (* 1 = 0.0382861 loss)
I0926 13:04:45.811854  4406 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I0926 13:05:00.433379  4406 solver.cpp:218] Iteration 98400 (6.83924 iter/s, 14.6215s/100 iters), loss = 0.0368759
I0926 13:05:00.433410  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0368761 (* 1 = 0.0368761 loss)
I0926 13:05:00.433416  4406 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I0926 13:05:14.328294  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:05:14.913637  4406 solver.cpp:330] Iteration 98500, Testing net (#0)
I0926 13:05:18.341717  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:05:18.485087  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8847
I0926 13:05:18.485122  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.487239 (* 1 = 0.487239 loss)
I0926 13:05:18.630146  4406 solver.cpp:218] Iteration 98500 (5.4955 iter/s, 18.1967s/100 iters), loss = 0.0359076
I0926 13:05:18.630178  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0359077 (* 1 = 0.0359077 loss)
I0926 13:05:18.630184  4406 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I0926 13:05:33.253432  4406 solver.cpp:218] Iteration 98600 (6.83844 iter/s, 14.6232s/100 iters), loss = 0.0811588
I0926 13:05:33.253473  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.081159 (* 1 = 0.081159 loss)
I0926 13:05:33.253479  4406 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I0926 13:05:47.882017  4406 solver.cpp:218] Iteration 98700 (6.83597 iter/s, 14.6285s/100 iters), loss = 0.0443791
I0926 13:05:47.882138  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0443793 (* 1 = 0.0443793 loss)
I0926 13:05:47.882145  4406 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I0926 13:06:02.513733  4406 solver.cpp:218] Iteration 98800 (6.83454 iter/s, 14.6316s/100 iters), loss = 0.0309952
I0926 13:06:02.513772  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0309953 (* 1 = 0.0309953 loss)
I0926 13:06:02.513778  4406 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I0926 13:06:17.135251  4406 solver.cpp:218] Iteration 98900 (6.83927 iter/s, 14.6214s/100 iters), loss = 0.0125023
I0926 13:06:17.135291  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125024 (* 1 = 0.0125024 loss)
I0926 13:06:17.135296  4406 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I0926 13:06:31.033718  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:06:31.619261  4406 solver.cpp:330] Iteration 99000, Testing net (#0)
I0926 13:06:35.045058  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:06:35.187676  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.886
I0926 13:06:35.187712  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.491764 (* 1 = 0.491764 loss)
I0926 13:06:35.332130  4406 solver.cpp:218] Iteration 99000 (5.49547 iter/s, 18.1968s/100 iters), loss = 0.0133191
I0926 13:06:35.332160  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0133193 (* 1 = 0.0133193 loss)
I0926 13:06:35.332166  4406 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I0926 13:06:49.961737  4406 solver.cpp:218] Iteration 99100 (6.83548 iter/s, 14.6295s/100 iters), loss = 0.0129549
I0926 13:06:49.961777  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129551 (* 1 = 0.0129551 loss)
I0926 13:06:49.961783  4406 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I0926 13:07:04.600466  4406 solver.cpp:218] Iteration 99200 (6.83123 iter/s, 14.6387s/100 iters), loss = 0.0418269
I0926 13:07:04.600582  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0418271 (* 1 = 0.0418271 loss)
I0926 13:07:04.600600  4406 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I0926 13:07:19.240232  4406 solver.cpp:218] Iteration 99300 (6.83078 iter/s, 14.6396s/100 iters), loss = 0.0148798
I0926 13:07:19.240272  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.01488 (* 1 = 0.01488 loss)
I0926 13:07:19.240278  4406 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I0926 13:07:33.873708  4406 solver.cpp:218] Iteration 99400 (6.83368 iter/s, 14.6334s/100 iters), loss = 0.0233285
I0926 13:07:33.873749  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0233287 (* 1 = 0.0233287 loss)
I0926 13:07:33.873754  4406 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I0926 13:07:47.781599  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:07:48.366792  4406 solver.cpp:330] Iteration 99500, Testing net (#0)
I0926 13:07:51.795337  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:07:51.938215  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8862
I0926 13:07:51.938251  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.493293 (* 1 = 0.493293 loss)
I0926 13:07:52.083137  4406 solver.cpp:218] Iteration 99500 (5.49169 iter/s, 18.2093s/100 iters), loss = 0.0275223
I0926 13:07:52.083165  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275225 (* 1 = 0.0275225 loss)
I0926 13:07:52.083173  4406 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I0926 13:08:06.706197  4406 solver.cpp:218] Iteration 99600 (6.83854 iter/s, 14.623s/100 iters), loss = 0.0987182
I0926 13:08:06.706226  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0987184 (* 1 = 0.0987184 loss)
I0926 13:08:06.706231  4406 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I0926 13:08:21.331579  4406 solver.cpp:218] Iteration 99700 (6.83746 iter/s, 14.6253s/100 iters), loss = 0.050615
I0926 13:08:21.331743  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0506152 (* 1 = 0.0506152 loss)
I0926 13:08:21.331750  4406 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I0926 13:08:35.954322  4406 solver.cpp:218] Iteration 99800 (6.83875 iter/s, 14.6225s/100 iters), loss = 0.0388625
I0926 13:08:35.954352  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0388627 (* 1 = 0.0388627 loss)
I0926 13:08:35.954358  4406 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I0926 13:08:50.579512  4406 solver.cpp:218] Iteration 99900 (6.83755 iter/s, 14.6251s/100 iters), loss = 0.0510405
I0926 13:08:50.579543  4406 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0510407 (* 1 = 0.0510407 loss)
I0926 13:08:50.579550  4406 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I0926 13:09:04.476554  4414 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:09:05.061590  4406 solver.cpp:447] Snapshotting to binary proto file xn/PENLU/snapshot/resnet/res56_penlu_alpha2s_eta1s_gauss_iter_100000.caffemodel
I0926 13:09:05.089113  4406 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/PENLU/snapshot/resnet/res56_penlu_alpha2s_eta1s_gauss_iter_100000.solverstate
I0926 13:09:05.129987  4406 solver.cpp:310] Iteration 100000, loss = 0.0435294
I0926 13:09:05.130012  4406 solver.cpp:330] Iteration 100000, Testing net (#0)
I0926 13:09:08.556466  4415 data_layer.cpp:73] Restarting data prefetching from start.
I0926 13:09:08.699717  4406 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8834
I0926 13:09:08.699743  4406 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.485056 (* 1 = 0.485056 loss)
I0926 13:09:08.699748  4406 solver.cpp:315] Optimization Done.
I0926 13:09:08.699749  4406 caffe.cpp:259] Optimization Done.
