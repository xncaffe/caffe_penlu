I1023 12:10:12.469529  4594 caffe.cpp:218] Using GPUs 0
I1023 12:10:12.494232  4594 caffe.cpp:223] GPU 0: GeForce GTX 1080
I1023 12:10:12.719331  4594 solver.cpp:44] Initializing solver from parameters: 
test_iter: 64
test_interval: 200
base_lr: 0.01
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 30000
snapshot_prefix: "xn/English_orange/snapshot/res20/res20_penlu_alpha0.25_etanostudy_2study_nodecay"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 10000
stepvalue: 20000
I1023 12:10:12.719458  4594 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1023 12:10:12.720990  4594 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1023 12:10:12.721000  4594 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1023 12:10:12.721132  4594 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I1023 12:10:12.721195  4594 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I1023 12:10:12.721653  4594 net.cpp:51] Initializing net from parameters: 
name: "resnet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 224
    mean_file: "/home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/English_orange/data/train1_lmdb"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1023 12:10:12.721956  4594 layer_factory.hpp:77] Creating layer Data1
I1023 12:10:12.722035  4594 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/English_orange/data/train1_lmdb
I1023 12:10:12.722052  4594 net.cpp:84] Creating Layer Data1
I1023 12:10:12.722057  4594 net.cpp:380] Data1 -> Data1
I1023 12:10:12.722072  4594 net.cpp:380] Data1 -> Data2
I1023 12:10:12.722080  4594 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto
I1023 12:10:12.724539  4594 data_layer.cpp:45] output data size: 8,3,224,224
I1023 12:10:12.732385  4594 net.cpp:122] Setting up Data1
I1023 12:10:12.732416  4594 net.cpp:129] Top shape: 8 3 224 224 (1204224)
I1023 12:10:12.732421  4594 net.cpp:129] Top shape: 8 (8)
I1023 12:10:12.732424  4594 net.cpp:137] Memory required for data: 4816928
I1023 12:10:12.732430  4594 layer_factory.hpp:77] Creating layer Convolution1
I1023 12:10:12.732451  4594 net.cpp:84] Creating Layer Convolution1
I1023 12:10:12.732466  4594 net.cpp:406] Convolution1 <- Data1
I1023 12:10:12.732484  4594 net.cpp:380] Convolution1 -> Convolution1
I1023 12:10:12.880527  4594 net.cpp:122] Setting up Convolution1
I1023 12:10:12.880551  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.880554  4594 net.cpp:137] Memory required for data: 30507040
I1023 12:10:12.880569  4594 layer_factory.hpp:77] Creating layer BatchNorm1
I1023 12:10:12.880591  4594 net.cpp:84] Creating Layer BatchNorm1
I1023 12:10:12.880597  4594 net.cpp:406] BatchNorm1 <- Convolution1
I1023 12:10:12.880602  4594 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1023 12:10:12.880759  4594 net.cpp:122] Setting up BatchNorm1
I1023 12:10:12.880764  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.880767  4594 net.cpp:137] Memory required for data: 56197152
I1023 12:10:12.880774  4594 layer_factory.hpp:77] Creating layer Scale1
I1023 12:10:12.880784  4594 net.cpp:84] Creating Layer Scale1
I1023 12:10:12.880786  4594 net.cpp:406] Scale1 <- Convolution1
I1023 12:10:12.880800  4594 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1023 12:10:12.880839  4594 layer_factory.hpp:77] Creating layer Scale1
I1023 12:10:12.880971  4594 net.cpp:122] Setting up Scale1
I1023 12:10:12.880977  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.880980  4594 net.cpp:137] Memory required for data: 81887264
I1023 12:10:12.880983  4594 layer_factory.hpp:77] Creating layer penlu1
I1023 12:10:12.880992  4594 net.cpp:84] Creating Layer penlu1
I1023 12:10:12.880995  4594 net.cpp:406] penlu1 <- Convolution1
I1023 12:10:12.881008  4594 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1023 12:10:12.882139  4594 net.cpp:122] Setting up penlu1
I1023 12:10:12.882149  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.882163  4594 net.cpp:137] Memory required for data: 107577376
I1023 12:10:12.882170  4594 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1023 12:10:12.882175  4594 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1023 12:10:12.882187  4594 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1023 12:10:12.882191  4594 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1023 12:10:12.882196  4594 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1023 12:10:12.882233  4594 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1023 12:10:12.882238  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.882251  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.882252  4594 net.cpp:137] Memory required for data: 158957600
I1023 12:10:12.882254  4594 layer_factory.hpp:77] Creating layer Convolution2
I1023 12:10:12.882272  4594 net.cpp:84] Creating Layer Convolution2
I1023 12:10:12.882275  4594 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1023 12:10:12.882279  4594 net.cpp:380] Convolution2 -> Convolution2
I1023 12:10:12.883718  4594 net.cpp:122] Setting up Convolution2
I1023 12:10:12.883730  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.883733  4594 net.cpp:137] Memory required for data: 184647712
I1023 12:10:12.883759  4594 layer_factory.hpp:77] Creating layer BatchNorm2
I1023 12:10:12.883765  4594 net.cpp:84] Creating Layer BatchNorm2
I1023 12:10:12.883769  4594 net.cpp:406] BatchNorm2 <- Convolution2
I1023 12:10:12.883774  4594 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1023 12:10:12.883934  4594 net.cpp:122] Setting up BatchNorm2
I1023 12:10:12.883939  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.883942  4594 net.cpp:137] Memory required for data: 210337824
I1023 12:10:12.883965  4594 layer_factory.hpp:77] Creating layer Scale2
I1023 12:10:12.883971  4594 net.cpp:84] Creating Layer Scale2
I1023 12:10:12.883975  4594 net.cpp:406] Scale2 <- Convolution2
I1023 12:10:12.883978  4594 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1023 12:10:12.884004  4594 layer_factory.hpp:77] Creating layer Scale2
I1023 12:10:12.884165  4594 net.cpp:122] Setting up Scale2
I1023 12:10:12.884171  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.884172  4594 net.cpp:137] Memory required for data: 236027936
I1023 12:10:12.884189  4594 layer_factory.hpp:77] Creating layer penlu2
I1023 12:10:12.884196  4594 net.cpp:84] Creating Layer penlu2
I1023 12:10:12.884199  4594 net.cpp:406] penlu2 <- Convolution2
I1023 12:10:12.884202  4594 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1023 12:10:12.885354  4594 net.cpp:122] Setting up penlu2
I1023 12:10:12.885365  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.885368  4594 net.cpp:137] Memory required for data: 261718048
I1023 12:10:12.885375  4594 layer_factory.hpp:77] Creating layer Convolution3
I1023 12:10:12.885383  4594 net.cpp:84] Creating Layer Convolution3
I1023 12:10:12.885386  4594 net.cpp:406] Convolution3 <- Convolution2
I1023 12:10:12.885391  4594 net.cpp:380] Convolution3 -> Convolution3
I1023 12:10:12.886327  4594 net.cpp:122] Setting up Convolution3
I1023 12:10:12.886338  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.886342  4594 net.cpp:137] Memory required for data: 287408160
I1023 12:10:12.886346  4594 layer_factory.hpp:77] Creating layer BatchNorm3
I1023 12:10:12.886353  4594 net.cpp:84] Creating Layer BatchNorm3
I1023 12:10:12.886355  4594 net.cpp:406] BatchNorm3 <- Convolution3
I1023 12:10:12.886359  4594 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1023 12:10:12.886510  4594 net.cpp:122] Setting up BatchNorm3
I1023 12:10:12.886515  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.886518  4594 net.cpp:137] Memory required for data: 313098272
I1023 12:10:12.886523  4594 layer_factory.hpp:77] Creating layer Scale3
I1023 12:10:12.886528  4594 net.cpp:84] Creating Layer Scale3
I1023 12:10:12.886533  4594 net.cpp:406] Scale3 <- Convolution3
I1023 12:10:12.886535  4594 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1023 12:10:12.886561  4594 layer_factory.hpp:77] Creating layer Scale3
I1023 12:10:12.887168  4594 net.cpp:122] Setting up Scale3
I1023 12:10:12.887179  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.887182  4594 net.cpp:137] Memory required for data: 338788384
I1023 12:10:12.887187  4594 layer_factory.hpp:77] Creating layer Eltwise1
I1023 12:10:12.887193  4594 net.cpp:84] Creating Layer Eltwise1
I1023 12:10:12.887197  4594 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1023 12:10:12.887200  4594 net.cpp:406] Eltwise1 <- Convolution3
I1023 12:10:12.887204  4594 net.cpp:380] Eltwise1 -> Eltwise1
I1023 12:10:12.887224  4594 net.cpp:122] Setting up Eltwise1
I1023 12:10:12.887229  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.887233  4594 net.cpp:137] Memory required for data: 364478496
I1023 12:10:12.887234  4594 layer_factory.hpp:77] Creating layer penlu3
I1023 12:10:12.887240  4594 net.cpp:84] Creating Layer penlu3
I1023 12:10:12.887243  4594 net.cpp:406] penlu3 <- Eltwise1
I1023 12:10:12.887246  4594 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1023 12:10:12.888386  4594 net.cpp:122] Setting up penlu3
I1023 12:10:12.888397  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.888412  4594 net.cpp:137] Memory required for data: 390168608
I1023 12:10:12.888418  4594 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1023 12:10:12.888424  4594 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1023 12:10:12.888427  4594 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1023 12:10:12.888432  4594 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1023 12:10:12.888438  4594 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1023 12:10:12.888463  4594 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1023 12:10:12.888468  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.888471  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.888473  4594 net.cpp:137] Memory required for data: 441548832
I1023 12:10:12.888478  4594 layer_factory.hpp:77] Creating layer Convolution4
I1023 12:10:12.888485  4594 net.cpp:84] Creating Layer Convolution4
I1023 12:10:12.888489  4594 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1023 12:10:12.888492  4594 net.cpp:380] Convolution4 -> Convolution4
I1023 12:10:12.889407  4594 net.cpp:122] Setting up Convolution4
I1023 12:10:12.889418  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.889422  4594 net.cpp:137] Memory required for data: 467238944
I1023 12:10:12.889426  4594 layer_factory.hpp:77] Creating layer BatchNorm4
I1023 12:10:12.889432  4594 net.cpp:84] Creating Layer BatchNorm4
I1023 12:10:12.889436  4594 net.cpp:406] BatchNorm4 <- Convolution4
I1023 12:10:12.889441  4594 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1023 12:10:12.889587  4594 net.cpp:122] Setting up BatchNorm4
I1023 12:10:12.889593  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.889596  4594 net.cpp:137] Memory required for data: 492929056
I1023 12:10:12.889605  4594 layer_factory.hpp:77] Creating layer Scale4
I1023 12:10:12.889609  4594 net.cpp:84] Creating Layer Scale4
I1023 12:10:12.889613  4594 net.cpp:406] Scale4 <- Convolution4
I1023 12:10:12.889616  4594 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1023 12:10:12.889643  4594 layer_factory.hpp:77] Creating layer Scale4
I1023 12:10:12.889766  4594 net.cpp:122] Setting up Scale4
I1023 12:10:12.889772  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.889775  4594 net.cpp:137] Memory required for data: 518619168
I1023 12:10:12.889780  4594 layer_factory.hpp:77] Creating layer penlu4
I1023 12:10:12.889786  4594 net.cpp:84] Creating Layer penlu4
I1023 12:10:12.889788  4594 net.cpp:406] penlu4 <- Convolution4
I1023 12:10:12.889792  4594 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1023 12:10:12.890952  4594 net.cpp:122] Setting up penlu4
I1023 12:10:12.890964  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.890966  4594 net.cpp:137] Memory required for data: 544309280
I1023 12:10:12.890971  4594 layer_factory.hpp:77] Creating layer Convolution5
I1023 12:10:12.890980  4594 net.cpp:84] Creating Layer Convolution5
I1023 12:10:12.890983  4594 net.cpp:406] Convolution5 <- Convolution4
I1023 12:10:12.890988  4594 net.cpp:380] Convolution5 -> Convolution5
I1023 12:10:12.892282  4594 net.cpp:122] Setting up Convolution5
I1023 12:10:12.892292  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.892297  4594 net.cpp:137] Memory required for data: 569999392
I1023 12:10:12.892302  4594 layer_factory.hpp:77] Creating layer BatchNorm5
I1023 12:10:12.892307  4594 net.cpp:84] Creating Layer BatchNorm5
I1023 12:10:12.892309  4594 net.cpp:406] BatchNorm5 <- Convolution5
I1023 12:10:12.892313  4594 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1023 12:10:12.892498  4594 net.cpp:122] Setting up BatchNorm5
I1023 12:10:12.892511  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.892516  4594 net.cpp:137] Memory required for data: 595689504
I1023 12:10:12.892524  4594 layer_factory.hpp:77] Creating layer Scale5
I1023 12:10:12.892534  4594 net.cpp:84] Creating Layer Scale5
I1023 12:10:12.892537  4594 net.cpp:406] Scale5 <- Convolution5
I1023 12:10:12.892545  4594 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1023 12:10:12.892613  4594 layer_factory.hpp:77] Creating layer Scale5
I1023 12:10:12.892783  4594 net.cpp:122] Setting up Scale5
I1023 12:10:12.892791  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.892804  4594 net.cpp:137] Memory required for data: 621379616
I1023 12:10:12.892809  4594 layer_factory.hpp:77] Creating layer Eltwise2
I1023 12:10:12.892814  4594 net.cpp:84] Creating Layer Eltwise2
I1023 12:10:12.892817  4594 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1023 12:10:12.892820  4594 net.cpp:406] Eltwise2 <- Convolution5
I1023 12:10:12.892824  4594 net.cpp:380] Eltwise2 -> Eltwise2
I1023 12:10:12.892840  4594 net.cpp:122] Setting up Eltwise2
I1023 12:10:12.892845  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.892848  4594 net.cpp:137] Memory required for data: 647069728
I1023 12:10:12.892850  4594 layer_factory.hpp:77] Creating layer penlu5
I1023 12:10:12.892855  4594 net.cpp:84] Creating Layer penlu5
I1023 12:10:12.892859  4594 net.cpp:406] penlu5 <- Eltwise2
I1023 12:10:12.892863  4594 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1023 12:10:12.894068  4594 net.cpp:122] Setting up penlu5
I1023 12:10:12.894083  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.894085  4594 net.cpp:137] Memory required for data: 672759840
I1023 12:10:12.894091  4594 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1023 12:10:12.894098  4594 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1023 12:10:12.894101  4594 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1023 12:10:12.894105  4594 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1023 12:10:12.894111  4594 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1023 12:10:12.894136  4594 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1023 12:10:12.894141  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.894143  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.894146  4594 net.cpp:137] Memory required for data: 724140064
I1023 12:10:12.894148  4594 layer_factory.hpp:77] Creating layer Convolution6
I1023 12:10:12.894155  4594 net.cpp:84] Creating Layer Convolution6
I1023 12:10:12.894160  4594 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1023 12:10:12.894163  4594 net.cpp:380] Convolution6 -> Convolution6
I1023 12:10:12.895673  4594 net.cpp:122] Setting up Convolution6
I1023 12:10:12.895684  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.895687  4594 net.cpp:137] Memory required for data: 749830176
I1023 12:10:12.895692  4594 layer_factory.hpp:77] Creating layer BatchNorm6
I1023 12:10:12.895699  4594 net.cpp:84] Creating Layer BatchNorm6
I1023 12:10:12.895701  4594 net.cpp:406] BatchNorm6 <- Convolution6
I1023 12:10:12.895706  4594 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1023 12:10:12.895861  4594 net.cpp:122] Setting up BatchNorm6
I1023 12:10:12.895867  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.895870  4594 net.cpp:137] Memory required for data: 775520288
I1023 12:10:12.895875  4594 layer_factory.hpp:77] Creating layer Scale6
I1023 12:10:12.895880  4594 net.cpp:84] Creating Layer Scale6
I1023 12:10:12.895884  4594 net.cpp:406] Scale6 <- Convolution6
I1023 12:10:12.895886  4594 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1023 12:10:12.895915  4594 layer_factory.hpp:77] Creating layer Scale6
I1023 12:10:12.896036  4594 net.cpp:122] Setting up Scale6
I1023 12:10:12.896044  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.896045  4594 net.cpp:137] Memory required for data: 801210400
I1023 12:10:12.896049  4594 layer_factory.hpp:77] Creating layer penlu6
I1023 12:10:12.896056  4594 net.cpp:84] Creating Layer penlu6
I1023 12:10:12.896060  4594 net.cpp:406] penlu6 <- Convolution6
I1023 12:10:12.896064  4594 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1023 12:10:12.897199  4594 net.cpp:122] Setting up penlu6
I1023 12:10:12.897209  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.897224  4594 net.cpp:137] Memory required for data: 826900512
I1023 12:10:12.897230  4594 layer_factory.hpp:77] Creating layer Convolution7
I1023 12:10:12.897239  4594 net.cpp:84] Creating Layer Convolution7
I1023 12:10:12.897243  4594 net.cpp:406] Convolution7 <- Convolution6
I1023 12:10:12.897248  4594 net.cpp:380] Convolution7 -> Convolution7
I1023 12:10:12.898207  4594 net.cpp:122] Setting up Convolution7
I1023 12:10:12.898218  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.898222  4594 net.cpp:137] Memory required for data: 852590624
I1023 12:10:12.898226  4594 layer_factory.hpp:77] Creating layer BatchNorm7
I1023 12:10:12.898232  4594 net.cpp:84] Creating Layer BatchNorm7
I1023 12:10:12.898236  4594 net.cpp:406] BatchNorm7 <- Convolution7
I1023 12:10:12.898241  4594 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1023 12:10:12.898402  4594 net.cpp:122] Setting up BatchNorm7
I1023 12:10:12.898408  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.898411  4594 net.cpp:137] Memory required for data: 878280736
I1023 12:10:12.898422  4594 layer_factory.hpp:77] Creating layer Scale7
I1023 12:10:12.898430  4594 net.cpp:84] Creating Layer Scale7
I1023 12:10:12.898433  4594 net.cpp:406] Scale7 <- Convolution7
I1023 12:10:12.898437  4594 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1023 12:10:12.898463  4594 layer_factory.hpp:77] Creating layer Scale7
I1023 12:10:12.898588  4594 net.cpp:122] Setting up Scale7
I1023 12:10:12.898596  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.898598  4594 net.cpp:137] Memory required for data: 903970848
I1023 12:10:12.898602  4594 layer_factory.hpp:77] Creating layer Eltwise3
I1023 12:10:12.898607  4594 net.cpp:84] Creating Layer Eltwise3
I1023 12:10:12.898610  4594 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1023 12:10:12.898613  4594 net.cpp:406] Eltwise3 <- Convolution7
I1023 12:10:12.898617  4594 net.cpp:380] Eltwise3 -> Eltwise3
I1023 12:10:12.898634  4594 net.cpp:122] Setting up Eltwise3
I1023 12:10:12.898639  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.898643  4594 net.cpp:137] Memory required for data: 929660960
I1023 12:10:12.898644  4594 layer_factory.hpp:77] Creating layer penlu7
I1023 12:10:12.898649  4594 net.cpp:84] Creating Layer penlu7
I1023 12:10:12.898653  4594 net.cpp:406] penlu7 <- Eltwise3
I1023 12:10:12.898656  4594 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1023 12:10:12.899794  4594 net.cpp:122] Setting up penlu7
I1023 12:10:12.899806  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.899808  4594 net.cpp:137] Memory required for data: 955351072
I1023 12:10:12.899813  4594 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1023 12:10:12.899819  4594 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1023 12:10:12.899822  4594 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1023 12:10:12.899826  4594 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1023 12:10:12.899832  4594 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1023 12:10:12.899855  4594 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1023 12:10:12.899860  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.899864  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.899868  4594 net.cpp:137] Memory required for data: 1006731296
I1023 12:10:12.899870  4594 layer_factory.hpp:77] Creating layer Convolution8
I1023 12:10:12.899878  4594 net.cpp:84] Creating Layer Convolution8
I1023 12:10:12.899881  4594 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1023 12:10:12.899884  4594 net.cpp:380] Convolution8 -> Convolution8
I1023 12:10:12.901603  4594 net.cpp:122] Setting up Convolution8
I1023 12:10:12.901614  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.901618  4594 net.cpp:137] Memory required for data: 1019576352
I1023 12:10:12.901623  4594 layer_factory.hpp:77] Creating layer BatchNorm8
I1023 12:10:12.901629  4594 net.cpp:84] Creating Layer BatchNorm8
I1023 12:10:12.901633  4594 net.cpp:406] BatchNorm8 <- Convolution8
I1023 12:10:12.901648  4594 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1023 12:10:12.901788  4594 net.cpp:122] Setting up BatchNorm8
I1023 12:10:12.901793  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.901796  4594 net.cpp:137] Memory required for data: 1032421408
I1023 12:10:12.901803  4594 layer_factory.hpp:77] Creating layer Scale8
I1023 12:10:12.901808  4594 net.cpp:84] Creating Layer Scale8
I1023 12:10:12.901811  4594 net.cpp:406] Scale8 <- Convolution8
I1023 12:10:12.901814  4594 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1023 12:10:12.901844  4594 layer_factory.hpp:77] Creating layer Scale8
I1023 12:10:12.901924  4594 net.cpp:122] Setting up Scale8
I1023 12:10:12.901931  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.901933  4594 net.cpp:137] Memory required for data: 1045266464
I1023 12:10:12.901937  4594 layer_factory.hpp:77] Creating layer Convolution9
I1023 12:10:12.901945  4594 net.cpp:84] Creating Layer Convolution9
I1023 12:10:12.901948  4594 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1023 12:10:12.901952  4594 net.cpp:380] Convolution9 -> Convolution9
I1023 12:10:12.902899  4594 net.cpp:122] Setting up Convolution9
I1023 12:10:12.902909  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.902912  4594 net.cpp:137] Memory required for data: 1058111520
I1023 12:10:12.902917  4594 layer_factory.hpp:77] Creating layer BatchNorm9
I1023 12:10:12.902923  4594 net.cpp:84] Creating Layer BatchNorm9
I1023 12:10:12.902926  4594 net.cpp:406] BatchNorm9 <- Convolution9
I1023 12:10:12.902930  4594 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1023 12:10:12.903059  4594 net.cpp:122] Setting up BatchNorm9
I1023 12:10:12.903064  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.903067  4594 net.cpp:137] Memory required for data: 1070956576
I1023 12:10:12.903071  4594 layer_factory.hpp:77] Creating layer Scale9
I1023 12:10:12.903076  4594 net.cpp:84] Creating Layer Scale9
I1023 12:10:12.903079  4594 net.cpp:406] Scale9 <- Convolution9
I1023 12:10:12.903084  4594 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1023 12:10:12.903110  4594 layer_factory.hpp:77] Creating layer Scale9
I1023 12:10:12.903187  4594 net.cpp:122] Setting up Scale9
I1023 12:10:12.903192  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.903195  4594 net.cpp:137] Memory required for data: 1083801632
I1023 12:10:12.903199  4594 layer_factory.hpp:77] Creating layer penlu8
I1023 12:10:12.903205  4594 net.cpp:84] Creating Layer penlu8
I1023 12:10:12.903209  4594 net.cpp:406] penlu8 <- Convolution9
I1023 12:10:12.903213  4594 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1023 12:10:12.904012  4594 net.cpp:122] Setting up penlu8
I1023 12:10:12.904022  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.904026  4594 net.cpp:137] Memory required for data: 1096646688
I1023 12:10:12.904031  4594 layer_factory.hpp:77] Creating layer Convolution10
I1023 12:10:12.904038  4594 net.cpp:84] Creating Layer Convolution10
I1023 12:10:12.904042  4594 net.cpp:406] Convolution10 <- Convolution9
I1023 12:10:12.904047  4594 net.cpp:380] Convolution10 -> Convolution10
I1023 12:10:12.905086  4594 net.cpp:122] Setting up Convolution10
I1023 12:10:12.905097  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.905100  4594 net.cpp:137] Memory required for data: 1109491744
I1023 12:10:12.905105  4594 layer_factory.hpp:77] Creating layer BatchNorm10
I1023 12:10:12.905112  4594 net.cpp:84] Creating Layer BatchNorm10
I1023 12:10:12.905114  4594 net.cpp:406] BatchNorm10 <- Convolution10
I1023 12:10:12.905118  4594 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1023 12:10:12.905248  4594 net.cpp:122] Setting up BatchNorm10
I1023 12:10:12.905254  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.905257  4594 net.cpp:137] Memory required for data: 1122336800
I1023 12:10:12.905262  4594 layer_factory.hpp:77] Creating layer Scale10
I1023 12:10:12.905267  4594 net.cpp:84] Creating Layer Scale10
I1023 12:10:12.905278  4594 net.cpp:406] Scale10 <- Convolution10
I1023 12:10:12.905282  4594 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1023 12:10:12.905309  4594 layer_factory.hpp:77] Creating layer Scale10
I1023 12:10:12.905391  4594 net.cpp:122] Setting up Scale10
I1023 12:10:12.905397  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.905400  4594 net.cpp:137] Memory required for data: 1135181856
I1023 12:10:12.905403  4594 layer_factory.hpp:77] Creating layer Eltwise4
I1023 12:10:12.905408  4594 net.cpp:84] Creating Layer Eltwise4
I1023 12:10:12.905411  4594 net.cpp:406] Eltwise4 <- Convolution8
I1023 12:10:12.905414  4594 net.cpp:406] Eltwise4 <- Convolution10
I1023 12:10:12.905418  4594 net.cpp:380] Eltwise4 -> Eltwise4
I1023 12:10:12.905436  4594 net.cpp:122] Setting up Eltwise4
I1023 12:10:12.905439  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.905442  4594 net.cpp:137] Memory required for data: 1148026912
I1023 12:10:12.905444  4594 layer_factory.hpp:77] Creating layer penlu9
I1023 12:10:12.905450  4594 net.cpp:84] Creating Layer penlu9
I1023 12:10:12.905453  4594 net.cpp:406] penlu9 <- Eltwise4
I1023 12:10:12.905457  4594 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1023 12:10:12.906234  4594 net.cpp:122] Setting up penlu9
I1023 12:10:12.906244  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.906247  4594 net.cpp:137] Memory required for data: 1160871968
I1023 12:10:12.906252  4594 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1023 12:10:12.906257  4594 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1023 12:10:12.906261  4594 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1023 12:10:12.906265  4594 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1023 12:10:12.906270  4594 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1023 12:10:12.906296  4594 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1023 12:10:12.906301  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.906303  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.906306  4594 net.cpp:137] Memory required for data: 1186562080
I1023 12:10:12.906308  4594 layer_factory.hpp:77] Creating layer Convolution11
I1023 12:10:12.906316  4594 net.cpp:84] Creating Layer Convolution11
I1023 12:10:12.906318  4594 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1023 12:10:12.906322  4594 net.cpp:380] Convolution11 -> Convolution11
I1023 12:10:12.907392  4594 net.cpp:122] Setting up Convolution11
I1023 12:10:12.907402  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.907405  4594 net.cpp:137] Memory required for data: 1199407136
I1023 12:10:12.907409  4594 layer_factory.hpp:77] Creating layer BatchNorm11
I1023 12:10:12.907416  4594 net.cpp:84] Creating Layer BatchNorm11
I1023 12:10:12.907420  4594 net.cpp:406] BatchNorm11 <- Convolution11
I1023 12:10:12.907423  4594 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1023 12:10:12.907562  4594 net.cpp:122] Setting up BatchNorm11
I1023 12:10:12.907568  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.907572  4594 net.cpp:137] Memory required for data: 1212252192
I1023 12:10:12.907577  4594 layer_factory.hpp:77] Creating layer Scale11
I1023 12:10:12.907583  4594 net.cpp:84] Creating Layer Scale11
I1023 12:10:12.907588  4594 net.cpp:406] Scale11 <- Convolution11
I1023 12:10:12.907590  4594 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1023 12:10:12.907618  4594 layer_factory.hpp:77] Creating layer Scale11
I1023 12:10:12.907724  4594 net.cpp:122] Setting up Scale11
I1023 12:10:12.907730  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.907732  4594 net.cpp:137] Memory required for data: 1225097248
I1023 12:10:12.907737  4594 layer_factory.hpp:77] Creating layer penlu10
I1023 12:10:12.907743  4594 net.cpp:84] Creating Layer penlu10
I1023 12:10:12.907747  4594 net.cpp:406] penlu10 <- Convolution11
I1023 12:10:12.907750  4594 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1023 12:10:12.908710  4594 net.cpp:122] Setting up penlu10
I1023 12:10:12.908730  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.908743  4594 net.cpp:137] Memory required for data: 1237942304
I1023 12:10:12.908749  4594 layer_factory.hpp:77] Creating layer Convolution12
I1023 12:10:12.908756  4594 net.cpp:84] Creating Layer Convolution12
I1023 12:10:12.908759  4594 net.cpp:406] Convolution12 <- Convolution11
I1023 12:10:12.908766  4594 net.cpp:380] Convolution12 -> Convolution12
I1023 12:10:12.909935  4594 net.cpp:122] Setting up Convolution12
I1023 12:10:12.909945  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.909958  4594 net.cpp:137] Memory required for data: 1250787360
I1023 12:10:12.909962  4594 layer_factory.hpp:77] Creating layer BatchNorm12
I1023 12:10:12.909968  4594 net.cpp:84] Creating Layer BatchNorm12
I1023 12:10:12.909971  4594 net.cpp:406] BatchNorm12 <- Convolution12
I1023 12:10:12.909976  4594 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1023 12:10:12.910137  4594 net.cpp:122] Setting up BatchNorm12
I1023 12:10:12.910142  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.910154  4594 net.cpp:137] Memory required for data: 1263632416
I1023 12:10:12.910159  4594 layer_factory.hpp:77] Creating layer Scale12
I1023 12:10:12.910166  4594 net.cpp:84] Creating Layer Scale12
I1023 12:10:12.910167  4594 net.cpp:406] Scale12 <- Convolution12
I1023 12:10:12.910171  4594 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1023 12:10:12.910209  4594 layer_factory.hpp:77] Creating layer Scale12
I1023 12:10:12.910320  4594 net.cpp:122] Setting up Scale12
I1023 12:10:12.910326  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.910328  4594 net.cpp:137] Memory required for data: 1276477472
I1023 12:10:12.910342  4594 layer_factory.hpp:77] Creating layer Eltwise5
I1023 12:10:12.910347  4594 net.cpp:84] Creating Layer Eltwise5
I1023 12:10:12.910351  4594 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1023 12:10:12.910353  4594 net.cpp:406] Eltwise5 <- Convolution12
I1023 12:10:12.910357  4594 net.cpp:380] Eltwise5 -> Eltwise5
I1023 12:10:12.910387  4594 net.cpp:122] Setting up Eltwise5
I1023 12:10:12.910390  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.910392  4594 net.cpp:137] Memory required for data: 1289322528
I1023 12:10:12.910404  4594 layer_factory.hpp:77] Creating layer penlu11
I1023 12:10:12.910409  4594 net.cpp:84] Creating Layer penlu11
I1023 12:10:12.910413  4594 net.cpp:406] penlu11 <- Eltwise5
I1023 12:10:12.910418  4594 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1023 12:10:12.911276  4594 net.cpp:122] Setting up penlu11
I1023 12:10:12.911288  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.911299  4594 net.cpp:137] Memory required for data: 1302167584
I1023 12:10:12.911304  4594 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1023 12:10:12.911309  4594 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1023 12:10:12.911314  4594 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1023 12:10:12.911317  4594 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1023 12:10:12.911322  4594 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1023 12:10:12.911348  4594 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1023 12:10:12.911355  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.911357  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.911360  4594 net.cpp:137] Memory required for data: 1327857696
I1023 12:10:12.911363  4594 layer_factory.hpp:77] Creating layer Convolution13
I1023 12:10:12.911370  4594 net.cpp:84] Creating Layer Convolution13
I1023 12:10:12.911372  4594 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1023 12:10:12.911377  4594 net.cpp:380] Convolution13 -> Convolution13
I1023 12:10:12.912518  4594 net.cpp:122] Setting up Convolution13
I1023 12:10:12.912528  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.912533  4594 net.cpp:137] Memory required for data: 1340702752
I1023 12:10:12.912547  4594 layer_factory.hpp:77] Creating layer BatchNorm13
I1023 12:10:12.912554  4594 net.cpp:84] Creating Layer BatchNorm13
I1023 12:10:12.912557  4594 net.cpp:406] BatchNorm13 <- Convolution13
I1023 12:10:12.912561  4594 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1023 12:10:12.912708  4594 net.cpp:122] Setting up BatchNorm13
I1023 12:10:12.912713  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.912715  4594 net.cpp:137] Memory required for data: 1353547808
I1023 12:10:12.912721  4594 layer_factory.hpp:77] Creating layer Scale13
I1023 12:10:12.912726  4594 net.cpp:84] Creating Layer Scale13
I1023 12:10:12.912730  4594 net.cpp:406] Scale13 <- Convolution13
I1023 12:10:12.912734  4594 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1023 12:10:12.912765  4594 layer_factory.hpp:77] Creating layer Scale13
I1023 12:10:12.912856  4594 net.cpp:122] Setting up Scale13
I1023 12:10:12.912863  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.912865  4594 net.cpp:137] Memory required for data: 1366392864
I1023 12:10:12.912869  4594 layer_factory.hpp:77] Creating layer penlu12
I1023 12:10:12.912876  4594 net.cpp:84] Creating Layer penlu12
I1023 12:10:12.912879  4594 net.cpp:406] penlu12 <- Convolution13
I1023 12:10:12.912885  4594 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1023 12:10:12.913696  4594 net.cpp:122] Setting up penlu12
I1023 12:10:12.913705  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.913709  4594 net.cpp:137] Memory required for data: 1379237920
I1023 12:10:12.913714  4594 layer_factory.hpp:77] Creating layer Convolution14
I1023 12:10:12.913722  4594 net.cpp:84] Creating Layer Convolution14
I1023 12:10:12.913725  4594 net.cpp:406] Convolution14 <- Convolution13
I1023 12:10:12.913731  4594 net.cpp:380] Convolution14 -> Convolution14
I1023 12:10:12.914860  4594 net.cpp:122] Setting up Convolution14
I1023 12:10:12.914871  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.914875  4594 net.cpp:137] Memory required for data: 1392082976
I1023 12:10:12.914892  4594 layer_factory.hpp:77] Creating layer BatchNorm14
I1023 12:10:12.914902  4594 net.cpp:84] Creating Layer BatchNorm14
I1023 12:10:12.914906  4594 net.cpp:406] BatchNorm14 <- Convolution14
I1023 12:10:12.914911  4594 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1023 12:10:12.915057  4594 net.cpp:122] Setting up BatchNorm14
I1023 12:10:12.915063  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.915066  4594 net.cpp:137] Memory required for data: 1404928032
I1023 12:10:12.915072  4594 layer_factory.hpp:77] Creating layer Scale14
I1023 12:10:12.915077  4594 net.cpp:84] Creating Layer Scale14
I1023 12:10:12.915081  4594 net.cpp:406] Scale14 <- Convolution14
I1023 12:10:12.915083  4594 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1023 12:10:12.915112  4594 layer_factory.hpp:77] Creating layer Scale14
I1023 12:10:12.915206  4594 net.cpp:122] Setting up Scale14
I1023 12:10:12.915212  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.915215  4594 net.cpp:137] Memory required for data: 1417773088
I1023 12:10:12.915220  4594 layer_factory.hpp:77] Creating layer Eltwise6
I1023 12:10:12.915225  4594 net.cpp:84] Creating Layer Eltwise6
I1023 12:10:12.915227  4594 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1023 12:10:12.915230  4594 net.cpp:406] Eltwise6 <- Convolution14
I1023 12:10:12.915235  4594 net.cpp:380] Eltwise6 -> Eltwise6
I1023 12:10:12.915253  4594 net.cpp:122] Setting up Eltwise6
I1023 12:10:12.915258  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.915261  4594 net.cpp:137] Memory required for data: 1430618144
I1023 12:10:12.915263  4594 layer_factory.hpp:77] Creating layer penlu13
I1023 12:10:12.915269  4594 net.cpp:84] Creating Layer penlu13
I1023 12:10:12.915272  4594 net.cpp:406] penlu13 <- Eltwise6
I1023 12:10:12.915277  4594 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1023 12:10:12.916105  4594 net.cpp:122] Setting up penlu13
I1023 12:10:12.916115  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.916126  4594 net.cpp:137] Memory required for data: 1443463200
I1023 12:10:12.916133  4594 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1023 12:10:12.916139  4594 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1023 12:10:12.916142  4594 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1023 12:10:12.916146  4594 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1023 12:10:12.916152  4594 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1023 12:10:12.916180  4594 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1023 12:10:12.916185  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.916189  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.916193  4594 net.cpp:137] Memory required for data: 1469153312
I1023 12:10:12.916194  4594 layer_factory.hpp:77] Creating layer Convolution15
I1023 12:10:12.916201  4594 net.cpp:84] Creating Layer Convolution15
I1023 12:10:12.916205  4594 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1023 12:10:12.916210  4594 net.cpp:380] Convolution15 -> Convolution15
I1023 12:10:12.917179  4594 net.cpp:122] Setting up Convolution15
I1023 12:10:12.917189  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.917193  4594 net.cpp:137] Memory required for data: 1475575840
I1023 12:10:12.917198  4594 layer_factory.hpp:77] Creating layer BatchNorm15
I1023 12:10:12.917203  4594 net.cpp:84] Creating Layer BatchNorm15
I1023 12:10:12.917207  4594 net.cpp:406] BatchNorm15 <- Convolution15
I1023 12:10:12.917212  4594 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1023 12:10:12.917357  4594 net.cpp:122] Setting up BatchNorm15
I1023 12:10:12.917362  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.917366  4594 net.cpp:137] Memory required for data: 1481998368
I1023 12:10:12.917371  4594 layer_factory.hpp:77] Creating layer Scale15
I1023 12:10:12.917376  4594 net.cpp:84] Creating Layer Scale15
I1023 12:10:12.917379  4594 net.cpp:406] Scale15 <- Convolution15
I1023 12:10:12.917383  4594 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1023 12:10:12.917412  4594 layer_factory.hpp:77] Creating layer Scale15
I1023 12:10:12.917496  4594 net.cpp:122] Setting up Scale15
I1023 12:10:12.917502  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.917505  4594 net.cpp:137] Memory required for data: 1488420896
I1023 12:10:12.917510  4594 layer_factory.hpp:77] Creating layer Convolution16
I1023 12:10:12.917516  4594 net.cpp:84] Creating Layer Convolution16
I1023 12:10:12.917520  4594 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1023 12:10:12.917524  4594 net.cpp:380] Convolution16 -> Convolution16
I1023 12:10:12.918843  4594 net.cpp:122] Setting up Convolution16
I1023 12:10:12.918853  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.918857  4594 net.cpp:137] Memory required for data: 1494843424
I1023 12:10:12.918861  4594 layer_factory.hpp:77] Creating layer BatchNorm16
I1023 12:10:12.918869  4594 net.cpp:84] Creating Layer BatchNorm16
I1023 12:10:12.918872  4594 net.cpp:406] BatchNorm16 <- Convolution16
I1023 12:10:12.918876  4594 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1023 12:10:12.919021  4594 net.cpp:122] Setting up BatchNorm16
I1023 12:10:12.919026  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.919030  4594 net.cpp:137] Memory required for data: 1501265952
I1023 12:10:12.919035  4594 layer_factory.hpp:77] Creating layer Scale16
I1023 12:10:12.919040  4594 net.cpp:84] Creating Layer Scale16
I1023 12:10:12.919044  4594 net.cpp:406] Scale16 <- Convolution16
I1023 12:10:12.919047  4594 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1023 12:10:12.919076  4594 layer_factory.hpp:77] Creating layer Scale16
I1023 12:10:12.919162  4594 net.cpp:122] Setting up Scale16
I1023 12:10:12.919167  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.919170  4594 net.cpp:137] Memory required for data: 1507688480
I1023 12:10:12.919175  4594 layer_factory.hpp:77] Creating layer penlu14
I1023 12:10:12.919188  4594 net.cpp:84] Creating Layer penlu14
I1023 12:10:12.919191  4594 net.cpp:406] penlu14 <- Convolution16
I1023 12:10:12.919195  4594 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1023 12:10:12.919446  4594 net.cpp:122] Setting up penlu14
I1023 12:10:12.919452  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.919456  4594 net.cpp:137] Memory required for data: 1514111008
I1023 12:10:12.919461  4594 layer_factory.hpp:77] Creating layer Convolution17
I1023 12:10:12.919468  4594 net.cpp:84] Creating Layer Convolution17
I1023 12:10:12.919471  4594 net.cpp:406] Convolution17 <- Convolution16
I1023 12:10:12.919476  4594 net.cpp:380] Convolution17 -> Convolution17
I1023 12:10:12.920859  4594 net.cpp:122] Setting up Convolution17
I1023 12:10:12.920869  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.920872  4594 net.cpp:137] Memory required for data: 1520533536
I1023 12:10:12.920877  4594 layer_factory.hpp:77] Creating layer BatchNorm17
I1023 12:10:12.920882  4594 net.cpp:84] Creating Layer BatchNorm17
I1023 12:10:12.920886  4594 net.cpp:406] BatchNorm17 <- Convolution17
I1023 12:10:12.920889  4594 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1023 12:10:12.921034  4594 net.cpp:122] Setting up BatchNorm17
I1023 12:10:12.921039  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.921042  4594 net.cpp:137] Memory required for data: 1526956064
I1023 12:10:12.921047  4594 layer_factory.hpp:77] Creating layer Scale17
I1023 12:10:12.921051  4594 net.cpp:84] Creating Layer Scale17
I1023 12:10:12.921054  4594 net.cpp:406] Scale17 <- Convolution17
I1023 12:10:12.921057  4594 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1023 12:10:12.921097  4594 layer_factory.hpp:77] Creating layer Scale17
I1023 12:10:12.921200  4594 net.cpp:122] Setting up Scale17
I1023 12:10:12.921206  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.921218  4594 net.cpp:137] Memory required for data: 1533378592
I1023 12:10:12.921222  4594 layer_factory.hpp:77] Creating layer Eltwise7
I1023 12:10:12.921226  4594 net.cpp:84] Creating Layer Eltwise7
I1023 12:10:12.921231  4594 net.cpp:406] Eltwise7 <- Convolution15
I1023 12:10:12.921232  4594 net.cpp:406] Eltwise7 <- Convolution17
I1023 12:10:12.921236  4594 net.cpp:380] Eltwise7 -> Eltwise7
I1023 12:10:12.921253  4594 net.cpp:122] Setting up Eltwise7
I1023 12:10:12.921257  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.921260  4594 net.cpp:137] Memory required for data: 1539801120
I1023 12:10:12.921263  4594 layer_factory.hpp:77] Creating layer penlu15
I1023 12:10:12.921268  4594 net.cpp:84] Creating Layer penlu15
I1023 12:10:12.921272  4594 net.cpp:406] penlu15 <- Eltwise7
I1023 12:10:12.921277  4594 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1023 12:10:12.922017  4594 net.cpp:122] Setting up penlu15
I1023 12:10:12.922026  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.922029  4594 net.cpp:137] Memory required for data: 1546223648
I1023 12:10:12.922034  4594 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1023 12:10:12.922039  4594 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1023 12:10:12.922041  4594 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1023 12:10:12.922044  4594 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1023 12:10:12.922049  4594 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1023 12:10:12.922075  4594 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1023 12:10:12.922080  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.922083  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.922086  4594 net.cpp:137] Memory required for data: 1559068704
I1023 12:10:12.922087  4594 layer_factory.hpp:77] Creating layer Convolution18
I1023 12:10:12.922094  4594 net.cpp:84] Creating Layer Convolution18
I1023 12:10:12.922096  4594 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1023 12:10:12.922101  4594 net.cpp:380] Convolution18 -> Convolution18
I1023 12:10:12.924194  4594 net.cpp:122] Setting up Convolution18
I1023 12:10:12.924203  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.924206  4594 net.cpp:137] Memory required for data: 1565491232
I1023 12:10:12.924211  4594 layer_factory.hpp:77] Creating layer BatchNorm18
I1023 12:10:12.924216  4594 net.cpp:84] Creating Layer BatchNorm18
I1023 12:10:12.924219  4594 net.cpp:406] BatchNorm18 <- Convolution18
I1023 12:10:12.924223  4594 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1023 12:10:12.924372  4594 net.cpp:122] Setting up BatchNorm18
I1023 12:10:12.924377  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.924379  4594 net.cpp:137] Memory required for data: 1571913760
I1023 12:10:12.924384  4594 layer_factory.hpp:77] Creating layer Scale18
I1023 12:10:12.924388  4594 net.cpp:84] Creating Layer Scale18
I1023 12:10:12.924391  4594 net.cpp:406] Scale18 <- Convolution18
I1023 12:10:12.924396  4594 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1023 12:10:12.924423  4594 layer_factory.hpp:77] Creating layer Scale18
I1023 12:10:12.924511  4594 net.cpp:122] Setting up Scale18
I1023 12:10:12.924516  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.924518  4594 net.cpp:137] Memory required for data: 1578336288
I1023 12:10:12.924522  4594 layer_factory.hpp:77] Creating layer penlu16
I1023 12:10:12.924527  4594 net.cpp:84] Creating Layer penlu16
I1023 12:10:12.924530  4594 net.cpp:406] penlu16 <- Convolution18
I1023 12:10:12.924535  4594 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1023 12:10:12.924739  4594 net.cpp:122] Setting up penlu16
I1023 12:10:12.924744  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.924746  4594 net.cpp:137] Memory required for data: 1584758816
I1023 12:10:12.924751  4594 layer_factory.hpp:77] Creating layer Convolution19
I1023 12:10:12.924757  4594 net.cpp:84] Creating Layer Convolution19
I1023 12:10:12.924759  4594 net.cpp:406] Convolution19 <- Convolution18
I1023 12:10:12.924764  4594 net.cpp:380] Convolution19 -> Convolution19
I1023 12:10:12.927036  4594 net.cpp:122] Setting up Convolution19
I1023 12:10:12.927045  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.927047  4594 net.cpp:137] Memory required for data: 1591181344
I1023 12:10:12.927052  4594 layer_factory.hpp:77] Creating layer BatchNorm19
I1023 12:10:12.927058  4594 net.cpp:84] Creating Layer BatchNorm19
I1023 12:10:12.927062  4594 net.cpp:406] BatchNorm19 <- Convolution19
I1023 12:10:12.927065  4594 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1023 12:10:12.927211  4594 net.cpp:122] Setting up BatchNorm19
I1023 12:10:12.927215  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.927218  4594 net.cpp:137] Memory required for data: 1597603872
I1023 12:10:12.927223  4594 layer_factory.hpp:77] Creating layer Scale19
I1023 12:10:12.927227  4594 net.cpp:84] Creating Layer Scale19
I1023 12:10:12.927230  4594 net.cpp:406] Scale19 <- Convolution19
I1023 12:10:12.927233  4594 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1023 12:10:12.927263  4594 layer_factory.hpp:77] Creating layer Scale19
I1023 12:10:12.927348  4594 net.cpp:122] Setting up Scale19
I1023 12:10:12.927353  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.927356  4594 net.cpp:137] Memory required for data: 1604026400
I1023 12:10:12.927359  4594 layer_factory.hpp:77] Creating layer Eltwise8
I1023 12:10:12.927363  4594 net.cpp:84] Creating Layer Eltwise8
I1023 12:10:12.927366  4594 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1023 12:10:12.927369  4594 net.cpp:406] Eltwise8 <- Convolution19
I1023 12:10:12.927373  4594 net.cpp:380] Eltwise8 -> Eltwise8
I1023 12:10:12.927389  4594 net.cpp:122] Setting up Eltwise8
I1023 12:10:12.927393  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.927395  4594 net.cpp:137] Memory required for data: 1610448928
I1023 12:10:12.927397  4594 layer_factory.hpp:77] Creating layer penlu17
I1023 12:10:12.927402  4594 net.cpp:84] Creating Layer penlu17
I1023 12:10:12.927405  4594 net.cpp:406] penlu17 <- Eltwise8
I1023 12:10:12.927417  4594 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1023 12:10:12.928174  4594 net.cpp:122] Setting up penlu17
I1023 12:10:12.928182  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.928184  4594 net.cpp:137] Memory required for data: 1616871456
I1023 12:10:12.928190  4594 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1023 12:10:12.928194  4594 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1023 12:10:12.928196  4594 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1023 12:10:12.928200  4594 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1023 12:10:12.928205  4594 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1023 12:10:12.928232  4594 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1023 12:10:12.928236  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.928239  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.928241  4594 net.cpp:137] Memory required for data: 1629716512
I1023 12:10:12.928243  4594 layer_factory.hpp:77] Creating layer Convolution20
I1023 12:10:12.928249  4594 net.cpp:84] Creating Layer Convolution20
I1023 12:10:12.928252  4594 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1023 12:10:12.928257  4594 net.cpp:380] Convolution20 -> Convolution20
I1023 12:10:12.930644  4594 net.cpp:122] Setting up Convolution20
I1023 12:10:12.930652  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.930655  4594 net.cpp:137] Memory required for data: 1636139040
I1023 12:10:12.930660  4594 layer_factory.hpp:77] Creating layer BatchNorm20
I1023 12:10:12.930665  4594 net.cpp:84] Creating Layer BatchNorm20
I1023 12:10:12.930667  4594 net.cpp:406] BatchNorm20 <- Convolution20
I1023 12:10:12.930671  4594 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1023 12:10:12.930819  4594 net.cpp:122] Setting up BatchNorm20
I1023 12:10:12.930824  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.930826  4594 net.cpp:137] Memory required for data: 1642561568
I1023 12:10:12.930831  4594 layer_factory.hpp:77] Creating layer Scale20
I1023 12:10:12.930836  4594 net.cpp:84] Creating Layer Scale20
I1023 12:10:12.930838  4594 net.cpp:406] Scale20 <- Convolution20
I1023 12:10:12.930841  4594 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1023 12:10:12.930871  4594 layer_factory.hpp:77] Creating layer Scale20
I1023 12:10:12.930958  4594 net.cpp:122] Setting up Scale20
I1023 12:10:12.930961  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.930963  4594 net.cpp:137] Memory required for data: 1648984096
I1023 12:10:12.930968  4594 layer_factory.hpp:77] Creating layer penlu18
I1023 12:10:12.930974  4594 net.cpp:84] Creating Layer penlu18
I1023 12:10:12.930976  4594 net.cpp:406] penlu18 <- Convolution20
I1023 12:10:12.930979  4594 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1023 12:10:12.931179  4594 net.cpp:122] Setting up penlu18
I1023 12:10:12.931182  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.931185  4594 net.cpp:137] Memory required for data: 1655406624
I1023 12:10:12.931190  4594 layer_factory.hpp:77] Creating layer Convolution21
I1023 12:10:12.931196  4594 net.cpp:84] Creating Layer Convolution21
I1023 12:10:12.931198  4594 net.cpp:406] Convolution21 <- Convolution20
I1023 12:10:12.931202  4594 net.cpp:380] Convolution21 -> Convolution21
I1023 12:10:12.932941  4594 net.cpp:122] Setting up Convolution21
I1023 12:10:12.932950  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.932952  4594 net.cpp:137] Memory required for data: 1661829152
I1023 12:10:12.932957  4594 layer_factory.hpp:77] Creating layer BatchNorm21
I1023 12:10:12.932962  4594 net.cpp:84] Creating Layer BatchNorm21
I1023 12:10:12.932965  4594 net.cpp:406] BatchNorm21 <- Convolution21
I1023 12:10:12.932968  4594 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1023 12:10:12.933116  4594 net.cpp:122] Setting up BatchNorm21
I1023 12:10:12.933121  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.933123  4594 net.cpp:137] Memory required for data: 1668251680
I1023 12:10:12.933135  4594 layer_factory.hpp:77] Creating layer Scale21
I1023 12:10:12.933140  4594 net.cpp:84] Creating Layer Scale21
I1023 12:10:12.933142  4594 net.cpp:406] Scale21 <- Convolution21
I1023 12:10:12.933146  4594 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1023 12:10:12.933174  4594 layer_factory.hpp:77] Creating layer Scale21
I1023 12:10:12.933262  4594 net.cpp:122] Setting up Scale21
I1023 12:10:12.933266  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.933269  4594 net.cpp:137] Memory required for data: 1674674208
I1023 12:10:12.933272  4594 layer_factory.hpp:77] Creating layer Eltwise9
I1023 12:10:12.933276  4594 net.cpp:84] Creating Layer Eltwise9
I1023 12:10:12.933279  4594 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1023 12:10:12.933281  4594 net.cpp:406] Eltwise9 <- Convolution21
I1023 12:10:12.933285  4594 net.cpp:380] Eltwise9 -> Eltwise9
I1023 12:10:12.933303  4594 net.cpp:122] Setting up Eltwise9
I1023 12:10:12.933307  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.933310  4594 net.cpp:137] Memory required for data: 1681096736
I1023 12:10:12.933311  4594 layer_factory.hpp:77] Creating layer penlu19
I1023 12:10:12.933316  4594 net.cpp:84] Creating Layer penlu19
I1023 12:10:12.933318  4594 net.cpp:406] penlu19 <- Eltwise9
I1023 12:10:12.933322  4594 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1023 12:10:12.934051  4594 net.cpp:122] Setting up penlu19
I1023 12:10:12.934059  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.934062  4594 net.cpp:137] Memory required for data: 1687519264
I1023 12:10:12.934067  4594 layer_factory.hpp:77] Creating layer Pooling1
I1023 12:10:12.934073  4594 net.cpp:84] Creating Layer Pooling1
I1023 12:10:12.934075  4594 net.cpp:406] Pooling1 <- Eltwise9
I1023 12:10:12.934079  4594 net.cpp:380] Pooling1 -> Pooling1
I1023 12:10:12.934231  4594 net.cpp:122] Setting up Pooling1
I1023 12:10:12.934237  4594 net.cpp:129] Top shape: 8 64 1 1 (512)
I1023 12:10:12.934240  4594 net.cpp:137] Memory required for data: 1687521312
I1023 12:10:12.934242  4594 layer_factory.hpp:77] Creating layer InnerProduct1
I1023 12:10:12.934250  4594 net.cpp:84] Creating Layer InnerProduct1
I1023 12:10:12.934253  4594 net.cpp:406] InnerProduct1 <- Pooling1
I1023 12:10:12.934257  4594 net.cpp:380] InnerProduct1 -> InnerProduct1
I1023 12:10:12.934362  4594 net.cpp:122] Setting up InnerProduct1
I1023 12:10:12.934366  4594 net.cpp:129] Top shape: 8 10 (80)
I1023 12:10:12.934368  4594 net.cpp:137] Memory required for data: 1687521632
I1023 12:10:12.934372  4594 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1023 12:10:12.934378  4594 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1023 12:10:12.934381  4594 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1023 12:10:12.934383  4594 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1023 12:10:12.934387  4594 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1023 12:10:12.934392  4594 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1023 12:10:12.934938  4594 net.cpp:122] Setting up SoftmaxWithLoss1
I1023 12:10:12.934947  4594 net.cpp:129] Top shape: (1)
I1023 12:10:12.934950  4594 net.cpp:132]     with loss weight 1
I1023 12:10:12.934962  4594 net.cpp:137] Memory required for data: 1687521636
I1023 12:10:12.934965  4594 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1023 12:10:12.934967  4594 net.cpp:198] InnerProduct1 needs backward computation.
I1023 12:10:12.934969  4594 net.cpp:198] Pooling1 needs backward computation.
I1023 12:10:12.934972  4594 net.cpp:198] penlu19 needs backward computation.
I1023 12:10:12.934973  4594 net.cpp:198] Eltwise9 needs backward computation.
I1023 12:10:12.934976  4594 net.cpp:198] Scale21 needs backward computation.
I1023 12:10:12.934978  4594 net.cpp:198] BatchNorm21 needs backward computation.
I1023 12:10:12.934980  4594 net.cpp:198] Convolution21 needs backward computation.
I1023 12:10:12.934983  4594 net.cpp:198] penlu18 needs backward computation.
I1023 12:10:12.934984  4594 net.cpp:198] Scale20 needs backward computation.
I1023 12:10:12.934993  4594 net.cpp:198] BatchNorm20 needs backward computation.
I1023 12:10:12.934995  4594 net.cpp:198] Convolution20 needs backward computation.
I1023 12:10:12.934998  4594 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1023 12:10:12.934999  4594 net.cpp:198] penlu17 needs backward computation.
I1023 12:10:12.935001  4594 net.cpp:198] Eltwise8 needs backward computation.
I1023 12:10:12.935003  4594 net.cpp:198] Scale19 needs backward computation.
I1023 12:10:12.935006  4594 net.cpp:198] BatchNorm19 needs backward computation.
I1023 12:10:12.935009  4594 net.cpp:198] Convolution19 needs backward computation.
I1023 12:10:12.935010  4594 net.cpp:198] penlu16 needs backward computation.
I1023 12:10:12.935012  4594 net.cpp:198] Scale18 needs backward computation.
I1023 12:10:12.935014  4594 net.cpp:198] BatchNorm18 needs backward computation.
I1023 12:10:12.935016  4594 net.cpp:198] Convolution18 needs backward computation.
I1023 12:10:12.935019  4594 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1023 12:10:12.935021  4594 net.cpp:198] penlu15 needs backward computation.
I1023 12:10:12.935024  4594 net.cpp:198] Eltwise7 needs backward computation.
I1023 12:10:12.935026  4594 net.cpp:198] Scale17 needs backward computation.
I1023 12:10:12.935029  4594 net.cpp:198] BatchNorm17 needs backward computation.
I1023 12:10:12.935030  4594 net.cpp:198] Convolution17 needs backward computation.
I1023 12:10:12.935032  4594 net.cpp:198] penlu14 needs backward computation.
I1023 12:10:12.935035  4594 net.cpp:198] Scale16 needs backward computation.
I1023 12:10:12.935037  4594 net.cpp:198] BatchNorm16 needs backward computation.
I1023 12:10:12.935039  4594 net.cpp:198] Convolution16 needs backward computation.
I1023 12:10:12.935041  4594 net.cpp:198] Scale15 needs backward computation.
I1023 12:10:12.935044  4594 net.cpp:198] BatchNorm15 needs backward computation.
I1023 12:10:12.935046  4594 net.cpp:198] Convolution15 needs backward computation.
I1023 12:10:12.935048  4594 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1023 12:10:12.935050  4594 net.cpp:198] penlu13 needs backward computation.
I1023 12:10:12.935052  4594 net.cpp:198] Eltwise6 needs backward computation.
I1023 12:10:12.935055  4594 net.cpp:198] Scale14 needs backward computation.
I1023 12:10:12.935057  4594 net.cpp:198] BatchNorm14 needs backward computation.
I1023 12:10:12.935060  4594 net.cpp:198] Convolution14 needs backward computation.
I1023 12:10:12.935061  4594 net.cpp:198] penlu12 needs backward computation.
I1023 12:10:12.935065  4594 net.cpp:198] Scale13 needs backward computation.
I1023 12:10:12.935066  4594 net.cpp:198] BatchNorm13 needs backward computation.
I1023 12:10:12.935068  4594 net.cpp:198] Convolution13 needs backward computation.
I1023 12:10:12.935071  4594 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1023 12:10:12.935073  4594 net.cpp:198] penlu11 needs backward computation.
I1023 12:10:12.935075  4594 net.cpp:198] Eltwise5 needs backward computation.
I1023 12:10:12.935078  4594 net.cpp:198] Scale12 needs backward computation.
I1023 12:10:12.935081  4594 net.cpp:198] BatchNorm12 needs backward computation.
I1023 12:10:12.935082  4594 net.cpp:198] Convolution12 needs backward computation.
I1023 12:10:12.935084  4594 net.cpp:198] penlu10 needs backward computation.
I1023 12:10:12.935086  4594 net.cpp:198] Scale11 needs backward computation.
I1023 12:10:12.935089  4594 net.cpp:198] BatchNorm11 needs backward computation.
I1023 12:10:12.935091  4594 net.cpp:198] Convolution11 needs backward computation.
I1023 12:10:12.935093  4594 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1023 12:10:12.935096  4594 net.cpp:198] penlu9 needs backward computation.
I1023 12:10:12.935098  4594 net.cpp:198] Eltwise4 needs backward computation.
I1023 12:10:12.935101  4594 net.cpp:198] Scale10 needs backward computation.
I1023 12:10:12.935103  4594 net.cpp:198] BatchNorm10 needs backward computation.
I1023 12:10:12.935106  4594 net.cpp:198] Convolution10 needs backward computation.
I1023 12:10:12.935111  4594 net.cpp:198] penlu8 needs backward computation.
I1023 12:10:12.935113  4594 net.cpp:198] Scale9 needs backward computation.
I1023 12:10:12.935115  4594 net.cpp:198] BatchNorm9 needs backward computation.
I1023 12:10:12.935117  4594 net.cpp:198] Convolution9 needs backward computation.
I1023 12:10:12.935120  4594 net.cpp:198] Scale8 needs backward computation.
I1023 12:10:12.935122  4594 net.cpp:198] BatchNorm8 needs backward computation.
I1023 12:10:12.935124  4594 net.cpp:198] Convolution8 needs backward computation.
I1023 12:10:12.935127  4594 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1023 12:10:12.935129  4594 net.cpp:198] penlu7 needs backward computation.
I1023 12:10:12.935132  4594 net.cpp:198] Eltwise3 needs backward computation.
I1023 12:10:12.935134  4594 net.cpp:198] Scale7 needs backward computation.
I1023 12:10:12.935137  4594 net.cpp:198] BatchNorm7 needs backward computation.
I1023 12:10:12.935139  4594 net.cpp:198] Convolution7 needs backward computation.
I1023 12:10:12.935142  4594 net.cpp:198] penlu6 needs backward computation.
I1023 12:10:12.935143  4594 net.cpp:198] Scale6 needs backward computation.
I1023 12:10:12.935145  4594 net.cpp:198] BatchNorm6 needs backward computation.
I1023 12:10:12.935148  4594 net.cpp:198] Convolution6 needs backward computation.
I1023 12:10:12.935150  4594 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1023 12:10:12.935153  4594 net.cpp:198] penlu5 needs backward computation.
I1023 12:10:12.935154  4594 net.cpp:198] Eltwise2 needs backward computation.
I1023 12:10:12.935158  4594 net.cpp:198] Scale5 needs backward computation.
I1023 12:10:12.935159  4594 net.cpp:198] BatchNorm5 needs backward computation.
I1023 12:10:12.935161  4594 net.cpp:198] Convolution5 needs backward computation.
I1023 12:10:12.935163  4594 net.cpp:198] penlu4 needs backward computation.
I1023 12:10:12.935166  4594 net.cpp:198] Scale4 needs backward computation.
I1023 12:10:12.935168  4594 net.cpp:198] BatchNorm4 needs backward computation.
I1023 12:10:12.935170  4594 net.cpp:198] Convolution4 needs backward computation.
I1023 12:10:12.935173  4594 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1023 12:10:12.935175  4594 net.cpp:198] penlu3 needs backward computation.
I1023 12:10:12.935178  4594 net.cpp:198] Eltwise1 needs backward computation.
I1023 12:10:12.935180  4594 net.cpp:198] Scale3 needs backward computation.
I1023 12:10:12.935183  4594 net.cpp:198] BatchNorm3 needs backward computation.
I1023 12:10:12.935184  4594 net.cpp:198] Convolution3 needs backward computation.
I1023 12:10:12.935186  4594 net.cpp:198] penlu2 needs backward computation.
I1023 12:10:12.935189  4594 net.cpp:198] Scale2 needs backward computation.
I1023 12:10:12.935191  4594 net.cpp:198] BatchNorm2 needs backward computation.
I1023 12:10:12.935194  4594 net.cpp:198] Convolution2 needs backward computation.
I1023 12:10:12.935195  4594 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1023 12:10:12.935199  4594 net.cpp:198] penlu1 needs backward computation.
I1023 12:10:12.935200  4594 net.cpp:198] Scale1 needs backward computation.
I1023 12:10:12.935202  4594 net.cpp:198] BatchNorm1 needs backward computation.
I1023 12:10:12.935204  4594 net.cpp:198] Convolution1 needs backward computation.
I1023 12:10:12.935207  4594 net.cpp:200] Data1 does not need backward computation.
I1023 12:10:12.935209  4594 net.cpp:242] This network produces output SoftmaxWithLoss1
I1023 12:10:12.935241  4594 net.cpp:255] Network initialization done.
I1023 12:10:12.936996  4594 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1023 12:10:12.937005  4594 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1023 12:10:12.937008  4594 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1023 12:10:12.937089  4594 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I1023 12:10:12.937558  4594 net.cpp:51] Initializing net from parameters: 
name: "resnet"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_file: "/home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/English_orange/data/val1_lmdb"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I1023 12:10:12.937808  4594 layer_factory.hpp:77] Creating layer Data1
I1023 12:10:12.937845  4594 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/English_orange/data/val1_lmdb
I1023 12:10:12.937856  4594 net.cpp:84] Creating Layer Data1
I1023 12:10:12.937860  4594 net.cpp:380] Data1 -> Data1
I1023 12:10:12.937866  4594 net.cpp:380] Data1 -> Data2
I1023 12:10:12.937871  4594 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto
I1023 12:10:12.939059  4594 data_layer.cpp:45] output data size: 8,3,224,224
I1023 12:10:12.947890  4594 net.cpp:122] Setting up Data1
I1023 12:10:12.947909  4594 net.cpp:129] Top shape: 8 3 224 224 (1204224)
I1023 12:10:12.947913  4594 net.cpp:129] Top shape: 8 (8)
I1023 12:10:12.947916  4594 net.cpp:137] Memory required for data: 4816928
I1023 12:10:12.947921  4594 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1023 12:10:12.947930  4594 net.cpp:84] Creating Layer Data2_Data1_1_split
I1023 12:10:12.947933  4594 net.cpp:406] Data2_Data1_1_split <- Data2
I1023 12:10:12.947952  4594 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1023 12:10:12.947965  4594 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1023 12:10:12.948047  4594 net.cpp:122] Setting up Data2_Data1_1_split
I1023 12:10:12.948053  4594 net.cpp:129] Top shape: 8 (8)
I1023 12:10:12.948056  4594 net.cpp:129] Top shape: 8 (8)
I1023 12:10:12.948058  4594 net.cpp:137] Memory required for data: 4816992
I1023 12:10:12.948060  4594 layer_factory.hpp:77] Creating layer Convolution1
I1023 12:10:12.948070  4594 net.cpp:84] Creating Layer Convolution1
I1023 12:10:12.948071  4594 net.cpp:406] Convolution1 <- Data1
I1023 12:10:12.948076  4594 net.cpp:380] Convolution1 -> Convolution1
I1023 12:10:12.949520  4594 net.cpp:122] Setting up Convolution1
I1023 12:10:12.949529  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.949532  4594 net.cpp:137] Memory required for data: 30507104
I1023 12:10:12.949539  4594 layer_factory.hpp:77] Creating layer BatchNorm1
I1023 12:10:12.949545  4594 net.cpp:84] Creating Layer BatchNorm1
I1023 12:10:12.949548  4594 net.cpp:406] BatchNorm1 <- Convolution1
I1023 12:10:12.949550  4594 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1023 12:10:12.949725  4594 net.cpp:122] Setting up BatchNorm1
I1023 12:10:12.949729  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.949733  4594 net.cpp:137] Memory required for data: 56197216
I1023 12:10:12.949739  4594 layer_factory.hpp:77] Creating layer Scale1
I1023 12:10:12.949745  4594 net.cpp:84] Creating Layer Scale1
I1023 12:10:12.949748  4594 net.cpp:406] Scale1 <- Convolution1
I1023 12:10:12.949750  4594 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1023 12:10:12.949779  4594 layer_factory.hpp:77] Creating layer Scale1
I1023 12:10:12.949918  4594 net.cpp:122] Setting up Scale1
I1023 12:10:12.949923  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.949924  4594 net.cpp:137] Memory required for data: 81887328
I1023 12:10:12.949928  4594 layer_factory.hpp:77] Creating layer penlu1
I1023 12:10:12.949934  4594 net.cpp:84] Creating Layer penlu1
I1023 12:10:12.949936  4594 net.cpp:406] penlu1 <- Convolution1
I1023 12:10:12.949939  4594 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1023 12:10:12.951153  4594 net.cpp:122] Setting up penlu1
I1023 12:10:12.951164  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.951166  4594 net.cpp:137] Memory required for data: 107577440
I1023 12:10:12.951174  4594 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1023 12:10:12.951179  4594 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1023 12:10:12.951182  4594 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1023 12:10:12.951185  4594 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1023 12:10:12.951191  4594 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1023 12:10:12.951220  4594 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1023 12:10:12.951223  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.951226  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.951228  4594 net.cpp:137] Memory required for data: 158957664
I1023 12:10:12.951231  4594 layer_factory.hpp:77] Creating layer Convolution2
I1023 12:10:12.951237  4594 net.cpp:84] Creating Layer Convolution2
I1023 12:10:12.951241  4594 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1023 12:10:12.951243  4594 net.cpp:380] Convolution2 -> Convolution2
I1023 12:10:12.951913  4594 net.cpp:122] Setting up Convolution2
I1023 12:10:12.951920  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.951923  4594 net.cpp:137] Memory required for data: 184647776
I1023 12:10:12.951927  4594 layer_factory.hpp:77] Creating layer BatchNorm2
I1023 12:10:12.951933  4594 net.cpp:84] Creating Layer BatchNorm2
I1023 12:10:12.951936  4594 net.cpp:406] BatchNorm2 <- Convolution2
I1023 12:10:12.951939  4594 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1023 12:10:12.952136  4594 net.cpp:122] Setting up BatchNorm2
I1023 12:10:12.952157  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.952160  4594 net.cpp:137] Memory required for data: 210337888
I1023 12:10:12.952165  4594 layer_factory.hpp:77] Creating layer Scale2
I1023 12:10:12.952170  4594 net.cpp:84] Creating Layer Scale2
I1023 12:10:12.952172  4594 net.cpp:406] Scale2 <- Convolution2
I1023 12:10:12.952177  4594 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1023 12:10:12.952209  4594 layer_factory.hpp:77] Creating layer Scale2
I1023 12:10:12.952356  4594 net.cpp:122] Setting up Scale2
I1023 12:10:12.952361  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.952363  4594 net.cpp:137] Memory required for data: 236028000
I1023 12:10:12.952369  4594 layer_factory.hpp:77] Creating layer penlu2
I1023 12:10:12.952374  4594 net.cpp:84] Creating Layer penlu2
I1023 12:10:12.952388  4594 net.cpp:406] penlu2 <- Convolution2
I1023 12:10:12.952390  4594 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1023 12:10:12.953737  4594 net.cpp:122] Setting up penlu2
I1023 12:10:12.953749  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.953752  4594 net.cpp:137] Memory required for data: 261718112
I1023 12:10:12.953758  4594 layer_factory.hpp:77] Creating layer Convolution3
I1023 12:10:12.953768  4594 net.cpp:84] Creating Layer Convolution3
I1023 12:10:12.953771  4594 net.cpp:406] Convolution3 <- Convolution2
I1023 12:10:12.953776  4594 net.cpp:380] Convolution3 -> Convolution3
I1023 12:10:12.955801  4594 net.cpp:122] Setting up Convolution3
I1023 12:10:12.955816  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.955818  4594 net.cpp:137] Memory required for data: 287408224
I1023 12:10:12.955824  4594 layer_factory.hpp:77] Creating layer BatchNorm3
I1023 12:10:12.955832  4594 net.cpp:84] Creating Layer BatchNorm3
I1023 12:10:12.955835  4594 net.cpp:406] BatchNorm3 <- Convolution3
I1023 12:10:12.955840  4594 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1023 12:10:12.956045  4594 net.cpp:122] Setting up BatchNorm3
I1023 12:10:12.956051  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.956054  4594 net.cpp:137] Memory required for data: 313098336
I1023 12:10:12.956059  4594 layer_factory.hpp:77] Creating layer Scale3
I1023 12:10:12.956064  4594 net.cpp:84] Creating Layer Scale3
I1023 12:10:12.956066  4594 net.cpp:406] Scale3 <- Convolution3
I1023 12:10:12.956070  4594 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1023 12:10:12.956104  4594 layer_factory.hpp:77] Creating layer Scale3
I1023 12:10:12.956233  4594 net.cpp:122] Setting up Scale3
I1023 12:10:12.956238  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.956239  4594 net.cpp:137] Memory required for data: 338788448
I1023 12:10:12.956243  4594 layer_factory.hpp:77] Creating layer Eltwise1
I1023 12:10:12.956248  4594 net.cpp:84] Creating Layer Eltwise1
I1023 12:10:12.956250  4594 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1023 12:10:12.956254  4594 net.cpp:406] Eltwise1 <- Convolution3
I1023 12:10:12.956256  4594 net.cpp:380] Eltwise1 -> Eltwise1
I1023 12:10:12.956275  4594 net.cpp:122] Setting up Eltwise1
I1023 12:10:12.956279  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.956281  4594 net.cpp:137] Memory required for data: 364478560
I1023 12:10:12.956284  4594 layer_factory.hpp:77] Creating layer penlu3
I1023 12:10:12.956290  4594 net.cpp:84] Creating Layer penlu3
I1023 12:10:12.956291  4594 net.cpp:406] penlu3 <- Eltwise1
I1023 12:10:12.956295  4594 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1023 12:10:12.957548  4594 net.cpp:122] Setting up penlu3
I1023 12:10:12.957561  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.957562  4594 net.cpp:137] Memory required for data: 390168672
I1023 12:10:12.957568  4594 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1023 12:10:12.957574  4594 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1023 12:10:12.957577  4594 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1023 12:10:12.957581  4594 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1023 12:10:12.957602  4594 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1023 12:10:12.957633  4594 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1023 12:10:12.957638  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.957640  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.957643  4594 net.cpp:137] Memory required for data: 441548896
I1023 12:10:12.957644  4594 layer_factory.hpp:77] Creating layer Convolution4
I1023 12:10:12.957651  4594 net.cpp:84] Creating Layer Convolution4
I1023 12:10:12.957654  4594 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1023 12:10:12.957659  4594 net.cpp:380] Convolution4 -> Convolution4
I1023 12:10:12.958817  4594 net.cpp:122] Setting up Convolution4
I1023 12:10:12.958825  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.958828  4594 net.cpp:137] Memory required for data: 467239008
I1023 12:10:12.958832  4594 layer_factory.hpp:77] Creating layer BatchNorm4
I1023 12:10:12.958838  4594 net.cpp:84] Creating Layer BatchNorm4
I1023 12:10:12.958840  4594 net.cpp:406] BatchNorm4 <- Convolution4
I1023 12:10:12.958844  4594 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1023 12:10:12.959414  4594 net.cpp:122] Setting up BatchNorm4
I1023 12:10:12.959419  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.959420  4594 net.cpp:137] Memory required for data: 492929120
I1023 12:10:12.959429  4594 layer_factory.hpp:77] Creating layer Scale4
I1023 12:10:12.959434  4594 net.cpp:84] Creating Layer Scale4
I1023 12:10:12.959436  4594 net.cpp:406] Scale4 <- Convolution4
I1023 12:10:12.959439  4594 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1023 12:10:12.959471  4594 layer_factory.hpp:77] Creating layer Scale4
I1023 12:10:12.959619  4594 net.cpp:122] Setting up Scale4
I1023 12:10:12.959623  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.959625  4594 net.cpp:137] Memory required for data: 518619232
I1023 12:10:12.959630  4594 layer_factory.hpp:77] Creating layer penlu4
I1023 12:10:12.959635  4594 net.cpp:84] Creating Layer penlu4
I1023 12:10:12.959637  4594 net.cpp:406] penlu4 <- Convolution4
I1023 12:10:12.959640  4594 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1023 12:10:12.960912  4594 net.cpp:122] Setting up penlu4
I1023 12:10:12.960923  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.960925  4594 net.cpp:137] Memory required for data: 544309344
I1023 12:10:12.960932  4594 layer_factory.hpp:77] Creating layer Convolution5
I1023 12:10:12.960939  4594 net.cpp:84] Creating Layer Convolution5
I1023 12:10:12.960942  4594 net.cpp:406] Convolution5 <- Convolution4
I1023 12:10:12.960947  4594 net.cpp:380] Convolution5 -> Convolution5
I1023 12:10:12.962391  4594 net.cpp:122] Setting up Convolution5
I1023 12:10:12.962401  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.962404  4594 net.cpp:137] Memory required for data: 569999456
I1023 12:10:12.962409  4594 layer_factory.hpp:77] Creating layer BatchNorm5
I1023 12:10:12.962414  4594 net.cpp:84] Creating Layer BatchNorm5
I1023 12:10:12.962417  4594 net.cpp:406] BatchNorm5 <- Convolution5
I1023 12:10:12.962421  4594 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1023 12:10:12.962620  4594 net.cpp:122] Setting up BatchNorm5
I1023 12:10:12.962625  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.962627  4594 net.cpp:137] Memory required for data: 595689568
I1023 12:10:12.962631  4594 layer_factory.hpp:77] Creating layer Scale5
I1023 12:10:12.962637  4594 net.cpp:84] Creating Layer Scale5
I1023 12:10:12.962640  4594 net.cpp:406] Scale5 <- Convolution5
I1023 12:10:12.962642  4594 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1023 12:10:12.962676  4594 layer_factory.hpp:77] Creating layer Scale5
I1023 12:10:12.963359  4594 net.cpp:122] Setting up Scale5
I1023 12:10:12.963367  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.963371  4594 net.cpp:137] Memory required for data: 621379680
I1023 12:10:12.963376  4594 layer_factory.hpp:77] Creating layer Eltwise2
I1023 12:10:12.963397  4594 net.cpp:84] Creating Layer Eltwise2
I1023 12:10:12.963400  4594 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1023 12:10:12.963403  4594 net.cpp:406] Eltwise2 <- Convolution5
I1023 12:10:12.963407  4594 net.cpp:380] Eltwise2 -> Eltwise2
I1023 12:10:12.963428  4594 net.cpp:122] Setting up Eltwise2
I1023 12:10:12.963433  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.963434  4594 net.cpp:137] Memory required for data: 647069792
I1023 12:10:12.963436  4594 layer_factory.hpp:77] Creating layer penlu5
I1023 12:10:12.963443  4594 net.cpp:84] Creating Layer penlu5
I1023 12:10:12.963445  4594 net.cpp:406] penlu5 <- Eltwise2
I1023 12:10:12.963449  4594 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1023 12:10:12.964778  4594 net.cpp:122] Setting up penlu5
I1023 12:10:12.964787  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.964790  4594 net.cpp:137] Memory required for data: 672759904
I1023 12:10:12.964795  4594 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1023 12:10:12.964800  4594 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1023 12:10:12.964802  4594 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1023 12:10:12.964807  4594 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1023 12:10:12.964810  4594 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1023 12:10:12.964840  4594 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1023 12:10:12.964844  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.964848  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.964850  4594 net.cpp:137] Memory required for data: 724140128
I1023 12:10:12.964853  4594 layer_factory.hpp:77] Creating layer Convolution6
I1023 12:10:12.964859  4594 net.cpp:84] Creating Layer Convolution6
I1023 12:10:12.964861  4594 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1023 12:10:12.964865  4594 net.cpp:380] Convolution6 -> Convolution6
I1023 12:10:12.966048  4594 net.cpp:122] Setting up Convolution6
I1023 12:10:12.966058  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.966060  4594 net.cpp:137] Memory required for data: 749830240
I1023 12:10:12.966065  4594 layer_factory.hpp:77] Creating layer BatchNorm6
I1023 12:10:12.966070  4594 net.cpp:84] Creating Layer BatchNorm6
I1023 12:10:12.966073  4594 net.cpp:406] BatchNorm6 <- Convolution6
I1023 12:10:12.966078  4594 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1023 12:10:12.966264  4594 net.cpp:122] Setting up BatchNorm6
I1023 12:10:12.966269  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.966271  4594 net.cpp:137] Memory required for data: 775520352
I1023 12:10:12.966276  4594 layer_factory.hpp:77] Creating layer Scale6
I1023 12:10:12.966281  4594 net.cpp:84] Creating Layer Scale6
I1023 12:10:12.966284  4594 net.cpp:406] Scale6 <- Convolution6
I1023 12:10:12.966286  4594 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1023 12:10:12.966320  4594 layer_factory.hpp:77] Creating layer Scale6
I1023 12:10:12.966467  4594 net.cpp:122] Setting up Scale6
I1023 12:10:12.966472  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.966475  4594 net.cpp:137] Memory required for data: 801210464
I1023 12:10:12.966477  4594 layer_factory.hpp:77] Creating layer penlu6
I1023 12:10:12.966483  4594 net.cpp:84] Creating Layer penlu6
I1023 12:10:12.966486  4594 net.cpp:406] penlu6 <- Convolution6
I1023 12:10:12.966490  4594 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1023 12:10:12.967756  4594 net.cpp:122] Setting up penlu6
I1023 12:10:12.967767  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.967770  4594 net.cpp:137] Memory required for data: 826900576
I1023 12:10:12.967777  4594 layer_factory.hpp:77] Creating layer Convolution7
I1023 12:10:12.967784  4594 net.cpp:84] Creating Layer Convolution7
I1023 12:10:12.967787  4594 net.cpp:406] Convolution7 <- Convolution6
I1023 12:10:12.967792  4594 net.cpp:380] Convolution7 -> Convolution7
I1023 12:10:12.969327  4594 net.cpp:122] Setting up Convolution7
I1023 12:10:12.969349  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.969352  4594 net.cpp:137] Memory required for data: 852590688
I1023 12:10:12.969357  4594 layer_factory.hpp:77] Creating layer BatchNorm7
I1023 12:10:12.969367  4594 net.cpp:84] Creating Layer BatchNorm7
I1023 12:10:12.969369  4594 net.cpp:406] BatchNorm7 <- Convolution7
I1023 12:10:12.969373  4594 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1023 12:10:12.969604  4594 net.cpp:122] Setting up BatchNorm7
I1023 12:10:12.969609  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.969610  4594 net.cpp:137] Memory required for data: 878280800
I1023 12:10:12.969624  4594 layer_factory.hpp:77] Creating layer Scale7
I1023 12:10:12.969630  4594 net.cpp:84] Creating Layer Scale7
I1023 12:10:12.969633  4594 net.cpp:406] Scale7 <- Convolution7
I1023 12:10:12.969636  4594 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1023 12:10:12.969674  4594 layer_factory.hpp:77] Creating layer Scale7
I1023 12:10:12.969893  4594 net.cpp:122] Setting up Scale7
I1023 12:10:12.969897  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.969899  4594 net.cpp:137] Memory required for data: 903970912
I1023 12:10:12.969913  4594 layer_factory.hpp:77] Creating layer Eltwise3
I1023 12:10:12.969918  4594 net.cpp:84] Creating Layer Eltwise3
I1023 12:10:12.969920  4594 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1023 12:10:12.969923  4594 net.cpp:406] Eltwise3 <- Convolution7
I1023 12:10:12.969938  4594 net.cpp:380] Eltwise3 -> Eltwise3
I1023 12:10:12.969957  4594 net.cpp:122] Setting up Eltwise3
I1023 12:10:12.969960  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.969964  4594 net.cpp:137] Memory required for data: 929661024
I1023 12:10:12.969965  4594 layer_factory.hpp:77] Creating layer penlu7
I1023 12:10:12.969970  4594 net.cpp:84] Creating Layer penlu7
I1023 12:10:12.969974  4594 net.cpp:406] penlu7 <- Eltwise3
I1023 12:10:12.969976  4594 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1023 12:10:12.971249  4594 net.cpp:122] Setting up penlu7
I1023 12:10:12.971261  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.971264  4594 net.cpp:137] Memory required for data: 955351136
I1023 12:10:12.971271  4594 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1023 12:10:12.971276  4594 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1023 12:10:12.971278  4594 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1023 12:10:12.971282  4594 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1023 12:10:12.971287  4594 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1023 12:10:12.971318  4594 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1023 12:10:12.971323  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.971325  4594 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1023 12:10:12.971328  4594 net.cpp:137] Memory required for data: 1006731360
I1023 12:10:12.971329  4594 layer_factory.hpp:77] Creating layer Convolution8
I1023 12:10:12.971338  4594 net.cpp:84] Creating Layer Convolution8
I1023 12:10:12.971339  4594 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1023 12:10:12.971344  4594 net.cpp:380] Convolution8 -> Convolution8
I1023 12:10:12.972540  4594 net.cpp:122] Setting up Convolution8
I1023 12:10:12.972549  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.972553  4594 net.cpp:137] Memory required for data: 1019576416
I1023 12:10:12.972558  4594 layer_factory.hpp:77] Creating layer BatchNorm8
I1023 12:10:12.972563  4594 net.cpp:84] Creating Layer BatchNorm8
I1023 12:10:12.972566  4594 net.cpp:406] BatchNorm8 <- Convolution8
I1023 12:10:12.972570  4594 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1023 12:10:12.972738  4594 net.cpp:122] Setting up BatchNorm8
I1023 12:10:12.972743  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.972744  4594 net.cpp:137] Memory required for data: 1032421472
I1023 12:10:12.972749  4594 layer_factory.hpp:77] Creating layer Scale8
I1023 12:10:12.972764  4594 net.cpp:84] Creating Layer Scale8
I1023 12:10:12.972766  4594 net.cpp:406] Scale8 <- Convolution8
I1023 12:10:12.972769  4594 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1023 12:10:12.972807  4594 layer_factory.hpp:77] Creating layer Scale8
I1023 12:10:12.972909  4594 net.cpp:122] Setting up Scale8
I1023 12:10:12.972914  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.972916  4594 net.cpp:137] Memory required for data: 1045266528
I1023 12:10:12.972920  4594 layer_factory.hpp:77] Creating layer Convolution9
I1023 12:10:12.972929  4594 net.cpp:84] Creating Layer Convolution9
I1023 12:10:12.972931  4594 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1023 12:10:12.972935  4594 net.cpp:380] Convolution9 -> Convolution9
I1023 12:10:12.974035  4594 net.cpp:122] Setting up Convolution9
I1023 12:10:12.974045  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.974047  4594 net.cpp:137] Memory required for data: 1058111584
I1023 12:10:12.974051  4594 layer_factory.hpp:77] Creating layer BatchNorm9
I1023 12:10:12.974056  4594 net.cpp:84] Creating Layer BatchNorm9
I1023 12:10:12.974059  4594 net.cpp:406] BatchNorm9 <- Convolution9
I1023 12:10:12.974064  4594 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1023 12:10:12.974225  4594 net.cpp:122] Setting up BatchNorm9
I1023 12:10:12.974231  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.974232  4594 net.cpp:137] Memory required for data: 1070956640
I1023 12:10:12.974237  4594 layer_factory.hpp:77] Creating layer Scale9
I1023 12:10:12.974242  4594 net.cpp:84] Creating Layer Scale9
I1023 12:10:12.974244  4594 net.cpp:406] Scale9 <- Convolution9
I1023 12:10:12.974248  4594 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1023 12:10:12.974278  4594 layer_factory.hpp:77] Creating layer Scale9
I1023 12:10:12.974380  4594 net.cpp:122] Setting up Scale9
I1023 12:10:12.974383  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.974385  4594 net.cpp:137] Memory required for data: 1083801696
I1023 12:10:12.974390  4594 layer_factory.hpp:77] Creating layer penlu8
I1023 12:10:12.974395  4594 net.cpp:84] Creating Layer penlu8
I1023 12:10:12.974396  4594 net.cpp:406] penlu8 <- Convolution9
I1023 12:10:12.974401  4594 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1023 12:10:12.975250  4594 net.cpp:122] Setting up penlu8
I1023 12:10:12.975258  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.975261  4594 net.cpp:137] Memory required for data: 1096646752
I1023 12:10:12.975266  4594 layer_factory.hpp:77] Creating layer Convolution10
I1023 12:10:12.975275  4594 net.cpp:84] Creating Layer Convolution10
I1023 12:10:12.975277  4594 net.cpp:406] Convolution10 <- Convolution9
I1023 12:10:12.975281  4594 net.cpp:380] Convolution10 -> Convolution10
I1023 12:10:12.977365  4594 net.cpp:122] Setting up Convolution10
I1023 12:10:12.977373  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.977376  4594 net.cpp:137] Memory required for data: 1109491808
I1023 12:10:12.977381  4594 layer_factory.hpp:77] Creating layer BatchNorm10
I1023 12:10:12.977386  4594 net.cpp:84] Creating Layer BatchNorm10
I1023 12:10:12.977390  4594 net.cpp:406] BatchNorm10 <- Convolution10
I1023 12:10:12.977393  4594 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1023 12:10:12.977557  4594 net.cpp:122] Setting up BatchNorm10
I1023 12:10:12.977562  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.977565  4594 net.cpp:137] Memory required for data: 1122336864
I1023 12:10:12.977569  4594 layer_factory.hpp:77] Creating layer Scale10
I1023 12:10:12.977573  4594 net.cpp:84] Creating Layer Scale10
I1023 12:10:12.977576  4594 net.cpp:406] Scale10 <- Convolution10
I1023 12:10:12.977579  4594 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1023 12:10:12.977612  4594 layer_factory.hpp:77] Creating layer Scale10
I1023 12:10:12.977706  4594 net.cpp:122] Setting up Scale10
I1023 12:10:12.977710  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.977725  4594 net.cpp:137] Memory required for data: 1135181920
I1023 12:10:12.977728  4594 layer_factory.hpp:77] Creating layer Eltwise4
I1023 12:10:12.977733  4594 net.cpp:84] Creating Layer Eltwise4
I1023 12:10:12.977735  4594 net.cpp:406] Eltwise4 <- Convolution8
I1023 12:10:12.977738  4594 net.cpp:406] Eltwise4 <- Convolution10
I1023 12:10:12.977742  4594 net.cpp:380] Eltwise4 -> Eltwise4
I1023 12:10:12.977762  4594 net.cpp:122] Setting up Eltwise4
I1023 12:10:12.977766  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.977768  4594 net.cpp:137] Memory required for data: 1148026976
I1023 12:10:12.977771  4594 layer_factory.hpp:77] Creating layer penlu9
I1023 12:10:12.977777  4594 net.cpp:84] Creating Layer penlu9
I1023 12:10:12.977778  4594 net.cpp:406] penlu9 <- Eltwise4
I1023 12:10:12.977783  4594 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1023 12:10:12.978612  4594 net.cpp:122] Setting up penlu9
I1023 12:10:12.978621  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.978622  4594 net.cpp:137] Memory required for data: 1160872032
I1023 12:10:12.978627  4594 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1023 12:10:12.978633  4594 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1023 12:10:12.978636  4594 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1023 12:10:12.978639  4594 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1023 12:10:12.978643  4594 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1023 12:10:12.978672  4594 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1023 12:10:12.978677  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.978678  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.978680  4594 net.cpp:137] Memory required for data: 1186562144
I1023 12:10:12.978683  4594 layer_factory.hpp:77] Creating layer Convolution11
I1023 12:10:12.978689  4594 net.cpp:84] Creating Layer Convolution11
I1023 12:10:12.978691  4594 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1023 12:10:12.978696  4594 net.cpp:380] Convolution11 -> Convolution11
I1023 12:10:12.979873  4594 net.cpp:122] Setting up Convolution11
I1023 12:10:12.979882  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.979885  4594 net.cpp:137] Memory required for data: 1199407200
I1023 12:10:12.979889  4594 layer_factory.hpp:77] Creating layer BatchNorm11
I1023 12:10:12.979894  4594 net.cpp:84] Creating Layer BatchNorm11
I1023 12:10:12.979897  4594 net.cpp:406] BatchNorm11 <- Convolution11
I1023 12:10:12.979902  4594 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1023 12:10:12.980092  4594 net.cpp:122] Setting up BatchNorm11
I1023 12:10:12.980096  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.980098  4594 net.cpp:137] Memory required for data: 1212252256
I1023 12:10:12.980103  4594 layer_factory.hpp:77] Creating layer Scale11
I1023 12:10:12.980108  4594 net.cpp:84] Creating Layer Scale11
I1023 12:10:12.980110  4594 net.cpp:406] Scale11 <- Convolution11
I1023 12:10:12.980113  4594 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1023 12:10:12.980144  4594 layer_factory.hpp:77] Creating layer Scale11
I1023 12:10:12.980239  4594 net.cpp:122] Setting up Scale11
I1023 12:10:12.980244  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.980247  4594 net.cpp:137] Memory required for data: 1225097312
I1023 12:10:12.980250  4594 layer_factory.hpp:77] Creating layer penlu10
I1023 12:10:12.980255  4594 net.cpp:84] Creating Layer penlu10
I1023 12:10:12.980258  4594 net.cpp:406] penlu10 <- Convolution11
I1023 12:10:12.980262  4594 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1023 12:10:12.981091  4594 net.cpp:122] Setting up penlu10
I1023 12:10:12.981098  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.981101  4594 net.cpp:137] Memory required for data: 1237942368
I1023 12:10:12.981106  4594 layer_factory.hpp:77] Creating layer Convolution12
I1023 12:10:12.981113  4594 net.cpp:84] Creating Layer Convolution12
I1023 12:10:12.981125  4594 net.cpp:406] Convolution12 <- Convolution11
I1023 12:10:12.981130  4594 net.cpp:380] Convolution12 -> Convolution12
I1023 12:10:12.981925  4594 net.cpp:122] Setting up Convolution12
I1023 12:10:12.981933  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.981936  4594 net.cpp:137] Memory required for data: 1250787424
I1023 12:10:12.981940  4594 layer_factory.hpp:77] Creating layer BatchNorm12
I1023 12:10:12.981945  4594 net.cpp:84] Creating Layer BatchNorm12
I1023 12:10:12.981947  4594 net.cpp:406] BatchNorm12 <- Convolution12
I1023 12:10:12.981951  4594 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1023 12:10:12.982112  4594 net.cpp:122] Setting up BatchNorm12
I1023 12:10:12.982117  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.982120  4594 net.cpp:137] Memory required for data: 1263632480
I1023 12:10:12.982125  4594 layer_factory.hpp:77] Creating layer Scale12
I1023 12:10:12.982128  4594 net.cpp:84] Creating Layer Scale12
I1023 12:10:12.982131  4594 net.cpp:406] Scale12 <- Convolution12
I1023 12:10:12.982134  4594 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1023 12:10:12.982164  4594 layer_factory.hpp:77] Creating layer Scale12
I1023 12:10:12.982264  4594 net.cpp:122] Setting up Scale12
I1023 12:10:12.982269  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.982270  4594 net.cpp:137] Memory required for data: 1276477536
I1023 12:10:12.982275  4594 layer_factory.hpp:77] Creating layer Eltwise5
I1023 12:10:12.982280  4594 net.cpp:84] Creating Layer Eltwise5
I1023 12:10:12.982281  4594 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1023 12:10:12.982285  4594 net.cpp:406] Eltwise5 <- Convolution12
I1023 12:10:12.982287  4594 net.cpp:380] Eltwise5 -> Eltwise5
I1023 12:10:12.982307  4594 net.cpp:122] Setting up Eltwise5
I1023 12:10:12.982311  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.982312  4594 net.cpp:137] Memory required for data: 1289322592
I1023 12:10:12.982316  4594 layer_factory.hpp:77] Creating layer penlu11
I1023 12:10:12.982321  4594 net.cpp:84] Creating Layer penlu11
I1023 12:10:12.982322  4594 net.cpp:406] penlu11 <- Eltwise5
I1023 12:10:12.982326  4594 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1023 12:10:12.983173  4594 net.cpp:122] Setting up penlu11
I1023 12:10:12.983181  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.983183  4594 net.cpp:137] Memory required for data: 1302167648
I1023 12:10:12.983189  4594 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1023 12:10:12.983193  4594 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1023 12:10:12.983196  4594 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1023 12:10:12.983198  4594 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1023 12:10:12.983203  4594 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1023 12:10:12.983235  4594 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1023 12:10:12.983240  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.983243  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.983245  4594 net.cpp:137] Memory required for data: 1327857760
I1023 12:10:12.983247  4594 layer_factory.hpp:77] Creating layer Convolution13
I1023 12:10:12.983255  4594 net.cpp:84] Creating Layer Convolution13
I1023 12:10:12.983258  4594 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1023 12:10:12.983261  4594 net.cpp:380] Convolution13 -> Convolution13
I1023 12:10:12.984449  4594 net.cpp:122] Setting up Convolution13
I1023 12:10:12.984458  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.984462  4594 net.cpp:137] Memory required for data: 1340702816
I1023 12:10:12.984465  4594 layer_factory.hpp:77] Creating layer BatchNorm13
I1023 12:10:12.984472  4594 net.cpp:84] Creating Layer BatchNorm13
I1023 12:10:12.984474  4594 net.cpp:406] BatchNorm13 <- Convolution13
I1023 12:10:12.984478  4594 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1023 12:10:12.984642  4594 net.cpp:122] Setting up BatchNorm13
I1023 12:10:12.984653  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.984655  4594 net.cpp:137] Memory required for data: 1353547872
I1023 12:10:12.984660  4594 layer_factory.hpp:77] Creating layer Scale13
I1023 12:10:12.984665  4594 net.cpp:84] Creating Layer Scale13
I1023 12:10:12.984668  4594 net.cpp:406] Scale13 <- Convolution13
I1023 12:10:12.984671  4594 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1023 12:10:12.984705  4594 layer_factory.hpp:77] Creating layer Scale13
I1023 12:10:12.984809  4594 net.cpp:122] Setting up Scale13
I1023 12:10:12.984813  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.984815  4594 net.cpp:137] Memory required for data: 1366392928
I1023 12:10:12.984819  4594 layer_factory.hpp:77] Creating layer penlu12
I1023 12:10:12.984825  4594 net.cpp:84] Creating Layer penlu12
I1023 12:10:12.984827  4594 net.cpp:406] penlu12 <- Convolution13
I1023 12:10:12.984832  4594 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1023 12:10:12.985684  4594 net.cpp:122] Setting up penlu12
I1023 12:10:12.985692  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.985695  4594 net.cpp:137] Memory required for data: 1379237984
I1023 12:10:12.985700  4594 layer_factory.hpp:77] Creating layer Convolution14
I1023 12:10:12.985711  4594 net.cpp:84] Creating Layer Convolution14
I1023 12:10:12.985714  4594 net.cpp:406] Convolution14 <- Convolution13
I1023 12:10:12.985719  4594 net.cpp:380] Convolution14 -> Convolution14
I1023 12:10:12.986892  4594 net.cpp:122] Setting up Convolution14
I1023 12:10:12.986901  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.986904  4594 net.cpp:137] Memory required for data: 1392083040
I1023 12:10:12.986922  4594 layer_factory.hpp:77] Creating layer BatchNorm14
I1023 12:10:12.986927  4594 net.cpp:84] Creating Layer BatchNorm14
I1023 12:10:12.986929  4594 net.cpp:406] BatchNorm14 <- Convolution14
I1023 12:10:12.986934  4594 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1023 12:10:12.987097  4594 net.cpp:122] Setting up BatchNorm14
I1023 12:10:12.987102  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.987104  4594 net.cpp:137] Memory required for data: 1404928096
I1023 12:10:12.987109  4594 layer_factory.hpp:77] Creating layer Scale14
I1023 12:10:12.987114  4594 net.cpp:84] Creating Layer Scale14
I1023 12:10:12.987116  4594 net.cpp:406] Scale14 <- Convolution14
I1023 12:10:12.987119  4594 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1023 12:10:12.987150  4594 layer_factory.hpp:77] Creating layer Scale14
I1023 12:10:12.987251  4594 net.cpp:122] Setting up Scale14
I1023 12:10:12.987257  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.987258  4594 net.cpp:137] Memory required for data: 1417773152
I1023 12:10:12.987262  4594 layer_factory.hpp:77] Creating layer Eltwise6
I1023 12:10:12.987267  4594 net.cpp:84] Creating Layer Eltwise6
I1023 12:10:12.987269  4594 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1023 12:10:12.987272  4594 net.cpp:406] Eltwise6 <- Convolution14
I1023 12:10:12.987275  4594 net.cpp:380] Eltwise6 -> Eltwise6
I1023 12:10:12.987295  4594 net.cpp:122] Setting up Eltwise6
I1023 12:10:12.987299  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.987301  4594 net.cpp:137] Memory required for data: 1430618208
I1023 12:10:12.987303  4594 layer_factory.hpp:77] Creating layer penlu13
I1023 12:10:12.987308  4594 net.cpp:84] Creating Layer penlu13
I1023 12:10:12.987311  4594 net.cpp:406] penlu13 <- Eltwise6
I1023 12:10:12.987314  4594 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1023 12:10:12.988198  4594 net.cpp:122] Setting up penlu13
I1023 12:10:12.988207  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.988210  4594 net.cpp:137] Memory required for data: 1443463264
I1023 12:10:12.988215  4594 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1023 12:10:12.988221  4594 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1023 12:10:12.988224  4594 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1023 12:10:12.988237  4594 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1023 12:10:12.988242  4594 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1023 12:10:12.988273  4594 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1023 12:10:12.988278  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.988281  4594 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1023 12:10:12.988282  4594 net.cpp:137] Memory required for data: 1469153376
I1023 12:10:12.988286  4594 layer_factory.hpp:77] Creating layer Convolution15
I1023 12:10:12.988291  4594 net.cpp:84] Creating Layer Convolution15
I1023 12:10:12.988293  4594 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1023 12:10:12.988298  4594 net.cpp:380] Convolution15 -> Convolution15
I1023 12:10:12.989317  4594 net.cpp:122] Setting up Convolution15
I1023 12:10:12.989326  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.989329  4594 net.cpp:137] Memory required for data: 1475575904
I1023 12:10:12.989333  4594 layer_factory.hpp:77] Creating layer BatchNorm15
I1023 12:10:12.989339  4594 net.cpp:84] Creating Layer BatchNorm15
I1023 12:10:12.989342  4594 net.cpp:406] BatchNorm15 <- Convolution15
I1023 12:10:12.989346  4594 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1023 12:10:12.989509  4594 net.cpp:122] Setting up BatchNorm15
I1023 12:10:12.989513  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.989516  4594 net.cpp:137] Memory required for data: 1481998432
I1023 12:10:12.989521  4594 layer_factory.hpp:77] Creating layer Scale15
I1023 12:10:12.989524  4594 net.cpp:84] Creating Layer Scale15
I1023 12:10:12.989527  4594 net.cpp:406] Scale15 <- Convolution15
I1023 12:10:12.989531  4594 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1023 12:10:12.989562  4594 layer_factory.hpp:77] Creating layer Scale15
I1023 12:10:12.989657  4594 net.cpp:122] Setting up Scale15
I1023 12:10:12.989661  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.989663  4594 net.cpp:137] Memory required for data: 1488420960
I1023 12:10:12.989667  4594 layer_factory.hpp:77] Creating layer Convolution16
I1023 12:10:12.989675  4594 net.cpp:84] Creating Layer Convolution16
I1023 12:10:12.989676  4594 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1023 12:10:12.989681  4594 net.cpp:380] Convolution16 -> Convolution16
I1023 12:10:12.991053  4594 net.cpp:122] Setting up Convolution16
I1023 12:10:12.991062  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.991065  4594 net.cpp:137] Memory required for data: 1494843488
I1023 12:10:12.991070  4594 layer_factory.hpp:77] Creating layer BatchNorm16
I1023 12:10:12.991075  4594 net.cpp:84] Creating Layer BatchNorm16
I1023 12:10:12.991077  4594 net.cpp:406] BatchNorm16 <- Convolution16
I1023 12:10:12.991081  4594 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1023 12:10:12.991243  4594 net.cpp:122] Setting up BatchNorm16
I1023 12:10:12.991248  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.991250  4594 net.cpp:137] Memory required for data: 1501266016
I1023 12:10:12.991255  4594 layer_factory.hpp:77] Creating layer Scale16
I1023 12:10:12.991259  4594 net.cpp:84] Creating Layer Scale16
I1023 12:10:12.991261  4594 net.cpp:406] Scale16 <- Convolution16
I1023 12:10:12.991266  4594 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1023 12:10:12.991295  4594 layer_factory.hpp:77] Creating layer Scale16
I1023 12:10:12.991390  4594 net.cpp:122] Setting up Scale16
I1023 12:10:12.991394  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.991396  4594 net.cpp:137] Memory required for data: 1507688544
I1023 12:10:12.991400  4594 layer_factory.hpp:77] Creating layer penlu14
I1023 12:10:12.991406  4594 net.cpp:84] Creating Layer penlu14
I1023 12:10:12.991410  4594 net.cpp:406] penlu14 <- Convolution16
I1023 12:10:12.991412  4594 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1023 12:10:12.992197  4594 net.cpp:122] Setting up penlu14
I1023 12:10:12.992205  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.992215  4594 net.cpp:137] Memory required for data: 1514111072
I1023 12:10:12.992220  4594 layer_factory.hpp:77] Creating layer Convolution17
I1023 12:10:12.992228  4594 net.cpp:84] Creating Layer Convolution17
I1023 12:10:12.992231  4594 net.cpp:406] Convolution17 <- Convolution16
I1023 12:10:12.992235  4594 net.cpp:380] Convolution17 -> Convolution17
I1023 12:10:12.994608  4594 net.cpp:122] Setting up Convolution17
I1023 12:10:12.994617  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.994621  4594 net.cpp:137] Memory required for data: 1520533600
I1023 12:10:12.994626  4594 layer_factory.hpp:77] Creating layer BatchNorm17
I1023 12:10:12.994630  4594 net.cpp:84] Creating Layer BatchNorm17
I1023 12:10:12.994633  4594 net.cpp:406] BatchNorm17 <- Convolution17
I1023 12:10:12.994637  4594 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1023 12:10:12.994801  4594 net.cpp:122] Setting up BatchNorm17
I1023 12:10:12.994805  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.994807  4594 net.cpp:137] Memory required for data: 1526956128
I1023 12:10:12.994812  4594 layer_factory.hpp:77] Creating layer Scale17
I1023 12:10:12.994817  4594 net.cpp:84] Creating Layer Scale17
I1023 12:10:12.994820  4594 net.cpp:406] Scale17 <- Convolution17
I1023 12:10:12.994823  4594 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1023 12:10:12.994855  4594 layer_factory.hpp:77] Creating layer Scale17
I1023 12:10:12.994953  4594 net.cpp:122] Setting up Scale17
I1023 12:10:12.994957  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.994959  4594 net.cpp:137] Memory required for data: 1533378656
I1023 12:10:12.994963  4594 layer_factory.hpp:77] Creating layer Eltwise7
I1023 12:10:12.994968  4594 net.cpp:84] Creating Layer Eltwise7
I1023 12:10:12.994971  4594 net.cpp:406] Eltwise7 <- Convolution15
I1023 12:10:12.994973  4594 net.cpp:406] Eltwise7 <- Convolution17
I1023 12:10:12.994977  4594 net.cpp:380] Eltwise7 -> Eltwise7
I1023 12:10:12.994997  4594 net.cpp:122] Setting up Eltwise7
I1023 12:10:12.995000  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.995002  4594 net.cpp:137] Memory required for data: 1539801184
I1023 12:10:12.995004  4594 layer_factory.hpp:77] Creating layer penlu15
I1023 12:10:12.995009  4594 net.cpp:84] Creating Layer penlu15
I1023 12:10:12.995012  4594 net.cpp:406] penlu15 <- Eltwise7
I1023 12:10:12.995015  4594 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1023 12:10:12.995249  4594 net.cpp:122] Setting up penlu15
I1023 12:10:12.995254  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.995255  4594 net.cpp:137] Memory required for data: 1546223712
I1023 12:10:12.995260  4594 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1023 12:10:12.995263  4594 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1023 12:10:12.995266  4594 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1023 12:10:12.995270  4594 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1023 12:10:12.995273  4594 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1023 12:10:12.995302  4594 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1023 12:10:12.995306  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.995309  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:12.995311  4594 net.cpp:137] Memory required for data: 1559068768
I1023 12:10:12.995313  4594 layer_factory.hpp:77] Creating layer Convolution18
I1023 12:10:12.995319  4594 net.cpp:84] Creating Layer Convolution18
I1023 12:10:12.995322  4594 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1023 12:10:12.995326  4594 net.cpp:380] Convolution18 -> Convolution18
I1023 12:10:12.997858  4594 net.cpp:122] Setting up Convolution18
I1023 12:10:13.000316  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.000324  4594 net.cpp:137] Memory required for data: 1565491296
I1023 12:10:13.000329  4594 layer_factory.hpp:77] Creating layer BatchNorm18
I1023 12:10:13.000335  4594 net.cpp:84] Creating Layer BatchNorm18
I1023 12:10:13.000347  4594 net.cpp:406] BatchNorm18 <- Convolution18
I1023 12:10:13.000352  4594 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1023 12:10:13.000540  4594 net.cpp:122] Setting up BatchNorm18
I1023 12:10:13.000545  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.000547  4594 net.cpp:137] Memory required for data: 1571913824
I1023 12:10:13.000552  4594 layer_factory.hpp:77] Creating layer Scale18
I1023 12:10:13.000558  4594 net.cpp:84] Creating Layer Scale18
I1023 12:10:13.000560  4594 net.cpp:406] Scale18 <- Convolution18
I1023 12:10:13.000564  4594 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1023 12:10:13.000599  4594 layer_factory.hpp:77] Creating layer Scale18
I1023 12:10:13.000705  4594 net.cpp:122] Setting up Scale18
I1023 12:10:13.000710  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.000712  4594 net.cpp:137] Memory required for data: 1578336352
I1023 12:10:13.000716  4594 layer_factory.hpp:77] Creating layer penlu16
I1023 12:10:13.000725  4594 net.cpp:84] Creating Layer penlu16
I1023 12:10:13.000727  4594 net.cpp:406] penlu16 <- Convolution18
I1023 12:10:13.000730  4594 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1023 12:10:13.001566  4594 net.cpp:122] Setting up penlu16
I1023 12:10:13.001576  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.001579  4594 net.cpp:137] Memory required for data: 1584758880
I1023 12:10:13.001585  4594 layer_factory.hpp:77] Creating layer Convolution19
I1023 12:10:13.001591  4594 net.cpp:84] Creating Layer Convolution19
I1023 12:10:13.001595  4594 net.cpp:406] Convolution19 <- Convolution18
I1023 12:10:13.001600  4594 net.cpp:380] Convolution19 -> Convolution19
I1023 12:10:13.003595  4594 net.cpp:122] Setting up Convolution19
I1023 12:10:13.003607  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.003608  4594 net.cpp:137] Memory required for data: 1591181408
I1023 12:10:13.003613  4594 layer_factory.hpp:77] Creating layer BatchNorm19
I1023 12:10:13.003618  4594 net.cpp:84] Creating Layer BatchNorm19
I1023 12:10:13.003621  4594 net.cpp:406] BatchNorm19 <- Convolution19
I1023 12:10:13.003625  4594 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1023 12:10:13.003799  4594 net.cpp:122] Setting up BatchNorm19
I1023 12:10:13.003804  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.003806  4594 net.cpp:137] Memory required for data: 1597603936
I1023 12:10:13.003811  4594 layer_factory.hpp:77] Creating layer Scale19
I1023 12:10:13.003816  4594 net.cpp:84] Creating Layer Scale19
I1023 12:10:13.003818  4594 net.cpp:406] Scale19 <- Convolution19
I1023 12:10:13.003821  4594 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1023 12:10:13.003866  4594 layer_factory.hpp:77] Creating layer Scale19
I1023 12:10:13.003993  4594 net.cpp:122] Setting up Scale19
I1023 12:10:13.004000  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.004003  4594 net.cpp:137] Memory required for data: 1604026464
I1023 12:10:13.004006  4594 layer_factory.hpp:77] Creating layer Eltwise8
I1023 12:10:13.004011  4594 net.cpp:84] Creating Layer Eltwise8
I1023 12:10:13.004015  4594 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1023 12:10:13.004019  4594 net.cpp:406] Eltwise8 <- Convolution19
I1023 12:10:13.004022  4594 net.cpp:380] Eltwise8 -> Eltwise8
I1023 12:10:13.004045  4594 net.cpp:122] Setting up Eltwise8
I1023 12:10:13.004050  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.004051  4594 net.cpp:137] Memory required for data: 1610448992
I1023 12:10:13.004055  4594 layer_factory.hpp:77] Creating layer penlu17
I1023 12:10:13.004060  4594 net.cpp:84] Creating Layer penlu17
I1023 12:10:13.004061  4594 net.cpp:406] penlu17 <- Eltwise8
I1023 12:10:13.004065  4594 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1023 12:10:13.004328  4594 net.cpp:122] Setting up penlu17
I1023 12:10:13.004333  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.004334  4594 net.cpp:137] Memory required for data: 1616871520
I1023 12:10:13.004339  4594 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1023 12:10:13.004353  4594 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1023 12:10:13.004355  4594 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1023 12:10:13.004359  4594 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1023 12:10:13.004362  4594 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1023 12:10:13.004393  4594 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1023 12:10:13.004397  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.004400  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.004402  4594 net.cpp:137] Memory required for data: 1629716576
I1023 12:10:13.004405  4594 layer_factory.hpp:77] Creating layer Convolution20
I1023 12:10:13.004411  4594 net.cpp:84] Creating Layer Convolution20
I1023 12:10:13.004413  4594 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1023 12:10:13.004418  4594 net.cpp:380] Convolution20 -> Convolution20
I1023 12:10:13.006595  4594 net.cpp:122] Setting up Convolution20
I1023 12:10:13.006604  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.006606  4594 net.cpp:137] Memory required for data: 1636139104
I1023 12:10:13.006611  4594 layer_factory.hpp:77] Creating layer BatchNorm20
I1023 12:10:13.006618  4594 net.cpp:84] Creating Layer BatchNorm20
I1023 12:10:13.006619  4594 net.cpp:406] BatchNorm20 <- Convolution20
I1023 12:10:13.006624  4594 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1023 12:10:13.006795  4594 net.cpp:122] Setting up BatchNorm20
I1023 12:10:13.006799  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.006801  4594 net.cpp:137] Memory required for data: 1642561632
I1023 12:10:13.006806  4594 layer_factory.hpp:77] Creating layer Scale20
I1023 12:10:13.006811  4594 net.cpp:84] Creating Layer Scale20
I1023 12:10:13.006814  4594 net.cpp:406] Scale20 <- Convolution20
I1023 12:10:13.006817  4594 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1023 12:10:13.006850  4594 layer_factory.hpp:77] Creating layer Scale20
I1023 12:10:13.006953  4594 net.cpp:122] Setting up Scale20
I1023 12:10:13.006956  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.006958  4594 net.cpp:137] Memory required for data: 1648984160
I1023 12:10:13.006963  4594 layer_factory.hpp:77] Creating layer penlu18
I1023 12:10:13.006968  4594 net.cpp:84] Creating Layer penlu18
I1023 12:10:13.006971  4594 net.cpp:406] penlu18 <- Convolution20
I1023 12:10:13.006974  4594 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1023 12:10:13.007748  4594 net.cpp:122] Setting up penlu18
I1023 12:10:13.007757  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.007760  4594 net.cpp:137] Memory required for data: 1655406688
I1023 12:10:13.007764  4594 layer_factory.hpp:77] Creating layer Convolution21
I1023 12:10:13.007771  4594 net.cpp:84] Creating Layer Convolution21
I1023 12:10:13.007773  4594 net.cpp:406] Convolution21 <- Convolution20
I1023 12:10:13.007781  4594 net.cpp:380] Convolution21 -> Convolution21
I1023 12:10:13.009613  4594 net.cpp:122] Setting up Convolution21
I1023 12:10:13.009624  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.009627  4594 net.cpp:137] Memory required for data: 1661829216
I1023 12:10:13.009631  4594 layer_factory.hpp:77] Creating layer BatchNorm21
I1023 12:10:13.009636  4594 net.cpp:84] Creating Layer BatchNorm21
I1023 12:10:13.009639  4594 net.cpp:406] BatchNorm21 <- Convolution21
I1023 12:10:13.009644  4594 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1023 12:10:13.009810  4594 net.cpp:122] Setting up BatchNorm21
I1023 12:10:13.009815  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.009817  4594 net.cpp:137] Memory required for data: 1668251744
I1023 12:10:13.009822  4594 layer_factory.hpp:77] Creating layer Scale21
I1023 12:10:13.009826  4594 net.cpp:84] Creating Layer Scale21
I1023 12:10:13.009829  4594 net.cpp:406] Scale21 <- Convolution21
I1023 12:10:13.009832  4594 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1023 12:10:13.009865  4594 layer_factory.hpp:77] Creating layer Scale21
I1023 12:10:13.009975  4594 net.cpp:122] Setting up Scale21
I1023 12:10:13.009979  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.009982  4594 net.cpp:137] Memory required for data: 1674674272
I1023 12:10:13.009985  4594 layer_factory.hpp:77] Creating layer Eltwise9
I1023 12:10:13.009990  4594 net.cpp:84] Creating Layer Eltwise9
I1023 12:10:13.009992  4594 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1023 12:10:13.009995  4594 net.cpp:406] Eltwise9 <- Convolution21
I1023 12:10:13.009999  4594 net.cpp:380] Eltwise9 -> Eltwise9
I1023 12:10:13.010018  4594 net.cpp:122] Setting up Eltwise9
I1023 12:10:13.010022  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.010025  4594 net.cpp:137] Memory required for data: 1681096800
I1023 12:10:13.010026  4594 layer_factory.hpp:77] Creating layer penlu19
I1023 12:10:13.010032  4594 net.cpp:84] Creating Layer penlu19
I1023 12:10:13.010035  4594 net.cpp:406] penlu19 <- Eltwise9
I1023 12:10:13.010038  4594 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1023 12:10:13.010262  4594 net.cpp:122] Setting up penlu19
I1023 12:10:13.010267  4594 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1023 12:10:13.010268  4594 net.cpp:137] Memory required for data: 1687519328
I1023 12:10:13.010272  4594 layer_factory.hpp:77] Creating layer Pooling1
I1023 12:10:13.010277  4594 net.cpp:84] Creating Layer Pooling1
I1023 12:10:13.010279  4594 net.cpp:406] Pooling1 <- Eltwise9
I1023 12:10:13.010282  4594 net.cpp:380] Pooling1 -> Pooling1
I1023 12:10:13.010424  4594 net.cpp:122] Setting up Pooling1
I1023 12:10:13.010430  4594 net.cpp:129] Top shape: 8 64 1 1 (512)
I1023 12:10:13.010433  4594 net.cpp:137] Memory required for data: 1687521376
I1023 12:10:13.010435  4594 layer_factory.hpp:77] Creating layer InnerProduct1
I1023 12:10:13.010440  4594 net.cpp:84] Creating Layer InnerProduct1
I1023 12:10:13.010442  4594 net.cpp:406] InnerProduct1 <- Pooling1
I1023 12:10:13.010447  4594 net.cpp:380] InnerProduct1 -> InnerProduct1
I1023 12:10:13.010560  4594 net.cpp:122] Setting up InnerProduct1
I1023 12:10:13.010563  4594 net.cpp:129] Top shape: 8 10 (80)
I1023 12:10:13.010565  4594 net.cpp:137] Memory required for data: 1687521696
I1023 12:10:13.010570  4594 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1023 12:10:13.010573  4594 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1023 12:10:13.010576  4594 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1023 12:10:13.010579  4594 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1023 12:10:13.010583  4594 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1023 12:10:13.010613  4594 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1023 12:10:13.010618  4594 net.cpp:129] Top shape: 8 10 (80)
I1023 12:10:13.010620  4594 net.cpp:129] Top shape: 8 10 (80)
I1023 12:10:13.010622  4594 net.cpp:137] Memory required for data: 1687522336
I1023 12:10:13.010624  4594 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1023 12:10:13.010629  4594 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1023 12:10:13.010632  4594 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I1023 12:10:13.010634  4594 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I1023 12:10:13.010638  4594 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1023 12:10:13.010643  4594 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1023 12:10:13.010841  4594 net.cpp:122] Setting up SoftmaxWithLoss1
I1023 12:10:13.010846  4594 net.cpp:129] Top shape: (1)
I1023 12:10:13.010849  4594 net.cpp:132]     with loss weight 1
I1023 12:10:13.010856  4594 net.cpp:137] Memory required for data: 1687522340
I1023 12:10:13.010859  4594 layer_factory.hpp:77] Creating layer Accuracy1
I1023 12:10:13.010864  4594 net.cpp:84] Creating Layer Accuracy1
I1023 12:10:13.010867  4594 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I1023 12:10:13.010870  4594 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I1023 12:10:13.010879  4594 net.cpp:380] Accuracy1 -> Accuracy1
I1023 12:10:13.010885  4594 net.cpp:122] Setting up Accuracy1
I1023 12:10:13.010890  4594 net.cpp:129] Top shape: (1)
I1023 12:10:13.010891  4594 net.cpp:137] Memory required for data: 1687522344
I1023 12:10:13.010893  4594 net.cpp:200] Accuracy1 does not need backward computation.
I1023 12:10:13.010896  4594 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1023 12:10:13.010898  4594 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1023 12:10:13.010901  4594 net.cpp:198] InnerProduct1 needs backward computation.
I1023 12:10:13.010903  4594 net.cpp:198] Pooling1 needs backward computation.
I1023 12:10:13.010905  4594 net.cpp:198] penlu19 needs backward computation.
I1023 12:10:13.010907  4594 net.cpp:198] Eltwise9 needs backward computation.
I1023 12:10:13.010910  4594 net.cpp:198] Scale21 needs backward computation.
I1023 12:10:13.010912  4594 net.cpp:198] BatchNorm21 needs backward computation.
I1023 12:10:13.010915  4594 net.cpp:198] Convolution21 needs backward computation.
I1023 12:10:13.010916  4594 net.cpp:198] penlu18 needs backward computation.
I1023 12:10:13.010918  4594 net.cpp:198] Scale20 needs backward computation.
I1023 12:10:13.010921  4594 net.cpp:198] BatchNorm20 needs backward computation.
I1023 12:10:13.010922  4594 net.cpp:198] Convolution20 needs backward computation.
I1023 12:10:13.010924  4594 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1023 12:10:13.010927  4594 net.cpp:198] penlu17 needs backward computation.
I1023 12:10:13.010929  4594 net.cpp:198] Eltwise8 needs backward computation.
I1023 12:10:13.010931  4594 net.cpp:198] Scale19 needs backward computation.
I1023 12:10:13.010933  4594 net.cpp:198] BatchNorm19 needs backward computation.
I1023 12:10:13.010936  4594 net.cpp:198] Convolution19 needs backward computation.
I1023 12:10:13.010937  4594 net.cpp:198] penlu16 needs backward computation.
I1023 12:10:13.010941  4594 net.cpp:198] Scale18 needs backward computation.
I1023 12:10:13.010942  4594 net.cpp:198] BatchNorm18 needs backward computation.
I1023 12:10:13.010944  4594 net.cpp:198] Convolution18 needs backward computation.
I1023 12:10:13.010946  4594 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1023 12:10:13.010949  4594 net.cpp:198] penlu15 needs backward computation.
I1023 12:10:13.010951  4594 net.cpp:198] Eltwise7 needs backward computation.
I1023 12:10:13.010954  4594 net.cpp:198] Scale17 needs backward computation.
I1023 12:10:13.010957  4594 net.cpp:198] BatchNorm17 needs backward computation.
I1023 12:10:13.010958  4594 net.cpp:198] Convolution17 needs backward computation.
I1023 12:10:13.010962  4594 net.cpp:198] penlu14 needs backward computation.
I1023 12:10:13.010963  4594 net.cpp:198] Scale16 needs backward computation.
I1023 12:10:13.010965  4594 net.cpp:198] BatchNorm16 needs backward computation.
I1023 12:10:13.010967  4594 net.cpp:198] Convolution16 needs backward computation.
I1023 12:10:13.010972  4594 net.cpp:198] Scale15 needs backward computation.
I1023 12:10:13.010973  4594 net.cpp:198] BatchNorm15 needs backward computation.
I1023 12:10:13.010977  4594 net.cpp:198] Convolution15 needs backward computation.
I1023 12:10:13.010978  4594 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1023 12:10:13.010982  4594 net.cpp:198] penlu13 needs backward computation.
I1023 12:10:13.010983  4594 net.cpp:198] Eltwise6 needs backward computation.
I1023 12:10:13.010987  4594 net.cpp:198] Scale14 needs backward computation.
I1023 12:10:13.030952  4594 net.cpp:198] BatchNorm14 needs backward computation.
I1023 12:10:13.030959  4594 net.cpp:198] Convolution14 needs backward computation.
I1023 12:10:13.030963  4594 net.cpp:198] penlu12 needs backward computation.
I1023 12:10:13.030966  4594 net.cpp:198] Scale13 needs backward computation.
I1023 12:10:13.030968  4594 net.cpp:198] BatchNorm13 needs backward computation.
I1023 12:10:13.030971  4594 net.cpp:198] Convolution13 needs backward computation.
I1023 12:10:13.030982  4594 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1023 12:10:13.030984  4594 net.cpp:198] penlu11 needs backward computation.
I1023 12:10:13.030987  4594 net.cpp:198] Eltwise5 needs backward computation.
I1023 12:10:13.030990  4594 net.cpp:198] Scale12 needs backward computation.
I1023 12:10:13.030992  4594 net.cpp:198] BatchNorm12 needs backward computation.
I1023 12:10:13.030994  4594 net.cpp:198] Convolution12 needs backward computation.
I1023 12:10:13.030997  4594 net.cpp:198] penlu10 needs backward computation.
I1023 12:10:13.030999  4594 net.cpp:198] Scale11 needs backward computation.
I1023 12:10:13.031002  4594 net.cpp:198] BatchNorm11 needs backward computation.
I1023 12:10:13.031004  4594 net.cpp:198] Convolution11 needs backward computation.
I1023 12:10:13.031008  4594 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1023 12:10:13.031010  4594 net.cpp:198] penlu9 needs backward computation.
I1023 12:10:13.031013  4594 net.cpp:198] Eltwise4 needs backward computation.
I1023 12:10:13.031015  4594 net.cpp:198] Scale10 needs backward computation.
I1023 12:10:13.031018  4594 net.cpp:198] BatchNorm10 needs backward computation.
I1023 12:10:13.031020  4594 net.cpp:198] Convolution10 needs backward computation.
I1023 12:10:13.031023  4594 net.cpp:198] penlu8 needs backward computation.
I1023 12:10:13.031026  4594 net.cpp:198] Scale9 needs backward computation.
I1023 12:10:13.031028  4594 net.cpp:198] BatchNorm9 needs backward computation.
I1023 12:10:13.031030  4594 net.cpp:198] Convolution9 needs backward computation.
I1023 12:10:13.031033  4594 net.cpp:198] Scale8 needs backward computation.
I1023 12:10:13.031036  4594 net.cpp:198] BatchNorm8 needs backward computation.
I1023 12:10:13.031038  4594 net.cpp:198] Convolution8 needs backward computation.
I1023 12:10:13.031041  4594 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1023 12:10:13.031044  4594 net.cpp:198] penlu7 needs backward computation.
I1023 12:10:13.031046  4594 net.cpp:198] Eltwise3 needs backward computation.
I1023 12:10:13.031049  4594 net.cpp:198] Scale7 needs backward computation.
I1023 12:10:13.031051  4594 net.cpp:198] BatchNorm7 needs backward computation.
I1023 12:10:13.031054  4594 net.cpp:198] Convolution7 needs backward computation.
I1023 12:10:13.031056  4594 net.cpp:198] penlu6 needs backward computation.
I1023 12:10:13.031059  4594 net.cpp:198] Scale6 needs backward computation.
I1023 12:10:13.031061  4594 net.cpp:198] BatchNorm6 needs backward computation.
I1023 12:10:13.031064  4594 net.cpp:198] Convolution6 needs backward computation.
I1023 12:10:13.031066  4594 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1023 12:10:13.031069  4594 net.cpp:198] penlu5 needs backward computation.
I1023 12:10:13.031071  4594 net.cpp:198] Eltwise2 needs backward computation.
I1023 12:10:13.031075  4594 net.cpp:198] Scale5 needs backward computation.
I1023 12:10:13.031076  4594 net.cpp:198] BatchNorm5 needs backward computation.
I1023 12:10:13.031080  4594 net.cpp:198] Convolution5 needs backward computation.
I1023 12:10:13.031081  4594 net.cpp:198] penlu4 needs backward computation.
I1023 12:10:13.031085  4594 net.cpp:198] Scale4 needs backward computation.
I1023 12:10:13.031086  4594 net.cpp:198] BatchNorm4 needs backward computation.
I1023 12:10:13.031088  4594 net.cpp:198] Convolution4 needs backward computation.
I1023 12:10:13.031091  4594 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1023 12:10:13.031095  4594 net.cpp:198] penlu3 needs backward computation.
I1023 12:10:13.031096  4594 net.cpp:198] Eltwise1 needs backward computation.
I1023 12:10:13.031100  4594 net.cpp:198] Scale3 needs backward computation.
I1023 12:10:13.031102  4594 net.cpp:198] BatchNorm3 needs backward computation.
I1023 12:10:13.031105  4594 net.cpp:198] Convolution3 needs backward computation.
I1023 12:10:13.031107  4594 net.cpp:198] penlu2 needs backward computation.
I1023 12:10:13.031110  4594 net.cpp:198] Scale2 needs backward computation.
I1023 12:10:13.031114  4594 net.cpp:198] BatchNorm2 needs backward computation.
I1023 12:10:13.031117  4594 net.cpp:198] Convolution2 needs backward computation.
I1023 12:10:13.031121  4594 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1023 12:10:13.031123  4594 net.cpp:198] penlu1 needs backward computation.
I1023 12:10:13.031126  4594 net.cpp:198] Scale1 needs backward computation.
I1023 12:10:13.031127  4594 net.cpp:198] BatchNorm1 needs backward computation.
I1023 12:10:13.031131  4594 net.cpp:198] Convolution1 needs backward computation.
I1023 12:10:13.031133  4594 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1023 12:10:13.031138  4594 net.cpp:200] Data1 does not need backward computation.
I1023 12:10:13.031141  4594 net.cpp:242] This network produces output Accuracy1
I1023 12:10:13.031143  4594 net.cpp:242] This network produces output SoftmaxWithLoss1
I1023 12:10:13.031182  4594 net.cpp:255] Network initialization done.
I1023 12:10:13.031466  4594 solver.cpp:56] Solver scaffolding done.
I1023 12:10:13.037688  4594 caffe.cpp:248] Starting Optimization
I1023 12:10:13.037696  4594 solver.cpp:272] Solving resnet
I1023 12:10:13.037699  4594 solver.cpp:273] Learning Rate Policy: multistep
I1023 12:10:13.040604  4594 solver.cpp:330] Iteration 0, Testing net (#0)
I1023 12:10:15.502687  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:10:15.640486  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0
I1023 12:10:15.640547  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I1023 12:10:15.871773  4594 solver.cpp:218] Iteration 0 (0 iter/s, 2.83398s/100 iters), loss = 2.29651
I1023 12:10:15.871803  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.29651 (* 1 = 2.29651 loss)
I1023 12:10:15.871812  4594 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1023 12:10:36.024983  4594 solver.cpp:218] Iteration 100 (4.96204 iter/s, 20.153s/100 iters), loss = 0.872307
I1023 12:10:36.025018  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.872307 (* 1 = 0.872307 loss)
I1023 12:10:36.025025  4594 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I1023 12:10:55.995744  4594 solver.cpp:330] Iteration 200, Testing net (#0)
I1023 12:10:58.418687  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:10:58.556535  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.117188
I1023 12:10:58.556596  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 28.582 (* 1 = 28.582 loss)
I1023 12:10:58.757053  4594 solver.cpp:218] Iteration 200 (4.39912 iter/s, 22.7318s/100 iters), loss = 1.01673
I1023 12:10:58.757086  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.01673 (* 1 = 1.01673 loss)
I1023 12:10:58.757093  4594 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I1023 12:11:18.756206  4594 solver.cpp:218] Iteration 300 (5.00027 iter/s, 19.9989s/100 iters), loss = 1.54574
I1023 12:11:18.756237  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.54574 (* 1 = 1.54574 loss)
I1023 12:11:18.756243  4594 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I1023 12:11:26.183524  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:11:38.557096  4594 solver.cpp:330] Iteration 400, Testing net (#0)
I1023 12:11:40.985716  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:11:41.123941  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.414062
I1023 12:11:41.124011  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.48363 (* 1 = 3.48363 loss)
I1023 12:11:41.325233  4594 solver.cpp:218] Iteration 400 (4.4309 iter/s, 22.5688s/100 iters), loss = 1.19256
I1023 12:11:41.325263  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.19256 (* 1 = 1.19256 loss)
I1023 12:11:41.325270  4594 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I1023 12:12:01.380825  4594 solver.cpp:218] Iteration 500 (4.9862 iter/s, 20.0554s/100 iters), loss = 0.554994
I1023 12:12:01.380973  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.554995 (* 1 = 0.554995 loss)
I1023 12:12:01.380983  4594 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I1023 12:12:21.240656  4594 solver.cpp:330] Iteration 600, Testing net (#0)
I1023 12:12:23.632542  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:12:23.808002  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.583984
I1023 12:12:23.808059  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.04854 (* 1 = 2.04854 loss)
I1023 12:12:24.009299  4594 solver.cpp:218] Iteration 600 (4.41928 iter/s, 22.6281s/100 iters), loss = 1.57076
I1023 12:12:24.009328  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.57076 (* 1 = 1.57076 loss)
I1023 12:12:24.009335  4594 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I1023 12:12:39.676515  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:12:44.068256  4594 solver.cpp:218] Iteration 700 (4.98535 iter/s, 20.0588s/100 iters), loss = 0.845645
I1023 12:12:44.068286  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.845645 (* 1 = 0.845645 loss)
I1023 12:12:44.068294  4594 sgd_solver.cpp:105] Iteration 700, lr = 0.01
I1023 12:13:03.917575  4594 solver.cpp:330] Iteration 800, Testing net (#0)
I1023 12:13:06.307904  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:13:06.484622  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.556641
I1023 12:13:06.484688  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.67576 (* 1 = 1.67576 loss)
I1023 12:13:06.686413  4594 solver.cpp:218] Iteration 800 (4.42127 iter/s, 22.6179s/100 iters), loss = 0.284946
I1023 12:13:06.686445  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284946 (* 1 = 0.284946 loss)
I1023 12:13:06.686452  4594 sgd_solver.cpp:105] Iteration 800, lr = 0.01
I1023 12:13:26.750828  4594 solver.cpp:218] Iteration 900 (4.984 iter/s, 20.0642s/100 iters), loss = 0.569353
I1023 12:13:26.750972  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.569353 (* 1 = 0.569353 loss)
I1023 12:13:26.750982  4594 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I1023 12:13:46.617796  4594 solver.cpp:330] Iteration 1000, Testing net (#0)
I1023 12:13:49.007328  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:13:49.184530  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.513672
I1023 12:13:49.184597  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.80189 (* 1 = 1.80189 loss)
I1023 12:13:49.386116  4594 solver.cpp:218] Iteration 1000 (4.41794 iter/s, 22.635s/100 iters), loss = 0.325001
I1023 12:13:49.386147  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.325001 (* 1 = 0.325001 loss)
I1023 12:13:49.386153  4594 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I1023 12:13:53.229359  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:14:09.454833  4594 solver.cpp:218] Iteration 1100 (4.98292 iter/s, 20.0686s/100 iters), loss = 0.161785
I1023 12:14:09.454978  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.161785 (* 1 = 0.161785 loss)
I1023 12:14:09.454988  4594 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I1023 12:14:29.321640  4594 solver.cpp:330] Iteration 1200, Testing net (#0)
I1023 12:14:31.711227  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:14:31.889336  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.572266
I1023 12:14:31.889400  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.47264 (* 1 = 1.47264 loss)
I1023 12:14:32.090533  4594 solver.cpp:218] Iteration 1200 (4.41785 iter/s, 22.6354s/100 iters), loss = 1.54723
I1023 12:14:32.090562  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.54723 (* 1 = 1.54723 loss)
I1023 12:14:32.090569  4594 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I1023 12:14:52.151062  4594 solver.cpp:218] Iteration 1300 (4.98495 iter/s, 20.0604s/100 iters), loss = 0.626008
I1023 12:14:52.151235  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.626008 (* 1 = 0.626008 loss)
I1023 12:14:52.151244  4594 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I1023 12:15:04.219779  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:15:12.021399  4594 solver.cpp:330] Iteration 1400, Testing net (#0)
I1023 12:15:14.372221  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:15:14.588662  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.585938
I1023 12:15:14.588721  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.37288 (* 1 = 1.37288 loss)
I1023 12:15:14.789954  4594 solver.cpp:218] Iteration 1400 (4.41723 iter/s, 22.6386s/100 iters), loss = 0.761062
I1023 12:15:14.789989  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.761061 (* 1 = 0.761061 loss)
I1023 12:15:14.789996  4594 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I1023 12:15:34.843199  4594 solver.cpp:218] Iteration 1500 (4.98675 iter/s, 20.0531s/100 iters), loss = 0.371528
I1023 12:15:34.843341  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.371528 (* 1 = 0.371528 loss)
I1023 12:15:34.843350  4594 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I1023 12:15:54.705152  4594 solver.cpp:330] Iteration 1600, Testing net (#0)
I1023 12:15:57.055608  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:15:57.271796  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.587891
I1023 12:15:57.271857  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.19495 (* 1 = 1.19495 loss)
I1023 12:15:57.472975  4594 solver.cpp:218] Iteration 1600 (4.419 iter/s, 22.6295s/100 iters), loss = 1.09276
I1023 12:15:57.473006  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.09276 (* 1 = 1.09276 loss)
I1023 12:15:57.473013  4594 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I1023 12:16:17.532968  4594 solver.cpp:218] Iteration 1700 (4.98508 iter/s, 20.0599s/100 iters), loss = 0.806989
I1023 12:16:17.533077  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.806988 (* 1 = 0.806988 loss)
I1023 12:16:17.533085  4594 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I1023 12:16:17.959542  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:16:37.397969  4594 solver.cpp:330] Iteration 1800, Testing net (#0)
I1023 12:16:39.748667  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:16:39.965709  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.496094
I1023 12:16:39.965770  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.5063 (* 1 = 2.5063 loss)
I1023 12:16:40.166872  4594 solver.cpp:218] Iteration 1800 (4.41819 iter/s, 22.6337s/100 iters), loss = 0.372314
I1023 12:16:40.166901  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372314 (* 1 = 0.372314 loss)
I1023 12:16:40.166908  4594 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I1023 12:17:00.222671  4594 solver.cpp:218] Iteration 1900 (4.98612 iter/s, 20.0557s/100 iters), loss = 0.684695
I1023 12:17:00.222815  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.684694 (* 1 = 0.684694 loss)
I1023 12:17:00.222823  4594 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I1023 12:17:20.081240  4594 solver.cpp:330] Iteration 2000, Testing net (#0)
I1023 12:17:22.433892  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:17:22.652823  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.703125
I1023 12:17:22.652875  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.14413 (* 1 = 1.14413 loss)
I1023 12:17:22.854710  4594 solver.cpp:218] Iteration 2000 (4.41856 iter/s, 22.6318s/100 iters), loss = 0.0177623
I1023 12:17:22.854743  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0177616 (* 1 = 0.0177616 loss)
I1023 12:17:22.854750  4594 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I1023 12:17:31.526036  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:17:42.959519  4594 solver.cpp:218] Iteration 2100 (4.97396 iter/s, 20.1047s/100 iters), loss = 0.861904
I1023 12:17:42.959550  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.861903 (* 1 = 0.861903 loss)
I1023 12:17:42.959558  4594 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I1023 12:18:02.852170  4594 solver.cpp:330] Iteration 2200, Testing net (#0)
I1023 12:18:05.167973  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:18:05.424691  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.646484
I1023 12:18:05.424751  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.09954 (* 1 = 1.09954 loss)
I1023 12:18:05.626838  4594 solver.cpp:218] Iteration 2200 (4.41166 iter/s, 22.6672s/100 iters), loss = 0.441971
I1023 12:18:05.626868  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44197 (* 1 = 0.44197 loss)
I1023 12:18:05.626875  4594 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I1023 12:18:25.725245  4594 solver.cpp:218] Iteration 2300 (4.97555 iter/s, 20.0983s/100 iters), loss = 0.083348
I1023 12:18:25.725277  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0833473 (* 1 = 0.0833473 loss)
I1023 12:18:25.725284  4594 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I1023 12:18:42.634181  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:18:45.626869  4594 solver.cpp:330] Iteration 2400, Testing net (#0)
I1023 12:18:47.944882  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:18:48.202013  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.646484
I1023 12:18:48.202080  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.44025 (* 1 = 1.44025 loss)
I1023 12:18:48.403451  4594 solver.cpp:218] Iteration 2400 (4.40954 iter/s, 22.6781s/100 iters), loss = 0.184681
I1023 12:18:48.403483  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18468 (* 1 = 0.18468 loss)
I1023 12:18:48.403491  4594 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I1023 12:19:08.491560  4594 solver.cpp:218] Iteration 2500 (4.9781 iter/s, 20.088s/100 iters), loss = 0.182979
I1023 12:19:08.491593  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.182979 (* 1 = 0.182979 loss)
I1023 12:19:08.491601  4594 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I1023 12:19:28.384560  4594 solver.cpp:330] Iteration 2600, Testing net (#0)
I1023 12:19:30.699784  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:19:30.958000  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.736328
I1023 12:19:30.958061  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.660515 (* 1 = 0.660515 loss)
I1023 12:19:31.159541  4594 solver.cpp:218] Iteration 2600 (4.41153 iter/s, 22.6679s/100 iters), loss = 0.100962
I1023 12:19:31.159574  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100962 (* 1 = 0.100962 loss)
I1023 12:19:31.159580  4594 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I1023 12:19:51.254283  4594 solver.cpp:218] Iteration 2700 (4.97645 iter/s, 20.0946s/100 iters), loss = 1.237
I1023 12:19:51.254317  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.237 (* 1 = 1.237 loss)
I1023 12:19:51.254323  4594 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I1023 12:19:56.308543  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:20:11.151952  4594 solver.cpp:330] Iteration 2800, Testing net (#0)
I1023 12:20:13.465260  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:20:13.724556  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.736328
I1023 12:20:13.724614  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.757989 (* 1 = 0.757989 loss)
I1023 12:20:13.926340  4594 solver.cpp:218] Iteration 2800 (4.41074 iter/s, 22.6719s/100 iters), loss = 0.0200486
I1023 12:20:13.926371  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0200475 (* 1 = 0.0200475 loss)
I1023 12:20:13.926378  4594 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I1023 12:20:34.020990  4594 solver.cpp:218] Iteration 2900 (4.97648 iter/s, 20.0945s/100 iters), loss = 0.291453
I1023 12:20:34.021024  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.291453 (* 1 = 0.291453 loss)
I1023 12:20:34.021031  4594 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I1023 12:20:53.917237  4594 solver.cpp:330] Iteration 3000, Testing net (#0)
I1023 12:20:56.194983  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:20:56.491405  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.753906
I1023 12:20:56.491467  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.648829 (* 1 = 0.648829 loss)
I1023 12:20:56.692845  4594 solver.cpp:218] Iteration 3000 (4.41078 iter/s, 22.6717s/100 iters), loss = 1.09535
I1023 12:20:56.692878  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.09535 (* 1 = 1.09535 loss)
I1023 12:20:56.692884  4594 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I1023 12:21:10.180259  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:21:16.789552  4594 solver.cpp:218] Iteration 3100 (4.97597 iter/s, 20.0966s/100 iters), loss = 0.89305
I1023 12:21:16.789587  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.893049 (* 1 = 0.893049 loss)
I1023 12:21:16.789593  4594 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I1023 12:21:36.673032  4594 solver.cpp:330] Iteration 3200, Testing net (#0)
I1023 12:21:38.948282  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:21:39.245745  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.724609
I1023 12:21:39.245810  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.05397 (* 1 = 1.05397 loss)
I1023 12:21:39.447518  4594 solver.cpp:218] Iteration 3200 (4.41348 iter/s, 22.6579s/100 iters), loss = 0.250572
I1023 12:21:39.447551  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250571 (* 1 = 0.250571 loss)
I1023 12:21:39.447558  4594 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I1023 12:21:59.543025  4594 solver.cpp:218] Iteration 3300 (4.97626 iter/s, 20.0954s/100 iters), loss = 0.195475
I1023 12:21:59.543056  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195474 (* 1 = 0.195474 loss)
I1023 12:21:59.543063  4594 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I1023 12:22:19.442354  4594 solver.cpp:330] Iteration 3400, Testing net (#0)
I1023 12:22:21.716543  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:22:22.014724  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.763672
I1023 12:22:22.014786  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.669861 (* 1 = 0.669861 loss)
I1023 12:22:22.216557  4594 solver.cpp:218] Iteration 3400 (4.41045 iter/s, 22.6734s/100 iters), loss = 0.0110384
I1023 12:22:22.216591  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110374 (* 1 = 0.0110374 loss)
I1023 12:22:22.216598  4594 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I1023 12:22:23.853883  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:22:42.316799  4594 solver.cpp:218] Iteration 3500 (4.97509 iter/s, 20.1001s/100 iters), loss = 0.693781
I1023 12:22:42.316833  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.69378 (* 1 = 0.69378 loss)
I1023 12:22:42.316839  4594 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I1023 12:23:02.217355  4594 solver.cpp:330] Iteration 3600, Testing net (#0)
I1023 12:23:04.491997  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:23:04.791851  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.734375
I1023 12:23:04.791910  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.01748 (* 1 = 1.01748 loss)
I1023 12:23:04.992986  4594 solver.cpp:218] Iteration 3600 (4.40993 iter/s, 22.6761s/100 iters), loss = 0.105046
I1023 12:23:04.993017  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105045 (* 1 = 0.105045 loss)
I1023 12:23:04.993024  4594 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I1023 12:23:25.082371  4594 solver.cpp:218] Iteration 3700 (4.97778 iter/s, 20.0893s/100 iters), loss = 0.010058
I1023 12:23:25.082404  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.010057 (* 1 = 0.010057 loss)
I1023 12:23:25.082412  4594 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I1023 12:23:34.959749  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:23:44.978350  4594 solver.cpp:330] Iteration 3800, Testing net (#0)
I1023 12:23:47.214913  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:23:47.551975  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9375
I1023 12:23:47.552034  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.215005 (* 1 = 0.215005 loss)
I1023 12:23:47.753772  4594 solver.cpp:218] Iteration 3800 (4.41086 iter/s, 22.6713s/100 iters), loss = 0.684385
I1023 12:23:47.753803  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.684384 (* 1 = 0.684384 loss)
I1023 12:23:47.753810  4594 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I1023 12:24:07.850083  4594 solver.cpp:218] Iteration 3900 (4.97606 iter/s, 20.0962s/100 iters), loss = 0.00448662
I1023 12:24:07.850217  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00448567 (* 1 = 0.00448567 loss)
I1023 12:24:07.850226  4594 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I1023 12:24:27.745880  4594 solver.cpp:330] Iteration 4000, Testing net (#0)
I1023 12:24:29.981971  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:24:30.319515  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.818359
I1023 12:24:30.319576  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.582003 (* 1 = 0.582003 loss)
I1023 12:24:30.521459  4594 solver.cpp:218] Iteration 4000 (4.41089 iter/s, 22.6712s/100 iters), loss = 0.183836
I1023 12:24:30.521492  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183836 (* 1 = 0.183836 loss)
I1023 12:24:30.521499  4594 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I1023 12:24:48.635090  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:24:50.620492  4594 solver.cpp:218] Iteration 4100 (4.97539 iter/s, 20.0989s/100 iters), loss = 0.0663461
I1023 12:24:50.620535  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0663452 (* 1 = 0.0663452 loss)
I1023 12:24:50.620543  4594 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I1023 12:25:10.515470  4594 solver.cpp:330] Iteration 4200, Testing net (#0)
I1023 12:25:12.749821  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:25:13.089030  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.822266
I1023 12:25:13.089095  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.91121 (* 1 = 0.91121 loss)
I1023 12:25:13.290813  4594 solver.cpp:218] Iteration 4200 (4.41108 iter/s, 22.6702s/100 iters), loss = 0.148298
I1023 12:25:13.290846  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.148297 (* 1 = 0.148297 loss)
I1023 12:25:13.290853  4594 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I1023 12:25:33.380000  4594 solver.cpp:218] Iteration 4300 (4.97783 iter/s, 20.0891s/100 iters), loss = 0.344754
I1023 12:25:33.380141  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.344753 (* 1 = 0.344753 loss)
I1023 12:25:33.380151  4594 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I1023 12:25:53.268282  4594 solver.cpp:330] Iteration 4400, Testing net (#0)
I1023 12:25:55.501710  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:25:55.840924  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.882812
I1023 12:25:55.840982  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.354641 (* 1 = 0.354641 loss)
I1023 12:25:56.042752  4594 solver.cpp:218] Iteration 4400 (4.41257 iter/s, 22.6625s/100 iters), loss = 0.014253
I1023 12:25:56.042786  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142514 (* 1 = 0.0142514 loss)
I1023 12:25:56.042794  4594 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I1023 12:26:02.505803  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:26:16.141898  4594 solver.cpp:218] Iteration 4500 (4.97536 iter/s, 20.0991s/100 iters), loss = 0.221465
I1023 12:26:16.142065  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221463 (* 1 = 0.221463 loss)
I1023 12:26:16.142083  4594 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I1023 12:26:36.037751  4594 solver.cpp:330] Iteration 4600, Testing net (#0)
I1023 12:26:38.234629  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:26:38.611910  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.867188
I1023 12:26:38.611977  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.459837 (* 1 = 0.459837 loss)
I1023 12:26:38.813751  4594 solver.cpp:218] Iteration 4600 (4.4108 iter/s, 22.6716s/100 iters), loss = 0.00821604
I1023 12:26:38.813786  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0082144 (* 1 = 0.0082144 loss)
I1023 12:26:38.813792  4594 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I1023 12:26:58.910233  4594 solver.cpp:218] Iteration 4700 (4.97602 iter/s, 20.0964s/100 iters), loss = 0.491075
I1023 12:26:58.910377  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.491073 (* 1 = 0.491073 loss)
I1023 12:26:58.910387  4594 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I1023 12:27:13.608836  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:27:18.815615  4594 solver.cpp:330] Iteration 4800, Testing net (#0)
I1023 12:27:21.012238  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:27:21.389789  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.904297
I1023 12:27:21.389853  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.227406 (* 1 = 0.227406 loss)
I1023 12:27:21.591737  4594 solver.cpp:218] Iteration 4800 (4.40892 iter/s, 22.6813s/100 iters), loss = 0.140467
I1023 12:27:21.591766  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140465 (* 1 = 0.140465 loss)
I1023 12:27:21.591773  4594 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I1023 12:27:41.676095  4594 solver.cpp:218] Iteration 4900 (4.97902 iter/s, 20.0843s/100 iters), loss = 0.0202915
I1023 12:27:41.676232  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.02029 (* 1 = 0.02029 loss)
I1023 12:27:41.676241  4594 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I1023 12:28:01.568763  4594 solver.cpp:330] Iteration 5000, Testing net (#0)
I1023 12:28:03.764111  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:28:04.143188  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.84375
I1023 12:28:04.143255  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.56478 (* 1 = 0.56478 loss)
I1023 12:28:04.344732  4594 solver.cpp:218] Iteration 5000 (4.41142 iter/s, 22.6684s/100 iters), loss = 0.0472299
I1023 12:28:04.344761  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0472284 (* 1 = 0.0472284 loss)
I1023 12:28:04.344769  4594 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1023 12:28:24.437742  4594 solver.cpp:218] Iteration 5100 (4.97688 iter/s, 20.0929s/100 iters), loss = 0.0358005
I1023 12:28:24.437876  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.035799 (* 1 = 0.035799 loss)
I1023 12:28:24.437885  4594 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1023 12:28:27.279176  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:28:44.336421  4594 solver.cpp:330] Iteration 5200, Testing net (#0)
I1023 12:28:46.530848  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:28:46.909232  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.890625
I1023 12:28:46.909298  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.319885 (* 1 = 0.319885 loss)
I1023 12:28:47.111258  4594 solver.cpp:218] Iteration 5200 (4.41047 iter/s, 22.6733s/100 iters), loss = 0.466753
I1023 12:28:47.111289  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.466752 (* 1 = 0.466752 loss)
I1023 12:28:47.111296  4594 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1023 12:29:07.207839  4594 solver.cpp:218] Iteration 5300 (4.97599 iter/s, 20.0965s/100 iters), loss = 0.260252
I1023 12:29:07.208011  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260251 (* 1 = 0.260251 loss)
I1023 12:29:07.208024  4594 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1023 12:29:27.102911  4594 solver.cpp:330] Iteration 5400, Testing net (#0)
I1023 12:29:29.259997  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:29:29.676864  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.871094
I1023 12:29:29.676929  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.381765 (* 1 = 0.381765 loss)
I1023 12:29:29.878338  4594 solver.cpp:218] Iteration 5400 (4.41106 iter/s, 22.6703s/100 iters), loss = 0.145838
I1023 12:29:29.878371  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145837 (* 1 = 0.145837 loss)
I1023 12:29:29.878378  4594 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1023 12:29:40.957455  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:29:49.976572  4594 solver.cpp:218] Iteration 5500 (4.97558 iter/s, 20.0981s/100 iters), loss = 0.207597
I1023 12:29:49.976603  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.207596 (* 1 = 0.207596 loss)
I1023 12:29:49.976611  4594 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1023 12:30:09.857250  4594 solver.cpp:330] Iteration 5600, Testing net (#0)
I1023 12:30:12.012989  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:30:12.431360  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.804688
I1023 12:30:12.431423  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.737933 (* 1 = 0.737933 loss)
I1023 12:30:12.633002  4594 solver.cpp:218] Iteration 5600 (4.41378 iter/s, 22.6563s/100 iters), loss = 0.0766352
I1023 12:30:12.633033  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0766339 (* 1 = 0.0766339 loss)
I1023 12:30:12.633039  4594 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1023 12:30:32.731061  4594 solver.cpp:218] Iteration 5700 (4.97563 iter/s, 20.098s/100 iters), loss = 0.0145046
I1023 12:30:32.731092  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0145033 (* 1 = 0.0145033 loss)
I1023 12:30:32.731099  4594 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1023 12:30:52.249980  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:30:52.630491  4594 solver.cpp:330] Iteration 5800, Testing net (#0)
I1023 12:30:54.785953  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:30:55.204135  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.939453
I1023 12:30:55.204195  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.155308 (* 1 = 0.155308 loss)
I1023 12:30:55.407297  4594 solver.cpp:218] Iteration 5800 (4.40992 iter/s, 22.6761s/100 iters), loss = 0.05239
I1023 12:30:55.407337  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0523886 (* 1 = 0.0523886 loss)
I1023 12:30:55.407344  4594 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1023 12:31:15.505928  4594 solver.cpp:218] Iteration 5900 (4.97549 iter/s, 20.0985s/100 iters), loss = 0.551219
I1023 12:31:15.505959  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.551217 (* 1 = 0.551217 loss)
I1023 12:31:15.505965  4594 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1023 12:31:35.405458  4594 solver.cpp:330] Iteration 6000, Testing net (#0)
I1023 12:31:37.559655  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:31:37.979586  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.863281
I1023 12:31:37.979645  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385002 (* 1 = 0.385002 loss)
I1023 12:31:38.187427  4594 solver.cpp:218] Iteration 6000 (4.4089 iter/s, 22.6814s/100 iters), loss = 0.128647
I1023 12:31:38.187458  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.128646 (* 1 = 0.128646 loss)
I1023 12:31:38.187463  4594 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1023 12:31:58.306140  4594 solver.cpp:218] Iteration 6100 (4.97052 iter/s, 20.1186s/100 iters), loss = 0.0119548
I1023 12:31:58.306172  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119534 (* 1 = 0.0119534 loss)
I1023 12:31:58.306180  4594 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1023 12:32:05.984596  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:32:18.232493  4594 solver.cpp:330] Iteration 6200, Testing net (#0)
I1023 12:32:20.350147  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:32:20.807986  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.951172
I1023 12:32:20.808043  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.127413 (* 1 = 0.127413 loss)
I1023 12:32:21.010052  4594 solver.cpp:218] Iteration 6200 (4.40455 iter/s, 22.7038s/100 iters), loss = 0.0104604
I1023 12:32:21.010083  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0104591 (* 1 = 0.0104591 loss)
I1023 12:32:21.010090  4594 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1023 12:32:41.135135  4594 solver.cpp:218] Iteration 6300 (4.96895 iter/s, 20.125s/100 iters), loss = 0.0354444
I1023 12:32:41.135273  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0354432 (* 1 = 0.0354432 loss)
I1023 12:32:41.135282  4594 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1023 12:33:01.064594  4594 solver.cpp:330] Iteration 6400, Testing net (#0)
I1023 12:33:03.182276  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:33:03.640365  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.949219
I1023 12:33:03.640422  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.137415 (* 1 = 0.137415 loss)
I1023 12:33:03.842798  4594 solver.cpp:218] Iteration 6400 (4.40384 iter/s, 22.7075s/100 iters), loss = 0.00785138
I1023 12:33:03.842831  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00785021 (* 1 = 0.00785021 loss)
I1023 12:33:03.842839  4594 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1023 12:33:19.769181  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:33:23.973105  4594 solver.cpp:218] Iteration 6500 (4.96766 iter/s, 20.1302s/100 iters), loss = 0.0163122
I1023 12:33:23.973147  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.016311 (* 1 = 0.016311 loss)
I1023 12:33:23.973155  4594 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1023 12:33:43.891486  4594 solver.cpp:330] Iteration 6600, Testing net (#0)
I1023 12:33:46.007800  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:33:46.467201  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.949219
I1023 12:33:46.467262  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.138804 (* 1 = 0.138804 loss)
I1023 12:33:46.668926  4594 solver.cpp:218] Iteration 6600 (4.40612 iter/s, 22.6957s/100 iters), loss = 0.0360247
I1023 12:33:46.668957  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0360237 (* 1 = 0.0360237 loss)
I1023 12:33:46.668964  4594 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1023 12:34:06.784528  4594 solver.cpp:218] Iteration 6700 (4.97129 iter/s, 20.1155s/100 iters), loss = 0.0337702
I1023 12:34:06.784658  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0337692 (* 1 = 0.0337692 loss)
I1023 12:34:06.784667  4594 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1023 12:34:26.701135  4594 solver.cpp:330] Iteration 6800, Testing net (#0)
I1023 12:34:28.815312  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:34:29.275418  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.794922
I1023 12:34:29.275478  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.858898 (* 1 = 0.858898 loss)
I1023 12:34:29.477596  4594 solver.cpp:218] Iteration 6800 (4.40667 iter/s, 22.6929s/100 iters), loss = 0.102063
I1023 12:34:29.477625  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.102062 (* 1 = 0.102062 loss)
I1023 12:34:29.477633  4594 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1023 12:34:33.535854  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:34:49.604945  4594 solver.cpp:218] Iteration 6900 (4.96838 iter/s, 20.1273s/100 iters), loss = 0.0769445
I1023 12:34:49.605118  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0769433 (* 1 = 0.0769433 loss)
I1023 12:34:49.605126  4594 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1023 12:35:09.531965  4594 solver.cpp:330] Iteration 7000, Testing net (#0)
I1023 12:35:11.610555  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:35:12.108427  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.951172
I1023 12:35:12.108492  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.148676 (* 1 = 0.148676 loss)
I1023 12:35:12.310703  4594 solver.cpp:218] Iteration 7000 (4.40421 iter/s, 22.7055s/100 iters), loss = 0.00542032
I1023 12:35:12.310734  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00541915 (* 1 = 0.00541915 loss)
I1023 12:35:12.310740  4594 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1023 12:35:32.435973  4594 solver.cpp:218] Iteration 7100 (4.9689 iter/s, 20.1252s/100 iters), loss = 0.045248
I1023 12:35:32.436122  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0452468 (* 1 = 0.0452468 loss)
I1023 12:35:32.436131  4594 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1023 12:35:44.945590  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:35:52.375674  4594 solver.cpp:330] Iteration 7200, Testing net (#0)
I1023 12:35:54.452752  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:35:54.952049  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.931641
I1023 12:35:54.952106  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.20481 (* 1 = 0.20481 loss)
I1023 12:35:55.153715  4594 solver.cpp:218] Iteration 7200 (4.40189 iter/s, 22.7175s/100 iters), loss = 0.289473
I1023 12:35:55.153745  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289471 (* 1 = 0.289471 loss)
I1023 12:35:55.153753  4594 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1023 12:36:15.265626  4594 solver.cpp:218] Iteration 7300 (4.9722 iter/s, 20.1118s/100 iters), loss = 0.365076
I1023 12:36:15.265771  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.365075 (* 1 = 0.365075 loss)
I1023 12:36:15.265780  4594 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1023 12:36:35.188961  4594 solver.cpp:330] Iteration 7400, Testing net (#0)
I1023 12:36:37.266103  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:36:37.764828  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 12:36:37.764884  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0974849 (* 1 = 0.0974849 loss)
I1023 12:36:37.966729  4594 solver.cpp:218] Iteration 7400 (4.40511 iter/s, 22.7009s/100 iters), loss = 0.0535904
I1023 12:36:37.966766  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0535892 (* 1 = 0.0535892 loss)
I1023 12:36:37.966773  4594 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1023 12:36:58.091986  4594 solver.cpp:218] Iteration 7500 (4.9689 iter/s, 20.1252s/100 iters), loss = 0.0692924
I1023 12:36:58.092129  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0692912 (* 1 = 0.0692912 loss)
I1023 12:36:58.092139  4594 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1023 12:36:58.721561  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:37:18.022316  4594 solver.cpp:330] Iteration 7600, Testing net (#0)
I1023 12:37:20.097406  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:37:20.598803  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.935547
I1023 12:37:20.598861  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.242197 (* 1 = 0.242197 loss)
I1023 12:37:20.800669  4594 solver.cpp:218] Iteration 7600 (4.40364 iter/s, 22.7085s/100 iters), loss = 0.249437
I1023 12:37:20.800704  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249436 (* 1 = 0.249436 loss)
I1023 12:37:20.800710  4594 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1023 12:37:40.924607  4594 solver.cpp:218] Iteration 7700 (4.96923 iter/s, 20.1239s/100 iters), loss = 0.278892
I1023 12:37:40.924763  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278891 (* 1 = 0.278891 loss)
I1023 12:37:40.924774  4594 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1023 12:38:00.853510  4594 solver.cpp:330] Iteration 7800, Testing net (#0)
I1023 12:38:02.892004  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:38:03.430783  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 12:38:03.430840  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0934823 (* 1 = 0.0934823 loss)
I1023 12:38:03.632658  4594 solver.cpp:218] Iteration 7800 (4.40376 iter/s, 22.7078s/100 iters), loss = 0.0220679
I1023 12:38:03.632694  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0220669 (* 1 = 0.0220669 loss)
I1023 12:38:03.632701  4594 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1023 12:38:12.518007  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:38:23.763521  4594 solver.cpp:218] Iteration 7900 (4.96752 iter/s, 20.1308s/100 iters), loss = 0.0247703
I1023 12:38:23.763556  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0247693 (* 1 = 0.0247693 loss)
I1023 12:38:23.763562  4594 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1023 12:38:43.683446  4594 solver.cpp:330] Iteration 8000, Testing net (#0)
I1023 12:38:45.721760  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:38:46.260006  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1023 12:38:46.260066  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.100741 (* 1 = 0.100741 loss)
I1023 12:38:46.462072  4594 solver.cpp:218] Iteration 8000 (4.40558 iter/s, 22.6985s/100 iters), loss = 0.0312442
I1023 12:38:46.462103  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0312433 (* 1 = 0.0312433 loss)
I1023 12:38:46.462110  4594 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1023 12:39:06.586859  4594 solver.cpp:218] Iteration 8100 (4.96902 iter/s, 20.1247s/100 iters), loss = 0.298481
I1023 12:39:06.586890  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.29848 (* 1 = 0.29848 loss)
I1023 12:39:06.586897  4594 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1023 12:39:23.723469  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:39:26.518079  4594 solver.cpp:330] Iteration 8200, Testing net (#0)
I1023 12:39:28.556932  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:39:29.096258  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.886719
I1023 12:39:29.096318  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.308766 (* 1 = 0.308766 loss)
I1023 12:39:29.299068  4594 solver.cpp:218] Iteration 8200 (4.40294 iter/s, 22.7121s/100 iters), loss = 0.53783
I1023 12:39:29.299104  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.537829 (* 1 = 0.537829 loss)
I1023 12:39:29.299111  4594 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1023 12:39:49.421792  4594 solver.cpp:218] Iteration 8300 (4.96953 iter/s, 20.1226s/100 iters), loss = 0.0296551
I1023 12:39:49.421823  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0296542 (* 1 = 0.0296542 loss)
I1023 12:39:49.421829  4594 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1023 12:40:09.350585  4594 solver.cpp:330] Iteration 8400, Testing net (#0)
I1023 12:40:11.386205  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:40:11.927053  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.908203
I1023 12:40:11.927117  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.280142 (* 1 = 0.280142 loss)
I1023 12:40:12.129022  4594 solver.cpp:218] Iteration 8400 (4.4039 iter/s, 22.7071s/100 iters), loss = 0.217201
I1023 12:40:12.129055  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2172 (* 1 = 0.2172 loss)
I1023 12:40:12.129060  4594 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1023 12:40:32.251726  4594 solver.cpp:218] Iteration 8500 (4.96953 iter/s, 20.1226s/100 iters), loss = 0.0118321
I1023 12:40:32.251760  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0118311 (* 1 = 0.0118311 loss)
I1023 12:40:32.251766  4594 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1023 12:40:37.715600  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:40:52.178932  4594 solver.cpp:330] Iteration 8600, Testing net (#0)
I1023 12:40:54.178411  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:40:54.755813  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.876953
I1023 12:40:54.755872  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.434001 (* 1 = 0.434001 loss)
I1023 12:40:54.957656  4594 solver.cpp:218] Iteration 8600 (4.40415 iter/s, 22.7058s/100 iters), loss = 0.100069
I1023 12:40:54.957687  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100068 (* 1 = 0.100068 loss)
I1023 12:40:54.957705  4594 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1023 12:41:15.084623  4594 solver.cpp:218] Iteration 8700 (4.96848 iter/s, 20.1269s/100 iters), loss = 0.22193
I1023 12:41:15.084655  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.221929 (* 1 = 0.221929 loss)
I1023 12:41:15.084661  4594 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1023 12:41:35.010614  4594 solver.cpp:330] Iteration 8800, Testing net (#0)
I1023 12:41:37.008476  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:41:37.587615  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.890625
I1023 12:41:37.587677  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.351505 (* 1 = 0.351505 loss)
I1023 12:41:37.789718  4594 solver.cpp:218] Iteration 8800 (4.40432 iter/s, 22.705s/100 iters), loss = 0.0584573
I1023 12:41:37.789750  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0584564 (* 1 = 0.0584564 loss)
I1023 12:41:37.789757  4594 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1023 12:41:51.503574  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:41:57.921767  4594 solver.cpp:218] Iteration 8900 (4.96723 iter/s, 20.132s/100 iters), loss = 0.017141
I1023 12:41:57.921800  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171402 (* 1 = 0.0171402 loss)
I1023 12:41:57.921808  4594 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1023 12:42:17.841027  4594 solver.cpp:330] Iteration 9000, Testing net (#0)
I1023 12:42:19.837258  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:42:20.417282  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9375
I1023 12:42:20.417348  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.17415 (* 1 = 0.17415 loss)
I1023 12:42:20.619146  4594 solver.cpp:218] Iteration 9000 (4.40581 iter/s, 22.6973s/100 iters), loss = 0.045923
I1023 12:42:20.619177  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0459221 (* 1 = 0.0459221 loss)
I1023 12:42:20.619184  4594 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1023 12:42:40.734872  4594 solver.cpp:218] Iteration 9100 (4.97126 iter/s, 20.1156s/100 iters), loss = 0.0583846
I1023 12:42:40.734905  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0583837 (* 1 = 0.0583837 loss)
I1023 12:42:40.734912  4594 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1023 12:43:00.649864  4594 solver.cpp:330] Iteration 9200, Testing net (#0)
I1023 12:43:02.644776  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:43:03.224931  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.935547
I1023 12:43:03.224992  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.236725 (* 1 = 0.236725 loss)
I1023 12:43:03.426972  4594 solver.cpp:218] Iteration 9200 (4.40684 iter/s, 22.692s/100 iters), loss = 0.016733
I1023 12:43:03.427002  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0167322 (* 1 = 0.0167322 loss)
I1023 12:43:03.427009  4594 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1023 12:43:05.266360  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:43:23.552821  4594 solver.cpp:218] Iteration 9300 (4.96876 iter/s, 20.1258s/100 iters), loss = 0.111099
I1023 12:43:23.552852  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111098 (* 1 = 0.111098 loss)
I1023 12:43:23.552858  4594 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1023 12:43:43.479025  4594 solver.cpp:330] Iteration 9400, Testing net (#0)
I1023 12:43:45.436511  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:43:46.054214  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.890625
I1023 12:43:46.054275  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.413532 (* 1 = 0.413532 loss)
I1023 12:43:46.256202  4594 solver.cpp:218] Iteration 9400 (4.40465 iter/s, 22.7033s/100 iters), loss = 0.00353914
I1023 12:43:46.256233  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00353841 (* 1 = 0.00353841 loss)
I1023 12:43:46.256239  4594 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1023 12:44:06.382344  4594 solver.cpp:218] Iteration 9500 (4.96868 iter/s, 20.1261s/100 iters), loss = 0.0425461
I1023 12:44:06.382375  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0425454 (* 1 = 0.0425454 loss)
I1023 12:44:06.382382  4594 sgd_solver.cpp:105] Iteration 9500, lr = 0.01
I1023 12:44:16.481452  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:44:26.317175  4594 solver.cpp:330] Iteration 9600, Testing net (#0)
I1023 12:44:28.273859  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:44:28.893599  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.859375
I1023 12:44:28.893649  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.422716 (* 1 = 0.422716 loss)
I1023 12:44:29.095633  4594 solver.cpp:218] Iteration 9600 (4.40273 iter/s, 22.7132s/100 iters), loss = 0.0279318
I1023 12:44:29.095664  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0279311 (* 1 = 0.0279311 loss)
I1023 12:44:29.095670  4594 sgd_solver.cpp:105] Iteration 9600, lr = 0.01
I1023 12:44:49.216068  4594 solver.cpp:218] Iteration 9700 (4.97009 iter/s, 20.1203s/100 iters), loss = 0.0131398
I1023 12:44:49.216214  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.013139 (* 1 = 0.013139 loss)
I1023 12:44:49.216223  4594 sgd_solver.cpp:105] Iteration 9700, lr = 0.01
I1023 12:45:09.139564  4594 solver.cpp:330] Iteration 9800, Testing net (#0)
I1023 12:45:11.095517  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:45:11.716188  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 12:45:11.716248  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0900868 (* 1 = 0.0900868 loss)
I1023 12:45:11.918174  4594 solver.cpp:218] Iteration 9800 (4.40492 iter/s, 22.7019s/100 iters), loss = 0.0316982
I1023 12:45:11.918210  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0316974 (* 1 = 0.0316974 loss)
I1023 12:45:11.918216  4594 sgd_solver.cpp:105] Iteration 9800, lr = 0.01
I1023 12:45:30.456919  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:45:32.044528  4594 solver.cpp:218] Iteration 9900 (4.96863 iter/s, 20.1263s/100 iters), loss = 0.0142726
I1023 12:45:32.044571  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142717 (* 1 = 0.0142717 loss)
I1023 12:45:32.044579  4594 sgd_solver.cpp:105] Iteration 9900, lr = 0.01
I1023 12:45:51.968487  4594 solver.cpp:330] Iteration 10000, Testing net (#0)
I1023 12:45:53.923617  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:45:54.544544  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.925781
I1023 12:45:54.544610  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.25011 (* 1 = 0.25011 loss)
I1023 12:45:54.746724  4594 solver.cpp:218] Iteration 10000 (4.40488 iter/s, 22.7021s/100 iters), loss = 0.0563517
I1023 12:45:54.746758  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0563509 (* 1 = 0.0563509 loss)
I1023 12:45:54.746764  4594 sgd_solver.cpp:46] MultiStep Status: Iteration 10000, step = 1
I1023 12:45:54.746768  4594 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1023 12:46:14.870250  4594 solver.cpp:218] Iteration 10100 (4.96933 iter/s, 20.1234s/100 iters), loss = 0.007464
I1023 12:46:14.870430  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0074631 (* 1 = 0.0074631 loss)
I1023 12:46:14.870440  4594 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1023 12:46:34.798564  4594 solver.cpp:330] Iteration 10200, Testing net (#0)
I1023 12:46:36.716878  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:46:37.374925  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1023 12:46:37.374991  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.109146 (* 1 = 0.109146 loss)
I1023 12:46:37.576840  4594 solver.cpp:218] Iteration 10200 (4.40405 iter/s, 22.7064s/100 iters), loss = 0.00907281
I1023 12:46:37.576872  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00907187 (* 1 = 0.00907187 loss)
I1023 12:46:37.576879  4594 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1023 12:46:44.246973  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:46:57.697175  4594 solver.cpp:218] Iteration 10300 (4.97012 iter/s, 20.1203s/100 iters), loss = 0.0140811
I1023 12:46:57.697324  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140802 (* 1 = 0.0140802 loss)
I1023 12:46:57.697333  4594 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1023 12:47:17.616812  4594 solver.cpp:330] Iteration 10400, Testing net (#0)
I1023 12:47:19.533125  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:47:20.192402  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 12:47:20.192467  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0916732 (* 1 = 0.0916732 loss)
I1023 12:47:20.394405  4594 solver.cpp:218] Iteration 10400 (4.40586 iter/s, 22.697s/100 iters), loss = 0.00360485
I1023 12:47:20.394435  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0036039 (* 1 = 0.0036039 loss)
I1023 12:47:20.394441  4594 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1023 12:47:40.519238  4594 solver.cpp:218] Iteration 10500 (4.96901 iter/s, 20.1248s/100 iters), loss = 0.00286916
I1023 12:47:40.519335  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00286825 (* 1 = 0.00286825 loss)
I1023 12:47:40.519343  4594 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1023 12:47:55.442891  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:48:00.454915  4594 solver.cpp:330] Iteration 10600, Testing net (#0)
I1023 12:48:02.370828  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:48:03.031581  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.972656
I1023 12:48:03.031641  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.084933 (* 1 = 0.084933 loss)
I1023 12:48:03.234005  4594 solver.cpp:218] Iteration 10600 (4.40245 iter/s, 22.7146s/100 iters), loss = 0.0890039
I1023 12:48:03.234036  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.089003 (* 1 = 0.089003 loss)
I1023 12:48:03.234042  4594 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1023 12:48:23.356842  4594 solver.cpp:218] Iteration 10700 (4.9695 iter/s, 20.1228s/100 iters), loss = 0.00223668
I1023 12:48:23.356987  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0022358 (* 1 = 0.0022358 loss)
I1023 12:48:23.356995  4594 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1023 12:48:43.285629  4594 solver.cpp:330] Iteration 10800, Testing net (#0)
I1023 12:48:45.200531  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:48:45.861522  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1023 12:48:45.861580  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0917928 (* 1 = 0.0917928 loss)
I1023 12:48:46.063427  4594 solver.cpp:218] Iteration 10800 (4.40405 iter/s, 22.7064s/100 iters), loss = 0.00105282
I1023 12:48:46.063460  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00105195 (* 1 = 0.00105195 loss)
I1023 12:48:46.063467  4594 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1023 12:49:06.185186  4594 solver.cpp:218] Iteration 10900 (4.96977 iter/s, 20.1217s/100 iters), loss = 0.011455
I1023 12:49:06.185343  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114541 (* 1 = 0.0114541 loss)
I1023 12:49:06.185365  4594 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1023 12:49:09.231230  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:49:26.111492  4594 solver.cpp:330] Iteration 11000, Testing net (#0)
I1023 12:49:27.988801  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:49:28.687875  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.953125
I1023 12:49:28.687939  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.103606 (* 1 = 0.103606 loss)
I1023 12:49:28.889734  4594 solver.cpp:218] Iteration 11000 (4.40444 iter/s, 22.7043s/100 iters), loss = 0.256163
I1023 12:49:28.889767  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256162 (* 1 = 0.256162 loss)
I1023 12:49:28.889773  4594 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1023 12:49:49.013025  4594 solver.cpp:218] Iteration 11100 (4.96939 iter/s, 20.1232s/100 iters), loss = 0.0231349
I1023 12:49:49.013172  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0231341 (* 1 = 0.0231341 loss)
I1023 12:49:49.013181  4594 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1023 12:50:08.939163  4594 solver.cpp:330] Iteration 11200, Testing net (#0)
I1023 12:50:10.815217  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:50:11.515425  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1023 12:50:11.515485  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0866104 (* 1 = 0.0866104 loss)
I1023 12:50:11.717605  4594 solver.cpp:218] Iteration 11200 (4.40444 iter/s, 22.7044s/100 iters), loss = 0.00142306
I1023 12:50:11.717641  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0014222 (* 1 = 0.0014222 loss)
I1023 12:50:11.717648  4594 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1023 12:50:23.217911  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:50:31.852746  4594 solver.cpp:218] Iteration 11300 (4.96646 iter/s, 20.1351s/100 iters), loss = 0.132029
I1023 12:50:31.852778  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.132028 (* 1 = 0.132028 loss)
I1023 12:50:31.852787  4594 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1023 12:50:51.769723  4594 solver.cpp:330] Iteration 11400, Testing net (#0)
I1023 12:50:53.645480  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:50:54.346577  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1023 12:50:54.346634  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0994351 (* 1 = 0.0994351 loss)
I1023 12:50:54.548564  4594 solver.cpp:218] Iteration 11400 (4.40612 iter/s, 22.6957s/100 iters), loss = 0.0556652
I1023 12:50:54.548595  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0556643 (* 1 = 0.0556643 loss)
I1023 12:50:54.548601  4594 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1023 12:51:14.666122  4594 solver.cpp:218] Iteration 11500 (4.9708 iter/s, 20.1175s/100 iters), loss = 0.0484544
I1023 12:51:14.666154  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0484535 (* 1 = 0.0484535 loss)
I1023 12:51:14.666160  4594 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1023 12:51:34.408344  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:51:34.587458  4594 solver.cpp:330] Iteration 11600, Testing net (#0)
I1023 12:51:36.462883  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:51:37.164273  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1023 12:51:37.164324  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0947651 (* 1 = 0.0947651 loss)
I1023 12:51:37.366370  4594 solver.cpp:218] Iteration 11600 (4.40525 iter/s, 22.7002s/100 iters), loss = 0.00392896
I1023 12:51:37.366406  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00392804 (* 1 = 0.00392804 loss)
I1023 12:51:37.366413  4594 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1023 12:51:57.490962  4594 solver.cpp:218] Iteration 11700 (4.96907 iter/s, 20.1245s/100 iters), loss = 0.0562457
I1023 12:51:57.490995  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0562448 (* 1 = 0.0562448 loss)
I1023 12:51:57.491003  4594 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1023 12:52:17.415693  4594 solver.cpp:330] Iteration 11800, Testing net (#0)
I1023 12:52:19.253813  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:52:19.992856  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 12:52:19.992914  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0743135 (* 1 = 0.0743135 loss)
I1023 12:52:20.194820  4594 solver.cpp:218] Iteration 11800 (4.40455 iter/s, 22.7038s/100 iters), loss = 0.00776917
I1023 12:52:20.194854  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00776824 (* 1 = 0.00776824 loss)
I1023 12:52:20.194860  4594 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1023 12:52:40.320835  4594 solver.cpp:218] Iteration 11900 (4.96871 iter/s, 20.1259s/100 iters), loss = 0.0206269
I1023 12:52:40.320868  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.020626 (* 1 = 0.020626 loss)
I1023 12:52:40.320874  4594 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1023 12:52:48.202364  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:53:00.251407  4594 solver.cpp:330] Iteration 12000, Testing net (#0)
I1023 12:53:02.088898  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:53:02.828519  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.958984
I1023 12:53:02.828588  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0972809 (* 1 = 0.0972809 loss)
I1023 12:53:03.030303  4594 solver.cpp:218] Iteration 12000 (4.40347 iter/s, 22.7094s/100 iters), loss = 0.00258895
I1023 12:53:03.030333  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00258805 (* 1 = 0.00258805 loss)
I1023 12:53:03.030350  4594 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1023 12:53:23.148854  4594 solver.cpp:218] Iteration 12100 (4.97056 iter/s, 20.1185s/100 iters), loss = 0.0323799
I1023 12:53:23.148998  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.032379 (* 1 = 0.032379 loss)
I1023 12:53:23.149016  4594 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1023 12:53:43.072366  4594 solver.cpp:330] Iteration 12200, Testing net (#0)
I1023 12:53:44.907804  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:53:45.648743  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.960938
I1023 12:53:45.648807  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0856403 (* 1 = 0.0856403 loss)
I1023 12:53:45.850735  4594 solver.cpp:218] Iteration 12200 (4.40496 iter/s, 22.7017s/100 iters), loss = 0.013838
I1023 12:53:45.850769  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0138371 (* 1 = 0.0138371 loss)
I1023 12:53:45.850775  4594 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1023 12:54:01.978984  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:54:05.981045  4594 solver.cpp:218] Iteration 12300 (4.96765 iter/s, 20.1302s/100 iters), loss = 0.0126964
I1023 12:54:05.981076  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126955 (* 1 = 0.0126955 loss)
I1023 12:54:05.981083  4594 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1023 12:54:25.902926  4594 solver.cpp:330] Iteration 12400, Testing net (#0)
I1023 12:54:27.735267  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:54:28.479128  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.958984
I1023 12:54:28.479179  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.103178 (* 1 = 0.103178 loss)
I1023 12:54:28.681258  4594 solver.cpp:218] Iteration 12400 (4.40526 iter/s, 22.7001s/100 iters), loss = 0.00354485
I1023 12:54:28.681289  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00354389 (* 1 = 0.00354389 loss)
I1023 12:54:28.681296  4594 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1023 12:54:48.805730  4594 solver.cpp:218] Iteration 12500 (4.96909 iter/s, 20.1244s/100 iters), loss = 0.0380263
I1023 12:54:48.805912  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380253 (* 1 = 0.0380253 loss)
I1023 12:54:48.805932  4594 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1023 12:55:08.732555  4594 solver.cpp:330] Iteration 12600, Testing net (#0)
I1023 12:55:10.529011  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:55:11.308949  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.960938
I1023 12:55:11.309006  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0917319 (* 1 = 0.0917319 loss)
I1023 12:55:11.510817  4594 solver.cpp:218] Iteration 12600 (4.40434 iter/s, 22.7049s/100 iters), loss = 0.152688
I1023 12:55:11.510849  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.152688 (* 1 = 0.152688 loss)
I1023 12:55:11.510855  4594 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1023 12:55:15.967083  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:55:31.630358  4594 solver.cpp:218] Iteration 12700 (4.97031 iter/s, 20.1195s/100 iters), loss = 0.00413612
I1023 12:55:31.630478  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00413513 (* 1 = 0.00413513 loss)
I1023 12:55:31.630486  4594 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1023 12:55:51.549410  4594 solver.cpp:330] Iteration 12800, Testing net (#0)
I1023 12:55:53.345134  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:55:54.125546  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.962891
I1023 12:55:54.125613  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0907375 (* 1 = 0.0907375 loss)
I1023 12:55:54.327512  4594 solver.cpp:218] Iteration 12800 (4.40587 iter/s, 22.697s/100 iters), loss = 0.11763
I1023 12:55:54.327543  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117629 (* 1 = 0.117629 loss)
I1023 12:55:54.327548  4594 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1023 12:56:14.450968  4594 solver.cpp:218] Iteration 12900 (4.96935 iter/s, 20.1234s/100 iters), loss = 0.00974641
I1023 12:56:14.451113  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00974543 (* 1 = 0.00974543 loss)
I1023 12:56:14.451122  4594 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1023 12:56:27.159898  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:56:34.386054  4594 solver.cpp:330] Iteration 13000, Testing net (#0)
I1023 12:56:36.182226  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:56:36.962572  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1023 12:56:36.962635  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.110575 (* 1 = 0.110575 loss)
I1023 12:56:37.164456  4594 solver.cpp:218] Iteration 13000 (4.40271 iter/s, 22.7133s/100 iters), loss = 0.0029694
I1023 12:56:37.164487  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00296843 (* 1 = 0.00296843 loss)
I1023 12:56:37.164494  4594 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1023 12:56:57.280941  4594 solver.cpp:218] Iteration 13100 (4.97107 iter/s, 20.1164s/100 iters), loss = 0.00598075
I1023 12:56:57.281114  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00597977 (* 1 = 0.00597977 loss)
I1023 12:56:57.281123  4594 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1023 12:57:17.209765  4594 solver.cpp:330] Iteration 13200, Testing net (#0)
I1023 12:57:19.004482  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:57:19.785459  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.964844
I1023 12:57:19.785512  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0876296 (* 1 = 0.0876296 loss)
I1023 12:57:19.986878  4594 solver.cpp:218] Iteration 13200 (4.40418 iter/s, 22.7057s/100 iters), loss = 0.00167005
I1023 12:57:19.986908  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00166906 (* 1 = 0.00166906 loss)
I1023 12:57:19.986915  4594 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1023 12:57:40.104830  4594 solver.cpp:218] Iteration 13300 (4.97071 iter/s, 20.1179s/100 iters), loss = 0.00226828
I1023 12:57:40.104967  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00226731 (* 1 = 0.00226731 loss)
I1023 12:57:40.104976  4594 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1023 12:57:40.937779  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:58:00.027420  4594 solver.cpp:330] Iteration 13400, Testing net (#0)
I1023 12:58:01.785508  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:58:02.605859  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.953125
I1023 12:58:02.605921  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.111686 (* 1 = 0.111686 loss)
I1023 12:58:02.807754  4594 solver.cpp:218] Iteration 13400 (4.40475 iter/s, 22.7027s/100 iters), loss = 0.00154712
I1023 12:58:02.807785  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00154616 (* 1 = 0.00154616 loss)
I1023 12:58:02.807792  4594 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1023 12:58:22.931460  4594 solver.cpp:218] Iteration 13500 (4.96928 iter/s, 20.1236s/100 iters), loss = 0.00747218
I1023 12:58:22.931537  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00747121 (* 1 = 0.00747121 loss)
I1023 12:58:22.931555  4594 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1023 12:58:42.859757  4594 solver.cpp:330] Iteration 13600, Testing net (#0)
I1023 12:58:44.615412  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:58:45.436193  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.958984
I1023 12:58:45.436260  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.105915 (* 1 = 0.105915 loss)
I1023 12:58:45.638276  4594 solver.cpp:218] Iteration 13600 (4.40399 iter/s, 22.7067s/100 iters), loss = 0.0100235
I1023 12:58:45.638308  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0100225 (* 1 = 0.0100225 loss)
I1023 12:58:45.638314  4594 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1023 12:58:54.725781  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:59:05.774560  4594 solver.cpp:218] Iteration 13700 (4.96618 iter/s, 20.1362s/100 iters), loss = 0.0109529
I1023 12:59:05.774592  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109519 (* 1 = 0.0109519 loss)
I1023 12:59:05.774600  4594 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1023 12:59:25.698734  4594 solver.cpp:330] Iteration 13800, Testing net (#0)
I1023 12:59:27.453341  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 12:59:28.275018  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 12:59:28.275075  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0889671 (* 1 = 0.0889671 loss)
I1023 12:59:28.476846  4594 solver.cpp:218] Iteration 13800 (4.40486 iter/s, 22.7022s/100 iters), loss = 0.0110299
I1023 12:59:28.476879  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.011029 (* 1 = 0.011029 loss)
I1023 12:59:28.476886  4594 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1023 12:59:48.594848  4594 solver.cpp:218] Iteration 13900 (4.97069 iter/s, 20.1179s/100 iters), loss = 0.0101107
I1023 12:59:48.594892  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0101097 (* 1 = 0.0101097 loss)
I1023 12:59:48.594908  4594 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1023 13:00:06.124702  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:00:08.517614  4594 solver.cpp:330] Iteration 14000, Testing net (#0)
I1023 13:00:10.273064  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:00:11.095057  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 13:00:11.095121  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0864881 (* 1 = 0.0864881 loss)
I1023 13:00:11.297178  4594 solver.cpp:218] Iteration 14000 (4.40485 iter/s, 22.7022s/100 iters), loss = 0.0154723
I1023 13:00:11.297210  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0154713 (* 1 = 0.0154713 loss)
I1023 13:00:11.297217  4594 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1023 13:00:31.418635  4594 solver.cpp:218] Iteration 14100 (4.96984 iter/s, 20.1214s/100 iters), loss = 0.0937021
I1023 13:00:31.418668  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.093701 (* 1 = 0.093701 loss)
I1023 13:00:31.418674  4594 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1023 13:00:51.343560  4594 solver.cpp:330] Iteration 14200, Testing net (#0)
I1023 13:00:53.061592  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:00:53.920886  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 13:00:53.920948  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0741381 (* 1 = 0.0741381 loss)
I1023 13:00:54.123366  4594 solver.cpp:218] Iteration 14200 (4.40439 iter/s, 22.7046s/100 iters), loss = 0.00603621
I1023 13:00:54.123396  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00603517 (* 1 = 0.00603517 loss)
I1023 13:00:54.123402  4594 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1023 13:01:14.249411  4594 solver.cpp:218] Iteration 14300 (4.96871 iter/s, 20.126s/100 iters), loss = 0.00188222
I1023 13:01:14.249454  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00188118 (* 1 = 0.00188118 loss)
I1023 13:01:14.249460  4594 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1023 13:01:19.914436  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:01:34.180100  4594 solver.cpp:330] Iteration 14400, Testing net (#0)
I1023 13:01:35.896718  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:01:36.756458  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:01:36.756517  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.067946 (* 1 = 0.067946 loss)
I1023 13:01:36.958287  4594 solver.cpp:218] Iteration 14400 (4.40358 iter/s, 22.7088s/100 iters), loss = 0.0113495
I1023 13:01:36.958322  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0113485 (* 1 = 0.0113485 loss)
I1023 13:01:36.958328  4594 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1023 13:01:57.078598  4594 solver.cpp:218] Iteration 14500 (4.97012 iter/s, 20.1202s/100 iters), loss = 0.0126377
I1023 13:01:57.078629  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126367 (* 1 = 0.0126367 loss)
I1023 13:01:57.078635  4594 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1023 13:02:17.002895  4594 solver.cpp:330] Iteration 14600, Testing net (#0)
I1023 13:02:18.717949  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:02:19.578774  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:02:19.578824  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0709664 (* 1 = 0.0709664 loss)
I1023 13:02:19.780817  4594 solver.cpp:218] Iteration 14600 (4.40487 iter/s, 22.7021s/100 iters), loss = 0.00722878
I1023 13:02:19.780848  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00722776 (* 1 = 0.00722776 loss)
I1023 13:02:19.780855  4594 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1023 13:02:33.695950  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:02:39.912636  4594 solver.cpp:218] Iteration 14700 (4.96728 iter/s, 20.1317s/100 iters), loss = 0.0426445
I1023 13:02:39.912667  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0426435 (* 1 = 0.0426435 loss)
I1023 13:02:39.912674  4594 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1023 13:02:59.831979  4594 solver.cpp:330] Iteration 14800, Testing net (#0)
I1023 13:03:01.546954  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:03:02.410210  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.972656
I1023 13:03:02.410257  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0798804 (* 1 = 0.0798804 loss)
I1023 13:03:02.612543  4594 solver.cpp:218] Iteration 14800 (4.40532 iter/s, 22.6998s/100 iters), loss = 0.0195063
I1023 13:03:02.612588  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0195054 (* 1 = 0.0195054 loss)
I1023 13:03:02.612596  4594 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1023 13:03:22.736379  4594 solver.cpp:218] Iteration 14900 (4.96925 iter/s, 20.1237s/100 iters), loss = 0.00873184
I1023 13:03:22.736426  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00873085 (* 1 = 0.00873085 loss)
I1023 13:03:22.736433  4594 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1023 13:03:42.661769  4594 solver.cpp:330] Iteration 15000, Testing net (#0)
I1023 13:03:44.337400  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:03:45.238279  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:03:45.238339  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.083379 (* 1 = 0.083379 loss)
I1023 13:03:45.440239  4594 solver.cpp:218] Iteration 15000 (4.40456 iter/s, 22.7038s/100 iters), loss = 0.0141357
I1023 13:03:45.440271  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0141347 (* 1 = 0.0141347 loss)
I1023 13:03:45.440277  4594 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1023 13:03:47.481245  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:04:05.559821  4594 solver.cpp:218] Iteration 15100 (4.9703 iter/s, 20.1195s/100 iters), loss = 0.00980051
I1023 13:04:05.559852  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00979956 (* 1 = 0.00979956 loss)
I1023 13:04:05.559859  4594 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1023 13:04:25.479131  4594 solver.cpp:330] Iteration 15200, Testing net (#0)
I1023 13:04:27.154392  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:04:28.055527  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:04:28.055580  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0797539 (* 1 = 0.0797539 loss)
I1023 13:04:28.257902  4594 solver.cpp:218] Iteration 15200 (4.40567 iter/s, 22.698s/100 iters), loss = 0.00687906
I1023 13:04:28.257935  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00687809 (* 1 = 0.00687809 loss)
I1023 13:04:28.257941  4594 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1023 13:04:48.382683  4594 solver.cpp:218] Iteration 15300 (4.96902 iter/s, 20.1247s/100 iters), loss = 0.0067835
I1023 13:04:48.382717  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00678253 (* 1 = 0.00678253 loss)
I1023 13:04:48.382725  4594 sgd_solver.cpp:105] Iteration 15300, lr = 0.001
I1023 13:04:58.876767  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:05:08.312381  4594 solver.cpp:330] Iteration 15400, Testing net (#0)
I1023 13:05:09.987231  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:05:10.889801  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.958984
I1023 13:05:10.889854  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.101878 (* 1 = 0.101878 loss)
I1023 13:05:11.091908  4594 solver.cpp:218] Iteration 15400 (4.40351 iter/s, 22.7091s/100 iters), loss = 0.00171902
I1023 13:05:11.091940  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00171805 (* 1 = 0.00171805 loss)
I1023 13:05:11.091948  4594 sgd_solver.cpp:105] Iteration 15400, lr = 0.001
I1023 13:05:31.218953  4594 solver.cpp:218] Iteration 15500 (4.96846 iter/s, 20.127s/100 iters), loss = 0.000320289
I1023 13:05:31.219127  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000319351 (* 1 = 0.000319351 loss)
I1023 13:05:31.219136  4594 sgd_solver.cpp:105] Iteration 15500, lr = 0.001
I1023 13:05:51.148847  4594 solver.cpp:330] Iteration 15600, Testing net (#0)
I1023 13:05:52.822898  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:05:53.725874  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 13:05:53.725922  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.082021 (* 1 = 0.082021 loss)
I1023 13:05:53.927683  4594 solver.cpp:218] Iteration 15600 (4.40363 iter/s, 22.7085s/100 iters), loss = 0.000877455
I1023 13:05:53.927716  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000876516 (* 1 = 0.000876516 loss)
I1023 13:05:53.927722  4594 sgd_solver.cpp:105] Iteration 15600, lr = 0.001
I1023 13:06:12.666785  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:06:14.052565  4594 solver.cpp:218] Iteration 15700 (4.96899 iter/s, 20.1248s/100 iters), loss = 0.0036733
I1023 13:06:14.052597  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00367237 (* 1 = 0.00367237 loss)
I1023 13:06:14.052603  4594 sgd_solver.cpp:105] Iteration 15700, lr = 0.001
I1023 13:06:33.973543  4594 solver.cpp:330] Iteration 15800, Testing net (#0)
I1023 13:06:35.609202  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:06:36.550622  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:06:36.550676  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0711686 (* 1 = 0.0711686 loss)
I1023 13:06:36.752578  4594 solver.cpp:218] Iteration 15800 (4.4053 iter/s, 22.6999s/100 iters), loss = 0.000534303
I1023 13:06:36.752615  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000533352 (* 1 = 0.000533352 loss)
I1023 13:06:36.752622  4594 sgd_solver.cpp:105] Iteration 15800, lr = 0.001
I1023 13:06:56.877995  4594 solver.cpp:218] Iteration 15900 (4.96886 iter/s, 20.1253s/100 iters), loss = 0.000953207
I1023 13:06:56.878140  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000952259 (* 1 = 0.000952259 loss)
I1023 13:06:56.878149  4594 sgd_solver.cpp:105] Iteration 15900, lr = 0.001
I1023 13:07:16.802811  4594 solver.cpp:330] Iteration 16000, Testing net (#0)
I1023 13:07:18.438146  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:07:19.379686  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:07:19.379739  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.070539 (* 1 = 0.070539 loss)
I1023 13:07:19.581614  4594 solver.cpp:218] Iteration 16000 (4.40462 iter/s, 22.7034s/100 iters), loss = 0.0109427
I1023 13:07:19.581651  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109418 (* 1 = 0.0109418 loss)
I1023 13:07:19.581658  4594 sgd_solver.cpp:105] Iteration 16000, lr = 0.001
I1023 13:07:26.455379  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:07:39.712275  4594 solver.cpp:218] Iteration 16100 (4.96757 iter/s, 20.1306s/100 iters), loss = 0.0108841
I1023 13:07:39.712401  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0108832 (* 1 = 0.0108832 loss)
I1023 13:07:39.712419  4594 sgd_solver.cpp:105] Iteration 16100, lr = 0.001
I1023 13:07:59.685221  4594 solver.cpp:330] Iteration 16200, Testing net (#0)
I1023 13:08:01.321349  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:08:02.263018  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:08:02.263065  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0827995 (* 1 = 0.0827995 loss)
I1023 13:08:02.464859  4594 solver.cpp:218] Iteration 16200 (4.39514 iter/s, 22.7524s/100 iters), loss = 0.013402
I1023 13:08:02.464890  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.013401 (* 1 = 0.013401 loss)
I1023 13:08:02.464896  4594 sgd_solver.cpp:105] Iteration 16200, lr = 0.001
I1023 13:08:22.582074  4594 solver.cpp:218] Iteration 16300 (4.97089 iter/s, 20.1171s/100 iters), loss = 0.127977
I1023 13:08:22.582240  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127976 (* 1 = 0.127976 loss)
I1023 13:08:22.582262  4594 sgd_solver.cpp:105] Iteration 16300, lr = 0.001
I1023 13:08:37.701082  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:08:42.508715  4594 solver.cpp:330] Iteration 16400, Testing net (#0)
I1023 13:08:44.141927  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:08:45.084484  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:08:45.084535  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.080534 (* 1 = 0.080534 loss)
I1023 13:08:45.286675  4594 solver.cpp:218] Iteration 16400 (4.40443 iter/s, 22.7044s/100 iters), loss = 0.0218306
I1023 13:08:45.286710  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0218297 (* 1 = 0.0218297 loss)
I1023 13:08:45.286717  4594 sgd_solver.cpp:105] Iteration 16400, lr = 0.001
I1023 13:09:05.409723  4594 solver.cpp:218] Iteration 16500 (4.96945 iter/s, 20.123s/100 iters), loss = 0.0184193
I1023 13:09:05.409869  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0184184 (* 1 = 0.0184184 loss)
I1023 13:09:05.409878  4594 sgd_solver.cpp:105] Iteration 16500, lr = 0.001
I1023 13:09:25.337601  4594 solver.cpp:330] Iteration 16600, Testing net (#0)
I1023 13:09:26.934406  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:09:27.914774  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:09:27.914829  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0894397 (* 1 = 0.0894397 loss)
I1023 13:09:28.117072  4594 solver.cpp:218] Iteration 16600 (4.4039 iter/s, 22.7072s/100 iters), loss = 0.00275985
I1023 13:09:28.117103  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.002759 (* 1 = 0.002759 loss)
I1023 13:09:28.117110  4594 sgd_solver.cpp:105] Iteration 16600, lr = 0.001
I1023 13:09:48.244223  4594 solver.cpp:218] Iteration 16700 (4.96843 iter/s, 20.1271s/100 iters), loss = 0.00570266
I1023 13:09:48.244374  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00570182 (* 1 = 0.00570182 loss)
I1023 13:09:48.244382  4594 sgd_solver.cpp:105] Iteration 16700, lr = 0.001
I1023 13:09:51.692049  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:10:08.175848  4594 solver.cpp:330] Iteration 16800, Testing net (#0)
I1023 13:10:09.770767  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:10:10.752501  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 13:10:10.752562  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0925085 (* 1 = 0.0925085 loss)
I1023 13:10:10.954063  4594 solver.cpp:218] Iteration 16800 (4.40342 iter/s, 22.7096s/100 iters), loss = 0.00538637
I1023 13:10:10.954094  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00538554 (* 1 = 0.00538554 loss)
I1023 13:10:10.954100  4594 sgd_solver.cpp:105] Iteration 16800, lr = 0.001
I1023 13:10:31.075022  4594 solver.cpp:218] Iteration 16900 (4.96996 iter/s, 20.1209s/100 iters), loss = 0.00802377
I1023 13:10:31.075139  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00802296 (* 1 = 0.00802296 loss)
I1023 13:10:31.075147  4594 sgd_solver.cpp:105] Iteration 16900, lr = 0.001
I1023 13:10:50.999263  4594 solver.cpp:330] Iteration 17000, Testing net (#0)
I1023 13:10:52.593256  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:10:53.575706  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:10:53.575762  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0815472 (* 1 = 0.0815472 loss)
I1023 13:10:53.777751  4594 solver.cpp:218] Iteration 17000 (4.40479 iter/s, 22.7026s/100 iters), loss = 0.0016467
I1023 13:10:53.777784  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0016459 (* 1 = 0.0016459 loss)
I1023 13:10:53.777793  4594 sgd_solver.cpp:105] Iteration 17000, lr = 0.001
I1023 13:11:05.481283  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:11:13.913930  4594 solver.cpp:218] Iteration 17100 (4.96621 iter/s, 20.1361s/100 iters), loss = 0.00297999
I1023 13:11:13.913964  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00297915 (* 1 = 0.00297915 loss)
I1023 13:11:13.913971  4594 sgd_solver.cpp:105] Iteration 17100, lr = 0.001
I1023 13:11:33.831830  4594 solver.cpp:330] Iteration 17200, Testing net (#0)
I1023 13:11:35.425264  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:11:36.408329  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.978516
I1023 13:11:36.408493  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0690842 (* 1 = 0.0690842 loss)
I1023 13:11:36.610829  4594 solver.cpp:218] Iteration 17200 (4.4059 iter/s, 22.6968s/100 iters), loss = 0.00254705
I1023 13:11:36.610860  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00254624 (* 1 = 0.00254624 loss)
I1023 13:11:36.610867  4594 sgd_solver.cpp:105] Iteration 17200, lr = 0.001
I1023 13:11:56.735317  4594 solver.cpp:218] Iteration 17300 (4.96909 iter/s, 20.1244s/100 iters), loss = 0.00394918
I1023 13:11:56.735349  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00394838 (* 1 = 0.00394838 loss)
I1023 13:11:56.735357  4594 sgd_solver.cpp:105] Iteration 17300, lr = 0.001
I1023 13:12:16.662663  4594 solver.cpp:330] Iteration 17400, Testing net (#0)
I1023 13:12:18.219117  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:12:19.239517  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.972656
I1023 13:12:19.239578  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0814165 (* 1 = 0.0814165 loss)
I1023 13:12:19.264147  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:12:19.441359  4594 solver.cpp:218] Iteration 17400 (4.40413 iter/s, 22.706s/100 iters), loss = 0.134369
I1023 13:12:19.441391  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.134368 (* 1 = 0.134368 loss)
I1023 13:12:19.441398  4594 sgd_solver.cpp:105] Iteration 17400, lr = 0.001
I1023 13:12:39.559994  4594 solver.cpp:218] Iteration 17500 (4.97054 iter/s, 20.1186s/100 iters), loss = 0.0589959
I1023 13:12:39.560026  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.058995 (* 1 = 0.058995 loss)
I1023 13:12:39.560032  4594 sgd_solver.cpp:105] Iteration 17500, lr = 0.001
I1023 13:12:59.480350  4594 solver.cpp:330] Iteration 17600, Testing net (#0)
I1023 13:13:01.037715  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:13:02.057016  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:13:02.057075  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0976123 (* 1 = 0.0976123 loss)
I1023 13:13:02.259150  4594 solver.cpp:218] Iteration 17600 (4.40547 iter/s, 22.6991s/100 iters), loss = 0.0304218
I1023 13:13:02.259181  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.030421 (* 1 = 0.030421 loss)
I1023 13:13:02.259188  4594 sgd_solver.cpp:105] Iteration 17600, lr = 0.001
I1023 13:13:22.384711  4594 solver.cpp:218] Iteration 17700 (4.96883 iter/s, 20.1255s/100 iters), loss = 0.210624
I1023 13:13:22.384740  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210623 (* 1 = 0.210623 loss)
I1023 13:13:22.384747  4594 sgd_solver.cpp:105] Iteration 17700, lr = 0.001
I1023 13:13:30.465538  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:13:42.313484  4594 solver.cpp:330] Iteration 17800, Testing net (#0)
I1023 13:13:43.867291  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:13:44.890177  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.960938
I1023 13:13:44.890238  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.105011 (* 1 = 0.105011 loss)
I1023 13:13:45.092417  4594 solver.cpp:218] Iteration 17800 (4.40381 iter/s, 22.7076s/100 iters), loss = 0.0520136
I1023 13:13:45.092448  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0520127 (* 1 = 0.0520127 loss)
I1023 13:13:45.092454  4594 sgd_solver.cpp:105] Iteration 17800, lr = 0.001
I1023 13:14:05.218245  4594 solver.cpp:218] Iteration 17900 (4.96876 iter/s, 20.1257s/100 iters), loss = 0.00818984
I1023 13:14:05.218403  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00818897 (* 1 = 0.00818897 loss)
I1023 13:14:05.218412  4594 sgd_solver.cpp:105] Iteration 17900, lr = 0.001
I1023 13:14:25.144152  4594 solver.cpp:330] Iteration 18000, Testing net (#0)
I1023 13:14:26.697509  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:14:27.720933  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:14:27.720979  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0817417 (* 1 = 0.0817417 loss)
I1023 13:14:27.923168  4594 solver.cpp:218] Iteration 18000 (4.40437 iter/s, 22.7047s/100 iters), loss = 0.0280244
I1023 13:14:27.923202  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0280235 (* 1 = 0.0280235 loss)
I1023 13:14:27.923209  4594 sgd_solver.cpp:105] Iteration 18000, lr = 0.001
I1023 13:14:44.447271  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:14:48.048422  4594 solver.cpp:218] Iteration 18100 (4.9689 iter/s, 20.1252s/100 iters), loss = 0.014017
I1023 13:14:48.048454  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140162 (* 1 = 0.0140162 loss)
I1023 13:14:48.048460  4594 sgd_solver.cpp:105] Iteration 18100, lr = 0.001
I1023 13:15:07.970073  4594 solver.cpp:330] Iteration 18200, Testing net (#0)
I1023 13:15:09.485668  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:15:10.546582  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:15:10.546643  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0850795 (* 1 = 0.0850795 loss)
I1023 13:15:10.748528  4594 solver.cpp:218] Iteration 18200 (4.40528 iter/s, 22.7s/100 iters), loss = 0.00729379
I1023 13:15:10.748564  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00729296 (* 1 = 0.00729296 loss)
I1023 13:15:10.748571  4594 sgd_solver.cpp:105] Iteration 18200, lr = 0.001
I1023 13:15:30.872306  4594 solver.cpp:218] Iteration 18300 (4.96927 iter/s, 20.1237s/100 iters), loss = 0.00484515
I1023 13:15:30.872448  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00484429 (* 1 = 0.00484429 loss)
I1023 13:15:30.872457  4594 sgd_solver.cpp:105] Iteration 18300, lr = 0.001
I1023 13:15:50.796895  4594 solver.cpp:330] Iteration 18400, Testing net (#0)
I1023 13:15:52.311596  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:15:53.373105  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:15:53.373168  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0855124 (* 1 = 0.0855124 loss)
I1023 13:15:53.574964  4594 solver.cpp:218] Iteration 18400 (4.40481 iter/s, 22.7025s/100 iters), loss = 0.00643456
I1023 13:15:53.574995  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00643371 (* 1 = 0.00643371 loss)
I1023 13:15:53.575002  4594 sgd_solver.cpp:105] Iteration 18400, lr = 0.001
I1023 13:15:58.233476  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:16:13.703057  4594 solver.cpp:218] Iteration 18500 (4.9682 iter/s, 20.128s/100 iters), loss = 0.00951504
I1023 13:16:13.703197  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00951421 (* 1 = 0.00951421 loss)
I1023 13:16:13.703207  4594 sgd_solver.cpp:105] Iteration 18500, lr = 0.001
I1023 13:16:33.629374  4594 solver.cpp:330] Iteration 18600, Testing net (#0)
I1023 13:16:35.143435  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:16:36.206001  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:16:36.206059  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0727452 (* 1 = 0.0727452 loss)
I1023 13:16:36.407661  4594 solver.cpp:218] Iteration 18600 (4.40443 iter/s, 22.7044s/100 iters), loss = 0.00445303
I1023 13:16:36.407696  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0044522 (* 1 = 0.0044522 loss)
I1023 13:16:36.407702  4594 sgd_solver.cpp:105] Iteration 18600, lr = 0.001
I1023 13:16:56.523815  4594 solver.cpp:218] Iteration 18700 (4.97115 iter/s, 20.1161s/100 iters), loss = 0.00873875
I1023 13:16:56.523927  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00873791 (* 1 = 0.00873791 loss)
I1023 13:16:56.523946  4594 sgd_solver.cpp:105] Iteration 18700, lr = 0.001
I1023 13:17:09.433516  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:17:16.455230  4594 solver.cpp:330] Iteration 18800, Testing net (#0)
I1023 13:17:17.969669  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:17:19.031633  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.974609
I1023 13:17:19.031697  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0725057 (* 1 = 0.0725057 loss)
I1023 13:17:19.234418  4594 solver.cpp:218] Iteration 18800 (4.40326 iter/s, 22.7104s/100 iters), loss = 0.052109
I1023 13:17:19.234450  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0521082 (* 1 = 0.0521082 loss)
I1023 13:17:19.234457  4594 sgd_solver.cpp:105] Iteration 18800, lr = 0.001
I1023 13:17:39.353955  4594 solver.cpp:218] Iteration 18900 (4.97031 iter/s, 20.1195s/100 iters), loss = 0.00616721
I1023 13:17:39.354097  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00616637 (* 1 = 0.00616637 loss)
I1023 13:17:39.354106  4594 sgd_solver.cpp:105] Iteration 18900, lr = 0.001
I1023 13:17:59.282236  4594 solver.cpp:330] Iteration 19000, Testing net (#0)
I1023 13:18:00.758581  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:18:01.860189  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:18:01.860247  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0792608 (* 1 = 0.0792608 loss)
I1023 13:18:02.062611  4594 solver.cpp:218] Iteration 19000 (4.40364 iter/s, 22.7085s/100 iters), loss = 0.00406951
I1023 13:18:02.062644  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00406867 (* 1 = 0.00406867 loss)
I1023 13:18:02.062651  4594 sgd_solver.cpp:105] Iteration 19000, lr = 0.001
I1023 13:18:22.192385  4594 solver.cpp:218] Iteration 19100 (4.96779 iter/s, 20.1297s/100 iters), loss = 0.000408579
I1023 13:18:22.192543  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000407732 (* 1 = 0.000407732 loss)
I1023 13:18:22.192551  4594 sgd_solver.cpp:105] Iteration 19100, lr = 0.001
I1023 13:18:23.227172  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:18:42.121937  4594 solver.cpp:330] Iteration 19200, Testing net (#0)
I1023 13:18:43.596815  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:18:44.699684  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.964844
I1023 13:18:44.699748  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0902508 (* 1 = 0.0902508 loss)
I1023 13:18:44.901468  4594 solver.cpp:218] Iteration 19200 (4.40356 iter/s, 22.7089s/100 iters), loss = 0.00334269
I1023 13:18:44.901499  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00334184 (* 1 = 0.00334184 loss)
I1023 13:18:44.901506  4594 sgd_solver.cpp:105] Iteration 19200, lr = 0.001
I1023 13:19:05.022822  4594 solver.cpp:218] Iteration 19300 (4.96986 iter/s, 20.1213s/100 iters), loss = 0.0112343
I1023 13:19:05.022958  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112334 (* 1 = 0.0112334 loss)
I1023 13:19:05.022966  4594 sgd_solver.cpp:105] Iteration 19300, lr = 0.001
I1023 13:19:24.946740  4594 solver.cpp:330] Iteration 19400, Testing net (#0)
I1023 13:19:26.420616  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:19:27.523054  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.972656
I1023 13:19:27.523110  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0894622 (* 1 = 0.0894622 loss)
I1023 13:19:27.724958  4594 solver.cpp:218] Iteration 19400 (4.40491 iter/s, 22.702s/100 iters), loss = 0.0012947
I1023 13:19:27.724997  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00129384 (* 1 = 0.00129384 loss)
I1023 13:19:27.725004  4594 sgd_solver.cpp:105] Iteration 19400, lr = 0.001
I1023 13:19:37.212852  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:19:47.860494  4594 solver.cpp:218] Iteration 19500 (4.96637 iter/s, 20.1355s/100 iters), loss = 0.0628961
I1023 13:19:47.860528  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0628953 (* 1 = 0.0628953 loss)
I1023 13:19:47.860535  4594 sgd_solver.cpp:105] Iteration 19500, lr = 0.001
I1023 13:20:07.777029  4594 solver.cpp:330] Iteration 19600, Testing net (#0)
I1023 13:20:09.250396  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:20:10.353590  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 13:20:10.353651  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0845166 (* 1 = 0.0845166 loss)
I1023 13:20:10.555423  4594 solver.cpp:218] Iteration 19600 (4.40629 iter/s, 22.6948s/100 iters), loss = 0.0212344
I1023 13:20:10.555454  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0212336 (* 1 = 0.0212336 loss)
I1023 13:20:10.555459  4594 sgd_solver.cpp:105] Iteration 19600, lr = 0.001
I1023 13:20:30.680011  4594 solver.cpp:218] Iteration 19700 (4.96907 iter/s, 20.1245s/100 iters), loss = 0.00115448
I1023 13:20:30.680042  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00115372 (* 1 = 0.00115372 loss)
I1023 13:20:30.680048  4594 sgd_solver.cpp:105] Iteration 19700, lr = 0.001
I1023 13:20:48.417799  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:20:50.609869  4594 solver.cpp:330] Iteration 19800, Testing net (#0)
I1023 13:20:52.048224  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:20:53.188688  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1023 13:20:53.188755  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0855861 (* 1 = 0.0855861 loss)
I1023 13:20:53.390873  4594 solver.cpp:218] Iteration 19800 (4.4032 iter/s, 22.7108s/100 iters), loss = 0.00308563
I1023 13:20:53.390904  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00308481 (* 1 = 0.00308481 loss)
I1023 13:20:53.390911  4594 sgd_solver.cpp:105] Iteration 19800, lr = 0.001
I1023 13:21:13.504914  4594 solver.cpp:218] Iteration 19900 (4.97167 iter/s, 20.114s/100 iters), loss = 0.00028122
I1023 13:21:13.504945  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0002804 (* 1 = 0.0002804 loss)
I1023 13:21:13.504951  4594 sgd_solver.cpp:105] Iteration 19900, lr = 0.001
I1023 13:21:33.424417  4594 solver.cpp:330] Iteration 20000, Testing net (#0)
I1023 13:21:34.858855  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:21:36.000196  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:21:36.000258  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.080977 (* 1 = 0.080977 loss)
I1023 13:21:36.202364  4594 solver.cpp:218] Iteration 20000 (4.4058 iter/s, 22.6974s/100 iters), loss = 0.0240081
I1023 13:21:36.202399  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0240073 (* 1 = 0.0240073 loss)
I1023 13:21:36.202405  4594 sgd_solver.cpp:46] MultiStep Status: Iteration 20000, step = 2
I1023 13:21:36.202409  4594 sgd_solver.cpp:105] Iteration 20000, lr = 0.0001
I1023 13:21:56.328578  4594 solver.cpp:218] Iteration 20100 (4.96866 iter/s, 20.1261s/100 iters), loss = 0.000994908
I1023 13:21:56.328610  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00099408 (* 1 = 0.00099408 loss)
I1023 13:21:56.328616  4594 sgd_solver.cpp:105] Iteration 20100, lr = 0.0001
I1023 13:22:02.198267  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:22:16.261251  4594 solver.cpp:330] Iteration 20200, Testing net (#0)
I1023 13:22:17.695718  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:22:18.839040  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:22:18.839095  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0831599 (* 1 = 0.0831599 loss)
I1023 13:22:19.040940  4594 solver.cpp:218] Iteration 20200 (4.4029 iter/s, 22.7123s/100 iters), loss = 0.00125401
I1023 13:22:19.040973  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00125316 (* 1 = 0.00125316 loss)
I1023 13:22:19.040980  4594 sgd_solver.cpp:105] Iteration 20200, lr = 0.0001
I1023 13:22:39.167878  4594 solver.cpp:218] Iteration 20300 (4.96848 iter/s, 20.1269s/100 iters), loss = 0.0130026
I1023 13:22:39.167909  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130017 (* 1 = 0.0130017 loss)
I1023 13:22:39.167917  4594 sgd_solver.cpp:105] Iteration 20300, lr = 0.0001
I1023 13:22:59.098325  4594 solver.cpp:330] Iteration 20400, Testing net (#0)
I1023 13:23:00.532212  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:23:01.676295  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:23:01.676357  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0845054 (* 1 = 0.0845054 loss)
I1023 13:23:01.878243  4594 solver.cpp:218] Iteration 20400 (4.40329 iter/s, 22.7103s/100 iters), loss = 0.00517984
I1023 13:23:01.878276  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.005179 (* 1 = 0.005179 loss)
I1023 13:23:01.878283  4594 sgd_solver.cpp:105] Iteration 20400, lr = 0.0001
I1023 13:23:15.994374  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:23:22.009050  4594 solver.cpp:218] Iteration 20500 (4.96753 iter/s, 20.1307s/100 iters), loss = 0.0311962
I1023 13:23:22.009083  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0311954 (* 1 = 0.0311954 loss)
I1023 13:23:22.009089  4594 sgd_solver.cpp:105] Iteration 20500, lr = 0.0001
I1023 13:23:41.927170  4594 solver.cpp:330] Iteration 20600, Testing net (#0)
I1023 13:23:43.323185  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:23:44.503979  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:23:44.504039  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0858064 (* 1 = 0.0858064 loss)
I1023 13:23:44.706130  4594 solver.cpp:218] Iteration 20600 (4.40587 iter/s, 22.697s/100 iters), loss = 0.00529958
I1023 13:23:44.706162  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00529872 (* 1 = 0.00529872 loss)
I1023 13:23:44.706169  4594 sgd_solver.cpp:105] Iteration 20600, lr = 0.0001
I1023 13:24:04.830339  4594 solver.cpp:218] Iteration 20700 (4.96916 iter/s, 20.1241s/100 iters), loss = 0.00141639
I1023 13:24:04.830370  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00141553 (* 1 = 0.00141553 loss)
I1023 13:24:04.830377  4594 sgd_solver.cpp:105] Iteration 20700, lr = 0.0001
I1023 13:24:24.757375  4594 solver.cpp:330] Iteration 20800, Testing net (#0)
I1023 13:24:26.151602  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:24:27.334270  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:24:27.334327  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0876696 (* 1 = 0.0876696 loss)
I1023 13:24:27.536408  4594 solver.cpp:218] Iteration 20800 (4.40413 iter/s, 22.706s/100 iters), loss = 0.00288234
I1023 13:24:27.536439  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00288149 (* 1 = 0.00288149 loss)
I1023 13:24:27.536447  4594 sgd_solver.cpp:105] Iteration 20800, lr = 0.0001
I1023 13:24:29.978138  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:24:47.664809  4594 solver.cpp:218] Iteration 20900 (4.96812 iter/s, 20.1283s/100 iters), loss = 0.0202552
I1023 13:24:47.664841  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0202544 (* 1 = 0.0202544 loss)
I1023 13:24:47.664847  4594 sgd_solver.cpp:105] Iteration 20900, lr = 0.0001
I1023 13:25:07.592538  4594 solver.cpp:330] Iteration 21000, Testing net (#0)
I1023 13:25:08.985553  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:25:10.169443  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:25:10.169505  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0914113 (* 1 = 0.0914113 loss)
I1023 13:25:10.371287  4594 solver.cpp:218] Iteration 21000 (4.40405 iter/s, 22.7064s/100 iters), loss = 0.00224906
I1023 13:25:10.371323  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0022482 (* 1 = 0.0022482 loss)
I1023 13:25:10.371330  4594 sgd_solver.cpp:105] Iteration 21000, lr = 0.0001
I1023 13:25:30.487193  4594 solver.cpp:218] Iteration 21100 (4.97121 iter/s, 20.1158s/100 iters), loss = 0.00289593
I1023 13:25:30.487224  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00289506 (* 1 = 0.00289506 loss)
I1023 13:25:30.487231  4594 sgd_solver.cpp:105] Iteration 21100, lr = 0.0001
I1023 13:25:41.181646  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:25:50.411957  4594 solver.cpp:330] Iteration 21200, Testing net (#0)
I1023 13:25:51.805488  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:25:52.987996  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:25:52.988049  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0902241 (* 1 = 0.0902241 loss)
I1023 13:25:53.190073  4594 solver.cpp:218] Iteration 21200 (4.40474 iter/s, 22.7028s/100 iters), loss = 0.062641
I1023 13:25:53.190102  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0626402 (* 1 = 0.0626402 loss)
I1023 13:25:53.190109  4594 sgd_solver.cpp:105] Iteration 21200, lr = 0.0001
I1023 13:26:13.316915  4594 solver.cpp:218] Iteration 21300 (4.96851 iter/s, 20.1268s/100 iters), loss = 0.0125123
I1023 13:26:13.317060  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125115 (* 1 = 0.0125115 loss)
I1023 13:26:13.317070  4594 sgd_solver.cpp:105] Iteration 21300, lr = 0.0001
I1023 13:26:33.247517  4594 solver.cpp:330] Iteration 21400, Testing net (#0)
I1023 13:26:34.603482  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:26:35.824283  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:26:35.824347  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.090606 (* 1 = 0.090606 loss)
I1023 13:26:36.026386  4594 solver.cpp:218] Iteration 21400 (4.40349 iter/s, 22.7093s/100 iters), loss = 0.00197882
I1023 13:26:36.026417  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00197791 (* 1 = 0.00197791 loss)
I1023 13:26:36.026423  4594 sgd_solver.cpp:105] Iteration 21400, lr = 0.0001
I1023 13:26:54.973675  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:26:56.156960  4594 solver.cpp:218] Iteration 21500 (4.96759 iter/s, 20.1305s/100 iters), loss = 0.00510221
I1023 13:26:56.156991  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00510128 (* 1 = 0.00510128 loss)
I1023 13:26:56.156998  4594 sgd_solver.cpp:105] Iteration 21500, lr = 0.0001
I1023 13:27:16.083950  4594 solver.cpp:330] Iteration 21600, Testing net (#0)
I1023 13:27:17.436853  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:27:18.659940  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:27:18.660019  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0915502 (* 1 = 0.0915502 loss)
I1023 13:27:18.861862  4594 solver.cpp:218] Iteration 21600 (4.40435 iter/s, 22.7048s/100 iters), loss = 0.0387496
I1023 13:27:18.861896  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0387486 (* 1 = 0.0387486 loss)
I1023 13:27:18.861902  4594 sgd_solver.cpp:105] Iteration 21600, lr = 0.0001
I1023 13:27:38.983170  4594 solver.cpp:218] Iteration 21700 (4.96988 iter/s, 20.1212s/100 iters), loss = 0.00140471
I1023 13:27:38.983342  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00140377 (* 1 = 0.00140377 loss)
I1023 13:27:38.983363  4594 sgd_solver.cpp:105] Iteration 21700, lr = 0.0001
I1023 13:27:58.906947  4594 solver.cpp:330] Iteration 21800, Testing net (#0)
I1023 13:28:00.260570  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:28:01.483731  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:28:01.483789  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0906482 (* 1 = 0.0906482 loss)
I1023 13:28:01.685416  4594 solver.cpp:218] Iteration 21800 (4.40489 iter/s, 22.702s/100 iters), loss = 0.0131685
I1023 13:28:01.685448  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0131675 (* 1 = 0.0131675 loss)
I1023 13:28:01.685456  4594 sgd_solver.cpp:105] Iteration 21800, lr = 0.0001
I1023 13:28:08.764571  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:28:21.816190  4594 solver.cpp:218] Iteration 21900 (4.96754 iter/s, 20.1307s/100 iters), loss = 0.00652723
I1023 13:28:21.816335  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00652629 (* 1 = 0.00652629 loss)
I1023 13:28:21.816344  4594 sgd_solver.cpp:105] Iteration 21900, lr = 0.0001
I1023 13:28:41.748602  4594 solver.cpp:330] Iteration 22000, Testing net (#0)
I1023 13:28:43.100447  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:28:44.324623  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:28:44.324684  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0913229 (* 1 = 0.0913229 loss)
I1023 13:28:44.526674  4594 solver.cpp:218] Iteration 22000 (4.40329 iter/s, 22.7103s/100 iters), loss = 0.0013692
I1023 13:28:44.526705  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00136828 (* 1 = 0.00136828 loss)
I1023 13:28:44.526711  4594 sgd_solver.cpp:105] Iteration 22000, lr = 0.0001
I1023 13:29:04.651433  4594 solver.cpp:218] Iteration 22100 (4.96902 iter/s, 20.1247s/100 iters), loss = 0.0802152
I1023 13:29:04.651573  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0802143 (* 1 = 0.0802143 loss)
I1023 13:29:04.651582  4594 sgd_solver.cpp:105] Iteration 22100, lr = 0.0001
I1023 13:29:20.177081  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:29:24.585665  4594 solver.cpp:330] Iteration 22200, Testing net (#0)
I1023 13:29:25.901398  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:29:27.163468  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:29:27.163519  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0891395 (* 1 = 0.0891395 loss)
I1023 13:29:27.365272  4594 solver.cpp:218] Iteration 22200 (4.40264 iter/s, 22.7136s/100 iters), loss = 0.00756824
I1023 13:29:27.365303  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00756731 (* 1 = 0.00756731 loss)
I1023 13:29:27.365309  4594 sgd_solver.cpp:105] Iteration 22200, lr = 0.0001
I1023 13:29:47.476649  4594 solver.cpp:218] Iteration 22300 (4.97233 iter/s, 20.1113s/100 iters), loss = 0.0173388
I1023 13:29:47.476783  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0173379 (* 1 = 0.0173379 loss)
I1023 13:29:47.476791  4594 sgd_solver.cpp:105] Iteration 22300, lr = 0.0001
I1023 13:30:07.395390  4594 solver.cpp:330] Iteration 22400, Testing net (#0)
I1023 13:30:08.709141  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:30:09.972087  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:30:09.972146  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0908 (* 1 = 0.0908 loss)
I1023 13:30:10.174183  4594 solver.cpp:218] Iteration 22400 (4.4058 iter/s, 22.6974s/100 iters), loss = 0.000508838
I1023 13:30:10.174214  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000507901 (* 1 = 0.000507901 loss)
I1023 13:30:10.174221  4594 sgd_solver.cpp:105] Iteration 22400, lr = 0.0001
I1023 13:30:30.297654  4594 solver.cpp:218] Iteration 22500 (4.96934 iter/s, 20.1234s/100 iters), loss = 0.0035789
I1023 13:30:30.297819  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00357798 (* 1 = 0.00357798 loss)
I1023 13:30:30.297828  4594 sgd_solver.cpp:105] Iteration 22500, lr = 0.0001
I1023 13:30:33.951589  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:30:50.231478  4594 solver.cpp:330] Iteration 22600, Testing net (#0)
I1023 13:30:51.544190  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:30:52.807646  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:30:52.807708  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0895302 (* 1 = 0.0895302 loss)
I1023 13:30:53.009573  4594 solver.cpp:218] Iteration 22600 (4.40302 iter/s, 22.7117s/100 iters), loss = 0.0229779
I1023 13:30:53.009606  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.022977 (* 1 = 0.022977 loss)
I1023 13:30:53.009613  4594 sgd_solver.cpp:105] Iteration 22600, lr = 0.0001
I1023 13:31:13.135145  4594 solver.cpp:218] Iteration 22700 (4.96882 iter/s, 20.1255s/100 iters), loss = 0.0516304
I1023 13:31:13.135259  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0516295 (* 1 = 0.0516295 loss)
I1023 13:31:13.135267  4594 sgd_solver.cpp:105] Iteration 22700, lr = 0.0001
I1023 13:31:33.063477  4594 solver.cpp:330] Iteration 22800, Testing net (#0)
I1023 13:31:34.375598  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:31:35.640369  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:31:35.640429  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.088287 (* 1 = 0.088287 loss)
I1023 13:31:35.842115  4594 solver.cpp:218] Iteration 22800 (4.40397 iter/s, 22.7068s/100 iters), loss = 0.00115885
I1023 13:31:35.842149  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00115794 (* 1 = 0.00115794 loss)
I1023 13:31:35.842156  4594 sgd_solver.cpp:105] Iteration 22800, lr = 0.0001
I1023 13:31:47.744199  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:31:55.971402  4594 solver.cpp:218] Iteration 22900 (4.96791 iter/s, 20.1292s/100 iters), loss = 0.00269672
I1023 13:31:55.971436  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0026958 (* 1 = 0.0026958 loss)
I1023 13:31:55.971441  4594 sgd_solver.cpp:105] Iteration 22900, lr = 0.0001
I1023 13:32:15.888240  4594 solver.cpp:330] Iteration 23000, Testing net (#0)
I1023 13:32:17.163002  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:32:18.465147  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:32:18.465332  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0876982 (* 1 = 0.0876982 loss)
I1023 13:32:18.667269  4594 solver.cpp:218] Iteration 23000 (4.4061 iter/s, 22.6958s/100 iters), loss = 0.00225602
I1023 13:32:18.667301  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00225509 (* 1 = 0.00225509 loss)
I1023 13:32:18.667307  4594 sgd_solver.cpp:105] Iteration 23000, lr = 0.0001
I1023 13:32:38.792925  4594 solver.cpp:218] Iteration 23100 (4.9688 iter/s, 20.1256s/100 iters), loss = 0.0022434
I1023 13:32:38.792968  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00224247 (* 1 = 0.00224247 loss)
I1023 13:32:38.792976  4594 sgd_solver.cpp:105] Iteration 23100, lr = 0.0001
I1023 13:32:58.721329  4594 solver.cpp:330] Iteration 23200, Testing net (#0)
I1023 13:32:59.995446  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:33:01.299695  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:33:01.299757  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0870778 (* 1 = 0.0870778 loss)
I1023 13:33:01.501987  4594 solver.cpp:218] Iteration 23200 (4.40355 iter/s, 22.709s/100 iters), loss = 0.028264
I1023 13:33:01.502017  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0282631 (* 1 = 0.0282631 loss)
I1023 13:33:01.502024  4594 sgd_solver.cpp:105] Iteration 23200, lr = 0.0001
I1023 13:33:01.528014  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:33:21.629513  4594 solver.cpp:218] Iteration 23300 (4.96834 iter/s, 20.1274s/100 iters), loss = 0.00594562
I1023 13:33:21.629544  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00594468 (* 1 = 0.00594468 loss)
I1023 13:33:21.629551  4594 sgd_solver.cpp:105] Iteration 23300, lr = 0.0001
I1023 13:33:41.556427  4594 solver.cpp:330] Iteration 23400, Testing net (#0)
I1023 13:33:42.828944  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:33:44.132822  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:33:44.132889  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0890259 (* 1 = 0.0890259 loss)
I1023 13:33:44.334473  4594 solver.cpp:218] Iteration 23400 (4.40434 iter/s, 22.7049s/100 iters), loss = 0.00524178
I1023 13:33:44.334503  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00524084 (* 1 = 0.00524084 loss)
I1023 13:33:44.334511  4594 sgd_solver.cpp:105] Iteration 23400, lr = 0.0001
I1023 13:34:04.454185  4594 solver.cpp:218] Iteration 23500 (4.97027 iter/s, 20.1196s/100 iters), loss = 0.00451102
I1023 13:34:04.454216  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00451008 (* 1 = 0.00451008 loss)
I1023 13:34:04.454223  4594 sgd_solver.cpp:105] Iteration 23500, lr = 0.0001
I1023 13:34:12.933643  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:34:24.375455  4594 solver.cpp:330] Iteration 23600, Testing net (#0)
I1023 13:34:25.647161  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:34:26.951737  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:34:26.951798  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0884816 (* 1 = 0.0884816 loss)
I1023 13:34:27.153774  4594 solver.cpp:218] Iteration 23600 (4.40538 iter/s, 22.6995s/100 iters), loss = 0.00362363
I1023 13:34:27.153803  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00362266 (* 1 = 0.00362266 loss)
I1023 13:34:27.153810  4594 sgd_solver.cpp:105] Iteration 23600, lr = 0.0001
I1023 13:34:47.277493  4594 solver.cpp:218] Iteration 23700 (4.96928 iter/s, 20.1236s/100 iters), loss = 0.00354629
I1023 13:34:47.277642  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00354533 (* 1 = 0.00354533 loss)
I1023 13:34:47.277652  4594 sgd_solver.cpp:105] Iteration 23700, lr = 0.0001
I1023 13:35:07.205498  4594 solver.cpp:330] Iteration 23800, Testing net (#0)
I1023 13:35:08.440698  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:35:09.782344  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:35:09.782400  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.095044 (* 1 = 0.095044 loss)
I1023 13:35:09.984199  4594 solver.cpp:218] Iteration 23800 (4.40402 iter/s, 22.7065s/100 iters), loss = 0.0675132
I1023 13:35:09.984233  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0675122 (* 1 = 0.0675122 loss)
I1023 13:35:09.984241  4594 sgd_solver.cpp:105] Iteration 23800, lr = 0.0001
I1023 13:35:26.717586  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:35:30.119707  4594 solver.cpp:218] Iteration 23900 (4.96637 iter/s, 20.1354s/100 iters), loss = 0.011099
I1023 13:35:30.119741  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.011098 (* 1 = 0.011098 loss)
I1023 13:35:30.119748  4594 sgd_solver.cpp:105] Iteration 23900, lr = 0.0001
I1023 13:35:50.045699  4594 solver.cpp:330] Iteration 24000, Testing net (#0)
I1023 13:35:51.279507  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:35:52.622054  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:35:52.622117  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.090013 (* 1 = 0.090013 loss)
I1023 13:35:52.823730  4594 solver.cpp:218] Iteration 24000 (4.40452 iter/s, 22.7039s/100 iters), loss = 0.00163838
I1023 13:35:52.823767  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00163744 (* 1 = 0.00163744 loss)
I1023 13:35:52.823774  4594 sgd_solver.cpp:105] Iteration 24000, lr = 0.0001
I1023 13:36:12.946745  4594 solver.cpp:218] Iteration 24100 (4.96945 iter/s, 20.1229s/100 iters), loss = 0.00217528
I1023 13:36:12.946907  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00217433 (* 1 = 0.00217433 loss)
I1023 13:36:12.946928  4594 sgd_solver.cpp:105] Iteration 24100, lr = 0.0001
I1023 13:36:32.870474  4594 solver.cpp:330] Iteration 24200, Testing net (#0)
I1023 13:36:34.102475  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:36:35.446866  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:36:35.446924  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.088849 (* 1 = 0.088849 loss)
I1023 13:36:35.648720  4594 solver.cpp:218] Iteration 24200 (4.40494 iter/s, 22.7018s/100 iters), loss = 0.00642037
I1023 13:36:35.648751  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00641942 (* 1 = 0.00641942 loss)
I1023 13:36:35.648757  4594 sgd_solver.cpp:105] Iteration 24200, lr = 0.0001
I1023 13:36:40.509809  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:36:55.777822  4594 solver.cpp:218] Iteration 24300 (4.96795 iter/s, 20.129s/100 iters), loss = 0.00877521
I1023 13:36:55.777942  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00877426 (* 1 = 0.00877426 loss)
I1023 13:36:55.777951  4594 sgd_solver.cpp:105] Iteration 24300, lr = 0.0001
I1023 13:37:15.702714  4594 solver.cpp:330] Iteration 24400, Testing net (#0)
I1023 13:37:16.934587  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:37:18.279286  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:37:18.279350  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0898907 (* 1 = 0.0898907 loss)
I1023 13:37:18.481401  4594 solver.cpp:218] Iteration 24400 (4.40462 iter/s, 22.7034s/100 iters), loss = 0.00323598
I1023 13:37:18.481431  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00323502 (* 1 = 0.00323502 loss)
I1023 13:37:18.481438  4594 sgd_solver.cpp:105] Iteration 24400, lr = 0.0001
I1023 13:37:38.606343  4594 solver.cpp:218] Iteration 24500 (4.96898 iter/s, 20.1249s/100 iters), loss = 0.00160113
I1023 13:37:38.606475  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00160017 (* 1 = 0.00160017 loss)
I1023 13:37:38.606484  4594 sgd_solver.cpp:105] Iteration 24500, lr = 0.0001
I1023 13:37:51.720075  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:37:58.542892  4594 solver.cpp:330] Iteration 24600, Testing net (#0)
I1023 13:37:59.738507  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:38:01.121552  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:38:01.121615  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0877047 (* 1 = 0.0877047 loss)
I1023 13:38:01.323261  4594 solver.cpp:218] Iteration 24600 (4.40204 iter/s, 22.7167s/100 iters), loss = 0.00669823
I1023 13:38:01.323292  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00669727 (* 1 = 0.00669727 loss)
I1023 13:38:01.323299  4594 sgd_solver.cpp:105] Iteration 24600, lr = 0.0001
I1023 13:38:21.432493  4594 solver.cpp:218] Iteration 24700 (4.97286 iter/s, 20.1092s/100 iters), loss = 0.00199351
I1023 13:38:21.432636  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00199255 (* 1 = 0.00199255 loss)
I1023 13:38:21.432646  4594 sgd_solver.cpp:105] Iteration 24700, lr = 0.0001
I1023 13:38:41.352895  4594 solver.cpp:330] Iteration 24800, Testing net (#0)
I1023 13:38:42.545380  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:38:43.928578  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:38:43.928637  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0928349 (* 1 = 0.0928349 loss)
I1023 13:38:44.130772  4594 solver.cpp:218] Iteration 24800 (4.40566 iter/s, 22.6981s/100 iters), loss = 0.0330567
I1023 13:38:44.130802  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0330558 (* 1 = 0.0330558 loss)
I1023 13:38:44.130810  4594 sgd_solver.cpp:105] Iteration 24800, lr = 0.0001
I1023 13:39:04.257314  4594 solver.cpp:218] Iteration 24900 (4.96858 iter/s, 20.1265s/100 iters), loss = 0.0231688
I1023 13:39:04.257464  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0231678 (* 1 = 0.0231678 loss)
I1023 13:39:04.257473  4594 sgd_solver.cpp:105] Iteration 24900, lr = 0.0001
I1023 13:39:05.692499  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:39:24.186236  4594 solver.cpp:330] Iteration 25000, Testing net (#0)
I1023 13:39:25.377975  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:39:26.762084  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:39:26.762148  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0911275 (* 1 = 0.0911275 loss)
I1023 13:39:26.964519  4594 solver.cpp:218] Iteration 25000 (4.40393 iter/s, 22.707s/100 iters), loss = 0.0993723
I1023 13:39:26.964550  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0993713 (* 1 = 0.0993713 loss)
I1023 13:39:26.964555  4594 sgd_solver.cpp:105] Iteration 25000, lr = 0.0001
I1023 13:39:47.091334  4594 solver.cpp:218] Iteration 25100 (4.96851 iter/s, 20.1267s/100 iters), loss = 0.0129403
I1023 13:39:47.091478  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0129392 (* 1 = 0.0129392 loss)
I1023 13:39:47.091487  4594 sgd_solver.cpp:105] Iteration 25100, lr = 0.0001
I1023 13:40:07.022025  4594 solver.cpp:330] Iteration 25200, Testing net (#0)
I1023 13:40:08.213052  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:40:09.597122  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:40:09.597182  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0898801 (* 1 = 0.0898801 loss)
I1023 13:40:09.798768  4594 solver.cpp:218] Iteration 25200 (4.40388 iter/s, 22.7072s/100 iters), loss = 0.00471895
I1023 13:40:09.798799  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00471789 (* 1 = 0.00471789 loss)
I1023 13:40:09.798805  4594 sgd_solver.cpp:105] Iteration 25200, lr = 0.0001
I1023 13:40:19.489008  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:40:29.934777  4594 solver.cpp:218] Iteration 25300 (4.96625 iter/s, 20.1359s/100 iters), loss = 0.0174921
I1023 13:40:29.934808  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0174911 (* 1 = 0.0174911 loss)
I1023 13:40:29.934814  4594 sgd_solver.cpp:105] Iteration 25300, lr = 0.0001
I1023 13:40:49.847198  4594 solver.cpp:330] Iteration 25400, Testing net (#0)
I1023 13:40:51.000988  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:40:52.424111  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:40:52.424180  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.087767 (* 1 = 0.087767 loss)
I1023 13:40:52.625897  4594 solver.cpp:218] Iteration 25400 (4.40703 iter/s, 22.691s/100 iters), loss = 0.12474
I1023 13:40:52.625929  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124739 (* 1 = 0.124739 loss)
I1023 13:40:52.625936  4594 sgd_solver.cpp:105] Iteration 25400, lr = 0.0001
I1023 13:41:12.751402  4594 solver.cpp:218] Iteration 25500 (4.96884 iter/s, 20.1254s/100 iters), loss = 0.00878182
I1023 13:41:12.751435  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00878078 (* 1 = 0.00878078 loss)
I1023 13:41:12.751441  4594 sgd_solver.cpp:105] Iteration 25500, lr = 0.0001
I1023 13:41:30.690399  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:41:32.679989  4594 solver.cpp:330] Iteration 25600, Testing net (#0)
I1023 13:41:33.833724  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:41:35.257244  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:41:35.257302  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0863895 (* 1 = 0.0863895 loss)
I1023 13:41:35.459250  4594 solver.cpp:218] Iteration 25600 (4.40378 iter/s, 22.7078s/100 iters), loss = 0.00373895
I1023 13:41:35.459282  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0037379 (* 1 = 0.0037379 loss)
I1023 13:41:35.459288  4594 sgd_solver.cpp:105] Iteration 25600, lr = 0.0001
I1023 13:41:55.582644  4594 solver.cpp:218] Iteration 25700 (4.96936 iter/s, 20.1233s/100 iters), loss = 0.00659959
I1023 13:41:55.582674  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00659855 (* 1 = 0.00659855 loss)
I1023 13:41:55.582681  4594 sgd_solver.cpp:105] Iteration 25700, lr = 0.0001
I1023 13:42:15.513247  4594 solver.cpp:330] Iteration 25800, Testing net (#0)
I1023 13:42:16.665529  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:42:18.090183  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:42:18.090250  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0863993 (* 1 = 0.0863993 loss)
I1023 13:42:18.292225  4594 solver.cpp:218] Iteration 25800 (4.40344 iter/s, 22.7095s/100 iters), loss = 0.011469
I1023 13:42:18.292259  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.011468 (* 1 = 0.011468 loss)
I1023 13:42:18.292266  4594 sgd_solver.cpp:105] Iteration 25800, lr = 0.0001
I1023 13:42:38.410996  4594 solver.cpp:218] Iteration 25900 (4.9705 iter/s, 20.1187s/100 iters), loss = 0.00282357
I1023 13:42:38.411027  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00282254 (* 1 = 0.00282254 loss)
I1023 13:42:38.411034  4594 sgd_solver.cpp:105] Iteration 25900, lr = 0.0001
I1023 13:42:44.479357  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:42:58.334801  4594 solver.cpp:330] Iteration 26000, Testing net (#0)
I1023 13:42:59.486320  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:43:00.912405  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:43:00.912467  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0858739 (* 1 = 0.0858739 loss)
I1023 13:43:01.114455  4594 solver.cpp:218] Iteration 26000 (4.40463 iter/s, 22.7034s/100 iters), loss = 0.00120561
I1023 13:43:01.114490  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00120458 (* 1 = 0.00120458 loss)
I1023 13:43:01.114497  4594 sgd_solver.cpp:105] Iteration 26000, lr = 0.0001
I1023 13:43:21.242357  4594 solver.cpp:218] Iteration 26100 (4.96825 iter/s, 20.1278s/100 iters), loss = 0.000463158
I1023 13:43:21.242388  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000462123 (* 1 = 0.000462123 loss)
I1023 13:43:21.242395  4594 sgd_solver.cpp:105] Iteration 26100, lr = 0.0001
I1023 13:43:41.168020  4594 solver.cpp:330] Iteration 26200, Testing net (#0)
I1023 13:43:42.281870  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:43:43.744884  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:43:43.744940  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0871443 (* 1 = 0.0871443 loss)
I1023 13:43:43.947026  4594 solver.cpp:218] Iteration 26200 (4.4044 iter/s, 22.7046s/100 iters), loss = 0.00778453
I1023 13:43:43.947058  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00778351 (* 1 = 0.00778351 loss)
I1023 13:43:43.947065  4594 sgd_solver.cpp:105] Iteration 26200, lr = 0.0001
I1023 13:43:58.466922  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:44:04.084269  4594 solver.cpp:218] Iteration 26300 (4.96594 iter/s, 20.1372s/100 iters), loss = 0.0073393
I1023 13:44:04.084298  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00733828 (* 1 = 0.00733828 loss)
I1023 13:44:04.084306  4594 sgd_solver.cpp:105] Iteration 26300, lr = 0.0001
I1023 13:44:24.010010  4594 solver.cpp:330] Iteration 26400, Testing net (#0)
I1023 13:44:25.122640  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:44:26.586971  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:44:26.587031  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.089461 (* 1 = 0.089461 loss)
I1023 13:44:26.788650  4594 solver.cpp:218] Iteration 26400 (4.40445 iter/s, 22.7043s/100 iters), loss = 0.00165149
I1023 13:44:26.788683  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00165048 (* 1 = 0.00165048 loss)
I1023 13:44:26.788691  4594 sgd_solver.cpp:105] Iteration 26400, lr = 0.0001
I1023 13:44:46.911113  4594 solver.cpp:218] Iteration 26500 (4.96959 iter/s, 20.1224s/100 iters), loss = 0.00327599
I1023 13:44:46.911144  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00327498 (* 1 = 0.00327498 loss)
I1023 13:44:46.911151  4594 sgd_solver.cpp:105] Iteration 26500, lr = 0.0001
I1023 13:45:06.833096  4594 solver.cpp:330] Iteration 26600, Testing net (#0)
I1023 13:45:07.945004  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:45:09.409468  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:45:09.409531  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0901027 (* 1 = 0.0901027 loss)
I1023 13:45:09.611449  4594 solver.cpp:218] Iteration 26600 (4.40524 iter/s, 22.7003s/100 iters), loss = 0.000349777
I1023 13:45:09.611485  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000348751 (* 1 = 0.000348751 loss)
I1023 13:45:09.611493  4594 sgd_solver.cpp:105] Iteration 26600, lr = 0.0001
I1023 13:45:12.255461  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:45:29.737339  4594 solver.cpp:218] Iteration 26700 (4.96874 iter/s, 20.1258s/100 iters), loss = 0.0171166
I1023 13:45:29.737370  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171156 (* 1 = 0.0171156 loss)
I1023 13:45:29.737377  4594 sgd_solver.cpp:105] Iteration 26700, lr = 0.0001
I1023 13:45:49.663236  4594 solver.cpp:330] Iteration 26800, Testing net (#0)
I1023 13:45:50.773859  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:45:52.239881  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:45:52.239929  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0885131 (* 1 = 0.0885131 loss)
I1023 13:45:52.441942  4594 solver.cpp:218] Iteration 26800 (4.40441 iter/s, 22.7045s/100 iters), loss = 0.0142425
I1023 13:45:52.441975  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0142415 (* 1 = 0.0142415 loss)
I1023 13:45:52.441982  4594 sgd_solver.cpp:105] Iteration 26800, lr = 0.0001
I1023 13:46:12.564535  4594 solver.cpp:218] Iteration 26900 (4.96956 iter/s, 20.1225s/100 iters), loss = 0.000858717
I1023 13:46:12.564566  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000857738 (* 1 = 0.000857738 loss)
I1023 13:46:12.564574  4594 sgd_solver.cpp:105] Iteration 26900, lr = 0.0001
I1023 13:46:23.463984  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:46:32.503509  4594 solver.cpp:330] Iteration 27000, Testing net (#0)
I1023 13:46:33.577026  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:46:35.079516  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:46:35.079576  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0868277 (* 1 = 0.0868277 loss)
I1023 13:46:35.281450  4594 solver.cpp:218] Iteration 27000 (4.40202 iter/s, 22.7168s/100 iters), loss = 0.0158071
I1023 13:46:35.281481  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158061 (* 1 = 0.0158061 loss)
I1023 13:46:35.281486  4594 sgd_solver.cpp:105] Iteration 27000, lr = 0.0001
I1023 13:46:55.400928  4594 solver.cpp:218] Iteration 27100 (4.97033 iter/s, 20.1194s/100 iters), loss = 0.00816481
I1023 13:46:55.401100  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00816384 (* 1 = 0.00816384 loss)
I1023 13:46:55.401109  4594 sgd_solver.cpp:105] Iteration 27100, lr = 0.0001
I1023 13:47:15.320127  4594 solver.cpp:330] Iteration 27200, Testing net (#0)
I1023 13:47:16.392226  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:47:17.896266  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:47:17.896323  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0893279 (* 1 = 0.0893279 loss)
I1023 13:47:18.098424  4594 solver.cpp:218] Iteration 27200 (4.40581 iter/s, 22.6973s/100 iters), loss = 0.00266944
I1023 13:47:18.098456  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00266846 (* 1 = 0.00266846 loss)
I1023 13:47:18.098464  4594 sgd_solver.cpp:105] Iteration 27200, lr = 0.0001
I1023 13:47:37.244478  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:47:38.225914  4594 solver.cpp:218] Iteration 27300 (4.96835 iter/s, 20.1274s/100 iters), loss = 0.00445569
I1023 13:47:38.225949  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0044547 (* 1 = 0.0044547 loss)
I1023 13:47:38.225955  4594 sgd_solver.cpp:105] Iteration 27300, lr = 0.0001
I1023 13:47:58.150650  4594 solver.cpp:330] Iteration 27400, Testing net (#0)
I1023 13:47:59.222439  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:48:00.728875  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:48:00.728922  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.088015 (* 1 = 0.088015 loss)
I1023 13:48:00.931356  4594 solver.cpp:218] Iteration 27400 (4.40425 iter/s, 22.7054s/100 iters), loss = 0.0057965
I1023 13:48:00.931387  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00579553 (* 1 = 0.00579553 loss)
I1023 13:48:00.931394  4594 sgd_solver.cpp:105] Iteration 27400, lr = 0.0001
I1023 13:48:21.059490  4594 solver.cpp:218] Iteration 27500 (4.96819 iter/s, 20.1281s/100 iters), loss = 0.000313819
I1023 13:48:21.059600  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000312845 (* 1 = 0.000312845 loss)
I1023 13:48:21.059608  4594 sgd_solver.cpp:105] Iteration 27500, lr = 0.0001
I1023 13:48:40.987504  4594 solver.cpp:330] Iteration 27600, Testing net (#0)
I1023 13:48:42.058286  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:48:43.564885  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:48:43.564934  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0887386 (* 1 = 0.0887386 loss)
I1023 13:48:43.766650  4594 solver.cpp:218] Iteration 27600 (4.40393 iter/s, 22.707s/100 iters), loss = 0.00787506
I1023 13:48:43.766683  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0078741 (* 1 = 0.0078741 loss)
I1023 13:48:43.766690  4594 sgd_solver.cpp:105] Iteration 27600, lr = 0.0001
I1023 13:48:51.243072  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:49:03.894320  4594 solver.cpp:218] Iteration 27700 (4.9683 iter/s, 20.1276s/100 iters), loss = 0.0507491
I1023 13:49:03.894351  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0507481 (* 1 = 0.0507481 loss)
I1023 13:49:03.894358  4594 sgd_solver.cpp:105] Iteration 27700, lr = 0.0001
I1023 13:49:23.821043  4594 solver.cpp:330] Iteration 27800, Testing net (#0)
I1023 13:49:24.854197  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:49:26.397912  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:49:26.397967  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0905767 (* 1 = 0.0905767 loss)
I1023 13:49:26.599577  4594 solver.cpp:218] Iteration 27800 (4.40428 iter/s, 22.7052s/100 iters), loss = 0.00331122
I1023 13:49:26.599612  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00331025 (* 1 = 0.00331025 loss)
I1023 13:49:26.599619  4594 sgd_solver.cpp:105] Iteration 27800, lr = 0.0001
I1023 13:49:46.723434  4594 solver.cpp:218] Iteration 27900 (4.96925 iter/s, 20.1238s/100 iters), loss = 0.016797
I1023 13:49:46.723466  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.016796 (* 1 = 0.016796 loss)
I1023 13:49:46.723472  4594 sgd_solver.cpp:105] Iteration 27900, lr = 0.0001
I1023 13:50:02.446385  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:50:06.655839  4594 solver.cpp:330] Iteration 28000, Testing net (#0)
I1023 13:50:07.688334  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:50:09.232947  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:50:09.233006  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0866811 (* 1 = 0.0866811 loss)
I1023 13:50:09.434927  4594 solver.cpp:218] Iteration 28000 (4.40307 iter/s, 22.7114s/100 iters), loss = 0.00589069
I1023 13:50:09.434959  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00588972 (* 1 = 0.00588972 loss)
I1023 13:50:09.434967  4594 sgd_solver.cpp:105] Iteration 28000, lr = 0.0001
I1023 13:50:29.553624  4594 solver.cpp:218] Iteration 28100 (4.97052 iter/s, 20.1186s/100 iters), loss = 0.000542327
I1023 13:50:29.553655  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000541366 (* 1 = 0.000541366 loss)
I1023 13:50:29.553661  4594 sgd_solver.cpp:105] Iteration 28100, lr = 0.0001
I1023 13:50:49.480942  4594 solver.cpp:330] Iteration 28200, Testing net (#0)
I1023 13:50:50.512764  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:50:52.058109  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:50:52.058174  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0874473 (* 1 = 0.0874473 loss)
I1023 13:50:52.260272  4594 solver.cpp:218] Iteration 28200 (4.40401 iter/s, 22.7066s/100 iters), loss = 0.00689451
I1023 13:50:52.260304  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00689354 (* 1 = 0.00689354 loss)
I1023 13:50:52.260311  4594 sgd_solver.cpp:105] Iteration 28200, lr = 0.0001
I1023 13:51:12.378553  4594 solver.cpp:218] Iteration 28300 (4.97062 iter/s, 20.1182s/100 iters), loss = 0.00145715
I1023 13:51:12.378584  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00145619 (* 1 = 0.00145619 loss)
I1023 13:51:12.378590  4594 sgd_solver.cpp:105] Iteration 28300, lr = 0.0001
I1023 13:51:16.231680  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:51:32.302794  4594 solver.cpp:330] Iteration 28400, Testing net (#0)
I1023 13:51:33.332952  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:51:34.879276  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:51:34.879339  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0859299 (* 1 = 0.0859299 loss)
I1023 13:51:35.081307  4594 solver.cpp:218] Iteration 28400 (4.40477 iter/s, 22.7027s/100 iters), loss = 0.00415875
I1023 13:51:35.081338  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0041578 (* 1 = 0.0041578 loss)
I1023 13:51:35.081346  4594 sgd_solver.cpp:105] Iteration 28400, lr = 0.0001
I1023 13:51:55.206212  4594 solver.cpp:218] Iteration 28500 (4.96899 iter/s, 20.1248s/100 iters), loss = 0.00352638
I1023 13:51:55.206243  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00352543 (* 1 = 0.00352543 loss)
I1023 13:51:55.206250  4594 sgd_solver.cpp:105] Iteration 28500, lr = 0.0001
I1023 13:52:15.133211  4594 solver.cpp:330] Iteration 28600, Testing net (#0)
I1023 13:52:16.126180  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:52:17.710034  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:52:17.710094  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0853122 (* 1 = 0.0853122 loss)
I1023 13:52:17.912020  4594 solver.cpp:218] Iteration 28600 (4.40418 iter/s, 22.7057s/100 iters), loss = 0.0486595
I1023 13:52:17.912051  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0486586 (* 1 = 0.0486586 loss)
I1023 13:52:17.912058  4594 sgd_solver.cpp:105] Iteration 28600, lr = 0.0001
I1023 13:52:30.019695  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:52:38.050916  4594 solver.cpp:218] Iteration 28700 (4.96554 iter/s, 20.1388s/100 iters), loss = 0.00209801
I1023 13:52:38.050947  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00209706 (* 1 = 0.00209706 loss)
I1023 13:52:38.050954  4594 sgd_solver.cpp:105] Iteration 28700, lr = 0.0001
I1023 13:52:57.970350  4594 solver.cpp:330] Iteration 28800, Testing net (#0)
I1023 13:52:58.962838  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:53:00.548455  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:53:00.548516  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0907813 (* 1 = 0.0907813 loss)
I1023 13:53:00.750483  4594 solver.cpp:218] Iteration 28800 (4.40539 iter/s, 22.6995s/100 iters), loss = 0.00313759
I1023 13:53:00.750515  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00313663 (* 1 = 0.00313663 loss)
I1023 13:53:00.750524  4594 sgd_solver.cpp:105] Iteration 28800, lr = 0.0001
I1023 13:53:20.868698  4594 solver.cpp:218] Iteration 28900 (4.97064 iter/s, 20.1181s/100 iters), loss = 0.145213
I1023 13:53:20.868728  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145212 (* 1 = 0.145212 loss)
I1023 13:53:20.868734  4594 sgd_solver.cpp:105] Iteration 28900, lr = 0.0001
I1023 13:53:40.792420  4594 solver.cpp:330] Iteration 29000, Testing net (#0)
I1023 13:53:41.783411  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:53:43.369547  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:53:43.369611  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0876803 (* 1 = 0.0876803 loss)
I1023 13:53:43.571205  4594 solver.cpp:218] Iteration 29000 (4.40482 iter/s, 22.7024s/100 iters), loss = 0.0170635
I1023 13:53:43.571236  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0170625 (* 1 = 0.0170625 loss)
I1023 13:53:43.571244  4594 sgd_solver.cpp:105] Iteration 29000, lr = 0.0001
I1023 13:53:43.999423  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:54:03.699934  4594 solver.cpp:218] Iteration 29100 (4.96804 iter/s, 20.1287s/100 iters), loss = 0.0107955
I1023 13:54:03.699967  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107946 (* 1 = 0.0107946 loss)
I1023 13:54:03.699975  4594 sgd_solver.cpp:105] Iteration 29100, lr = 0.0001
I1023 13:54:23.623368  4594 solver.cpp:330] Iteration 29200, Testing net (#0)
I1023 13:54:24.613009  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:54:26.200171  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:54:26.200232  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0884294 (* 1 = 0.0884294 loss)
I1023 13:54:26.402257  4594 solver.cpp:218] Iteration 29200 (4.40485 iter/s, 22.7022s/100 iters), loss = 0.00682833
I1023 13:54:26.402290  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00682741 (* 1 = 0.00682741 loss)
I1023 13:54:26.402297  4594 sgd_solver.cpp:105] Iteration 29200, lr = 0.0001
I1023 13:54:46.528851  4594 solver.cpp:218] Iteration 29300 (4.96857 iter/s, 20.1265s/100 iters), loss = 0.00171896
I1023 13:54:46.528882  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00171804 (* 1 = 0.00171804 loss)
I1023 13:54:46.528888  4594 sgd_solver.cpp:105] Iteration 29300, lr = 0.0001
I1023 13:54:55.213949  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:55:06.458451  4594 solver.cpp:330] Iteration 29400, Testing net (#0)
I1023 13:55:07.411156  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:55:09.035377  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:55:09.035442  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0864804 (* 1 = 0.0864804 loss)
I1023 13:55:09.237213  4594 solver.cpp:218] Iteration 29400 (4.40368 iter/s, 22.7083s/100 iters), loss = 0.0121857
I1023 13:55:09.237244  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121848 (* 1 = 0.0121848 loss)
I1023 13:55:09.237251  4594 sgd_solver.cpp:105] Iteration 29400, lr = 0.0001
I1023 13:55:29.356339  4594 solver.cpp:218] Iteration 29500 (4.97041 iter/s, 20.119s/100 iters), loss = 0.00345574
I1023 13:55:29.356499  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00345482 (* 1 = 0.00345482 loss)
I1023 13:55:29.356509  4594 sgd_solver.cpp:105] Iteration 29500, lr = 0.0001
I1023 13:55:49.275228  4594 solver.cpp:330] Iteration 29600, Testing net (#0)
I1023 13:55:50.227660  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:55:51.851686  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:55:51.851753  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0866967 (* 1 = 0.0866967 loss)
I1023 13:55:52.053946  4594 solver.cpp:218] Iteration 29600 (4.40579 iter/s, 22.6974s/100 iters), loss = 0.000756629
I1023 13:55:52.053975  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000755704 (* 1 = 0.000755704 loss)
I1023 13:55:52.053982  4594 sgd_solver.cpp:105] Iteration 29600, lr = 0.0001
I1023 13:56:08.986534  4603 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:56:12.185894  4594 solver.cpp:218] Iteration 29700 (4.96725 iter/s, 20.1319s/100 iters), loss = 0.00567672
I1023 13:56:12.185931  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00567577 (* 1 = 0.00567577 loss)
I1023 13:56:12.185938  4594 sgd_solver.cpp:105] Iteration 29700, lr = 0.0001
I1023 13:56:32.109477  4594 solver.cpp:330] Iteration 29800, Testing net (#0)
I1023 13:56:33.060329  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:56:34.686162  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1023 13:56:34.686225  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0869827 (* 1 = 0.0869827 loss)
I1023 13:56:34.888139  4594 solver.cpp:218] Iteration 29800 (4.40487 iter/s, 22.7022s/100 iters), loss = 0.0221573
I1023 13:56:34.888167  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0221563 (* 1 = 0.0221563 loss)
I1023 13:56:34.888175  4594 sgd_solver.cpp:105] Iteration 29800, lr = 0.0001
I1023 13:56:55.015089  4594 solver.cpp:218] Iteration 29900 (4.96848 iter/s, 20.1269s/100 iters), loss = 0.031389
I1023 13:56:55.015229  4594 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313881 (* 1 = 0.0313881 loss)
I1023 13:56:55.015239  4594 sgd_solver.cpp:105] Iteration 29900, lr = 0.0001
I1023 13:57:14.943578  4594 solver.cpp:447] Snapshotting to binary proto file xn/English_orange/snapshot/res20/res20_penlu_alpha0.25_etanostudy_2study_nodecay_iter_30000.caffemodel
I1023 13:57:14.953323  4594 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/English_orange/snapshot/res20/res20_penlu_alpha0.25_etanostudy_2study_nodecay_iter_30000.solverstate
I1023 13:57:15.009692  4594 solver.cpp:310] Iteration 30000, loss = 0.0284606
I1023 13:57:15.009739  4594 solver.cpp:330] Iteration 30000, Testing net (#0)
I1023 13:57:15.960276  4604 data_layer.cpp:73] Restarting data prefetching from start.
I1023 13:57:17.587101  4594 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1023 13:57:17.587153  4594 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0866708 (* 1 = 0.0866708 loss)
I1023 13:57:17.587158  4594 solver.cpp:315] Optimization Done.
I1023 13:57:17.587162  4594 caffe.cpp:259] Optimization Done.
