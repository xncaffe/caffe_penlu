I1022 21:09:53.334009  3959 caffe.cpp:218] Using GPUs 0
I1022 21:09:53.362532  3959 caffe.cpp:223] GPU 0: GeForce GTX 1080
I1022 21:09:53.574793  3959 solver.cpp:44] Initializing solver from parameters: 
test_iter: 64
test_interval: 200
base_lr: 0.01
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
snapshot: 30000
snapshot_prefix: "xn/English_orange/snapshot/res20/res20_penlu_alpha0.25_etanostudy_2study_2decay"
solver_mode: GPU
device_id: 0
net: "/home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt"
train_state {
  level: 0
  stage: ""
}
stepvalue: 10000
stepvalue: 20000
I1022 21:09:53.574939  3959 solver.cpp:87] Creating training net from net file: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1022 21:09:53.576544  3959 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1022 21:09:53.576553  3959 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1022 21:09:53.576701  3959 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Data1
I1022 21:09:53.576768  3959 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I1022 21:09:53.577265  3959 net.cpp:51] Initializing net from parameters: 
name: "resnet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 224
    mean_file: "/home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/English_orange/data/train1_lmdb"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1022 21:09:53.577585  3959 layer_factory.hpp:77] Creating layer Data1
I1022 21:09:53.577666  3959 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/English_orange/data/train1_lmdb
I1022 21:09:53.577682  3959 net.cpp:84] Creating Layer Data1
I1022 21:09:53.577687  3959 net.cpp:380] Data1 -> Data1
I1022 21:09:53.577705  3959 net.cpp:380] Data1 -> Data2
I1022 21:09:53.577714  3959 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto
I1022 21:09:53.580235  3959 data_layer.cpp:45] output data size: 8,3,224,224
I1022 21:09:53.588146  3959 net.cpp:122] Setting up Data1
I1022 21:09:53.588167  3959 net.cpp:129] Top shape: 8 3 224 224 (1204224)
I1022 21:09:53.588172  3959 net.cpp:129] Top shape: 8 (8)
I1022 21:09:53.588176  3959 net.cpp:137] Memory required for data: 4816928
I1022 21:09:53.588183  3959 layer_factory.hpp:77] Creating layer Convolution1
I1022 21:09:53.588204  3959 net.cpp:84] Creating Layer Convolution1
I1022 21:09:53.588209  3959 net.cpp:406] Convolution1 <- Data1
I1022 21:09:53.588218  3959 net.cpp:380] Convolution1 -> Convolution1
I1022 21:09:53.735837  3959 net.cpp:122] Setting up Convolution1
I1022 21:09:53.735862  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.735865  3959 net.cpp:137] Memory required for data: 30507040
I1022 21:09:53.735880  3959 layer_factory.hpp:77] Creating layer BatchNorm1
I1022 21:09:53.735900  3959 net.cpp:84] Creating Layer BatchNorm1
I1022 21:09:53.735904  3959 net.cpp:406] BatchNorm1 <- Convolution1
I1022 21:09:53.735908  3959 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1022 21:09:53.736088  3959 net.cpp:122] Setting up BatchNorm1
I1022 21:09:53.736093  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.736096  3959 net.cpp:137] Memory required for data: 56197152
I1022 21:09:53.736104  3959 layer_factory.hpp:77] Creating layer Scale1
I1022 21:09:53.736112  3959 net.cpp:84] Creating Layer Scale1
I1022 21:09:53.736126  3959 net.cpp:406] Scale1 <- Convolution1
I1022 21:09:53.736129  3959 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1022 21:09:53.736188  3959 layer_factory.hpp:77] Creating layer Scale1
I1022 21:09:53.736358  3959 net.cpp:122] Setting up Scale1
I1022 21:09:53.736364  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.736366  3959 net.cpp:137] Memory required for data: 81887264
I1022 21:09:53.736371  3959 layer_factory.hpp:77] Creating layer penlu1
I1022 21:09:53.736379  3959 net.cpp:84] Creating Layer penlu1
I1022 21:09:53.736382  3959 net.cpp:406] penlu1 <- Convolution1
I1022 21:09:53.736395  3959 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1022 21:09:53.737521  3959 net.cpp:122] Setting up penlu1
I1022 21:09:53.737531  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.737534  3959 net.cpp:137] Memory required for data: 107577376
I1022 21:09:53.737541  3959 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1022 21:09:53.737560  3959 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1022 21:09:53.737563  3959 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1022 21:09:53.737567  3959 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1022 21:09:53.737583  3959 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1022 21:09:53.737627  3959 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1022 21:09:53.737632  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.737635  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.737646  3959 net.cpp:137] Memory required for data: 158957600
I1022 21:09:53.737648  3959 layer_factory.hpp:77] Creating layer Convolution2
I1022 21:09:53.737666  3959 net.cpp:84] Creating Layer Convolution2
I1022 21:09:53.737670  3959 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1022 21:09:53.737682  3959 net.cpp:380] Convolution2 -> Convolution2
I1022 21:09:53.739123  3959 net.cpp:122] Setting up Convolution2
I1022 21:09:53.739135  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.739137  3959 net.cpp:137] Memory required for data: 184647712
I1022 21:09:53.739163  3959 layer_factory.hpp:77] Creating layer BatchNorm2
I1022 21:09:53.739169  3959 net.cpp:84] Creating Layer BatchNorm2
I1022 21:09:53.739184  3959 net.cpp:406] BatchNorm2 <- Convolution2
I1022 21:09:53.739188  3959 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1022 21:09:53.739351  3959 net.cpp:122] Setting up BatchNorm2
I1022 21:09:53.739357  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.739359  3959 net.cpp:137] Memory required for data: 210337824
I1022 21:09:53.739364  3959 layer_factory.hpp:77] Creating layer Scale2
I1022 21:09:53.739370  3959 net.cpp:84] Creating Layer Scale2
I1022 21:09:53.739372  3959 net.cpp:406] Scale2 <- Convolution2
I1022 21:09:53.739375  3959 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1022 21:09:53.739436  3959 layer_factory.hpp:77] Creating layer Scale2
I1022 21:09:53.739604  3959 net.cpp:122] Setting up Scale2
I1022 21:09:53.739610  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.739612  3959 net.cpp:137] Memory required for data: 236027936
I1022 21:09:53.739619  3959 layer_factory.hpp:77] Creating layer penlu2
I1022 21:09:53.739624  3959 net.cpp:84] Creating Layer penlu2
I1022 21:09:53.739626  3959 net.cpp:406] penlu2 <- Convolution2
I1022 21:09:53.739640  3959 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1022 21:09:53.740770  3959 net.cpp:122] Setting up penlu2
I1022 21:09:53.740780  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.740783  3959 net.cpp:137] Memory required for data: 261718048
I1022 21:09:53.740788  3959 layer_factory.hpp:77] Creating layer Convolution3
I1022 21:09:53.740797  3959 net.cpp:84] Creating Layer Convolution3
I1022 21:09:53.740809  3959 net.cpp:406] Convolution3 <- Convolution2
I1022 21:09:53.740813  3959 net.cpp:380] Convolution3 -> Convolution3
I1022 21:09:53.741705  3959 net.cpp:122] Setting up Convolution3
I1022 21:09:53.741715  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.741719  3959 net.cpp:137] Memory required for data: 287408160
I1022 21:09:53.741724  3959 layer_factory.hpp:77] Creating layer BatchNorm3
I1022 21:09:53.741727  3959 net.cpp:84] Creating Layer BatchNorm3
I1022 21:09:53.741730  3959 net.cpp:406] BatchNorm3 <- Convolution3
I1022 21:09:53.741744  3959 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1022 21:09:53.741904  3959 net.cpp:122] Setting up BatchNorm3
I1022 21:09:53.741909  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.741910  3959 net.cpp:137] Memory required for data: 313098272
I1022 21:09:53.741915  3959 layer_factory.hpp:77] Creating layer Scale3
I1022 21:09:53.741919  3959 net.cpp:84] Creating Layer Scale3
I1022 21:09:53.741922  3959 net.cpp:406] Scale3 <- Convolution3
I1022 21:09:53.741925  3959 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1022 21:09:53.741968  3959 layer_factory.hpp:77] Creating layer Scale3
I1022 21:09:53.742599  3959 net.cpp:122] Setting up Scale3
I1022 21:09:53.742609  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.742611  3959 net.cpp:137] Memory required for data: 338788384
I1022 21:09:53.742616  3959 layer_factory.hpp:77] Creating layer Eltwise1
I1022 21:09:53.742621  3959 net.cpp:84] Creating Layer Eltwise1
I1022 21:09:53.742624  3959 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1022 21:09:53.742637  3959 net.cpp:406] Eltwise1 <- Convolution3
I1022 21:09:53.742641  3959 net.cpp:380] Eltwise1 -> Eltwise1
I1022 21:09:53.742671  3959 net.cpp:122] Setting up Eltwise1
I1022 21:09:53.742676  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.742677  3959 net.cpp:137] Memory required for data: 364478496
I1022 21:09:53.742679  3959 layer_factory.hpp:77] Creating layer penlu3
I1022 21:09:53.742684  3959 net.cpp:84] Creating Layer penlu3
I1022 21:09:53.742687  3959 net.cpp:406] penlu3 <- Eltwise1
I1022 21:09:53.742691  3959 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1022 21:09:53.743824  3959 net.cpp:122] Setting up penlu3
I1022 21:09:53.743834  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.743847  3959 net.cpp:137] Memory required for data: 390168608
I1022 21:09:53.743862  3959 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1022 21:09:53.743868  3959 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1022 21:09:53.743870  3959 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1022 21:09:53.743876  3959 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1022 21:09:53.743882  3959 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1022 21:09:53.743916  3959 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1022 21:09:53.743921  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.743934  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.743937  3959 net.cpp:137] Memory required for data: 441548832
I1022 21:09:53.743938  3959 layer_factory.hpp:77] Creating layer Convolution4
I1022 21:09:53.743947  3959 net.cpp:84] Creating Layer Convolution4
I1022 21:09:53.743968  3959 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1022 21:09:53.743973  3959 net.cpp:380] Convolution4 -> Convolution4
I1022 21:09:53.744922  3959 net.cpp:122] Setting up Convolution4
I1022 21:09:53.744932  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.744946  3959 net.cpp:137] Memory required for data: 467238944
I1022 21:09:53.744951  3959 layer_factory.hpp:77] Creating layer BatchNorm4
I1022 21:09:53.744956  3959 net.cpp:84] Creating Layer BatchNorm4
I1022 21:09:53.744969  3959 net.cpp:406] BatchNorm4 <- Convolution4
I1022 21:09:53.744973  3959 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1022 21:09:53.745141  3959 net.cpp:122] Setting up BatchNorm4
I1022 21:09:53.745146  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.745157  3959 net.cpp:137] Memory required for data: 492929056
I1022 21:09:53.745167  3959 layer_factory.hpp:77] Creating layer Scale4
I1022 21:09:53.745172  3959 net.cpp:84] Creating Layer Scale4
I1022 21:09:53.745174  3959 net.cpp:406] Scale4 <- Convolution4
I1022 21:09:53.745177  3959 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1022 21:09:53.745204  3959 layer_factory.hpp:77] Creating layer Scale4
I1022 21:09:53.745333  3959 net.cpp:122] Setting up Scale4
I1022 21:09:53.745338  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.745342  3959 net.cpp:137] Memory required for data: 518619168
I1022 21:09:53.745345  3959 layer_factory.hpp:77] Creating layer penlu4
I1022 21:09:53.745352  3959 net.cpp:84] Creating Layer penlu4
I1022 21:09:53.745354  3959 net.cpp:406] penlu4 <- Convolution4
I1022 21:09:53.745358  3959 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1022 21:09:53.746520  3959 net.cpp:122] Setting up penlu4
I1022 21:09:53.746531  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.746534  3959 net.cpp:137] Memory required for data: 544309280
I1022 21:09:53.746539  3959 layer_factory.hpp:77] Creating layer Convolution5
I1022 21:09:53.746547  3959 net.cpp:84] Creating Layer Convolution5
I1022 21:09:53.746551  3959 net.cpp:406] Convolution5 <- Convolution4
I1022 21:09:53.746556  3959 net.cpp:380] Convolution5 -> Convolution5
I1022 21:09:53.747898  3959 net.cpp:122] Setting up Convolution5
I1022 21:09:53.747913  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.747917  3959 net.cpp:137] Memory required for data: 569999392
I1022 21:09:53.747922  3959 layer_factory.hpp:77] Creating layer BatchNorm5
I1022 21:09:53.747930  3959 net.cpp:84] Creating Layer BatchNorm5
I1022 21:09:53.747933  3959 net.cpp:406] BatchNorm5 <- Convolution5
I1022 21:09:53.747938  3959 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1022 21:09:53.748179  3959 net.cpp:122] Setting up BatchNorm5
I1022 21:09:53.748199  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.748203  3959 net.cpp:137] Memory required for data: 595689504
I1022 21:09:53.748209  3959 layer_factory.hpp:77] Creating layer Scale5
I1022 21:09:53.748224  3959 net.cpp:84] Creating Layer Scale5
I1022 21:09:53.748227  3959 net.cpp:406] Scale5 <- Convolution5
I1022 21:09:53.748231  3959 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1022 21:09:53.748302  3959 layer_factory.hpp:77] Creating layer Scale5
I1022 21:09:53.748476  3959 net.cpp:122] Setting up Scale5
I1022 21:09:53.748482  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.748486  3959 net.cpp:137] Memory required for data: 621379616
I1022 21:09:53.748499  3959 layer_factory.hpp:77] Creating layer Eltwise2
I1022 21:09:53.748505  3959 net.cpp:84] Creating Layer Eltwise2
I1022 21:09:53.748508  3959 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1022 21:09:53.748522  3959 net.cpp:406] Eltwise2 <- Convolution5
I1022 21:09:53.748528  3959 net.cpp:380] Eltwise2 -> Eltwise2
I1022 21:09:53.748564  3959 net.cpp:122] Setting up Eltwise2
I1022 21:09:53.748569  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.748571  3959 net.cpp:137] Memory required for data: 647069728
I1022 21:09:53.748584  3959 layer_factory.hpp:77] Creating layer penlu5
I1022 21:09:53.748589  3959 net.cpp:84] Creating Layer penlu5
I1022 21:09:53.748592  3959 net.cpp:406] penlu5 <- Eltwise2
I1022 21:09:53.748597  3959 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1022 21:09:53.749913  3959 net.cpp:122] Setting up penlu5
I1022 21:09:53.749927  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.749930  3959 net.cpp:137] Memory required for data: 672759840
I1022 21:09:53.749945  3959 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1022 21:09:53.749951  3959 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1022 21:09:53.749954  3959 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1022 21:09:53.749960  3959 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1022 21:09:53.749966  3959 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1022 21:09:53.749992  3959 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1022 21:09:53.749997  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.750001  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.750005  3959 net.cpp:137] Memory required for data: 724140064
I1022 21:09:53.750006  3959 layer_factory.hpp:77] Creating layer Convolution6
I1022 21:09:53.750015  3959 net.cpp:84] Creating Layer Convolution6
I1022 21:09:53.750018  3959 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1022 21:09:53.750022  3959 net.cpp:380] Convolution6 -> Convolution6
I1022 21:09:53.751564  3959 net.cpp:122] Setting up Convolution6
I1022 21:09:53.751575  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.751579  3959 net.cpp:137] Memory required for data: 749830176
I1022 21:09:53.751583  3959 layer_factory.hpp:77] Creating layer BatchNorm6
I1022 21:09:53.751590  3959 net.cpp:84] Creating Layer BatchNorm6
I1022 21:09:53.751592  3959 net.cpp:406] BatchNorm6 <- Convolution6
I1022 21:09:53.751597  3959 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1022 21:09:53.751755  3959 net.cpp:122] Setting up BatchNorm6
I1022 21:09:53.751761  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.751765  3959 net.cpp:137] Memory required for data: 775520288
I1022 21:09:53.751770  3959 layer_factory.hpp:77] Creating layer Scale6
I1022 21:09:53.751775  3959 net.cpp:84] Creating Layer Scale6
I1022 21:09:53.751777  3959 net.cpp:406] Scale6 <- Convolution6
I1022 21:09:53.751781  3959 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1022 21:09:53.751809  3959 layer_factory.hpp:77] Creating layer Scale6
I1022 21:09:53.751925  3959 net.cpp:122] Setting up Scale6
I1022 21:09:53.751931  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.751935  3959 net.cpp:137] Memory required for data: 801210400
I1022 21:09:53.751938  3959 layer_factory.hpp:77] Creating layer penlu6
I1022 21:09:53.751945  3959 net.cpp:84] Creating Layer penlu6
I1022 21:09:53.751948  3959 net.cpp:406] penlu6 <- Convolution6
I1022 21:09:53.751952  3959 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1022 21:09:53.753106  3959 net.cpp:122] Setting up penlu6
I1022 21:09:53.753116  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.753131  3959 net.cpp:137] Memory required for data: 826900512
I1022 21:09:53.753137  3959 layer_factory.hpp:77] Creating layer Convolution7
I1022 21:09:53.753145  3959 net.cpp:84] Creating Layer Convolution7
I1022 21:09:53.753149  3959 net.cpp:406] Convolution7 <- Convolution6
I1022 21:09:53.753154  3959 net.cpp:380] Convolution7 -> Convolution7
I1022 21:09:53.754127  3959 net.cpp:122] Setting up Convolution7
I1022 21:09:53.754138  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.754142  3959 net.cpp:137] Memory required for data: 852590624
I1022 21:09:53.754146  3959 layer_factory.hpp:77] Creating layer BatchNorm7
I1022 21:09:53.754153  3959 net.cpp:84] Creating Layer BatchNorm7
I1022 21:09:53.754156  3959 net.cpp:406] BatchNorm7 <- Convolution7
I1022 21:09:53.754159  3959 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1022 21:09:53.754319  3959 net.cpp:122] Setting up BatchNorm7
I1022 21:09:53.754325  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.754328  3959 net.cpp:137] Memory required for data: 878280736
I1022 21:09:53.754339  3959 layer_factory.hpp:77] Creating layer Scale7
I1022 21:09:53.754348  3959 net.cpp:84] Creating Layer Scale7
I1022 21:09:53.754351  3959 net.cpp:406] Scale7 <- Convolution7
I1022 21:09:53.754354  3959 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1022 21:09:53.754382  3959 layer_factory.hpp:77] Creating layer Scale7
I1022 21:09:53.754513  3959 net.cpp:122] Setting up Scale7
I1022 21:09:53.754519  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.754521  3959 net.cpp:137] Memory required for data: 903970848
I1022 21:09:53.754535  3959 layer_factory.hpp:77] Creating layer Eltwise3
I1022 21:09:53.754540  3959 net.cpp:84] Creating Layer Eltwise3
I1022 21:09:53.754544  3959 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1022 21:09:53.754547  3959 net.cpp:406] Eltwise3 <- Convolution7
I1022 21:09:53.754550  3959 net.cpp:380] Eltwise3 -> Eltwise3
I1022 21:09:53.754578  3959 net.cpp:122] Setting up Eltwise3
I1022 21:09:53.754583  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.754586  3959 net.cpp:137] Memory required for data: 929660960
I1022 21:09:53.754588  3959 layer_factory.hpp:77] Creating layer penlu7
I1022 21:09:53.754592  3959 net.cpp:84] Creating Layer penlu7
I1022 21:09:53.754595  3959 net.cpp:406] penlu7 <- Eltwise3
I1022 21:09:53.754600  3959 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1022 21:09:53.755781  3959 net.cpp:122] Setting up penlu7
I1022 21:09:53.755792  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.755795  3959 net.cpp:137] Memory required for data: 955351072
I1022 21:09:53.755801  3959 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1022 21:09:53.755806  3959 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1022 21:09:53.755810  3959 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1022 21:09:53.755813  3959 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1022 21:09:53.755820  3959 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1022 21:09:53.755844  3959 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1022 21:09:53.755849  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.755853  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.755856  3959 net.cpp:137] Memory required for data: 1006731296
I1022 21:09:53.755858  3959 layer_factory.hpp:77] Creating layer Convolution8
I1022 21:09:53.755867  3959 net.cpp:84] Creating Layer Convolution8
I1022 21:09:53.755870  3959 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1022 21:09:53.755874  3959 net.cpp:380] Convolution8 -> Convolution8
I1022 21:09:53.757639  3959 net.cpp:122] Setting up Convolution8
I1022 21:09:53.757650  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.757653  3959 net.cpp:137] Memory required for data: 1019576352
I1022 21:09:53.757658  3959 layer_factory.hpp:77] Creating layer BatchNorm8
I1022 21:09:53.757665  3959 net.cpp:84] Creating Layer BatchNorm8
I1022 21:09:53.757669  3959 net.cpp:406] BatchNorm8 <- Convolution8
I1022 21:09:53.757683  3959 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1022 21:09:53.757828  3959 net.cpp:122] Setting up BatchNorm8
I1022 21:09:53.757834  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.757838  3959 net.cpp:137] Memory required for data: 1032421408
I1022 21:09:53.757843  3959 layer_factory.hpp:77] Creating layer Scale8
I1022 21:09:53.757849  3959 net.cpp:84] Creating Layer Scale8
I1022 21:09:53.757853  3959 net.cpp:406] Scale8 <- Convolution8
I1022 21:09:53.757855  3959 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1022 21:09:53.757886  3959 layer_factory.hpp:77] Creating layer Scale8
I1022 21:09:53.757973  3959 net.cpp:122] Setting up Scale8
I1022 21:09:53.757979  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.757982  3959 net.cpp:137] Memory required for data: 1045266464
I1022 21:09:53.757985  3959 layer_factory.hpp:77] Creating layer Convolution9
I1022 21:09:53.757994  3959 net.cpp:84] Creating Layer Convolution9
I1022 21:09:53.757997  3959 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1022 21:09:53.758002  3959 net.cpp:380] Convolution9 -> Convolution9
I1022 21:09:53.758954  3959 net.cpp:122] Setting up Convolution9
I1022 21:09:53.758963  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.758966  3959 net.cpp:137] Memory required for data: 1058111520
I1022 21:09:53.758971  3959 layer_factory.hpp:77] Creating layer BatchNorm9
I1022 21:09:53.758977  3959 net.cpp:84] Creating Layer BatchNorm9
I1022 21:09:53.758980  3959 net.cpp:406] BatchNorm9 <- Convolution9
I1022 21:09:53.758985  3959 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1022 21:09:53.759119  3959 net.cpp:122] Setting up BatchNorm9
I1022 21:09:53.759124  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.759126  3959 net.cpp:137] Memory required for data: 1070956576
I1022 21:09:53.759131  3959 layer_factory.hpp:77] Creating layer Scale9
I1022 21:09:53.759135  3959 net.cpp:84] Creating Layer Scale9
I1022 21:09:53.759138  3959 net.cpp:406] Scale9 <- Convolution9
I1022 21:09:53.759142  3959 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1022 21:09:53.759168  3959 layer_factory.hpp:77] Creating layer Scale9
I1022 21:09:53.759250  3959 net.cpp:122] Setting up Scale9
I1022 21:09:53.759255  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.759258  3959 net.cpp:137] Memory required for data: 1083801632
I1022 21:09:53.759263  3959 layer_factory.hpp:77] Creating layer penlu8
I1022 21:09:53.759268  3959 net.cpp:84] Creating Layer penlu8
I1022 21:09:53.759271  3959 net.cpp:406] penlu8 <- Convolution9
I1022 21:09:53.759276  3959 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1022 21:09:53.760092  3959 net.cpp:122] Setting up penlu8
I1022 21:09:53.760102  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.760105  3959 net.cpp:137] Memory required for data: 1096646688
I1022 21:09:53.760110  3959 layer_factory.hpp:77] Creating layer Convolution10
I1022 21:09:53.760118  3959 net.cpp:84] Creating Layer Convolution10
I1022 21:09:53.760121  3959 net.cpp:406] Convolution10 <- Convolution9
I1022 21:09:53.760126  3959 net.cpp:380] Convolution10 -> Convolution10
I1022 21:09:53.761193  3959 net.cpp:122] Setting up Convolution10
I1022 21:09:53.761203  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.761205  3959 net.cpp:137] Memory required for data: 1109491744
I1022 21:09:53.761210  3959 layer_factory.hpp:77] Creating layer BatchNorm10
I1022 21:09:53.761216  3959 net.cpp:84] Creating Layer BatchNorm10
I1022 21:09:53.761220  3959 net.cpp:406] BatchNorm10 <- Convolution10
I1022 21:09:53.761225  3959 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1022 21:09:53.761358  3959 net.cpp:122] Setting up BatchNorm10
I1022 21:09:53.761364  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.761368  3959 net.cpp:137] Memory required for data: 1122336800
I1022 21:09:53.761373  3959 layer_factory.hpp:77] Creating layer Scale10
I1022 21:09:53.761379  3959 net.cpp:84] Creating Layer Scale10
I1022 21:09:53.761389  3959 net.cpp:406] Scale10 <- Convolution10
I1022 21:09:53.761394  3959 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1022 21:09:53.761421  3959 layer_factory.hpp:77] Creating layer Scale10
I1022 21:09:53.761510  3959 net.cpp:122] Setting up Scale10
I1022 21:09:53.761517  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.761519  3959 net.cpp:137] Memory required for data: 1135181856
I1022 21:09:53.761523  3959 layer_factory.hpp:77] Creating layer Eltwise4
I1022 21:09:53.761528  3959 net.cpp:84] Creating Layer Eltwise4
I1022 21:09:53.761531  3959 net.cpp:406] Eltwise4 <- Convolution8
I1022 21:09:53.761534  3959 net.cpp:406] Eltwise4 <- Convolution10
I1022 21:09:53.761539  3959 net.cpp:380] Eltwise4 -> Eltwise4
I1022 21:09:53.761555  3959 net.cpp:122] Setting up Eltwise4
I1022 21:09:53.761561  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.761564  3959 net.cpp:137] Memory required for data: 1148026912
I1022 21:09:53.761566  3959 layer_factory.hpp:77] Creating layer penlu9
I1022 21:09:53.761571  3959 net.cpp:84] Creating Layer penlu9
I1022 21:09:53.761574  3959 net.cpp:406] penlu9 <- Eltwise4
I1022 21:09:53.761579  3959 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1022 21:09:53.762365  3959 net.cpp:122] Setting up penlu9
I1022 21:09:53.762375  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.762377  3959 net.cpp:137] Memory required for data: 1160871968
I1022 21:09:53.762383  3959 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1022 21:09:53.762388  3959 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1022 21:09:53.762392  3959 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1022 21:09:53.762397  3959 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1022 21:09:53.762401  3959 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1022 21:09:53.762426  3959 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1022 21:09:53.762431  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.762435  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.762439  3959 net.cpp:137] Memory required for data: 1186562080
I1022 21:09:53.762440  3959 layer_factory.hpp:77] Creating layer Convolution11
I1022 21:09:53.762449  3959 net.cpp:84] Creating Layer Convolution11
I1022 21:09:53.762451  3959 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1022 21:09:53.762455  3959 net.cpp:380] Convolution11 -> Convolution11
I1022 21:09:53.763583  3959 net.cpp:122] Setting up Convolution11
I1022 21:09:53.763594  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.763597  3959 net.cpp:137] Memory required for data: 1199407136
I1022 21:09:53.763602  3959 layer_factory.hpp:77] Creating layer BatchNorm11
I1022 21:09:53.763609  3959 net.cpp:84] Creating Layer BatchNorm11
I1022 21:09:53.763613  3959 net.cpp:406] BatchNorm11 <- Convolution11
I1022 21:09:53.763617  3959 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1022 21:09:53.763814  3959 net.cpp:122] Setting up BatchNorm11
I1022 21:09:53.763826  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.763833  3959 net.cpp:137] Memory required for data: 1212252192
I1022 21:09:53.763841  3959 layer_factory.hpp:77] Creating layer Scale11
I1022 21:09:53.763851  3959 net.cpp:84] Creating Layer Scale11
I1022 21:09:53.763857  3959 net.cpp:406] Scale11 <- Convolution11
I1022 21:09:53.763864  3959 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1022 21:09:53.763906  3959 layer_factory.hpp:77] Creating layer Scale11
I1022 21:09:53.764011  3959 net.cpp:122] Setting up Scale11
I1022 21:09:53.764019  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.764020  3959 net.cpp:137] Memory required for data: 1225097248
I1022 21:09:53.764025  3959 layer_factory.hpp:77] Creating layer penlu10
I1022 21:09:53.764031  3959 net.cpp:84] Creating Layer penlu10
I1022 21:09:53.764035  3959 net.cpp:406] penlu10 <- Convolution11
I1022 21:09:53.764039  3959 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1022 21:09:53.764916  3959 net.cpp:122] Setting up penlu10
I1022 21:09:53.764935  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.764937  3959 net.cpp:137] Memory required for data: 1237942304
I1022 21:09:53.764943  3959 layer_factory.hpp:77] Creating layer Convolution12
I1022 21:09:53.764950  3959 net.cpp:84] Creating Layer Convolution12
I1022 21:09:53.764953  3959 net.cpp:406] Convolution12 <- Convolution11
I1022 21:09:53.764958  3959 net.cpp:380] Convolution12 -> Convolution12
I1022 21:09:53.766232  3959 net.cpp:122] Setting up Convolution12
I1022 21:09:53.766242  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.766245  3959 net.cpp:137] Memory required for data: 1250787360
I1022 21:09:53.766250  3959 layer_factory.hpp:77] Creating layer BatchNorm12
I1022 21:09:53.766257  3959 net.cpp:84] Creating Layer BatchNorm12
I1022 21:09:53.766260  3959 net.cpp:406] BatchNorm12 <- Convolution12
I1022 21:09:53.766264  3959 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1022 21:09:53.766415  3959 net.cpp:122] Setting up BatchNorm12
I1022 21:09:53.766433  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.766435  3959 net.cpp:137] Memory required for data: 1263632416
I1022 21:09:53.766450  3959 layer_factory.hpp:77] Creating layer Scale12
I1022 21:09:53.766455  3959 net.cpp:84] Creating Layer Scale12
I1022 21:09:53.766468  3959 net.cpp:406] Scale12 <- Convolution12
I1022 21:09:53.766470  3959 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1022 21:09:53.766508  3959 layer_factory.hpp:77] Creating layer Scale12
I1022 21:09:53.766597  3959 net.cpp:122] Setting up Scale12
I1022 21:09:53.766602  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.766604  3959 net.cpp:137] Memory required for data: 1276477472
I1022 21:09:53.766608  3959 layer_factory.hpp:77] Creating layer Eltwise5
I1022 21:09:53.766613  3959 net.cpp:84] Creating Layer Eltwise5
I1022 21:09:53.766618  3959 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1022 21:09:53.766620  3959 net.cpp:406] Eltwise5 <- Convolution12
I1022 21:09:53.766624  3959 net.cpp:380] Eltwise5 -> Eltwise5
I1022 21:09:53.766641  3959 net.cpp:122] Setting up Eltwise5
I1022 21:09:53.766646  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.766649  3959 net.cpp:137] Memory required for data: 1289322528
I1022 21:09:53.766651  3959 layer_factory.hpp:77] Creating layer penlu11
I1022 21:09:53.766656  3959 net.cpp:84] Creating Layer penlu11
I1022 21:09:53.766659  3959 net.cpp:406] penlu11 <- Eltwise5
I1022 21:09:53.766662  3959 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1022 21:09:53.767457  3959 net.cpp:122] Setting up penlu11
I1022 21:09:53.767467  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.767469  3959 net.cpp:137] Memory required for data: 1302167584
I1022 21:09:53.767474  3959 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1022 21:09:53.767479  3959 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1022 21:09:53.767482  3959 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1022 21:09:53.767487  3959 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1022 21:09:53.767491  3959 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1022 21:09:53.767516  3959 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1022 21:09:53.767521  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.767525  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.767529  3959 net.cpp:137] Memory required for data: 1327857696
I1022 21:09:53.767530  3959 layer_factory.hpp:77] Creating layer Convolution13
I1022 21:09:53.767537  3959 net.cpp:84] Creating Layer Convolution13
I1022 21:09:53.767540  3959 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1022 21:09:53.767544  3959 net.cpp:380] Convolution13 -> Convolution13
I1022 21:09:53.768633  3959 net.cpp:122] Setting up Convolution13
I1022 21:09:53.768643  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.768647  3959 net.cpp:137] Memory required for data: 1340702752
I1022 21:09:53.768661  3959 layer_factory.hpp:77] Creating layer BatchNorm13
I1022 21:09:53.768667  3959 net.cpp:84] Creating Layer BatchNorm13
I1022 21:09:53.768671  3959 net.cpp:406] BatchNorm13 <- Convolution13
I1022 21:09:53.768674  3959 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1022 21:09:53.768812  3959 net.cpp:122] Setting up BatchNorm13
I1022 21:09:53.768818  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.768821  3959 net.cpp:137] Memory required for data: 1353547808
I1022 21:09:53.768826  3959 layer_factory.hpp:77] Creating layer Scale13
I1022 21:09:53.768831  3959 net.cpp:84] Creating Layer Scale13
I1022 21:09:53.768834  3959 net.cpp:406] Scale13 <- Convolution13
I1022 21:09:53.768837  3959 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1022 21:09:53.768867  3959 layer_factory.hpp:77] Creating layer Scale13
I1022 21:09:53.768956  3959 net.cpp:122] Setting up Scale13
I1022 21:09:53.768962  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.768965  3959 net.cpp:137] Memory required for data: 1366392864
I1022 21:09:53.768968  3959 layer_factory.hpp:77] Creating layer penlu12
I1022 21:09:53.768975  3959 net.cpp:84] Creating Layer penlu12
I1022 21:09:53.768978  3959 net.cpp:406] penlu12 <- Convolution13
I1022 21:09:53.768982  3959 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1022 21:09:53.769767  3959 net.cpp:122] Setting up penlu12
I1022 21:09:53.769776  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.769780  3959 net.cpp:137] Memory required for data: 1379237920
I1022 21:09:53.769785  3959 layer_factory.hpp:77] Creating layer Convolution14
I1022 21:09:53.769793  3959 net.cpp:84] Creating Layer Convolution14
I1022 21:09:53.769795  3959 net.cpp:406] Convolution14 <- Convolution13
I1022 21:09:53.769800  3959 net.cpp:380] Convolution14 -> Convolution14
I1022 21:09:53.770902  3959 net.cpp:122] Setting up Convolution14
I1022 21:09:53.770912  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.770915  3959 net.cpp:137] Memory required for data: 1392082976
I1022 21:09:53.770933  3959 layer_factory.hpp:77] Creating layer BatchNorm14
I1022 21:09:53.770943  3959 net.cpp:84] Creating Layer BatchNorm14
I1022 21:09:53.770947  3959 net.cpp:406] BatchNorm14 <- Convolution14
I1022 21:09:53.770951  3959 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1022 21:09:53.771090  3959 net.cpp:122] Setting up BatchNorm14
I1022 21:09:53.771095  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.771097  3959 net.cpp:137] Memory required for data: 1404928032
I1022 21:09:53.771102  3959 layer_factory.hpp:77] Creating layer Scale14
I1022 21:09:53.771107  3959 net.cpp:84] Creating Layer Scale14
I1022 21:09:53.771111  3959 net.cpp:406] Scale14 <- Convolution14
I1022 21:09:53.771113  3959 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1022 21:09:53.771140  3959 layer_factory.hpp:77] Creating layer Scale14
I1022 21:09:53.771229  3959 net.cpp:122] Setting up Scale14
I1022 21:09:53.771234  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.771237  3959 net.cpp:137] Memory required for data: 1417773088
I1022 21:09:53.771241  3959 layer_factory.hpp:77] Creating layer Eltwise6
I1022 21:09:53.771246  3959 net.cpp:84] Creating Layer Eltwise6
I1022 21:09:53.771250  3959 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1022 21:09:53.771251  3959 net.cpp:406] Eltwise6 <- Convolution14
I1022 21:09:53.771255  3959 net.cpp:380] Eltwise6 -> Eltwise6
I1022 21:09:53.771272  3959 net.cpp:122] Setting up Eltwise6
I1022 21:09:53.771276  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.771280  3959 net.cpp:137] Memory required for data: 1430618144
I1022 21:09:53.771281  3959 layer_factory.hpp:77] Creating layer penlu13
I1022 21:09:53.771287  3959 net.cpp:84] Creating Layer penlu13
I1022 21:09:53.771291  3959 net.cpp:406] penlu13 <- Eltwise6
I1022 21:09:53.771293  3959 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1022 21:09:53.772122  3959 net.cpp:122] Setting up penlu13
I1022 21:09:53.772132  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.772143  3959 net.cpp:137] Memory required for data: 1443463200
I1022 21:09:53.772150  3959 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1022 21:09:53.772155  3959 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1022 21:09:53.772157  3959 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1022 21:09:53.772161  3959 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1022 21:09:53.772166  3959 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1022 21:09:53.772192  3959 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1022 21:09:53.772197  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.772200  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.772203  3959 net.cpp:137] Memory required for data: 1469153312
I1022 21:09:53.772207  3959 layer_factory.hpp:77] Creating layer Convolution15
I1022 21:09:53.772212  3959 net.cpp:84] Creating Layer Convolution15
I1022 21:09:53.772215  3959 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1022 21:09:53.772220  3959 net.cpp:380] Convolution15 -> Convolution15
I1022 21:09:53.773145  3959 net.cpp:122] Setting up Convolution15
I1022 21:09:53.773154  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.773159  3959 net.cpp:137] Memory required for data: 1475575840
I1022 21:09:53.773162  3959 layer_factory.hpp:77] Creating layer BatchNorm15
I1022 21:09:53.773167  3959 net.cpp:84] Creating Layer BatchNorm15
I1022 21:09:53.773170  3959 net.cpp:406] BatchNorm15 <- Convolution15
I1022 21:09:53.773176  3959 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1022 21:09:53.773315  3959 net.cpp:122] Setting up BatchNorm15
I1022 21:09:53.773320  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.773324  3959 net.cpp:137] Memory required for data: 1481998368
I1022 21:09:53.773329  3959 layer_factory.hpp:77] Creating layer Scale15
I1022 21:09:53.773332  3959 net.cpp:84] Creating Layer Scale15
I1022 21:09:53.773335  3959 net.cpp:406] Scale15 <- Convolution15
I1022 21:09:53.773339  3959 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1022 21:09:53.773366  3959 layer_factory.hpp:77] Creating layer Scale15
I1022 21:09:53.773448  3959 net.cpp:122] Setting up Scale15
I1022 21:09:53.773453  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.773457  3959 net.cpp:137] Memory required for data: 1488420896
I1022 21:09:53.773460  3959 layer_factory.hpp:77] Creating layer Convolution16
I1022 21:09:53.773468  3959 net.cpp:84] Creating Layer Convolution16
I1022 21:09:53.773470  3959 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1022 21:09:53.773475  3959 net.cpp:380] Convolution16 -> Convolution16
I1022 21:09:53.774729  3959 net.cpp:122] Setting up Convolution16
I1022 21:09:53.774739  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.774742  3959 net.cpp:137] Memory required for data: 1494843424
I1022 21:09:53.774747  3959 layer_factory.hpp:77] Creating layer BatchNorm16
I1022 21:09:53.774752  3959 net.cpp:84] Creating Layer BatchNorm16
I1022 21:09:53.774756  3959 net.cpp:406] BatchNorm16 <- Convolution16
I1022 21:09:53.774760  3959 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1022 21:09:53.774897  3959 net.cpp:122] Setting up BatchNorm16
I1022 21:09:53.774904  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.774905  3959 net.cpp:137] Memory required for data: 1501265952
I1022 21:09:53.774910  3959 layer_factory.hpp:77] Creating layer Scale16
I1022 21:09:53.774915  3959 net.cpp:84] Creating Layer Scale16
I1022 21:09:53.774919  3959 net.cpp:406] Scale16 <- Convolution16
I1022 21:09:53.774921  3959 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1022 21:09:53.774950  3959 layer_factory.hpp:77] Creating layer Scale16
I1022 21:09:53.775033  3959 net.cpp:122] Setting up Scale16
I1022 21:09:53.775038  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.775041  3959 net.cpp:137] Memory required for data: 1507688480
I1022 21:09:53.775045  3959 layer_factory.hpp:77] Creating layer penlu14
I1022 21:09:53.775058  3959 net.cpp:84] Creating Layer penlu14
I1022 21:09:53.775061  3959 net.cpp:406] penlu14 <- Convolution16
I1022 21:09:53.775065  3959 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1022 21:09:53.775305  3959 net.cpp:122] Setting up penlu14
I1022 21:09:53.775311  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.775315  3959 net.cpp:137] Memory required for data: 1514111008
I1022 21:09:53.775319  3959 layer_factory.hpp:77] Creating layer Convolution17
I1022 21:09:53.775326  3959 net.cpp:84] Creating Layer Convolution17
I1022 21:09:53.775329  3959 net.cpp:406] Convolution17 <- Convolution16
I1022 21:09:53.775333  3959 net.cpp:380] Convolution17 -> Convolution17
I1022 21:09:53.776650  3959 net.cpp:122] Setting up Convolution17
I1022 21:09:53.776659  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.776661  3959 net.cpp:137] Memory required for data: 1520533536
I1022 21:09:53.776665  3959 layer_factory.hpp:77] Creating layer BatchNorm17
I1022 21:09:53.776671  3959 net.cpp:84] Creating Layer BatchNorm17
I1022 21:09:53.776674  3959 net.cpp:406] BatchNorm17 <- Convolution17
I1022 21:09:53.776679  3959 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1022 21:09:53.776814  3959 net.cpp:122] Setting up BatchNorm17
I1022 21:09:53.776819  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.776823  3959 net.cpp:137] Memory required for data: 1526956064
I1022 21:09:53.776826  3959 layer_factory.hpp:77] Creating layer Scale17
I1022 21:09:53.776831  3959 net.cpp:84] Creating Layer Scale17
I1022 21:09:53.776834  3959 net.cpp:406] Scale17 <- Convolution17
I1022 21:09:53.776836  3959 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1022 21:09:53.776865  3959 layer_factory.hpp:77] Creating layer Scale17
I1022 21:09:53.776945  3959 net.cpp:122] Setting up Scale17
I1022 21:09:53.776952  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.776953  3959 net.cpp:137] Memory required for data: 1533378592
I1022 21:09:53.776957  3959 layer_factory.hpp:77] Creating layer Eltwise7
I1022 21:09:53.776962  3959 net.cpp:84] Creating Layer Eltwise7
I1022 21:09:53.776965  3959 net.cpp:406] Eltwise7 <- Convolution15
I1022 21:09:53.776968  3959 net.cpp:406] Eltwise7 <- Convolution17
I1022 21:09:53.776971  3959 net.cpp:380] Eltwise7 -> Eltwise7
I1022 21:09:53.776988  3959 net.cpp:122] Setting up Eltwise7
I1022 21:09:53.776993  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.776995  3959 net.cpp:137] Memory required for data: 1539801120
I1022 21:09:53.776998  3959 layer_factory.hpp:77] Creating layer penlu15
I1022 21:09:53.777003  3959 net.cpp:84] Creating Layer penlu15
I1022 21:09:53.777005  3959 net.cpp:406] penlu15 <- Eltwise7
I1022 21:09:53.777009  3959 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1022 21:09:53.777736  3959 net.cpp:122] Setting up penlu15
I1022 21:09:53.777745  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.777746  3959 net.cpp:137] Memory required for data: 1546223648
I1022 21:09:53.777751  3959 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1022 21:09:53.777755  3959 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1022 21:09:53.777758  3959 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1022 21:09:53.777761  3959 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1022 21:09:53.777766  3959 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1022 21:09:53.777791  3959 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1022 21:09:53.777796  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.777798  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.777801  3959 net.cpp:137] Memory required for data: 1559068704
I1022 21:09:53.777802  3959 layer_factory.hpp:77] Creating layer Convolution18
I1022 21:09:53.777808  3959 net.cpp:84] Creating Layer Convolution18
I1022 21:09:53.777812  3959 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1022 21:09:53.777814  3959 net.cpp:380] Convolution18 -> Convolution18
I1022 21:09:53.779814  3959 net.cpp:122] Setting up Convolution18
I1022 21:09:53.779824  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.779825  3959 net.cpp:137] Memory required for data: 1565491232
I1022 21:09:53.779830  3959 layer_factory.hpp:77] Creating layer BatchNorm18
I1022 21:09:53.779834  3959 net.cpp:84] Creating Layer BatchNorm18
I1022 21:09:53.779837  3959 net.cpp:406] BatchNorm18 <- Convolution18
I1022 21:09:53.779841  3959 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1022 21:09:53.779990  3959 net.cpp:122] Setting up BatchNorm18
I1022 21:09:53.779995  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.779997  3959 net.cpp:137] Memory required for data: 1571913760
I1022 21:09:53.780002  3959 layer_factory.hpp:77] Creating layer Scale18
I1022 21:09:53.780006  3959 net.cpp:84] Creating Layer Scale18
I1022 21:09:53.780009  3959 net.cpp:406] Scale18 <- Convolution18
I1022 21:09:53.780014  3959 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1022 21:09:53.780041  3959 layer_factory.hpp:77] Creating layer Scale18
I1022 21:09:53.780125  3959 net.cpp:122] Setting up Scale18
I1022 21:09:53.780130  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.780133  3959 net.cpp:137] Memory required for data: 1578336288
I1022 21:09:53.780135  3959 layer_factory.hpp:77] Creating layer penlu16
I1022 21:09:53.780141  3959 net.cpp:84] Creating Layer penlu16
I1022 21:09:53.780143  3959 net.cpp:406] penlu16 <- Convolution18
I1022 21:09:53.780148  3959 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1022 21:09:53.780344  3959 net.cpp:122] Setting up penlu16
I1022 21:09:53.780349  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.780351  3959 net.cpp:137] Memory required for data: 1584758816
I1022 21:09:53.780356  3959 layer_factory.hpp:77] Creating layer Convolution19
I1022 21:09:53.780361  3959 net.cpp:84] Creating Layer Convolution19
I1022 21:09:53.780364  3959 net.cpp:406] Convolution19 <- Convolution18
I1022 21:09:53.780369  3959 net.cpp:380] Convolution19 -> Convolution19
I1022 21:09:53.782541  3959 net.cpp:122] Setting up Convolution19
I1022 21:09:53.782549  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.782552  3959 net.cpp:137] Memory required for data: 1591181344
I1022 21:09:53.782557  3959 layer_factory.hpp:77] Creating layer BatchNorm19
I1022 21:09:53.782562  3959 net.cpp:84] Creating Layer BatchNorm19
I1022 21:09:53.782565  3959 net.cpp:406] BatchNorm19 <- Convolution19
I1022 21:09:53.782568  3959 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1022 21:09:53.782708  3959 net.cpp:122] Setting up BatchNorm19
I1022 21:09:53.782712  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.782716  3959 net.cpp:137] Memory required for data: 1597603872
I1022 21:09:53.782719  3959 layer_factory.hpp:77] Creating layer Scale19
I1022 21:09:53.782723  3959 net.cpp:84] Creating Layer Scale19
I1022 21:09:53.782727  3959 net.cpp:406] Scale19 <- Convolution19
I1022 21:09:53.782729  3959 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1022 21:09:53.782758  3959 layer_factory.hpp:77] Creating layer Scale19
I1022 21:09:53.782842  3959 net.cpp:122] Setting up Scale19
I1022 21:09:53.782846  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.782850  3959 net.cpp:137] Memory required for data: 1604026400
I1022 21:09:53.782852  3959 layer_factory.hpp:77] Creating layer Eltwise8
I1022 21:09:53.782856  3959 net.cpp:84] Creating Layer Eltwise8
I1022 21:09:53.782860  3959 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1022 21:09:53.782862  3959 net.cpp:406] Eltwise8 <- Convolution19
I1022 21:09:53.782866  3959 net.cpp:380] Eltwise8 -> Eltwise8
I1022 21:09:53.782881  3959 net.cpp:122] Setting up Eltwise8
I1022 21:09:53.782884  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.782886  3959 net.cpp:137] Memory required for data: 1610448928
I1022 21:09:53.782888  3959 layer_factory.hpp:77] Creating layer penlu17
I1022 21:09:53.782893  3959 net.cpp:84] Creating Layer penlu17
I1022 21:09:53.782896  3959 net.cpp:406] penlu17 <- Eltwise8
I1022 21:09:53.782907  3959 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1022 21:09:53.783602  3959 net.cpp:122] Setting up penlu17
I1022 21:09:53.783610  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.783612  3959 net.cpp:137] Memory required for data: 1616871456
I1022 21:09:53.783617  3959 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1022 21:09:53.783622  3959 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1022 21:09:53.783624  3959 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1022 21:09:53.783627  3959 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1022 21:09:53.783632  3959 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1022 21:09:53.783658  3959 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1022 21:09:53.783663  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.783665  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.783668  3959 net.cpp:137] Memory required for data: 1629716512
I1022 21:09:53.783669  3959 layer_factory.hpp:77] Creating layer Convolution20
I1022 21:09:53.783675  3959 net.cpp:84] Creating Layer Convolution20
I1022 21:09:53.783679  3959 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1022 21:09:53.783684  3959 net.cpp:380] Convolution20 -> Convolution20
I1022 21:09:53.786025  3959 net.cpp:122] Setting up Convolution20
I1022 21:09:53.786033  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.786036  3959 net.cpp:137] Memory required for data: 1636139040
I1022 21:09:53.786041  3959 layer_factory.hpp:77] Creating layer BatchNorm20
I1022 21:09:53.786046  3959 net.cpp:84] Creating Layer BatchNorm20
I1022 21:09:53.786048  3959 net.cpp:406] BatchNorm20 <- Convolution20
I1022 21:09:53.786052  3959 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1022 21:09:53.786196  3959 net.cpp:122] Setting up BatchNorm20
I1022 21:09:53.786201  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.786203  3959 net.cpp:137] Memory required for data: 1642561568
I1022 21:09:53.786208  3959 layer_factory.hpp:77] Creating layer Scale20
I1022 21:09:53.786212  3959 net.cpp:84] Creating Layer Scale20
I1022 21:09:53.786214  3959 net.cpp:406] Scale20 <- Convolution20
I1022 21:09:53.786217  3959 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1022 21:09:53.786245  3959 layer_factory.hpp:77] Creating layer Scale20
I1022 21:09:53.786330  3959 net.cpp:122] Setting up Scale20
I1022 21:09:53.786335  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.786337  3959 net.cpp:137] Memory required for data: 1648984096
I1022 21:09:53.786341  3959 layer_factory.hpp:77] Creating layer penlu18
I1022 21:09:53.786346  3959 net.cpp:84] Creating Layer penlu18
I1022 21:09:53.786348  3959 net.cpp:406] penlu18 <- Convolution20
I1022 21:09:53.786352  3959 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1022 21:09:53.786545  3959 net.cpp:122] Setting up penlu18
I1022 21:09:53.786550  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.786551  3959 net.cpp:137] Memory required for data: 1655406624
I1022 21:09:53.786556  3959 layer_factory.hpp:77] Creating layer Convolution21
I1022 21:09:53.786562  3959 net.cpp:84] Creating Layer Convolution21
I1022 21:09:53.786564  3959 net.cpp:406] Convolution21 <- Convolution20
I1022 21:09:53.786568  3959 net.cpp:380] Convolution21 -> Convolution21
I1022 21:09:53.788240  3959 net.cpp:122] Setting up Convolution21
I1022 21:09:53.788249  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.788252  3959 net.cpp:137] Memory required for data: 1661829152
I1022 21:09:53.788256  3959 layer_factory.hpp:77] Creating layer BatchNorm21
I1022 21:09:53.788261  3959 net.cpp:84] Creating Layer BatchNorm21
I1022 21:09:53.788264  3959 net.cpp:406] BatchNorm21 <- Convolution21
I1022 21:09:53.788267  3959 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1022 21:09:53.788414  3959 net.cpp:122] Setting up BatchNorm21
I1022 21:09:53.788419  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.788420  3959 net.cpp:137] Memory required for data: 1668251680
I1022 21:09:53.788431  3959 layer_factory.hpp:77] Creating layer Scale21
I1022 21:09:53.788436  3959 net.cpp:84] Creating Layer Scale21
I1022 21:09:53.788439  3959 net.cpp:406] Scale21 <- Convolution21
I1022 21:09:53.788442  3959 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1022 21:09:53.788470  3959 layer_factory.hpp:77] Creating layer Scale21
I1022 21:09:53.788555  3959 net.cpp:122] Setting up Scale21
I1022 21:09:53.788559  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.788561  3959 net.cpp:137] Memory required for data: 1674674208
I1022 21:09:53.788565  3959 layer_factory.hpp:77] Creating layer Eltwise9
I1022 21:09:53.788568  3959 net.cpp:84] Creating Layer Eltwise9
I1022 21:09:53.788571  3959 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1022 21:09:53.788573  3959 net.cpp:406] Eltwise9 <- Convolution21
I1022 21:09:53.788578  3959 net.cpp:380] Eltwise9 -> Eltwise9
I1022 21:09:53.788594  3959 net.cpp:122] Setting up Eltwise9
I1022 21:09:53.788599  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.788600  3959 net.cpp:137] Memory required for data: 1681096736
I1022 21:09:53.788602  3959 layer_factory.hpp:77] Creating layer penlu19
I1022 21:09:53.788606  3959 net.cpp:84] Creating Layer penlu19
I1022 21:09:53.788609  3959 net.cpp:406] penlu19 <- Eltwise9
I1022 21:09:53.788612  3959 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1022 21:09:53.789330  3959 net.cpp:122] Setting up penlu19
I1022 21:09:53.789338  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.789340  3959 net.cpp:137] Memory required for data: 1687519264
I1022 21:09:53.789345  3959 layer_factory.hpp:77] Creating layer Pooling1
I1022 21:09:53.789351  3959 net.cpp:84] Creating Layer Pooling1
I1022 21:09:53.789355  3959 net.cpp:406] Pooling1 <- Eltwise9
I1022 21:09:53.789357  3959 net.cpp:380] Pooling1 -> Pooling1
I1022 21:09:53.789505  3959 net.cpp:122] Setting up Pooling1
I1022 21:09:53.789512  3959 net.cpp:129] Top shape: 8 64 1 1 (512)
I1022 21:09:53.789515  3959 net.cpp:137] Memory required for data: 1687521312
I1022 21:09:53.789517  3959 layer_factory.hpp:77] Creating layer InnerProduct1
I1022 21:09:53.789526  3959 net.cpp:84] Creating Layer InnerProduct1
I1022 21:09:53.789528  3959 net.cpp:406] InnerProduct1 <- Pooling1
I1022 21:09:53.789532  3959 net.cpp:380] InnerProduct1 -> InnerProduct1
I1022 21:09:53.789634  3959 net.cpp:122] Setting up InnerProduct1
I1022 21:09:53.789639  3959 net.cpp:129] Top shape: 8 10 (80)
I1022 21:09:53.789641  3959 net.cpp:137] Memory required for data: 1687521632
I1022 21:09:53.789644  3959 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1022 21:09:53.789650  3959 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1022 21:09:53.789652  3959 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1022 21:09:53.789655  3959 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1022 21:09:53.789659  3959 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1022 21:09:53.789664  3959 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1022 21:09:53.790199  3959 net.cpp:122] Setting up SoftmaxWithLoss1
I1022 21:09:53.790208  3959 net.cpp:129] Top shape: (1)
I1022 21:09:53.790210  3959 net.cpp:132]     with loss weight 1
I1022 21:09:53.790223  3959 net.cpp:137] Memory required for data: 1687521636
I1022 21:09:53.790226  3959 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1022 21:09:53.790228  3959 net.cpp:198] InnerProduct1 needs backward computation.
I1022 21:09:53.790230  3959 net.cpp:198] Pooling1 needs backward computation.
I1022 21:09:53.790233  3959 net.cpp:198] penlu19 needs backward computation.
I1022 21:09:53.790235  3959 net.cpp:198] Eltwise9 needs backward computation.
I1022 21:09:53.790237  3959 net.cpp:198] Scale21 needs backward computation.
I1022 21:09:53.790240  3959 net.cpp:198] BatchNorm21 needs backward computation.
I1022 21:09:53.790241  3959 net.cpp:198] Convolution21 needs backward computation.
I1022 21:09:53.790243  3959 net.cpp:198] penlu18 needs backward computation.
I1022 21:09:53.790246  3959 net.cpp:198] Scale20 needs backward computation.
I1022 21:09:53.790253  3959 net.cpp:198] BatchNorm20 needs backward computation.
I1022 21:09:53.790256  3959 net.cpp:198] Convolution20 needs backward computation.
I1022 21:09:53.790258  3959 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1022 21:09:53.790261  3959 net.cpp:198] penlu17 needs backward computation.
I1022 21:09:53.790262  3959 net.cpp:198] Eltwise8 needs backward computation.
I1022 21:09:53.790266  3959 net.cpp:198] Scale19 needs backward computation.
I1022 21:09:53.790267  3959 net.cpp:198] BatchNorm19 needs backward computation.
I1022 21:09:53.790269  3959 net.cpp:198] Convolution19 needs backward computation.
I1022 21:09:53.790271  3959 net.cpp:198] penlu16 needs backward computation.
I1022 21:09:53.790273  3959 net.cpp:198] Scale18 needs backward computation.
I1022 21:09:53.790276  3959 net.cpp:198] BatchNorm18 needs backward computation.
I1022 21:09:53.790277  3959 net.cpp:198] Convolution18 needs backward computation.
I1022 21:09:53.790279  3959 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1022 21:09:53.790282  3959 net.cpp:198] penlu15 needs backward computation.
I1022 21:09:53.790283  3959 net.cpp:198] Eltwise7 needs backward computation.
I1022 21:09:53.790287  3959 net.cpp:198] Scale17 needs backward computation.
I1022 21:09:53.790288  3959 net.cpp:198] BatchNorm17 needs backward computation.
I1022 21:09:53.790290  3959 net.cpp:198] Convolution17 needs backward computation.
I1022 21:09:53.790292  3959 net.cpp:198] penlu14 needs backward computation.
I1022 21:09:53.790294  3959 net.cpp:198] Scale16 needs backward computation.
I1022 21:09:53.790297  3959 net.cpp:198] BatchNorm16 needs backward computation.
I1022 21:09:53.790298  3959 net.cpp:198] Convolution16 needs backward computation.
I1022 21:09:53.790300  3959 net.cpp:198] Scale15 needs backward computation.
I1022 21:09:53.790302  3959 net.cpp:198] BatchNorm15 needs backward computation.
I1022 21:09:53.790305  3959 net.cpp:198] Convolution15 needs backward computation.
I1022 21:09:53.790307  3959 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1022 21:09:53.790309  3959 net.cpp:198] penlu13 needs backward computation.
I1022 21:09:53.790313  3959 net.cpp:198] Eltwise6 needs backward computation.
I1022 21:09:53.790314  3959 net.cpp:198] Scale14 needs backward computation.
I1022 21:09:53.790316  3959 net.cpp:198] BatchNorm14 needs backward computation.
I1022 21:09:53.790318  3959 net.cpp:198] Convolution14 needs backward computation.
I1022 21:09:53.790321  3959 net.cpp:198] penlu12 needs backward computation.
I1022 21:09:53.790323  3959 net.cpp:198] Scale13 needs backward computation.
I1022 21:09:53.790325  3959 net.cpp:198] BatchNorm13 needs backward computation.
I1022 21:09:53.790328  3959 net.cpp:198] Convolution13 needs backward computation.
I1022 21:09:53.790330  3959 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1022 21:09:53.790333  3959 net.cpp:198] penlu11 needs backward computation.
I1022 21:09:53.790334  3959 net.cpp:198] Eltwise5 needs backward computation.
I1022 21:09:53.790338  3959 net.cpp:198] Scale12 needs backward computation.
I1022 21:09:53.790339  3959 net.cpp:198] BatchNorm12 needs backward computation.
I1022 21:09:53.790341  3959 net.cpp:198] Convolution12 needs backward computation.
I1022 21:09:53.790343  3959 net.cpp:198] penlu10 needs backward computation.
I1022 21:09:53.790345  3959 net.cpp:198] Scale11 needs backward computation.
I1022 21:09:53.790347  3959 net.cpp:198] BatchNorm11 needs backward computation.
I1022 21:09:53.790350  3959 net.cpp:198] Convolution11 needs backward computation.
I1022 21:09:53.790352  3959 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1022 21:09:53.790354  3959 net.cpp:198] penlu9 needs backward computation.
I1022 21:09:53.790357  3959 net.cpp:198] Eltwise4 needs backward computation.
I1022 21:09:53.790359  3959 net.cpp:198] Scale10 needs backward computation.
I1022 21:09:53.790362  3959 net.cpp:198] BatchNorm10 needs backward computation.
I1022 21:09:53.790364  3959 net.cpp:198] Convolution10 needs backward computation.
I1022 21:09:53.790370  3959 net.cpp:198] penlu8 needs backward computation.
I1022 21:09:53.790374  3959 net.cpp:198] Scale9 needs backward computation.
I1022 21:09:53.790375  3959 net.cpp:198] BatchNorm9 needs backward computation.
I1022 21:09:53.790377  3959 net.cpp:198] Convolution9 needs backward computation.
I1022 21:09:53.790380  3959 net.cpp:198] Scale8 needs backward computation.
I1022 21:09:53.790382  3959 net.cpp:198] BatchNorm8 needs backward computation.
I1022 21:09:53.790385  3959 net.cpp:198] Convolution8 needs backward computation.
I1022 21:09:53.790386  3959 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1022 21:09:53.790390  3959 net.cpp:198] penlu7 needs backward computation.
I1022 21:09:53.790391  3959 net.cpp:198] Eltwise3 needs backward computation.
I1022 21:09:53.790393  3959 net.cpp:198] Scale7 needs backward computation.
I1022 21:09:53.790396  3959 net.cpp:198] BatchNorm7 needs backward computation.
I1022 21:09:53.790398  3959 net.cpp:198] Convolution7 needs backward computation.
I1022 21:09:53.790400  3959 net.cpp:198] penlu6 needs backward computation.
I1022 21:09:53.790402  3959 net.cpp:198] Scale6 needs backward computation.
I1022 21:09:53.790405  3959 net.cpp:198] BatchNorm6 needs backward computation.
I1022 21:09:53.790406  3959 net.cpp:198] Convolution6 needs backward computation.
I1022 21:09:53.790410  3959 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1022 21:09:53.790411  3959 net.cpp:198] penlu5 needs backward computation.
I1022 21:09:53.790413  3959 net.cpp:198] Eltwise2 needs backward computation.
I1022 21:09:53.790416  3959 net.cpp:198] Scale5 needs backward computation.
I1022 21:09:53.790418  3959 net.cpp:198] BatchNorm5 needs backward computation.
I1022 21:09:53.790421  3959 net.cpp:198] Convolution5 needs backward computation.
I1022 21:09:53.790422  3959 net.cpp:198] penlu4 needs backward computation.
I1022 21:09:53.790424  3959 net.cpp:198] Scale4 needs backward computation.
I1022 21:09:53.790426  3959 net.cpp:198] BatchNorm4 needs backward computation.
I1022 21:09:53.790428  3959 net.cpp:198] Convolution4 needs backward computation.
I1022 21:09:53.790431  3959 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1022 21:09:53.790433  3959 net.cpp:198] penlu3 needs backward computation.
I1022 21:09:53.790436  3959 net.cpp:198] Eltwise1 needs backward computation.
I1022 21:09:53.790438  3959 net.cpp:198] Scale3 needs backward computation.
I1022 21:09:53.790441  3959 net.cpp:198] BatchNorm3 needs backward computation.
I1022 21:09:53.790442  3959 net.cpp:198] Convolution3 needs backward computation.
I1022 21:09:53.790446  3959 net.cpp:198] penlu2 needs backward computation.
I1022 21:09:53.790447  3959 net.cpp:198] Scale2 needs backward computation.
I1022 21:09:53.790449  3959 net.cpp:198] BatchNorm2 needs backward computation.
I1022 21:09:53.790452  3959 net.cpp:198] Convolution2 needs backward computation.
I1022 21:09:53.790453  3959 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1022 21:09:53.790457  3959 net.cpp:198] penlu1 needs backward computation.
I1022 21:09:53.790458  3959 net.cpp:198] Scale1 needs backward computation.
I1022 21:09:53.790460  3959 net.cpp:198] BatchNorm1 needs backward computation.
I1022 21:09:53.790462  3959 net.cpp:198] Convolution1 needs backward computation.
I1022 21:09:53.790464  3959 net.cpp:200] Data1 does not need backward computation.
I1022 21:09:53.790467  3959 net.cpp:242] This network produces output SoftmaxWithLoss1
I1022 21:09:53.790499  3959 net.cpp:255] Network initialization done.
I1022 21:09:53.792265  3959 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1022 21:09:53.792273  3959 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1022 21:09:53.792279  3959 solver.cpp:172] Creating test net (#0) specified by net file: /home/x306/caffe/xn/English_orange/neural/res20/res20_penlu_gauss.prototxt
I1022 21:09:53.792358  3959 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Data1
I1022 21:09:53.792892  3959 net.cpp:51] Initializing net from parameters: 
name: "resnet"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_file: "/home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto"
  }
  data_param {
    source: "/home/x306/caffe/xn/English_orange/data/val1_lmdb"
    batch_size: 8
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu1"
  type: "PENLU"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu2"
  type: "PENLU"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu3"
  type: "PENLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu4"
  type: "PENLU"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu5"
  type: "PENLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu6"
  type: "PENLU"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.118
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu7"
  type: "PENLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.25
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu8"
  type: "PENLU"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu9"
  type: "PENLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu10"
  type: "PENLU"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu11"
  type: "PENLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu12"
  type: "PENLU"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.083
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu13"
  type: "PENLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.17677669
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu14"
  type: "PENLU"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu15"
  type: "PENLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu16"
  type: "PENLU"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu17"
  type: "PENLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "penlu18"
  type: "PENLU"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.059
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "penlu19"
  type: "PENLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 2
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  penlu_param {
    alpha_filler {
      type: "constant"
      value: 0.25
    }
    beta_filler {
      type: "constant"
      value: 1
    }
    eta_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I1022 21:09:53.793138  3959 layer_factory.hpp:77] Creating layer Data1
I1022 21:09:53.793174  3959 db_lmdb.cpp:35] Opened lmdb /home/x306/caffe/xn/English_orange/data/val1_lmdb
I1022 21:09:53.793184  3959 net.cpp:84] Creating Layer Data1
I1022 21:09:53.793189  3959 net.cpp:380] Data1 -> Data1
I1022 21:09:53.793195  3959 net.cpp:380] Data1 -> Data2
I1022 21:09:53.793200  3959 data_transformer.cpp:25] Loading mean file from: /home/x306/caffe/xn/English_orange/data/orange1_mean.binaryproto
I1022 21:09:53.794364  3959 data_layer.cpp:45] output data size: 8,3,224,224
I1022 21:09:53.803138  3959 net.cpp:122] Setting up Data1
I1022 21:09:53.803159  3959 net.cpp:129] Top shape: 8 3 224 224 (1204224)
I1022 21:09:53.803164  3959 net.cpp:129] Top shape: 8 (8)
I1022 21:09:53.803166  3959 net.cpp:137] Memory required for data: 4816928
I1022 21:09:53.803170  3959 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1022 21:09:53.803179  3959 net.cpp:84] Creating Layer Data2_Data1_1_split
I1022 21:09:53.803181  3959 net.cpp:406] Data2_Data1_1_split <- Data2
I1022 21:09:53.803200  3959 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1022 21:09:53.803207  3959 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1022 21:09:53.803247  3959 net.cpp:122] Setting up Data2_Data1_1_split
I1022 21:09:53.803252  3959 net.cpp:129] Top shape: 8 (8)
I1022 21:09:53.803256  3959 net.cpp:129] Top shape: 8 (8)
I1022 21:09:53.803257  3959 net.cpp:137] Memory required for data: 4816992
I1022 21:09:53.803259  3959 layer_factory.hpp:77] Creating layer Convolution1
I1022 21:09:53.803267  3959 net.cpp:84] Creating Layer Convolution1
I1022 21:09:53.803270  3959 net.cpp:406] Convolution1 <- Data1
I1022 21:09:53.803274  3959 net.cpp:380] Convolution1 -> Convolution1
I1022 21:09:53.804327  3959 net.cpp:122] Setting up Convolution1
I1022 21:09:53.804337  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.804339  3959 net.cpp:137] Memory required for data: 30507104
I1022 21:09:53.804347  3959 layer_factory.hpp:77] Creating layer BatchNorm1
I1022 21:09:53.804352  3959 net.cpp:84] Creating Layer BatchNorm1
I1022 21:09:53.804354  3959 net.cpp:406] BatchNorm1 <- Convolution1
I1022 21:09:53.804358  3959 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1022 21:09:53.804883  3959 net.cpp:122] Setting up BatchNorm1
I1022 21:09:53.804888  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.804889  3959 net.cpp:137] Memory required for data: 56197216
I1022 21:09:53.804896  3959 layer_factory.hpp:77] Creating layer Scale1
I1022 21:09:53.804903  3959 net.cpp:84] Creating Layer Scale1
I1022 21:09:53.804904  3959 net.cpp:406] Scale1 <- Convolution1
I1022 21:09:53.804908  3959 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1022 21:09:53.804936  3959 layer_factory.hpp:77] Creating layer Scale1
I1022 21:09:53.805073  3959 net.cpp:122] Setting up Scale1
I1022 21:09:53.805078  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.805080  3959 net.cpp:137] Memory required for data: 81887328
I1022 21:09:53.805083  3959 layer_factory.hpp:77] Creating layer penlu1
I1022 21:09:53.805089  3959 net.cpp:84] Creating Layer penlu1
I1022 21:09:53.805091  3959 net.cpp:406] penlu1 <- Convolution1
I1022 21:09:53.805095  3959 net.cpp:367] penlu1 -> Convolution1 (in-place)
I1022 21:09:53.806299  3959 net.cpp:122] Setting up penlu1
I1022 21:09:53.806309  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.806313  3959 net.cpp:137] Memory required for data: 107577440
I1022 21:09:53.806320  3959 layer_factory.hpp:77] Creating layer Convolution1_penlu1_0_split
I1022 21:09:53.806325  3959 net.cpp:84] Creating Layer Convolution1_penlu1_0_split
I1022 21:09:53.806329  3959 net.cpp:406] Convolution1_penlu1_0_split <- Convolution1
I1022 21:09:53.806331  3959 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_0
I1022 21:09:53.806336  3959 net.cpp:380] Convolution1_penlu1_0_split -> Convolution1_penlu1_0_split_1
I1022 21:09:53.806365  3959 net.cpp:122] Setting up Convolution1_penlu1_0_split
I1022 21:09:53.806368  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.806370  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.806372  3959 net.cpp:137] Memory required for data: 158957664
I1022 21:09:53.806375  3959 layer_factory.hpp:77] Creating layer Convolution2
I1022 21:09:53.806382  3959 net.cpp:84] Creating Layer Convolution2
I1022 21:09:53.806385  3959 net.cpp:406] Convolution2 <- Convolution1_penlu1_0_split_0
I1022 21:09:53.806388  3959 net.cpp:380] Convolution2 -> Convolution2
I1022 21:09:53.807049  3959 net.cpp:122] Setting up Convolution2
I1022 21:09:53.807056  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.807059  3959 net.cpp:137] Memory required for data: 184647776
I1022 21:09:53.807062  3959 layer_factory.hpp:77] Creating layer BatchNorm2
I1022 21:09:53.807068  3959 net.cpp:84] Creating Layer BatchNorm2
I1022 21:09:53.807071  3959 net.cpp:406] BatchNorm2 <- Convolution2
I1022 21:09:53.807075  3959 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1022 21:09:53.807248  3959 net.cpp:122] Setting up BatchNorm2
I1022 21:09:53.807271  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.807273  3959 net.cpp:137] Memory required for data: 210337888
I1022 21:09:53.807278  3959 layer_factory.hpp:77] Creating layer Scale2
I1022 21:09:53.807283  3959 net.cpp:84] Creating Layer Scale2
I1022 21:09:53.807286  3959 net.cpp:406] Scale2 <- Convolution2
I1022 21:09:53.807288  3959 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1022 21:09:53.807320  3959 layer_factory.hpp:77] Creating layer Scale2
I1022 21:09:53.807462  3959 net.cpp:122] Setting up Scale2
I1022 21:09:53.807467  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.807468  3959 net.cpp:137] Memory required for data: 236028000
I1022 21:09:53.807476  3959 layer_factory.hpp:77] Creating layer penlu2
I1022 21:09:53.807482  3959 net.cpp:84] Creating Layer penlu2
I1022 21:09:53.807483  3959 net.cpp:406] penlu2 <- Convolution2
I1022 21:09:53.807487  3959 net.cpp:367] penlu2 -> Convolution2 (in-place)
I1022 21:09:53.808816  3959 net.cpp:122] Setting up penlu2
I1022 21:09:53.808832  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.808835  3959 net.cpp:137] Memory required for data: 261718112
I1022 21:09:53.808842  3959 layer_factory.hpp:77] Creating layer Convolution3
I1022 21:09:53.808856  3959 net.cpp:84] Creating Layer Convolution3
I1022 21:09:53.808859  3959 net.cpp:406] Convolution3 <- Convolution2
I1022 21:09:53.808866  3959 net.cpp:380] Convolution3 -> Convolution3
I1022 21:09:53.810845  3959 net.cpp:122] Setting up Convolution3
I1022 21:09:53.810858  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.810861  3959 net.cpp:137] Memory required for data: 287408224
I1022 21:09:53.810866  3959 layer_factory.hpp:77] Creating layer BatchNorm3
I1022 21:09:53.810873  3959 net.cpp:84] Creating Layer BatchNorm3
I1022 21:09:53.810876  3959 net.cpp:406] BatchNorm3 <- Convolution3
I1022 21:09:53.810880  3959 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1022 21:09:53.811051  3959 net.cpp:122] Setting up BatchNorm3
I1022 21:09:53.811056  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.811058  3959 net.cpp:137] Memory required for data: 313098336
I1022 21:09:53.811062  3959 layer_factory.hpp:77] Creating layer Scale3
I1022 21:09:53.811069  3959 net.cpp:84] Creating Layer Scale3
I1022 21:09:53.811070  3959 net.cpp:406] Scale3 <- Convolution3
I1022 21:09:53.811074  3959 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1022 21:09:53.811106  3959 layer_factory.hpp:77] Creating layer Scale3
I1022 21:09:53.811226  3959 net.cpp:122] Setting up Scale3
I1022 21:09:53.811230  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.811233  3959 net.cpp:137] Memory required for data: 338788448
I1022 21:09:53.811236  3959 layer_factory.hpp:77] Creating layer Eltwise1
I1022 21:09:53.811240  3959 net.cpp:84] Creating Layer Eltwise1
I1022 21:09:53.811244  3959 net.cpp:406] Eltwise1 <- Convolution1_penlu1_0_split_1
I1022 21:09:53.811246  3959 net.cpp:406] Eltwise1 <- Convolution3
I1022 21:09:53.811249  3959 net.cpp:380] Eltwise1 -> Eltwise1
I1022 21:09:53.811267  3959 net.cpp:122] Setting up Eltwise1
I1022 21:09:53.811270  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.811272  3959 net.cpp:137] Memory required for data: 364478560
I1022 21:09:53.811275  3959 layer_factory.hpp:77] Creating layer penlu3
I1022 21:09:53.811281  3959 net.cpp:84] Creating Layer penlu3
I1022 21:09:53.811283  3959 net.cpp:406] penlu3 <- Eltwise1
I1022 21:09:53.811286  3959 net.cpp:367] penlu3 -> Eltwise1 (in-place)
I1022 21:09:53.812536  3959 net.cpp:122] Setting up penlu3
I1022 21:09:53.812549  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.812551  3959 net.cpp:137] Memory required for data: 390168672
I1022 21:09:53.812557  3959 layer_factory.hpp:77] Creating layer Eltwise1_penlu3_0_split
I1022 21:09:53.812564  3959 net.cpp:84] Creating Layer Eltwise1_penlu3_0_split
I1022 21:09:53.812567  3959 net.cpp:406] Eltwise1_penlu3_0_split <- Eltwise1
I1022 21:09:53.812571  3959 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_0
I1022 21:09:53.812592  3959 net.cpp:380] Eltwise1_penlu3_0_split -> Eltwise1_penlu3_0_split_1
I1022 21:09:53.812620  3959 net.cpp:122] Setting up Eltwise1_penlu3_0_split
I1022 21:09:53.812624  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.812628  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.812629  3959 net.cpp:137] Memory required for data: 441548896
I1022 21:09:53.812631  3959 layer_factory.hpp:77] Creating layer Convolution4
I1022 21:09:53.812639  3959 net.cpp:84] Creating Layer Convolution4
I1022 21:09:53.812641  3959 net.cpp:406] Convolution4 <- Eltwise1_penlu3_0_split_0
I1022 21:09:53.812645  3959 net.cpp:380] Convolution4 -> Convolution4
I1022 21:09:53.813776  3959 net.cpp:122] Setting up Convolution4
I1022 21:09:53.813784  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.813787  3959 net.cpp:137] Memory required for data: 467239008
I1022 21:09:53.813791  3959 layer_factory.hpp:77] Creating layer BatchNorm4
I1022 21:09:53.813797  3959 net.cpp:84] Creating Layer BatchNorm4
I1022 21:09:53.813799  3959 net.cpp:406] BatchNorm4 <- Convolution4
I1022 21:09:53.813803  3959 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1022 21:09:53.814371  3959 net.cpp:122] Setting up BatchNorm4
I1022 21:09:53.814376  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.814378  3959 net.cpp:137] Memory required for data: 492929120
I1022 21:09:53.814388  3959 layer_factory.hpp:77] Creating layer Scale4
I1022 21:09:53.814393  3959 net.cpp:84] Creating Layer Scale4
I1022 21:09:53.814395  3959 net.cpp:406] Scale4 <- Convolution4
I1022 21:09:53.814399  3959 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1022 21:09:53.814430  3959 layer_factory.hpp:77] Creating layer Scale4
I1022 21:09:53.814570  3959 net.cpp:122] Setting up Scale4
I1022 21:09:53.814575  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.814577  3959 net.cpp:137] Memory required for data: 518619232
I1022 21:09:53.814580  3959 layer_factory.hpp:77] Creating layer penlu4
I1022 21:09:53.814586  3959 net.cpp:84] Creating Layer penlu4
I1022 21:09:53.814589  3959 net.cpp:406] penlu4 <- Convolution4
I1022 21:09:53.814591  3959 net.cpp:367] penlu4 -> Convolution4 (in-place)
I1022 21:09:53.815824  3959 net.cpp:122] Setting up penlu4
I1022 21:09:53.815836  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.815838  3959 net.cpp:137] Memory required for data: 544309344
I1022 21:09:53.815843  3959 layer_factory.hpp:77] Creating layer Convolution5
I1022 21:09:53.815851  3959 net.cpp:84] Creating Layer Convolution5
I1022 21:09:53.815855  3959 net.cpp:406] Convolution5 <- Convolution4
I1022 21:09:53.815860  3959 net.cpp:380] Convolution5 -> Convolution5
I1022 21:09:53.817293  3959 net.cpp:122] Setting up Convolution5
I1022 21:09:53.817303  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.817306  3959 net.cpp:137] Memory required for data: 569999456
I1022 21:09:53.817312  3959 layer_factory.hpp:77] Creating layer BatchNorm5
I1022 21:09:53.817317  3959 net.cpp:84] Creating Layer BatchNorm5
I1022 21:09:53.817318  3959 net.cpp:406] BatchNorm5 <- Convolution5
I1022 21:09:53.817322  3959 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1022 21:09:53.817505  3959 net.cpp:122] Setting up BatchNorm5
I1022 21:09:53.817509  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.817512  3959 net.cpp:137] Memory required for data: 595689568
I1022 21:09:53.817517  3959 layer_factory.hpp:77] Creating layer Scale5
I1022 21:09:53.817522  3959 net.cpp:84] Creating Layer Scale5
I1022 21:09:53.817524  3959 net.cpp:406] Scale5 <- Convolution5
I1022 21:09:53.817528  3959 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1022 21:09:53.817559  3959 layer_factory.hpp:77] Creating layer Scale5
I1022 21:09:53.818219  3959 net.cpp:122] Setting up Scale5
I1022 21:09:53.818228  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.818230  3959 net.cpp:137] Memory required for data: 621379680
I1022 21:09:53.818235  3959 layer_factory.hpp:77] Creating layer Eltwise2
I1022 21:09:53.818254  3959 net.cpp:84] Creating Layer Eltwise2
I1022 21:09:53.818259  3959 net.cpp:406] Eltwise2 <- Eltwise1_penlu3_0_split_1
I1022 21:09:53.818261  3959 net.cpp:406] Eltwise2 <- Convolution5
I1022 21:09:53.818264  3959 net.cpp:380] Eltwise2 -> Eltwise2
I1022 21:09:53.818284  3959 net.cpp:122] Setting up Eltwise2
I1022 21:09:53.818289  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.818290  3959 net.cpp:137] Memory required for data: 647069792
I1022 21:09:53.818292  3959 layer_factory.hpp:77] Creating layer penlu5
I1022 21:09:53.818298  3959 net.cpp:84] Creating Layer penlu5
I1022 21:09:53.818300  3959 net.cpp:406] penlu5 <- Eltwise2
I1022 21:09:53.818303  3959 net.cpp:367] penlu5 -> Eltwise2 (in-place)
I1022 21:09:53.819490  3959 net.cpp:122] Setting up penlu5
I1022 21:09:53.819500  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.819502  3959 net.cpp:137] Memory required for data: 672759904
I1022 21:09:53.819509  3959 layer_factory.hpp:77] Creating layer Eltwise2_penlu5_0_split
I1022 21:09:53.819514  3959 net.cpp:84] Creating Layer Eltwise2_penlu5_0_split
I1022 21:09:53.819516  3959 net.cpp:406] Eltwise2_penlu5_0_split <- Eltwise2
I1022 21:09:53.819519  3959 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_0
I1022 21:09:53.819524  3959 net.cpp:380] Eltwise2_penlu5_0_split -> Eltwise2_penlu5_0_split_1
I1022 21:09:53.819556  3959 net.cpp:122] Setting up Eltwise2_penlu5_0_split
I1022 21:09:53.819561  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.819564  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.819566  3959 net.cpp:137] Memory required for data: 724140128
I1022 21:09:53.819568  3959 layer_factory.hpp:77] Creating layer Convolution6
I1022 21:09:53.819576  3959 net.cpp:84] Creating Layer Convolution6
I1022 21:09:53.819578  3959 net.cpp:406] Convolution6 <- Eltwise2_penlu5_0_split_0
I1022 21:09:53.819582  3959 net.cpp:380] Convolution6 -> Convolution6
I1022 21:09:53.821102  3959 net.cpp:122] Setting up Convolution6
I1022 21:09:53.821111  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.821115  3959 net.cpp:137] Memory required for data: 749830240
I1022 21:09:53.821118  3959 layer_factory.hpp:77] Creating layer BatchNorm6
I1022 21:09:53.821123  3959 net.cpp:84] Creating Layer BatchNorm6
I1022 21:09:53.821126  3959 net.cpp:406] BatchNorm6 <- Convolution6
I1022 21:09:53.821130  3959 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1022 21:09:53.821310  3959 net.cpp:122] Setting up BatchNorm6
I1022 21:09:53.821315  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.821316  3959 net.cpp:137] Memory required for data: 775520352
I1022 21:09:53.821321  3959 layer_factory.hpp:77] Creating layer Scale6
I1022 21:09:53.821326  3959 net.cpp:84] Creating Layer Scale6
I1022 21:09:53.821328  3959 net.cpp:406] Scale6 <- Convolution6
I1022 21:09:53.821331  3959 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1022 21:09:53.821362  3959 layer_factory.hpp:77] Creating layer Scale6
I1022 21:09:53.821507  3959 net.cpp:122] Setting up Scale6
I1022 21:09:53.821511  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.821513  3959 net.cpp:137] Memory required for data: 801210464
I1022 21:09:53.821517  3959 layer_factory.hpp:77] Creating layer penlu6
I1022 21:09:53.821522  3959 net.cpp:84] Creating Layer penlu6
I1022 21:09:53.821524  3959 net.cpp:406] penlu6 <- Convolution6
I1022 21:09:53.821528  3959 net.cpp:367] penlu6 -> Convolution6 (in-place)
I1022 21:09:53.822769  3959 net.cpp:122] Setting up penlu6
I1022 21:09:53.822780  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.822783  3959 net.cpp:137] Memory required for data: 826900576
I1022 21:09:53.822788  3959 layer_factory.hpp:77] Creating layer Convolution7
I1022 21:09:53.822796  3959 net.cpp:84] Creating Layer Convolution7
I1022 21:09:53.822799  3959 net.cpp:406] Convolution7 <- Convolution6
I1022 21:09:53.822804  3959 net.cpp:380] Convolution7 -> Convolution7
I1022 21:09:53.824329  3959 net.cpp:122] Setting up Convolution7
I1022 21:09:53.824339  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.824342  3959 net.cpp:137] Memory required for data: 852590688
I1022 21:09:53.824347  3959 layer_factory.hpp:77] Creating layer BatchNorm7
I1022 21:09:53.824357  3959 net.cpp:84] Creating Layer BatchNorm7
I1022 21:09:53.824359  3959 net.cpp:406] BatchNorm7 <- Convolution7
I1022 21:09:53.824363  3959 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1022 21:09:53.824561  3959 net.cpp:122] Setting up BatchNorm7
I1022 21:09:53.824566  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.824568  3959 net.cpp:137] Memory required for data: 878280800
I1022 21:09:53.824580  3959 layer_factory.hpp:77] Creating layer Scale7
I1022 21:09:53.824586  3959 net.cpp:84] Creating Layer Scale7
I1022 21:09:53.824589  3959 net.cpp:406] Scale7 <- Convolution7
I1022 21:09:53.824592  3959 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1022 21:09:53.824628  3959 layer_factory.hpp:77] Creating layer Scale7
I1022 21:09:53.824796  3959 net.cpp:122] Setting up Scale7
I1022 21:09:53.824800  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.824802  3959 net.cpp:137] Memory required for data: 903970912
I1022 21:09:53.824806  3959 layer_factory.hpp:77] Creating layer Eltwise3
I1022 21:09:53.824811  3959 net.cpp:84] Creating Layer Eltwise3
I1022 21:09:53.824813  3959 net.cpp:406] Eltwise3 <- Eltwise2_penlu5_0_split_1
I1022 21:09:53.824816  3959 net.cpp:406] Eltwise3 <- Convolution7
I1022 21:09:53.824820  3959 net.cpp:380] Eltwise3 -> Eltwise3
I1022 21:09:53.824837  3959 net.cpp:122] Setting up Eltwise3
I1022 21:09:53.824841  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.824843  3959 net.cpp:137] Memory required for data: 929661024
I1022 21:09:53.824846  3959 layer_factory.hpp:77] Creating layer penlu7
I1022 21:09:53.824851  3959 net.cpp:84] Creating Layer penlu7
I1022 21:09:53.824853  3959 net.cpp:406] penlu7 <- Eltwise3
I1022 21:09:53.824856  3959 net.cpp:367] penlu7 -> Eltwise3 (in-place)
I1022 21:09:53.826102  3959 net.cpp:122] Setting up penlu7
I1022 21:09:53.826113  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.826115  3959 net.cpp:137] Memory required for data: 955351136
I1022 21:09:53.826122  3959 layer_factory.hpp:77] Creating layer Eltwise3_penlu7_0_split
I1022 21:09:53.826125  3959 net.cpp:84] Creating Layer Eltwise3_penlu7_0_split
I1022 21:09:53.826128  3959 net.cpp:406] Eltwise3_penlu7_0_split <- Eltwise3
I1022 21:09:53.826133  3959 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_0
I1022 21:09:53.826136  3959 net.cpp:380] Eltwise3_penlu7_0_split -> Eltwise3_penlu7_0_split_1
I1022 21:09:53.826164  3959 net.cpp:122] Setting up Eltwise3_penlu7_0_split
I1022 21:09:53.826169  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.826171  3959 net.cpp:129] Top shape: 8 16 224 224 (6422528)
I1022 21:09:53.826174  3959 net.cpp:137] Memory required for data: 1006731360
I1022 21:09:53.826175  3959 layer_factory.hpp:77] Creating layer Convolution8
I1022 21:09:53.826182  3959 net.cpp:84] Creating Layer Convolution8
I1022 21:09:53.826185  3959 net.cpp:406] Convolution8 <- Eltwise3_penlu7_0_split_0
I1022 21:09:53.826189  3959 net.cpp:380] Convolution8 -> Convolution8
I1022 21:09:53.827316  3959 net.cpp:122] Setting up Convolution8
I1022 21:09:53.827325  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.827327  3959 net.cpp:137] Memory required for data: 1019576416
I1022 21:09:53.827332  3959 layer_factory.hpp:77] Creating layer BatchNorm8
I1022 21:09:53.827338  3959 net.cpp:84] Creating Layer BatchNorm8
I1022 21:09:53.827342  3959 net.cpp:406] BatchNorm8 <- Convolution8
I1022 21:09:53.827344  3959 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1022 21:09:53.827505  3959 net.cpp:122] Setting up BatchNorm8
I1022 21:09:53.827510  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.827512  3959 net.cpp:137] Memory required for data: 1032421472
I1022 21:09:53.827517  3959 layer_factory.hpp:77] Creating layer Scale8
I1022 21:09:53.827531  3959 net.cpp:84] Creating Layer Scale8
I1022 21:09:53.827534  3959 net.cpp:406] Scale8 <- Convolution8
I1022 21:09:53.827538  3959 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1022 21:09:53.827572  3959 layer_factory.hpp:77] Creating layer Scale8
I1022 21:09:53.827713  3959 net.cpp:122] Setting up Scale8
I1022 21:09:53.827718  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.827720  3959 net.cpp:137] Memory required for data: 1045266528
I1022 21:09:53.827724  3959 layer_factory.hpp:77] Creating layer Convolution9
I1022 21:09:53.827731  3959 net.cpp:84] Creating Layer Convolution9
I1022 21:09:53.827734  3959 net.cpp:406] Convolution9 <- Eltwise3_penlu7_0_split_1
I1022 21:09:53.827739  3959 net.cpp:380] Convolution9 -> Convolution9
I1022 21:09:53.828874  3959 net.cpp:122] Setting up Convolution9
I1022 21:09:53.828894  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.828896  3959 net.cpp:137] Memory required for data: 1058111584
I1022 21:09:53.828902  3959 layer_factory.hpp:77] Creating layer BatchNorm9
I1022 21:09:53.828907  3959 net.cpp:84] Creating Layer BatchNorm9
I1022 21:09:53.828909  3959 net.cpp:406] BatchNorm9 <- Convolution9
I1022 21:09:53.828914  3959 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1022 21:09:53.829082  3959 net.cpp:122] Setting up BatchNorm9
I1022 21:09:53.829087  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.829088  3959 net.cpp:137] Memory required for data: 1070956640
I1022 21:09:53.829093  3959 layer_factory.hpp:77] Creating layer Scale9
I1022 21:09:53.829098  3959 net.cpp:84] Creating Layer Scale9
I1022 21:09:53.829100  3959 net.cpp:406] Scale9 <- Convolution9
I1022 21:09:53.829103  3959 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1022 21:09:53.829133  3959 layer_factory.hpp:77] Creating layer Scale9
I1022 21:09:53.829254  3959 net.cpp:122] Setting up Scale9
I1022 21:09:53.829258  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.829260  3959 net.cpp:137] Memory required for data: 1083801696
I1022 21:09:53.829264  3959 layer_factory.hpp:77] Creating layer penlu8
I1022 21:09:53.829269  3959 net.cpp:84] Creating Layer penlu8
I1022 21:09:53.829272  3959 net.cpp:406] penlu8 <- Convolution9
I1022 21:09:53.829277  3959 net.cpp:367] penlu8 -> Convolution9 (in-place)
I1022 21:09:53.830160  3959 net.cpp:122] Setting up penlu8
I1022 21:09:53.830168  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.830171  3959 net.cpp:137] Memory required for data: 1096646752
I1022 21:09:53.830176  3959 layer_factory.hpp:77] Creating layer Convolution10
I1022 21:09:53.830183  3959 net.cpp:84] Creating Layer Convolution10
I1022 21:09:53.830186  3959 net.cpp:406] Convolution10 <- Convolution9
I1022 21:09:53.830190  3959 net.cpp:380] Convolution10 -> Convolution10
I1022 21:09:53.832296  3959 net.cpp:122] Setting up Convolution10
I1022 21:09:53.832305  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.832309  3959 net.cpp:137] Memory required for data: 1109491808
I1022 21:09:53.832314  3959 layer_factory.hpp:77] Creating layer BatchNorm10
I1022 21:09:53.832319  3959 net.cpp:84] Creating Layer BatchNorm10
I1022 21:09:53.832322  3959 net.cpp:406] BatchNorm10 <- Convolution10
I1022 21:09:53.832325  3959 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1022 21:09:53.832490  3959 net.cpp:122] Setting up BatchNorm10
I1022 21:09:53.832495  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.832497  3959 net.cpp:137] Memory required for data: 1122336864
I1022 21:09:53.832502  3959 layer_factory.hpp:77] Creating layer Scale10
I1022 21:09:53.832507  3959 net.cpp:84] Creating Layer Scale10
I1022 21:09:53.832509  3959 net.cpp:406] Scale10 <- Convolution10
I1022 21:09:53.832512  3959 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1022 21:09:53.832545  3959 layer_factory.hpp:77] Creating layer Scale10
I1022 21:09:53.832638  3959 net.cpp:122] Setting up Scale10
I1022 21:09:53.832643  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.832656  3959 net.cpp:137] Memory required for data: 1135181920
I1022 21:09:53.832661  3959 layer_factory.hpp:77] Creating layer Eltwise4
I1022 21:09:53.832666  3959 net.cpp:84] Creating Layer Eltwise4
I1022 21:09:53.832669  3959 net.cpp:406] Eltwise4 <- Convolution8
I1022 21:09:53.832671  3959 net.cpp:406] Eltwise4 <- Convolution10
I1022 21:09:53.832676  3959 net.cpp:380] Eltwise4 -> Eltwise4
I1022 21:09:53.832696  3959 net.cpp:122] Setting up Eltwise4
I1022 21:09:53.832700  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.832702  3959 net.cpp:137] Memory required for data: 1148026976
I1022 21:09:53.832705  3959 layer_factory.hpp:77] Creating layer penlu9
I1022 21:09:53.832710  3959 net.cpp:84] Creating Layer penlu9
I1022 21:09:53.832711  3959 net.cpp:406] penlu9 <- Eltwise4
I1022 21:09:53.832717  3959 net.cpp:367] penlu9 -> Eltwise4 (in-place)
I1022 21:09:53.833547  3959 net.cpp:122] Setting up penlu9
I1022 21:09:53.833555  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.833557  3959 net.cpp:137] Memory required for data: 1160872032
I1022 21:09:53.833562  3959 layer_factory.hpp:77] Creating layer Eltwise4_penlu9_0_split
I1022 21:09:53.833567  3959 net.cpp:84] Creating Layer Eltwise4_penlu9_0_split
I1022 21:09:53.833570  3959 net.cpp:406] Eltwise4_penlu9_0_split <- Eltwise4
I1022 21:09:53.833573  3959 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_0
I1022 21:09:53.833578  3959 net.cpp:380] Eltwise4_penlu9_0_split -> Eltwise4_penlu9_0_split_1
I1022 21:09:53.833606  3959 net.cpp:122] Setting up Eltwise4_penlu9_0_split
I1022 21:09:53.833611  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.833613  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.833616  3959 net.cpp:137] Memory required for data: 1186562144
I1022 21:09:53.833617  3959 layer_factory.hpp:77] Creating layer Convolution11
I1022 21:09:53.833623  3959 net.cpp:84] Creating Layer Convolution11
I1022 21:09:53.833626  3959 net.cpp:406] Convolution11 <- Eltwise4_penlu9_0_split_0
I1022 21:09:53.833631  3959 net.cpp:380] Convolution11 -> Convolution11
I1022 21:09:53.834810  3959 net.cpp:122] Setting up Convolution11
I1022 21:09:53.834820  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.834821  3959 net.cpp:137] Memory required for data: 1199407200
I1022 21:09:53.834826  3959 layer_factory.hpp:77] Creating layer BatchNorm11
I1022 21:09:53.834831  3959 net.cpp:84] Creating Layer BatchNorm11
I1022 21:09:53.834834  3959 net.cpp:406] BatchNorm11 <- Convolution11
I1022 21:09:53.834838  3959 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1022 21:09:53.834997  3959 net.cpp:122] Setting up BatchNorm11
I1022 21:09:53.835002  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.835005  3959 net.cpp:137] Memory required for data: 1212252256
I1022 21:09:53.835009  3959 layer_factory.hpp:77] Creating layer Scale11
I1022 21:09:53.835012  3959 net.cpp:84] Creating Layer Scale11
I1022 21:09:53.835016  3959 net.cpp:406] Scale11 <- Convolution11
I1022 21:09:53.835018  3959 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1022 21:09:53.835048  3959 layer_factory.hpp:77] Creating layer Scale11
I1022 21:09:53.835142  3959 net.cpp:122] Setting up Scale11
I1022 21:09:53.835147  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.835150  3959 net.cpp:137] Memory required for data: 1225097312
I1022 21:09:53.835153  3959 layer_factory.hpp:77] Creating layer penlu10
I1022 21:09:53.835158  3959 net.cpp:84] Creating Layer penlu10
I1022 21:09:53.835161  3959 net.cpp:406] penlu10 <- Convolution11
I1022 21:09:53.835165  3959 net.cpp:367] penlu10 -> Convolution11 (in-place)
I1022 21:09:53.836035  3959 net.cpp:122] Setting up penlu10
I1022 21:09:53.836045  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.836046  3959 net.cpp:137] Memory required for data: 1237942368
I1022 21:09:53.836051  3959 layer_factory.hpp:77] Creating layer Convolution12
I1022 21:09:53.836058  3959 net.cpp:84] Creating Layer Convolution12
I1022 21:09:53.836069  3959 net.cpp:406] Convolution12 <- Convolution11
I1022 21:09:53.836076  3959 net.cpp:380] Convolution12 -> Convolution12
I1022 21:09:53.836871  3959 net.cpp:122] Setting up Convolution12
I1022 21:09:53.836879  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.836881  3959 net.cpp:137] Memory required for data: 1250787424
I1022 21:09:53.836885  3959 layer_factory.hpp:77] Creating layer BatchNorm12
I1022 21:09:53.836890  3959 net.cpp:84] Creating Layer BatchNorm12
I1022 21:09:53.836894  3959 net.cpp:406] BatchNorm12 <- Convolution12
I1022 21:09:53.836896  3959 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1022 21:09:53.837057  3959 net.cpp:122] Setting up BatchNorm12
I1022 21:09:53.837061  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.837064  3959 net.cpp:137] Memory required for data: 1263632480
I1022 21:09:53.837069  3959 layer_factory.hpp:77] Creating layer Scale12
I1022 21:09:53.837072  3959 net.cpp:84] Creating Layer Scale12
I1022 21:09:53.837075  3959 net.cpp:406] Scale12 <- Convolution12
I1022 21:09:53.837080  3959 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1022 21:09:53.837110  3959 layer_factory.hpp:77] Creating layer Scale12
I1022 21:09:53.837208  3959 net.cpp:122] Setting up Scale12
I1022 21:09:53.837213  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.837215  3959 net.cpp:137] Memory required for data: 1276477536
I1022 21:09:53.837219  3959 layer_factory.hpp:77] Creating layer Eltwise5
I1022 21:09:53.837224  3959 net.cpp:84] Creating Layer Eltwise5
I1022 21:09:53.837226  3959 net.cpp:406] Eltwise5 <- Eltwise4_penlu9_0_split_1
I1022 21:09:53.837229  3959 net.cpp:406] Eltwise5 <- Convolution12
I1022 21:09:53.837232  3959 net.cpp:380] Eltwise5 -> Eltwise5
I1022 21:09:53.837251  3959 net.cpp:122] Setting up Eltwise5
I1022 21:09:53.837255  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.837256  3959 net.cpp:137] Memory required for data: 1289322592
I1022 21:09:53.837258  3959 layer_factory.hpp:77] Creating layer penlu11
I1022 21:09:53.837265  3959 net.cpp:84] Creating Layer penlu11
I1022 21:09:53.837266  3959 net.cpp:406] penlu11 <- Eltwise5
I1022 21:09:53.837270  3959 net.cpp:367] penlu11 -> Eltwise5 (in-place)
I1022 21:09:53.838124  3959 net.cpp:122] Setting up penlu11
I1022 21:09:53.838131  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.838135  3959 net.cpp:137] Memory required for data: 1302167648
I1022 21:09:53.838140  3959 layer_factory.hpp:77] Creating layer Eltwise5_penlu11_0_split
I1022 21:09:53.838143  3959 net.cpp:84] Creating Layer Eltwise5_penlu11_0_split
I1022 21:09:53.838145  3959 net.cpp:406] Eltwise5_penlu11_0_split <- Eltwise5
I1022 21:09:53.838150  3959 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_0
I1022 21:09:53.838153  3959 net.cpp:380] Eltwise5_penlu11_0_split -> Eltwise5_penlu11_0_split_1
I1022 21:09:53.838186  3959 net.cpp:122] Setting up Eltwise5_penlu11_0_split
I1022 21:09:53.838189  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.838193  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.838196  3959 net.cpp:137] Memory required for data: 1327857760
I1022 21:09:53.838197  3959 layer_factory.hpp:77] Creating layer Convolution13
I1022 21:09:53.838204  3959 net.cpp:84] Creating Layer Convolution13
I1022 21:09:53.838207  3959 net.cpp:406] Convolution13 <- Eltwise5_penlu11_0_split_0
I1022 21:09:53.838212  3959 net.cpp:380] Convolution13 -> Convolution13
I1022 21:09:53.839380  3959 net.cpp:122] Setting up Convolution13
I1022 21:09:53.839387  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.839390  3959 net.cpp:137] Memory required for data: 1340702816
I1022 21:09:53.839395  3959 layer_factory.hpp:77] Creating layer BatchNorm13
I1022 21:09:53.839401  3959 net.cpp:84] Creating Layer BatchNorm13
I1022 21:09:53.839403  3959 net.cpp:406] BatchNorm13 <- Convolution13
I1022 21:09:53.839407  3959 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1022 21:09:53.839568  3959 net.cpp:122] Setting up BatchNorm13
I1022 21:09:53.839581  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.839583  3959 net.cpp:137] Memory required for data: 1353547872
I1022 21:09:53.839588  3959 layer_factory.hpp:77] Creating layer Scale13
I1022 21:09:53.839593  3959 net.cpp:84] Creating Layer Scale13
I1022 21:09:53.839596  3959 net.cpp:406] Scale13 <- Convolution13
I1022 21:09:53.839599  3959 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1022 21:09:53.839633  3959 layer_factory.hpp:77] Creating layer Scale13
I1022 21:09:53.839735  3959 net.cpp:122] Setting up Scale13
I1022 21:09:53.839740  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.839741  3959 net.cpp:137] Memory required for data: 1366392928
I1022 21:09:53.839745  3959 layer_factory.hpp:77] Creating layer penlu12
I1022 21:09:53.839751  3959 net.cpp:84] Creating Layer penlu12
I1022 21:09:53.839753  3959 net.cpp:406] penlu12 <- Convolution13
I1022 21:09:53.839757  3959 net.cpp:367] penlu12 -> Convolution13 (in-place)
I1022 21:09:53.840642  3959 net.cpp:122] Setting up penlu12
I1022 21:09:53.840651  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.840653  3959 net.cpp:137] Memory required for data: 1379237984
I1022 21:09:53.840658  3959 layer_factory.hpp:77] Creating layer Convolution14
I1022 21:09:53.840669  3959 net.cpp:84] Creating Layer Convolution14
I1022 21:09:53.840672  3959 net.cpp:406] Convolution14 <- Convolution13
I1022 21:09:53.840677  3959 net.cpp:380] Convolution14 -> Convolution14
I1022 21:09:53.841858  3959 net.cpp:122] Setting up Convolution14
I1022 21:09:53.841867  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.841871  3959 net.cpp:137] Memory required for data: 1392083040
I1022 21:09:53.841887  3959 layer_factory.hpp:77] Creating layer BatchNorm14
I1022 21:09:53.841892  3959 net.cpp:84] Creating Layer BatchNorm14
I1022 21:09:53.841894  3959 net.cpp:406] BatchNorm14 <- Convolution14
I1022 21:09:53.841899  3959 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1022 21:09:53.842063  3959 net.cpp:122] Setting up BatchNorm14
I1022 21:09:53.842067  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.842070  3959 net.cpp:137] Memory required for data: 1404928096
I1022 21:09:53.842075  3959 layer_factory.hpp:77] Creating layer Scale14
I1022 21:09:53.842079  3959 net.cpp:84] Creating Layer Scale14
I1022 21:09:53.842082  3959 net.cpp:406] Scale14 <- Convolution14
I1022 21:09:53.842085  3959 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1022 21:09:53.842116  3959 layer_factory.hpp:77] Creating layer Scale14
I1022 21:09:53.842216  3959 net.cpp:122] Setting up Scale14
I1022 21:09:53.842221  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.842223  3959 net.cpp:137] Memory required for data: 1417773152
I1022 21:09:53.842226  3959 layer_factory.hpp:77] Creating layer Eltwise6
I1022 21:09:53.842231  3959 net.cpp:84] Creating Layer Eltwise6
I1022 21:09:53.842234  3959 net.cpp:406] Eltwise6 <- Eltwise5_penlu11_0_split_1
I1022 21:09:53.842237  3959 net.cpp:406] Eltwise6 <- Convolution14
I1022 21:09:53.842241  3959 net.cpp:380] Eltwise6 -> Eltwise6
I1022 21:09:53.842259  3959 net.cpp:122] Setting up Eltwise6
I1022 21:09:53.842262  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.842264  3959 net.cpp:137] Memory required for data: 1430618208
I1022 21:09:53.842267  3959 layer_factory.hpp:77] Creating layer penlu13
I1022 21:09:53.842273  3959 net.cpp:84] Creating Layer penlu13
I1022 21:09:53.842275  3959 net.cpp:406] penlu13 <- Eltwise6
I1022 21:09:53.842279  3959 net.cpp:367] penlu13 -> Eltwise6 (in-place)
I1022 21:09:53.843127  3959 net.cpp:122] Setting up penlu13
I1022 21:09:53.843135  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.843137  3959 net.cpp:137] Memory required for data: 1443463264
I1022 21:09:53.843142  3959 layer_factory.hpp:77] Creating layer Eltwise6_penlu13_0_split
I1022 21:09:53.843148  3959 net.cpp:84] Creating Layer Eltwise6_penlu13_0_split
I1022 21:09:53.843152  3959 net.cpp:406] Eltwise6_penlu13_0_split <- Eltwise6
I1022 21:09:53.843165  3959 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_0
I1022 21:09:53.843170  3959 net.cpp:380] Eltwise6_penlu13_0_split -> Eltwise6_penlu13_0_split_1
I1022 21:09:53.843201  3959 net.cpp:122] Setting up Eltwise6_penlu13_0_split
I1022 21:09:53.843205  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.843209  3959 net.cpp:129] Top shape: 8 32 112 112 (3211264)
I1022 21:09:53.843210  3959 net.cpp:137] Memory required for data: 1469153376
I1022 21:09:53.843212  3959 layer_factory.hpp:77] Creating layer Convolution15
I1022 21:09:53.843219  3959 net.cpp:84] Creating Layer Convolution15
I1022 21:09:53.843221  3959 net.cpp:406] Convolution15 <- Eltwise6_penlu13_0_split_0
I1022 21:09:53.843225  3959 net.cpp:380] Convolution15 -> Convolution15
I1022 21:09:53.844274  3959 net.cpp:122] Setting up Convolution15
I1022 21:09:53.844283  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.844285  3959 net.cpp:137] Memory required for data: 1475575904
I1022 21:09:53.844290  3959 layer_factory.hpp:77] Creating layer BatchNorm15
I1022 21:09:53.844295  3959 net.cpp:84] Creating Layer BatchNorm15
I1022 21:09:53.844298  3959 net.cpp:406] BatchNorm15 <- Convolution15
I1022 21:09:53.844302  3959 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1022 21:09:53.844465  3959 net.cpp:122] Setting up BatchNorm15
I1022 21:09:53.844470  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.844471  3959 net.cpp:137] Memory required for data: 1481998432
I1022 21:09:53.844476  3959 layer_factory.hpp:77] Creating layer Scale15
I1022 21:09:53.844480  3959 net.cpp:84] Creating Layer Scale15
I1022 21:09:53.844482  3959 net.cpp:406] Scale15 <- Convolution15
I1022 21:09:53.844485  3959 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1022 21:09:53.844516  3959 layer_factory.hpp:77] Creating layer Scale15
I1022 21:09:53.844612  3959 net.cpp:122] Setting up Scale15
I1022 21:09:53.844616  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.844619  3959 net.cpp:137] Memory required for data: 1488420960
I1022 21:09:53.844622  3959 layer_factory.hpp:77] Creating layer Convolution16
I1022 21:09:53.844630  3959 net.cpp:84] Creating Layer Convolution16
I1022 21:09:53.844632  3959 net.cpp:406] Convolution16 <- Eltwise6_penlu13_0_split_1
I1022 21:09:53.844637  3959 net.cpp:380] Convolution16 -> Convolution16
I1022 21:09:53.846012  3959 net.cpp:122] Setting up Convolution16
I1022 21:09:53.846021  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.846024  3959 net.cpp:137] Memory required for data: 1494843488
I1022 21:09:53.846029  3959 layer_factory.hpp:77] Creating layer BatchNorm16
I1022 21:09:53.846032  3959 net.cpp:84] Creating Layer BatchNorm16
I1022 21:09:53.846035  3959 net.cpp:406] BatchNorm16 <- Convolution16
I1022 21:09:53.846040  3959 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1022 21:09:53.846199  3959 net.cpp:122] Setting up BatchNorm16
I1022 21:09:53.846204  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.846206  3959 net.cpp:137] Memory required for data: 1501266016
I1022 21:09:53.846210  3959 layer_factory.hpp:77] Creating layer Scale16
I1022 21:09:53.846215  3959 net.cpp:84] Creating Layer Scale16
I1022 21:09:53.846217  3959 net.cpp:406] Scale16 <- Convolution16
I1022 21:09:53.846221  3959 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1022 21:09:53.846251  3959 layer_factory.hpp:77] Creating layer Scale16
I1022 21:09:53.846344  3959 net.cpp:122] Setting up Scale16
I1022 21:09:53.846349  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.846351  3959 net.cpp:137] Memory required for data: 1507688544
I1022 21:09:53.846355  3959 layer_factory.hpp:77] Creating layer penlu14
I1022 21:09:53.846360  3959 net.cpp:84] Creating Layer penlu14
I1022 21:09:53.846364  3959 net.cpp:406] penlu14 <- Convolution16
I1022 21:09:53.846367  3959 net.cpp:367] penlu14 -> Convolution16 (in-place)
I1022 21:09:53.847129  3959 net.cpp:122] Setting up penlu14
I1022 21:09:53.847138  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.847148  3959 net.cpp:137] Memory required for data: 1514111072
I1022 21:09:53.847153  3959 layer_factory.hpp:77] Creating layer Convolution17
I1022 21:09:53.847162  3959 net.cpp:84] Creating Layer Convolution17
I1022 21:09:53.847164  3959 net.cpp:406] Convolution17 <- Convolution16
I1022 21:09:53.847168  3959 net.cpp:380] Convolution17 -> Convolution17
I1022 21:09:53.849565  3959 net.cpp:122] Setting up Convolution17
I1022 21:09:53.849575  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.849577  3959 net.cpp:137] Memory required for data: 1520533600
I1022 21:09:53.849582  3959 layer_factory.hpp:77] Creating layer BatchNorm17
I1022 21:09:53.849587  3959 net.cpp:84] Creating Layer BatchNorm17
I1022 21:09:53.849591  3959 net.cpp:406] BatchNorm17 <- Convolution17
I1022 21:09:53.849594  3959 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1022 21:09:53.849756  3959 net.cpp:122] Setting up BatchNorm17
I1022 21:09:53.849761  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.849763  3959 net.cpp:137] Memory required for data: 1526956128
I1022 21:09:53.849768  3959 layer_factory.hpp:77] Creating layer Scale17
I1022 21:09:53.849773  3959 net.cpp:84] Creating Layer Scale17
I1022 21:09:53.849776  3959 net.cpp:406] Scale17 <- Convolution17
I1022 21:09:53.849778  3959 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1022 21:09:53.849810  3959 layer_factory.hpp:77] Creating layer Scale17
I1022 21:09:53.849910  3959 net.cpp:122] Setting up Scale17
I1022 21:09:53.849913  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.849915  3959 net.cpp:137] Memory required for data: 1533378656
I1022 21:09:53.849920  3959 layer_factory.hpp:77] Creating layer Eltwise7
I1022 21:09:53.849925  3959 net.cpp:84] Creating Layer Eltwise7
I1022 21:09:53.849927  3959 net.cpp:406] Eltwise7 <- Convolution15
I1022 21:09:53.849931  3959 net.cpp:406] Eltwise7 <- Convolution17
I1022 21:09:53.849933  3959 net.cpp:380] Eltwise7 -> Eltwise7
I1022 21:09:53.849952  3959 net.cpp:122] Setting up Eltwise7
I1022 21:09:53.849956  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.849958  3959 net.cpp:137] Memory required for data: 1539801184
I1022 21:09:53.849961  3959 layer_factory.hpp:77] Creating layer penlu15
I1022 21:09:53.849967  3959 net.cpp:84] Creating Layer penlu15
I1022 21:09:53.849968  3959 net.cpp:406] penlu15 <- Eltwise7
I1022 21:09:53.849972  3959 net.cpp:367] penlu15 -> Eltwise7 (in-place)
I1022 21:09:53.850213  3959 net.cpp:122] Setting up penlu15
I1022 21:09:53.850217  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.850219  3959 net.cpp:137] Memory required for data: 1546223712
I1022 21:09:53.850224  3959 layer_factory.hpp:77] Creating layer Eltwise7_penlu15_0_split
I1022 21:09:53.850229  3959 net.cpp:84] Creating Layer Eltwise7_penlu15_0_split
I1022 21:09:53.850230  3959 net.cpp:406] Eltwise7_penlu15_0_split <- Eltwise7
I1022 21:09:53.850234  3959 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_0
I1022 21:09:53.850237  3959 net.cpp:380] Eltwise7_penlu15_0_split -> Eltwise7_penlu15_0_split_1
I1022 21:09:53.850266  3959 net.cpp:122] Setting up Eltwise7_penlu15_0_split
I1022 21:09:53.850270  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.850273  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.850275  3959 net.cpp:137] Memory required for data: 1559068768
I1022 21:09:53.850277  3959 layer_factory.hpp:77] Creating layer Convolution18
I1022 21:09:53.850283  3959 net.cpp:84] Creating Layer Convolution18
I1022 21:09:53.850286  3959 net.cpp:406] Convolution18 <- Eltwise7_penlu15_0_split_0
I1022 21:09:53.850291  3959 net.cpp:380] Convolution18 -> Convolution18
I1022 21:09:53.852841  3959 net.cpp:122] Setting up Convolution18
I1022 21:09:53.855945  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.855949  3959 net.cpp:137] Memory required for data: 1565491296
I1022 21:09:53.855955  3959 layer_factory.hpp:77] Creating layer BatchNorm18
I1022 21:09:53.855974  3959 net.cpp:84] Creating Layer BatchNorm18
I1022 21:09:53.855984  3959 net.cpp:406] BatchNorm18 <- Convolution18
I1022 21:09:53.855990  3959 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1022 21:09:53.856171  3959 net.cpp:122] Setting up BatchNorm18
I1022 21:09:53.856178  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.856179  3959 net.cpp:137] Memory required for data: 1571913824
I1022 21:09:53.856185  3959 layer_factory.hpp:77] Creating layer Scale18
I1022 21:09:53.856190  3959 net.cpp:84] Creating Layer Scale18
I1022 21:09:53.856192  3959 net.cpp:406] Scale18 <- Convolution18
I1022 21:09:53.856196  3959 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1022 21:09:53.856230  3959 layer_factory.hpp:77] Creating layer Scale18
I1022 21:09:53.856338  3959 net.cpp:122] Setting up Scale18
I1022 21:09:53.856343  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.856344  3959 net.cpp:137] Memory required for data: 1578336352
I1022 21:09:53.856349  3959 layer_factory.hpp:77] Creating layer penlu16
I1022 21:09:53.856354  3959 net.cpp:84] Creating Layer penlu16
I1022 21:09:53.856357  3959 net.cpp:406] penlu16 <- Convolution18
I1022 21:09:53.856361  3959 net.cpp:367] penlu16 -> Convolution18 (in-place)
I1022 21:09:53.857164  3959 net.cpp:122] Setting up penlu16
I1022 21:09:53.857172  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.857175  3959 net.cpp:137] Memory required for data: 1584758880
I1022 21:09:53.857179  3959 layer_factory.hpp:77] Creating layer Convolution19
I1022 21:09:53.857187  3959 net.cpp:84] Creating Layer Convolution19
I1022 21:09:53.857188  3959 net.cpp:406] Convolution19 <- Convolution18
I1022 21:09:53.857194  3959 net.cpp:380] Convolution19 -> Convolution19
I1022 21:09:53.859268  3959 net.cpp:122] Setting up Convolution19
I1022 21:09:53.859278  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.859282  3959 net.cpp:137] Memory required for data: 1591181408
I1022 21:09:53.859287  3959 layer_factory.hpp:77] Creating layer BatchNorm19
I1022 21:09:53.859292  3959 net.cpp:84] Creating Layer BatchNorm19
I1022 21:09:53.859293  3959 net.cpp:406] BatchNorm19 <- Convolution19
I1022 21:09:53.859298  3959 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1022 21:09:53.859465  3959 net.cpp:122] Setting up BatchNorm19
I1022 21:09:53.859470  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.859472  3959 net.cpp:137] Memory required for data: 1597603936
I1022 21:09:53.859477  3959 layer_factory.hpp:77] Creating layer Scale19
I1022 21:09:53.859482  3959 net.cpp:84] Creating Layer Scale19
I1022 21:09:53.859484  3959 net.cpp:406] Scale19 <- Convolution19
I1022 21:09:53.859488  3959 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1022 21:09:53.859520  3959 layer_factory.hpp:77] Creating layer Scale19
I1022 21:09:53.859629  3959 net.cpp:122] Setting up Scale19
I1022 21:09:53.859634  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.859637  3959 net.cpp:137] Memory required for data: 1604026464
I1022 21:09:53.859642  3959 layer_factory.hpp:77] Creating layer Eltwise8
I1022 21:09:53.859645  3959 net.cpp:84] Creating Layer Eltwise8
I1022 21:09:53.859648  3959 net.cpp:406] Eltwise8 <- Eltwise7_penlu15_0_split_1
I1022 21:09:53.859650  3959 net.cpp:406] Eltwise8 <- Convolution19
I1022 21:09:53.859654  3959 net.cpp:380] Eltwise8 -> Eltwise8
I1022 21:09:53.859674  3959 net.cpp:122] Setting up Eltwise8
I1022 21:09:53.859679  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.859681  3959 net.cpp:137] Memory required for data: 1610448992
I1022 21:09:53.859683  3959 layer_factory.hpp:77] Creating layer penlu17
I1022 21:09:53.859688  3959 net.cpp:84] Creating Layer penlu17
I1022 21:09:53.859691  3959 net.cpp:406] penlu17 <- Eltwise8
I1022 21:09:53.859695  3959 net.cpp:367] penlu17 -> Eltwise8 (in-place)
I1022 21:09:53.859947  3959 net.cpp:122] Setting up penlu17
I1022 21:09:53.859952  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.859954  3959 net.cpp:137] Memory required for data: 1616871520
I1022 21:09:53.859966  3959 layer_factory.hpp:77] Creating layer Eltwise8_penlu17_0_split
I1022 21:09:53.859978  3959 net.cpp:84] Creating Layer Eltwise8_penlu17_0_split
I1022 21:09:53.859982  3959 net.cpp:406] Eltwise8_penlu17_0_split <- Eltwise8
I1022 21:09:53.859984  3959 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_0
I1022 21:09:53.859998  3959 net.cpp:380] Eltwise8_penlu17_0_split -> Eltwise8_penlu17_0_split_1
I1022 21:09:53.860028  3959 net.cpp:122] Setting up Eltwise8_penlu17_0_split
I1022 21:09:53.860033  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.860036  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.860038  3959 net.cpp:137] Memory required for data: 1629716576
I1022 21:09:53.860040  3959 layer_factory.hpp:77] Creating layer Convolution20
I1022 21:09:53.860046  3959 net.cpp:84] Creating Layer Convolution20
I1022 21:09:53.860049  3959 net.cpp:406] Convolution20 <- Eltwise8_penlu17_0_split_0
I1022 21:09:53.860054  3959 net.cpp:380] Convolution20 -> Convolution20
I1022 21:09:53.862241  3959 net.cpp:122] Setting up Convolution20
I1022 21:09:53.862248  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.862251  3959 net.cpp:137] Memory required for data: 1636139104
I1022 21:09:53.862257  3959 layer_factory.hpp:77] Creating layer BatchNorm20
I1022 21:09:53.862262  3959 net.cpp:84] Creating Layer BatchNorm20
I1022 21:09:53.862264  3959 net.cpp:406] BatchNorm20 <- Convolution20
I1022 21:09:53.862268  3959 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1022 21:09:53.862439  3959 net.cpp:122] Setting up BatchNorm20
I1022 21:09:53.862444  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.862447  3959 net.cpp:137] Memory required for data: 1642561632
I1022 21:09:53.862452  3959 layer_factory.hpp:77] Creating layer Scale20
I1022 21:09:53.862457  3959 net.cpp:84] Creating Layer Scale20
I1022 21:09:53.862458  3959 net.cpp:406] Scale20 <- Convolution20
I1022 21:09:53.862462  3959 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1022 21:09:53.862494  3959 layer_factory.hpp:77] Creating layer Scale20
I1022 21:09:53.862597  3959 net.cpp:122] Setting up Scale20
I1022 21:09:53.862602  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.862604  3959 net.cpp:137] Memory required for data: 1648984160
I1022 21:09:53.862608  3959 layer_factory.hpp:77] Creating layer penlu18
I1022 21:09:53.862614  3959 net.cpp:84] Creating Layer penlu18
I1022 21:09:53.862617  3959 net.cpp:406] penlu18 <- Convolution20
I1022 21:09:53.862620  3959 net.cpp:367] penlu18 -> Convolution20 (in-place)
I1022 21:09:53.863400  3959 net.cpp:122] Setting up penlu18
I1022 21:09:53.863409  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.863411  3959 net.cpp:137] Memory required for data: 1655406688
I1022 21:09:53.863416  3959 layer_factory.hpp:77] Creating layer Convolution21
I1022 21:09:53.863423  3959 net.cpp:84] Creating Layer Convolution21
I1022 21:09:53.863425  3959 net.cpp:406] Convolution21 <- Convolution20
I1022 21:09:53.863432  3959 net.cpp:380] Convolution21 -> Convolution21
I1022 21:09:53.865245  3959 net.cpp:122] Setting up Convolution21
I1022 21:09:53.865254  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.865257  3959 net.cpp:137] Memory required for data: 1661829216
I1022 21:09:53.865262  3959 layer_factory.hpp:77] Creating layer BatchNorm21
I1022 21:09:53.865267  3959 net.cpp:84] Creating Layer BatchNorm21
I1022 21:09:53.865269  3959 net.cpp:406] BatchNorm21 <- Convolution21
I1022 21:09:53.865273  3959 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1022 21:09:53.865440  3959 net.cpp:122] Setting up BatchNorm21
I1022 21:09:53.865445  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.865447  3959 net.cpp:137] Memory required for data: 1668251744
I1022 21:09:53.865453  3959 layer_factory.hpp:77] Creating layer Scale21
I1022 21:09:53.865456  3959 net.cpp:84] Creating Layer Scale21
I1022 21:09:53.865459  3959 net.cpp:406] Scale21 <- Convolution21
I1022 21:09:53.865463  3959 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1022 21:09:53.865495  3959 layer_factory.hpp:77] Creating layer Scale21
I1022 21:09:53.865607  3959 net.cpp:122] Setting up Scale21
I1022 21:09:53.865612  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.865613  3959 net.cpp:137] Memory required for data: 1674674272
I1022 21:09:53.865617  3959 layer_factory.hpp:77] Creating layer Eltwise9
I1022 21:09:53.865622  3959 net.cpp:84] Creating Layer Eltwise9
I1022 21:09:53.865624  3959 net.cpp:406] Eltwise9 <- Eltwise8_penlu17_0_split_1
I1022 21:09:53.865628  3959 net.cpp:406] Eltwise9 <- Convolution21
I1022 21:09:53.865631  3959 net.cpp:380] Eltwise9 -> Eltwise9
I1022 21:09:53.865649  3959 net.cpp:122] Setting up Eltwise9
I1022 21:09:53.865653  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.865655  3959 net.cpp:137] Memory required for data: 1681096800
I1022 21:09:53.865658  3959 layer_factory.hpp:77] Creating layer penlu19
I1022 21:09:53.865664  3959 net.cpp:84] Creating Layer penlu19
I1022 21:09:53.865665  3959 net.cpp:406] penlu19 <- Eltwise9
I1022 21:09:53.865669  3959 net.cpp:367] penlu19 -> Eltwise9 (in-place)
I1022 21:09:53.865887  3959 net.cpp:122] Setting up penlu19
I1022 21:09:53.865892  3959 net.cpp:129] Top shape: 8 64 56 56 (1605632)
I1022 21:09:53.865895  3959 net.cpp:137] Memory required for data: 1687519328
I1022 21:09:53.865900  3959 layer_factory.hpp:77] Creating layer Pooling1
I1022 21:09:53.865903  3959 net.cpp:84] Creating Layer Pooling1
I1022 21:09:53.865906  3959 net.cpp:406] Pooling1 <- Eltwise9
I1022 21:09:53.865909  3959 net.cpp:380] Pooling1 -> Pooling1
I1022 21:09:53.866051  3959 net.cpp:122] Setting up Pooling1
I1022 21:09:53.866058  3959 net.cpp:129] Top shape: 8 64 1 1 (512)
I1022 21:09:53.866060  3959 net.cpp:137] Memory required for data: 1687521376
I1022 21:09:53.866062  3959 layer_factory.hpp:77] Creating layer InnerProduct1
I1022 21:09:53.866067  3959 net.cpp:84] Creating Layer InnerProduct1
I1022 21:09:53.866070  3959 net.cpp:406] InnerProduct1 <- Pooling1
I1022 21:09:53.866075  3959 net.cpp:380] InnerProduct1 -> InnerProduct1
I1022 21:09:53.866184  3959 net.cpp:122] Setting up InnerProduct1
I1022 21:09:53.866189  3959 net.cpp:129] Top shape: 8 10 (80)
I1022 21:09:53.866191  3959 net.cpp:137] Memory required for data: 1687521696
I1022 21:09:53.866195  3959 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1022 21:09:53.866199  3959 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1022 21:09:53.866201  3959 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1022 21:09:53.866205  3959 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1022 21:09:53.866209  3959 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1022 21:09:53.866240  3959 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1022 21:09:53.866245  3959 net.cpp:129] Top shape: 8 10 (80)
I1022 21:09:53.866247  3959 net.cpp:129] Top shape: 8 10 (80)
I1022 21:09:53.866250  3959 net.cpp:137] Memory required for data: 1687522336
I1022 21:09:53.866251  3959 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1022 21:09:53.866256  3959 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1022 21:09:53.866258  3959 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I1022 21:09:53.866261  3959 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I1022 21:09:53.866264  3959 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1022 21:09:53.866269  3959 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1022 21:09:53.866468  3959 net.cpp:122] Setting up SoftmaxWithLoss1
I1022 21:09:53.866475  3959 net.cpp:129] Top shape: (1)
I1022 21:09:53.866478  3959 net.cpp:132]     with loss weight 1
I1022 21:09:53.866485  3959 net.cpp:137] Memory required for data: 1687522340
I1022 21:09:53.866487  3959 layer_factory.hpp:77] Creating layer Accuracy1
I1022 21:09:53.866497  3959 net.cpp:84] Creating Layer Accuracy1
I1022 21:09:53.866498  3959 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I1022 21:09:53.866502  3959 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_1
I1022 21:09:53.866510  3959 net.cpp:380] Accuracy1 -> Accuracy1
I1022 21:09:53.866518  3959 net.cpp:122] Setting up Accuracy1
I1022 21:09:53.866520  3959 net.cpp:129] Top shape: (1)
I1022 21:09:53.866523  3959 net.cpp:137] Memory required for data: 1687522344
I1022 21:09:53.866524  3959 net.cpp:200] Accuracy1 does not need backward computation.
I1022 21:09:53.866528  3959 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1022 21:09:53.866529  3959 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1022 21:09:53.866533  3959 net.cpp:198] InnerProduct1 needs backward computation.
I1022 21:09:53.866534  3959 net.cpp:198] Pooling1 needs backward computation.
I1022 21:09:53.866536  3959 net.cpp:198] penlu19 needs backward computation.
I1022 21:09:53.866539  3959 net.cpp:198] Eltwise9 needs backward computation.
I1022 21:09:53.866541  3959 net.cpp:198] Scale21 needs backward computation.
I1022 21:09:53.866544  3959 net.cpp:198] BatchNorm21 needs backward computation.
I1022 21:09:53.866545  3959 net.cpp:198] Convolution21 needs backward computation.
I1022 21:09:53.866547  3959 net.cpp:198] penlu18 needs backward computation.
I1022 21:09:53.866549  3959 net.cpp:198] Scale20 needs backward computation.
I1022 21:09:53.866551  3959 net.cpp:198] BatchNorm20 needs backward computation.
I1022 21:09:53.866554  3959 net.cpp:198] Convolution20 needs backward computation.
I1022 21:09:53.866555  3959 net.cpp:198] Eltwise8_penlu17_0_split needs backward computation.
I1022 21:09:53.866557  3959 net.cpp:198] penlu17 needs backward computation.
I1022 21:09:53.866559  3959 net.cpp:198] Eltwise8 needs backward computation.
I1022 21:09:53.866562  3959 net.cpp:198] Scale19 needs backward computation.
I1022 21:09:53.866564  3959 net.cpp:198] BatchNorm19 needs backward computation.
I1022 21:09:53.866566  3959 net.cpp:198] Convolution19 needs backward computation.
I1022 21:09:53.866569  3959 net.cpp:198] penlu16 needs backward computation.
I1022 21:09:53.866570  3959 net.cpp:198] Scale18 needs backward computation.
I1022 21:09:53.866574  3959 net.cpp:198] BatchNorm18 needs backward computation.
I1022 21:09:53.866575  3959 net.cpp:198] Convolution18 needs backward computation.
I1022 21:09:53.866577  3959 net.cpp:198] Eltwise7_penlu15_0_split needs backward computation.
I1022 21:09:53.866580  3959 net.cpp:198] penlu15 needs backward computation.
I1022 21:09:53.866582  3959 net.cpp:198] Eltwise7 needs backward computation.
I1022 21:09:53.866585  3959 net.cpp:198] Scale17 needs backward computation.
I1022 21:09:53.866587  3959 net.cpp:198] BatchNorm17 needs backward computation.
I1022 21:09:53.866590  3959 net.cpp:198] Convolution17 needs backward computation.
I1022 21:09:53.866593  3959 net.cpp:198] penlu14 needs backward computation.
I1022 21:09:53.866595  3959 net.cpp:198] Scale16 needs backward computation.
I1022 21:09:53.866597  3959 net.cpp:198] BatchNorm16 needs backward computation.
I1022 21:09:53.866600  3959 net.cpp:198] Convolution16 needs backward computation.
I1022 21:09:53.866602  3959 net.cpp:198] Scale15 needs backward computation.
I1022 21:09:53.866605  3959 net.cpp:198] BatchNorm15 needs backward computation.
I1022 21:09:53.866606  3959 net.cpp:198] Convolution15 needs backward computation.
I1022 21:09:53.866608  3959 net.cpp:198] Eltwise6_penlu13_0_split needs backward computation.
I1022 21:09:53.866611  3959 net.cpp:198] penlu13 needs backward computation.
I1022 21:09:53.866613  3959 net.cpp:198] Eltwise6 needs backward computation.
I1022 21:09:53.866616  3959 net.cpp:198] Scale14 needs backward computation.
I1022 21:09:53.886771  3959 net.cpp:198] BatchNorm14 needs backward computation.
I1022 21:09:53.886780  3959 net.cpp:198] Convolution14 needs backward computation.
I1022 21:09:53.886782  3959 net.cpp:198] penlu12 needs backward computation.
I1022 21:09:53.886785  3959 net.cpp:198] Scale13 needs backward computation.
I1022 21:09:53.886786  3959 net.cpp:198] BatchNorm13 needs backward computation.
I1022 21:09:53.886790  3959 net.cpp:198] Convolution13 needs backward computation.
I1022 21:09:53.886801  3959 net.cpp:198] Eltwise5_penlu11_0_split needs backward computation.
I1022 21:09:53.886803  3959 net.cpp:198] penlu11 needs backward computation.
I1022 21:09:53.886806  3959 net.cpp:198] Eltwise5 needs backward computation.
I1022 21:09:53.886809  3959 net.cpp:198] Scale12 needs backward computation.
I1022 21:09:53.886811  3959 net.cpp:198] BatchNorm12 needs backward computation.
I1022 21:09:53.886813  3959 net.cpp:198] Convolution12 needs backward computation.
I1022 21:09:53.886816  3959 net.cpp:198] penlu10 needs backward computation.
I1022 21:09:53.886818  3959 net.cpp:198] Scale11 needs backward computation.
I1022 21:09:53.886821  3959 net.cpp:198] BatchNorm11 needs backward computation.
I1022 21:09:53.886823  3959 net.cpp:198] Convolution11 needs backward computation.
I1022 21:09:53.886826  3959 net.cpp:198] Eltwise4_penlu9_0_split needs backward computation.
I1022 21:09:53.886828  3959 net.cpp:198] penlu9 needs backward computation.
I1022 21:09:53.886831  3959 net.cpp:198] Eltwise4 needs backward computation.
I1022 21:09:53.886834  3959 net.cpp:198] Scale10 needs backward computation.
I1022 21:09:53.886837  3959 net.cpp:198] BatchNorm10 needs backward computation.
I1022 21:09:53.886838  3959 net.cpp:198] Convolution10 needs backward computation.
I1022 21:09:53.886842  3959 net.cpp:198] penlu8 needs backward computation.
I1022 21:09:53.886844  3959 net.cpp:198] Scale9 needs backward computation.
I1022 21:09:53.886847  3959 net.cpp:198] BatchNorm9 needs backward computation.
I1022 21:09:53.886848  3959 net.cpp:198] Convolution9 needs backward computation.
I1022 21:09:53.886852  3959 net.cpp:198] Scale8 needs backward computation.
I1022 21:09:53.886854  3959 net.cpp:198] BatchNorm8 needs backward computation.
I1022 21:09:53.886857  3959 net.cpp:198] Convolution8 needs backward computation.
I1022 21:09:53.886859  3959 net.cpp:198] Eltwise3_penlu7_0_split needs backward computation.
I1022 21:09:53.886862  3959 net.cpp:198] penlu7 needs backward computation.
I1022 21:09:53.886864  3959 net.cpp:198] Eltwise3 needs backward computation.
I1022 21:09:53.886868  3959 net.cpp:198] Scale7 needs backward computation.
I1022 21:09:53.886869  3959 net.cpp:198] BatchNorm7 needs backward computation.
I1022 21:09:53.886873  3959 net.cpp:198] Convolution7 needs backward computation.
I1022 21:09:53.886874  3959 net.cpp:198] penlu6 needs backward computation.
I1022 21:09:53.886878  3959 net.cpp:198] Scale6 needs backward computation.
I1022 21:09:53.886879  3959 net.cpp:198] BatchNorm6 needs backward computation.
I1022 21:09:53.886881  3959 net.cpp:198] Convolution6 needs backward computation.
I1022 21:09:53.886884  3959 net.cpp:198] Eltwise2_penlu5_0_split needs backward computation.
I1022 21:09:53.886888  3959 net.cpp:198] penlu5 needs backward computation.
I1022 21:09:53.886889  3959 net.cpp:198] Eltwise2 needs backward computation.
I1022 21:09:53.886893  3959 net.cpp:198] Scale5 needs backward computation.
I1022 21:09:53.886894  3959 net.cpp:198] BatchNorm5 needs backward computation.
I1022 21:09:53.886898  3959 net.cpp:198] Convolution5 needs backward computation.
I1022 21:09:53.886899  3959 net.cpp:198] penlu4 needs backward computation.
I1022 21:09:53.886903  3959 net.cpp:198] Scale4 needs backward computation.
I1022 21:09:53.886904  3959 net.cpp:198] BatchNorm4 needs backward computation.
I1022 21:09:53.886906  3959 net.cpp:198] Convolution4 needs backward computation.
I1022 21:09:53.886909  3959 net.cpp:198] Eltwise1_penlu3_0_split needs backward computation.
I1022 21:09:53.886912  3959 net.cpp:198] penlu3 needs backward computation.
I1022 21:09:53.886915  3959 net.cpp:198] Eltwise1 needs backward computation.
I1022 21:09:53.886919  3959 net.cpp:198] Scale3 needs backward computation.
I1022 21:09:53.886920  3959 net.cpp:198] BatchNorm3 needs backward computation.
I1022 21:09:53.886922  3959 net.cpp:198] Convolution3 needs backward computation.
I1022 21:09:53.886925  3959 net.cpp:198] penlu2 needs backward computation.
I1022 21:09:53.886927  3959 net.cpp:198] Scale2 needs backward computation.
I1022 21:09:53.886934  3959 net.cpp:198] BatchNorm2 needs backward computation.
I1022 21:09:53.886936  3959 net.cpp:198] Convolution2 needs backward computation.
I1022 21:09:53.886940  3959 net.cpp:198] Convolution1_penlu1_0_split needs backward computation.
I1022 21:09:53.886941  3959 net.cpp:198] penlu1 needs backward computation.
I1022 21:09:53.886946  3959 net.cpp:198] Scale1 needs backward computation.
I1022 21:09:53.886948  3959 net.cpp:198] BatchNorm1 needs backward computation.
I1022 21:09:53.886950  3959 net.cpp:198] Convolution1 needs backward computation.
I1022 21:09:53.886955  3959 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1022 21:09:53.886957  3959 net.cpp:200] Data1 does not need backward computation.
I1022 21:09:53.886960  3959 net.cpp:242] This network produces output Accuracy1
I1022 21:09:53.886962  3959 net.cpp:242] This network produces output SoftmaxWithLoss1
I1022 21:09:53.886999  3959 net.cpp:255] Network initialization done.
I1022 21:09:53.887285  3959 solver.cpp:56] Solver scaffolding done.
I1022 21:09:53.893470  3959 caffe.cpp:248] Starting Optimization
I1022 21:09:53.893477  3959 solver.cpp:272] Solving resnet
I1022 21:09:53.893479  3959 solver.cpp:273] Learning Rate Policy: multistep
I1022 21:09:53.896334  3959 solver.cpp:330] Iteration 0, Testing net (#0)
I1022 21:09:56.367048  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:09:56.505365  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0
I1022 21:09:56.505424  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3365 (* 1 = 87.3365 loss)
I1022 21:09:56.736928  3959 solver.cpp:218] Iteration 0 (0.0800631 iter/s, 2.84337s/100 iters), loss = 2.33573
I1022 21:09:56.736956  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.33573 (* 1 = 2.33573 loss)
I1022 21:09:56.736971  3959 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1022 21:10:16.913878  3959 solver.cpp:218] Iteration 100 (4.95619 iter/s, 20.1768s/100 iters), loss = 1.00837
I1022 21:10:16.913909  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.00837 (* 1 = 1.00837 loss)
I1022 21:10:16.913915  3959 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I1022 21:10:37.219718  3959 solver.cpp:330] Iteration 200, Testing net (#0)
I1022 21:10:39.651438  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:10:39.788641  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.117188
I1022 21:10:39.788702  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 30.9374 (* 1 = 30.9374 loss)
I1022 21:10:39.989917  3959 solver.cpp:218] Iteration 200 (4.33353 iter/s, 23.0759s/100 iters), loss = 0.967895
I1022 21:10:39.989950  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.967895 (* 1 = 0.967895 loss)
I1022 21:10:39.989958  3959 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I1022 21:11:00.157414  3959 solver.cpp:218] Iteration 300 (4.95851 iter/s, 20.1673s/100 iters), loss = 1.50887
I1022 21:11:00.157456  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.50887 (* 1 = 1.50887 loss)
I1022 21:11:00.157464  3959 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I1022 21:11:07.668844  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:11:20.145967  3959 solver.cpp:330] Iteration 400, Testing net (#0)
I1022 21:11:22.584938  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:11:22.723031  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.337891
I1022 21:11:22.723088  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 8.31571 (* 1 = 8.31571 loss)
I1022 21:11:22.924492  3959 solver.cpp:218] Iteration 400 (4.39233 iter/s, 22.7669s/100 iters), loss = 1.11618
I1022 21:11:22.924523  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.11618 (* 1 = 1.11618 loss)
I1022 21:11:22.924530  3959 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I1022 21:11:43.355679  3959 solver.cpp:218] Iteration 500 (4.89451 iter/s, 20.4311s/100 iters), loss = 0.510526
I1022 21:11:43.355839  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.510526 (* 1 = 0.510526 loss)
I1022 21:11:43.355849  3959 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I1022 21:12:03.265636  3959 solver.cpp:330] Iteration 600, Testing net (#0)
I1022 21:12:05.662343  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:12:05.838552  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.306641
I1022 21:12:05.838609  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 6.46059 (* 1 = 6.46059 loss)
I1022 21:12:06.040451  3959 solver.cpp:218] Iteration 600 (4.40829 iter/s, 22.6845s/100 iters), loss = 1.27294
I1022 21:12:06.040482  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.27294 (* 1 = 1.27294 loss)
I1022 21:12:06.040488  3959 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I1022 21:12:21.816079  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:12:26.217209  3959 solver.cpp:218] Iteration 700 (4.95622 iter/s, 20.1767s/100 iters), loss = 0.792242
I1022 21:12:26.217242  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.792242 (* 1 = 0.792242 loss)
I1022 21:12:26.217248  3959 sgd_solver.cpp:105] Iteration 700, lr = 0.01
I1022 21:12:46.120388  3959 solver.cpp:330] Iteration 800, Testing net (#0)
I1022 21:12:48.516801  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:12:48.693472  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.386719
I1022 21:12:48.693532  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.49337 (* 1 = 4.49337 loss)
I1022 21:12:48.895452  3959 solver.cpp:218] Iteration 800 (4.40953 iter/s, 22.6781s/100 iters), loss = 0.184325
I1022 21:12:48.895484  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184325 (* 1 = 0.184325 loss)
I1022 21:12:48.895490  3959 sgd_solver.cpp:105] Iteration 800, lr = 0.01
I1022 21:13:09.005612  3959 solver.cpp:218] Iteration 900 (4.97264 iter/s, 20.1101s/100 iters), loss = 0.790365
I1022 21:13:09.005733  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.790365 (* 1 = 0.790365 loss)
I1022 21:13:09.005741  3959 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I1022 21:13:28.917444  3959 solver.cpp:330] Iteration 1000, Testing net (#0)
I1022 21:13:31.314034  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:13:31.491894  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.375
I1022 21:13:31.491966  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.86794 (* 1 = 3.86794 loss)
I1022 21:13:31.693565  3959 solver.cpp:218] Iteration 1000 (4.40767 iter/s, 22.6877s/100 iters), loss = 0.116178
I1022 21:13:31.693598  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.116178 (* 1 = 0.116178 loss)
I1022 21:13:31.693604  3959 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I1022 21:13:35.542193  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:13:51.807972  3959 solver.cpp:218] Iteration 1100 (4.97159 iter/s, 20.1143s/100 iters), loss = 0.0534312
I1022 21:13:51.808094  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0534309 (* 1 = 0.0534309 loss)
I1022 21:13:51.808113  3959 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I1022 21:14:11.724088  3959 solver.cpp:330] Iteration 1200, Testing net (#0)
I1022 21:14:14.119509  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:14:14.298593  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.603516
I1022 21:14:14.298660  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.32467 (* 1 = 1.32467 loss)
I1022 21:14:14.500282  3959 solver.cpp:218] Iteration 1200 (4.40682 iter/s, 22.6921s/100 iters), loss = 1.58964
I1022 21:14:14.500314  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.58964 (* 1 = 1.58964 loss)
I1022 21:14:14.500320  3959 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I1022 21:14:34.607609  3959 solver.cpp:218] Iteration 1300 (4.97334 iter/s, 20.1072s/100 iters), loss = 0.489774
I1022 21:14:34.607748  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.489773 (* 1 = 0.489773 loss)
I1022 21:14:34.607758  3959 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I1022 21:14:46.704665  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:14:54.528410  3959 solver.cpp:330] Iteration 1400, Testing net (#0)
I1022 21:14:56.886953  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:14:57.104351  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.373047
I1022 21:14:57.104415  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.22772 (* 1 = 4.22772 loss)
I1022 21:14:57.306254  3959 solver.cpp:218] Iteration 1400 (4.40559 iter/s, 22.6984s/100 iters), loss = 0.720515
I1022 21:14:57.306288  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.720514 (* 1 = 0.720514 loss)
I1022 21:14:57.306294  3959 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I1022 21:15:17.415480  3959 solver.cpp:218] Iteration 1500 (4.97287 iter/s, 20.1091s/100 iters), loss = 0.266465
I1022 21:15:17.415582  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.266465 (* 1 = 0.266465 loss)
I1022 21:15:17.415592  3959 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I1022 21:15:37.329883  3959 solver.cpp:330] Iteration 1600, Testing net (#0)
I1022 21:15:39.689035  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:15:39.906911  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.552734
I1022 21:15:39.906971  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.57394 (* 1 = 1.57394 loss)
I1022 21:15:40.108814  3959 solver.cpp:218] Iteration 1600 (4.40662 iter/s, 22.6932s/100 iters), loss = 0.65367
I1022 21:15:40.108846  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.65367 (* 1 = 0.65367 loss)
I1022 21:15:40.108852  3959 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I1022 21:16:00.221508  3959 solver.cpp:218] Iteration 1700 (4.97201 iter/s, 20.1126s/100 iters), loss = 0.750241
I1022 21:16:00.221628  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.75024 (* 1 = 0.75024 loss)
I1022 21:16:00.221637  3959 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I1022 21:16:00.649229  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:16:20.138137  3959 solver.cpp:330] Iteration 1800, Testing net (#0)
I1022 21:16:22.495460  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:16:22.714478  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.626953
I1022 21:16:22.714531  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.57181 (* 1 = 1.57181 loss)
I1022 21:16:22.915958  3959 solver.cpp:218] Iteration 1800 (4.4064 iter/s, 22.6943s/100 iters), loss = 0.419798
I1022 21:16:22.915993  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419797 (* 1 = 0.419797 loss)
I1022 21:16:22.916000  3959 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I1022 21:16:43.024032  3959 solver.cpp:218] Iteration 1900 (4.97315 iter/s, 20.108s/100 iters), loss = 0.446745
I1022 21:16:43.024188  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.446744 (* 1 = 0.446744 loss)
I1022 21:16:43.024199  3959 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I1022 21:17:02.932380  3959 solver.cpp:330] Iteration 2000, Testing net (#0)
I1022 21:17:05.289672  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:17:05.509692  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.785156
I1022 21:17:05.509743  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.582145 (* 1 = 0.582145 loss)
I1022 21:17:05.711551  3959 solver.cpp:218] Iteration 2000 (4.40775 iter/s, 22.6873s/100 iters), loss = 0.0103464
I1022 21:17:05.711583  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0103457 (* 1 = 0.0103457 loss)
I1022 21:17:05.711591  3959 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I1022 21:17:14.391270  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:17:25.837983  3959 solver.cpp:218] Iteration 2100 (4.96862 iter/s, 20.1263s/100 iters), loss = 0.42148
I1022 21:17:25.838013  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.421479 (* 1 = 0.421479 loss)
I1022 21:17:25.838021  3959 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I1022 21:17:45.751813  3959 solver.cpp:330] Iteration 2200, Testing net (#0)
I1022 21:17:48.071434  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:17:48.328454  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.757812
I1022 21:17:48.328512  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.701473 (* 1 = 0.701473 loss)
I1022 21:17:48.530339  3959 solver.cpp:218] Iteration 2200 (4.40679 iter/s, 22.6923s/100 iters), loss = 0.188733
I1022 21:17:48.530374  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.188733 (* 1 = 0.188733 loss)
I1022 21:17:48.530380  3959 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I1022 21:18:08.646827  3959 solver.cpp:218] Iteration 2300 (4.97107 iter/s, 20.1164s/100 iters), loss = 0.098685
I1022 21:18:08.646878  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0986847 (* 1 = 0.0986847 loss)
I1022 21:18:08.646886  3959 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I1022 21:18:25.572901  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:18:28.569469  3959 solver.cpp:330] Iteration 2400, Testing net (#0)
I1022 21:18:30.889241  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:18:31.147240  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.755859
I1022 21:18:31.147306  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.846559 (* 1 = 0.846559 loss)
I1022 21:18:31.348899  3959 solver.cpp:218] Iteration 2400 (4.40491 iter/s, 22.702s/100 iters), loss = 0.036547
I1022 21:18:31.348933  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0365465 (* 1 = 0.0365465 loss)
I1022 21:18:31.348942  3959 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I1022 21:18:51.460222  3959 solver.cpp:218] Iteration 2500 (4.97235 iter/s, 20.1112s/100 iters), loss = 0.166462
I1022 21:18:51.460255  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166461 (* 1 = 0.166461 loss)
I1022 21:18:51.460263  3959 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I1022 21:19:11.375282  3959 solver.cpp:330] Iteration 2600, Testing net (#0)
I1022 21:19:13.693581  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:19:13.952621  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.820312
I1022 21:19:13.952680  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.607379 (* 1 = 0.607379 loss)
I1022 21:19:14.154381  3959 solver.cpp:218] Iteration 2600 (4.40644 iter/s, 22.6941s/100 iters), loss = 0.0470923
I1022 21:19:14.154409  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0470914 (* 1 = 0.0470914 loss)
I1022 21:19:14.154417  3959 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I1022 21:19:34.280695  3959 solver.cpp:218] Iteration 2700 (4.96864 iter/s, 20.1262s/100 iters), loss = 1.28606
I1022 21:19:34.280725  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.28606 (* 1 = 1.28606 loss)
I1022 21:19:34.280731  3959 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I1022 21:19:39.336902  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:19:54.209251  3959 solver.cpp:330] Iteration 2800, Testing net (#0)
I1022 21:19:56.527062  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:19:56.787766  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.628906
I1022 21:19:56.787827  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.55083 (* 1 = 1.55083 loss)
I1022 21:19:56.989890  3959 solver.cpp:218] Iteration 2800 (4.40352 iter/s, 22.7091s/100 iters), loss = 0.213145
I1022 21:19:56.989919  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213145 (* 1 = 0.213145 loss)
I1022 21:19:56.989925  3959 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I1022 21:20:17.116150  3959 solver.cpp:218] Iteration 2900 (4.96866 iter/s, 20.1262s/100 iters), loss = 0.364608
I1022 21:20:17.116180  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364607 (* 1 = 0.364607 loss)
I1022 21:20:17.116186  3959 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I1022 21:20:37.044668  3959 solver.cpp:330] Iteration 3000, Testing net (#0)
I1022 21:20:39.328527  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:20:39.626571  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.837891
I1022 21:20:39.626621  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.407456 (* 1 = 0.407456 loss)
I1022 21:20:39.828402  3959 solver.cpp:218] Iteration 3000 (4.40293 iter/s, 22.7122s/100 iters), loss = 0.754204
I1022 21:20:39.828434  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.754203 (* 1 = 0.754203 loss)
I1022 21:20:39.828441  3959 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I1022 21:20:53.343466  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:20:59.970584  3959 solver.cpp:218] Iteration 3100 (4.96473 iter/s, 20.1421s/100 iters), loss = 0.27052
I1022 21:20:59.970616  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27052 (* 1 = 0.27052 loss)
I1022 21:20:59.970623  3959 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I1022 21:21:19.899919  3959 solver.cpp:330] Iteration 3200, Testing net (#0)
I1022 21:21:22.180155  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:21:22.479635  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.828125
I1022 21:21:22.479692  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.499522 (* 1 = 0.499522 loss)
I1022 21:21:22.681852  3959 solver.cpp:218] Iteration 3200 (4.40312 iter/s, 22.7112s/100 iters), loss = 0.21611
I1022 21:21:22.681885  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.21611 (* 1 = 0.21611 loss)
I1022 21:21:22.681890  3959 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I1022 21:21:42.823015  3959 solver.cpp:218] Iteration 3300 (4.96498 iter/s, 20.1411s/100 iters), loss = 0.13865
I1022 21:21:42.823047  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138649 (* 1 = 0.138649 loss)
I1022 21:21:42.823055  3959 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I1022 21:22:02.764299  3959 solver.cpp:330] Iteration 3400, Testing net (#0)
I1022 21:22:05.043517  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:22:05.342880  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.884766
I1022 21:22:05.342938  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.266613 (* 1 = 0.266613 loss)
I1022 21:22:05.544800  3959 solver.cpp:218] Iteration 3400 (4.40108 iter/s, 22.7217s/100 iters), loss = 0.0335593
I1022 21:22:05.544831  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.033559 (* 1 = 0.033559 loss)
I1022 21:22:05.544838  3959 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I1022 21:22:07.182582  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:22:25.683593  3959 solver.cpp:218] Iteration 3500 (4.96556 iter/s, 20.1387s/100 iters), loss = 0.5579
I1022 21:22:25.683624  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.557899 (* 1 = 0.557899 loss)
I1022 21:22:25.683630  3959 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I1022 21:22:45.624486  3959 solver.cpp:330] Iteration 3600, Testing net (#0)
I1022 21:22:47.902947  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:22:48.204742  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.804688
I1022 21:22:48.204800  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.479661 (* 1 = 0.479661 loss)
I1022 21:22:48.406718  3959 solver.cpp:218] Iteration 3600 (4.40082 iter/s, 22.723s/100 iters), loss = 0.151135
I1022 21:22:48.406750  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151135 (* 1 = 0.151135 loss)
I1022 21:22:48.406756  3959 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I1022 21:23:08.542095  3959 solver.cpp:218] Iteration 3700 (4.96641 iter/s, 20.1353s/100 iters), loss = 0.0119583
I1022 21:23:08.542127  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119579 (* 1 = 0.0119579 loss)
I1022 21:23:08.542134  3959 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I1022 21:23:18.437299  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:23:28.477098  3959 solver.cpp:330] Iteration 3800, Testing net (#0)
I1022 21:23:30.718969  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:23:31.055991  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.859375
I1022 21:23:31.056048  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.44471 (* 1 = 0.44471 loss)
I1022 21:23:31.257884  3959 solver.cpp:218] Iteration 3800 (4.40224 iter/s, 22.7157s/100 iters), loss = 0.431797
I1022 21:23:31.257918  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.431796 (* 1 = 0.431796 loss)
I1022 21:23:31.257925  3959 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I1022 21:23:51.394848  3959 solver.cpp:218] Iteration 3900 (4.96601 iter/s, 20.1369s/100 iters), loss = 0.0119284
I1022 21:23:51.394996  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119278 (* 1 = 0.0119278 loss)
I1022 21:23:51.395006  3959 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I1022 21:24:11.334480  3959 solver.cpp:330] Iteration 4000, Testing net (#0)
I1022 21:24:13.574683  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:24:13.913012  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.847656
I1022 21:24:13.913067  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361961 (* 1 = 0.361961 loss)
I1022 21:24:14.115353  3959 solver.cpp:218] Iteration 4000 (4.40135 iter/s, 22.7203s/100 iters), loss = 0.101083
I1022 21:24:14.115383  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.101082 (* 1 = 0.101082 loss)
I1022 21:24:14.115391  3959 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I1022 21:24:32.261652  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:24:34.252840  3959 solver.cpp:218] Iteration 4100 (4.96589 iter/s, 20.1374s/100 iters), loss = 0.258402
I1022 21:24:34.252868  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.258402 (* 1 = 0.258402 loss)
I1022 21:24:34.252876  3959 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I1022 21:24:54.190052  3959 solver.cpp:330] Iteration 4200, Testing net (#0)
I1022 21:24:56.428479  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:24:56.767990  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.902344
I1022 21:24:56.768051  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.296404 (* 1 = 0.296404 loss)
I1022 21:24:56.969907  3959 solver.cpp:218] Iteration 4200 (4.402 iter/s, 22.717s/100 iters), loss = 0.0847871
I1022 21:24:56.969939  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0847867 (* 1 = 0.0847867 loss)
I1022 21:24:56.969946  3959 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I1022 21:25:17.097291  3959 solver.cpp:218] Iteration 4300 (4.96838 iter/s, 20.1273s/100 iters), loss = 0.807794
I1022 21:25:17.097432  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.807794 (* 1 = 0.807794 loss)
I1022 21:25:17.097441  3959 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I1022 21:25:37.031206  3959 solver.cpp:330] Iteration 4400, Testing net (#0)
I1022 21:25:39.270081  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:25:39.610072  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.726562
I1022 21:25:39.610136  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.26108 (* 1 = 1.26108 loss)
I1022 21:25:39.812216  3959 solver.cpp:218] Iteration 4400 (4.40243 iter/s, 22.7147s/100 iters), loss = 0.0102429
I1022 21:25:39.812247  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0102427 (* 1 = 0.0102427 loss)
I1022 21:25:39.812254  3959 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I1022 21:25:46.283210  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:25:59.950029  3959 solver.cpp:218] Iteration 4500 (4.9658 iter/s, 20.1377s/100 iters), loss = 0.0908497
I1022 21:25:59.950197  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0908494 (* 1 = 0.0908494 loss)
I1022 21:25:59.950209  3959 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I1022 21:26:19.892424  3959 solver.cpp:330] Iteration 4600, Testing net (#0)
I1022 21:26:22.093380  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:26:22.471348  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.714844
I1022 21:26:22.471407  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.08082 (* 1 = 1.08082 loss)
I1022 21:26:22.673317  3959 solver.cpp:218] Iteration 4600 (4.40082 iter/s, 22.7231s/100 iters), loss = 0.0245934
I1022 21:26:22.673351  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.024593 (* 1 = 0.024593 loss)
I1022 21:26:22.673357  3959 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I1022 21:26:42.813388  3959 solver.cpp:218] Iteration 4700 (4.96525 iter/s, 20.14s/100 iters), loss = 0.375872
I1022 21:26:42.813532  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.375871 (* 1 = 0.375871 loss)
I1022 21:26:42.813541  3959 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I1022 21:26:57.544641  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:27:02.762233  3959 solver.cpp:330] Iteration 4800, Testing net (#0)
I1022 21:27:04.963616  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:27:05.341948  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.871094
I1022 21:27:05.342005  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.346775 (* 1 = 0.346775 loss)
I1022 21:27:05.543817  3959 solver.cpp:218] Iteration 4800 (4.39943 iter/s, 22.7302s/100 iters), loss = 0.122389
I1022 21:27:05.543848  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.122389 (* 1 = 0.122389 loss)
I1022 21:27:05.543854  3959 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I1022 21:27:25.671773  3959 solver.cpp:218] Iteration 4900 (4.96824 iter/s, 20.1279s/100 iters), loss = 0.0411719
I1022 21:27:25.671887  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0411716 (* 1 = 0.0411716 loss)
I1022 21:27:25.671898  3959 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I1022 21:27:45.609112  3959 solver.cpp:330] Iteration 5000, Testing net (#0)
I1022 21:27:47.808269  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:27:48.188207  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.867188
I1022 21:27:48.188261  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.511307 (* 1 = 0.511307 loss)
I1022 21:27:48.390319  3959 solver.cpp:218] Iteration 5000 (4.40172 iter/s, 22.7184s/100 iters), loss = 0.135051
I1022 21:27:48.390352  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.13505 (* 1 = 0.13505 loss)
I1022 21:27:48.390369  3959 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1022 21:28:08.528926  3959 solver.cpp:218] Iteration 5100 (4.96561 iter/s, 20.1385s/100 iters), loss = 0.00551571
I1022 21:28:08.529083  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00551529 (* 1 = 0.00551529 loss)
I1022 21:28:08.529091  3959 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1022 21:28:11.376890  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:28:28.472774  3959 solver.cpp:330] Iteration 5200, Testing net (#0)
I1022 21:28:30.672183  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:28:31.052603  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.939453
I1022 21:28:31.052656  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.1622 (* 1 = 0.1622 loss)
I1022 21:28:31.254813  3959 solver.cpp:218] Iteration 5200 (4.40031 iter/s, 22.7257s/100 iters), loss = 0.0611768
I1022 21:28:31.254849  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0611765 (* 1 = 0.0611765 loss)
I1022 21:28:31.254856  3959 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1022 21:28:51.405763  3959 solver.cpp:218] Iteration 5300 (4.96257 iter/s, 20.1509s/100 iters), loss = 0.058746
I1022 21:28:51.405931  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0587458 (* 1 = 0.0587458 loss)
I1022 21:28:51.405942  3959 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1022 21:29:11.360405  3959 solver.cpp:330] Iteration 5400, Testing net (#0)
I1022 21:29:13.520483  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:29:13.938740  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8125
I1022 21:29:13.938802  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.574028 (* 1 = 0.574028 loss)
I1022 21:29:14.140830  3959 solver.cpp:218] Iteration 5400 (4.39853 iter/s, 22.7348s/100 iters), loss = 0.333505
I1022 21:29:14.140859  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.333505 (* 1 = 0.333505 loss)
I1022 21:29:14.140866  3959 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1022 21:29:25.243733  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:29:34.296593  3959 solver.cpp:218] Iteration 5500 (4.96138 iter/s, 20.1557s/100 iters), loss = 0.0787484
I1022 21:29:34.296623  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.078748 (* 1 = 0.078748 loss)
I1022 21:29:34.296630  3959 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1022 21:29:54.235014  3959 solver.cpp:330] Iteration 5600, Testing net (#0)
I1022 21:29:56.395705  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:29:56.815454  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.857422
I1022 21:29:56.815501  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.46601 (* 1 = 0.46601 loss)
I1022 21:29:57.017787  3959 solver.cpp:218] Iteration 5600 (4.40119 iter/s, 22.7211s/100 iters), loss = 0.0517877
I1022 21:29:57.017817  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0517872 (* 1 = 0.0517872 loss)
I1022 21:29:57.017823  3959 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1022 21:30:17.170352  3959 solver.cpp:218] Iteration 5700 (4.96217 iter/s, 20.1525s/100 iters), loss = 0.00444277
I1022 21:30:17.170383  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00444226 (* 1 = 0.00444226 loss)
I1022 21:30:17.170390  3959 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1022 21:30:36.737835  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:30:37.125787  3959 solver.cpp:330] Iteration 5800, Testing net (#0)
I1022 21:30:39.285184  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:30:39.704777  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.761719
I1022 21:30:39.704835  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.679316 (* 1 = 0.679316 loss)
I1022 21:30:39.907476  3959 solver.cpp:218] Iteration 5800 (4.39811 iter/s, 22.737s/100 iters), loss = 0.163498
I1022 21:30:39.907508  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163497 (* 1 = 0.163497 loss)
I1022 21:30:39.907516  3959 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1022 21:31:00.056291  3959 solver.cpp:218] Iteration 5900 (4.96309 iter/s, 20.1487s/100 iters), loss = 0.337947
I1022 21:31:00.056320  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.337946 (* 1 = 0.337946 loss)
I1022 21:31:00.056326  3959 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1022 21:31:20.004928  3959 solver.cpp:330] Iteration 6000, Testing net (#0)
I1022 21:31:22.163828  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:31:22.585067  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.753906
I1022 21:31:22.585132  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.690853 (* 1 = 0.690853 loss)
I1022 21:31:22.787145  3959 solver.cpp:218] Iteration 6000 (4.39932 iter/s, 22.7308s/100 iters), loss = 0.0902961
I1022 21:31:22.787174  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0902953 (* 1 = 0.0902953 loss)
I1022 21:31:22.787180  3959 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1022 21:31:42.929612  3959 solver.cpp:218] Iteration 6100 (4.96466 iter/s, 20.1424s/100 iters), loss = 0.00525904
I1022 21:31:42.929641  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00525838 (* 1 = 0.00525838 loss)
I1022 21:31:42.929647  3959 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1022 21:31:50.608821  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:32:02.878912  3959 solver.cpp:330] Iteration 6200, Testing net (#0)
I1022 21:32:05.000365  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:32:05.459007  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.960938
I1022 21:32:05.459061  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.117014 (* 1 = 0.117014 loss)
I1022 21:32:05.661154  3959 solver.cpp:218] Iteration 6200 (4.39919 iter/s, 22.7315s/100 iters), loss = 0.0541801
I1022 21:32:05.661185  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0541796 (* 1 = 0.0541796 loss)
I1022 21:32:05.661191  3959 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1022 21:32:25.806571  3959 solver.cpp:218] Iteration 6300 (4.96393 iter/s, 20.1453s/100 iters), loss = 0.0230455
I1022 21:32:25.806720  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0230449 (* 1 = 0.0230449 loss)
I1022 21:32:25.806730  3959 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1022 21:32:45.757692  3959 solver.cpp:330] Iteration 6400, Testing net (#0)
I1022 21:32:47.876991  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:32:48.336303  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.96875
I1022 21:32:48.336362  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.119985 (* 1 = 0.119985 loss)
I1022 21:32:48.538488  3959 solver.cpp:218] Iteration 6400 (4.39914 iter/s, 22.7317s/100 iters), loss = 0.0124623
I1022 21:32:48.538518  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0124617 (* 1 = 0.0124617 loss)
I1022 21:32:48.538525  3959 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1022 21:33:04.477604  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:33:08.692441  3959 solver.cpp:218] Iteration 6500 (4.96183 iter/s, 20.1539s/100 iters), loss = 0.0117001
I1022 21:33:08.692471  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116995 (* 1 = 0.0116995 loss)
I1022 21:33:08.692476  3959 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1022 21:33:28.637344  3959 solver.cpp:330] Iteration 6600, Testing net (#0)
I1022 21:33:30.756166  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:33:31.216208  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.941406
I1022 21:33:31.216271  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.145386 (* 1 = 0.145386 loss)
I1022 21:33:31.418140  3959 solver.cpp:218] Iteration 6600 (4.40032 iter/s, 22.7256s/100 iters), loss = 0.0606814
I1022 21:33:31.418170  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0606807 (* 1 = 0.0606807 loss)
I1022 21:33:31.418176  3959 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1022 21:33:51.559278  3959 solver.cpp:218] Iteration 6700 (4.96498 iter/s, 20.1411s/100 iters), loss = 0.110593
I1022 21:33:51.559427  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110593 (* 1 = 0.110593 loss)
I1022 21:33:51.559435  3959 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1022 21:34:11.502641  3959 solver.cpp:330] Iteration 6800, Testing net (#0)
I1022 21:34:13.621379  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:34:14.082090  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.955078
I1022 21:34:14.082142  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.113923 (* 1 = 0.113923 loss)
I1022 21:34:14.284425  3959 solver.cpp:218] Iteration 6800 (4.40045 iter/s, 22.7249s/100 iters), loss = 0.154858
I1022 21:34:14.284454  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154857 (* 1 = 0.154857 loss)
I1022 21:34:14.284461  3959 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1022 21:34:18.339097  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:34:34.433825  3959 solver.cpp:218] Iteration 6900 (4.96295 iter/s, 20.1493s/100 iters), loss = 0.492696
I1022 21:34:34.433998  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.492695 (* 1 = 0.492695 loss)
I1022 21:34:34.434008  3959 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1022 21:34:54.384333  3959 solver.cpp:330] Iteration 7000, Testing net (#0)
I1022 21:34:56.466652  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:34:56.965067  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.841797
I1022 21:34:56.965127  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.571384 (* 1 = 0.571384 loss)
I1022 21:34:57.167660  3959 solver.cpp:218] Iteration 7000 (4.39877 iter/s, 22.7336s/100 iters), loss = 0.00327149
I1022 21:34:57.167692  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00327082 (* 1 = 0.00327082 loss)
I1022 21:34:57.167701  3959 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1022 21:35:17.314178  3959 solver.cpp:218] Iteration 7100 (4.96366 iter/s, 20.1464s/100 iters), loss = 0.244185
I1022 21:35:17.314327  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244185 (* 1 = 0.244185 loss)
I1022 21:35:17.314334  3959 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1022 21:35:29.828733  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:35:37.271994  3959 solver.cpp:330] Iteration 7200, Testing net (#0)
I1022 21:35:39.350924  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:35:39.850730  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.857422
I1022 21:35:39.850791  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.504558 (* 1 = 0.504558 loss)
I1022 21:35:40.052667  3959 solver.cpp:218] Iteration 7200 (4.39787 iter/s, 22.7383s/100 iters), loss = 0.316708
I1022 21:35:40.052700  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316707 (* 1 = 0.316707 loss)
I1022 21:35:40.052706  3959 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1022 21:36:00.185818  3959 solver.cpp:218] Iteration 7300 (4.96695 iter/s, 20.1331s/100 iters), loss = 0.448068
I1022 21:36:00.185963  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.448067 (* 1 = 0.448067 loss)
I1022 21:36:00.185972  3959 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1022 21:36:20.131876  3959 solver.cpp:330] Iteration 7400, Testing net (#0)
I1022 21:36:22.209491  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:36:22.710146  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.880859
I1022 21:36:22.710201  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.337902 (* 1 = 0.337902 loss)
I1022 21:36:22.912253  3959 solver.cpp:218] Iteration 7400 (4.4002 iter/s, 22.7262s/100 iters), loss = 0.0290973
I1022 21:36:22.912284  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0290965 (* 1 = 0.0290965 loss)
I1022 21:36:22.912292  3959 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1022 21:36:43.056888  3959 solver.cpp:218] Iteration 7500 (4.96412 iter/s, 20.1446s/100 iters), loss = 0.1532
I1022 21:36:43.057032  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.153199 (* 1 = 0.153199 loss)
I1022 21:36:43.057041  3959 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1022 21:36:43.683342  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:37:03.008805  3959 solver.cpp:330] Iteration 7600, Testing net (#0)
I1022 21:37:05.088114  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:37:05.588335  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.972656
I1022 21:37:05.588393  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0966251 (* 1 = 0.0966251 loss)
I1022 21:37:05.790535  3959 solver.cpp:218] Iteration 7600 (4.3988 iter/s, 22.7334s/100 iters), loss = 0.286472
I1022 21:37:05.790565  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.286471 (* 1 = 0.286471 loss)
I1022 21:37:05.790570  3959 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1022 21:37:25.934175  3959 solver.cpp:218] Iteration 7700 (4.96437 iter/s, 20.1436s/100 iters), loss = 0.0235715
I1022 21:37:25.934350  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0235707 (* 1 = 0.0235707 loss)
I1022 21:37:25.934358  3959 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1022 21:37:45.880219  3959 solver.cpp:330] Iteration 7800, Testing net (#0)
I1022 21:37:47.919771  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:37:48.458799  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.966797
I1022 21:37:48.458856  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.107235 (* 1 = 0.107235 loss)
I1022 21:37:48.660858  3959 solver.cpp:218] Iteration 7800 (4.40016 iter/s, 22.7265s/100 iters), loss = 0.00771791
I1022 21:37:48.660888  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00771711 (* 1 = 0.00771711 loss)
I1022 21:37:48.660895  3959 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1022 21:37:57.548559  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:38:08.814301  3959 solver.cpp:218] Iteration 7900 (4.96195 iter/s, 20.1534s/100 iters), loss = 0.293519
I1022 21:38:08.814329  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293518 (* 1 = 0.293518 loss)
I1022 21:38:08.814335  3959 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1022 21:38:28.756642  3959 solver.cpp:330] Iteration 8000, Testing net (#0)
I1022 21:38:30.796152  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:38:31.336129  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1022 21:38:31.336184  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.1131 (* 1 = 0.1131 loss)
I1022 21:38:31.538544  3959 solver.cpp:218] Iteration 8000 (4.4006 iter/s, 22.7242s/100 iters), loss = 0.0670182
I1022 21:38:31.538573  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0670173 (* 1 = 0.0670173 loss)
I1022 21:38:31.538580  3959 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1022 21:38:51.684803  3959 solver.cpp:218] Iteration 8100 (4.96372 iter/s, 20.1462s/100 iters), loss = 0.081589
I1022 21:38:51.684834  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0815881 (* 1 = 0.0815881 loss)
I1022 21:38:51.684839  3959 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1022 21:39:08.836061  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:39:11.639616  3959 solver.cpp:330] Iteration 8200, Testing net (#0)
I1022 21:39:13.677958  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:39:14.217739  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.865234
I1022 21:39:14.217802  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.395996 (* 1 = 0.395996 loss)
I1022 21:39:14.420620  3959 solver.cpp:218] Iteration 8200 (4.39836 iter/s, 22.7357s/100 iters), loss = 0.493081
I1022 21:39:14.420650  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.49308 (* 1 = 0.49308 loss)
I1022 21:39:14.420656  3959 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1022 21:39:34.564528  3959 solver.cpp:218] Iteration 8300 (4.9643 iter/s, 20.1438s/100 iters), loss = 0.00578754
I1022 21:39:34.564558  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0057866 (* 1 = 0.0057866 loss)
I1022 21:39:34.564564  3959 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1022 21:39:54.514314  3959 solver.cpp:330] Iteration 8400, Testing net (#0)
I1022 21:39:56.551846  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:39:57.092700  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.880859
I1022 21:39:57.092763  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.441201 (* 1 = 0.441201 loss)
I1022 21:39:57.294667  3959 solver.cpp:218] Iteration 8400 (4.39946 iter/s, 22.7301s/100 iters), loss = 0.22352
I1022 21:39:57.294697  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223519 (* 1 = 0.223519 loss)
I1022 21:39:57.294704  3959 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1022 21:40:17.438691  3959 solver.cpp:218] Iteration 8500 (4.96427 iter/s, 20.1439s/100 iters), loss = 0.0878148
I1022 21:40:17.438721  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0878139 (* 1 = 0.0878139 loss)
I1022 21:40:17.438727  3959 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1022 21:40:22.898449  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:40:37.384658  3959 solver.cpp:330] Iteration 8600, Testing net (#0)
I1022 21:40:39.384600  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:40:39.963579  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917969
I1022 21:40:39.963644  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.239207 (* 1 = 0.239207 loss)
I1022 21:40:40.165776  3959 solver.cpp:218] Iteration 8600 (4.40005 iter/s, 22.727s/100 iters), loss = 0.316645
I1022 21:40:40.165803  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316644 (* 1 = 0.316644 loss)
I1022 21:40:40.165810  3959 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1022 21:41:00.312608  3959 solver.cpp:218] Iteration 8700 (4.96358 iter/s, 20.1467s/100 iters), loss = 0.748749
I1022 21:41:00.312641  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.748748 (* 1 = 0.748748 loss)
I1022 21:41:00.312647  3959 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1022 21:41:20.260692  3959 solver.cpp:330] Iteration 8800, Testing net (#0)
I1022 21:41:22.261044  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:41:22.840338  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.894531
I1022 21:41:22.840399  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.327143 (* 1 = 0.327143 loss)
I1022 21:41:23.042448  3959 solver.cpp:218] Iteration 8800 (4.39952 iter/s, 22.7297s/100 iters), loss = 0.0107582
I1022 21:41:23.042477  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107571 (* 1 = 0.0107571 loss)
I1022 21:41:23.042484  3959 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1022 21:41:36.764431  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:41:43.196913  3959 solver.cpp:218] Iteration 8900 (4.9617 iter/s, 20.1544s/100 iters), loss = 0.103445
I1022 21:41:43.196947  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103444 (* 1 = 0.103444 loss)
I1022 21:41:43.196954  3959 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1022 21:42:03.139773  3959 solver.cpp:330] Iteration 9000, Testing net (#0)
I1022 21:42:05.137862  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:42:05.718153  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1022 21:42:05.718217  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.128232 (* 1 = 0.128232 loss)
I1022 21:42:05.920075  3959 solver.cpp:218] Iteration 9000 (4.40081 iter/s, 22.7231s/100 iters), loss = 0.138245
I1022 21:42:05.920105  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138244 (* 1 = 0.138244 loss)
I1022 21:42:05.920112  3959 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1022 21:42:26.061967  3959 solver.cpp:218] Iteration 9100 (4.9648 iter/s, 20.1418s/100 iters), loss = 0.107088
I1022 21:42:26.061998  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107087 (* 1 = 0.107087 loss)
I1022 21:42:26.062006  3959 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1022 21:42:46.005401  3959 solver.cpp:330] Iteration 9200, Testing net (#0)
I1022 21:42:48.003221  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:42:48.585079  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.822266
I1022 21:42:48.585144  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.511792 (* 1 = 0.511792 loss)
I1022 21:42:48.787715  3959 solver.cpp:218] Iteration 9200 (4.40031 iter/s, 22.7257s/100 iters), loss = 0.0416891
I1022 21:42:48.787745  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0416882 (* 1 = 0.0416882 loss)
I1022 21:42:48.787752  3959 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1022 21:42:50.623664  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:43:08.937564  3959 solver.cpp:218] Iteration 9300 (4.96284 iter/s, 20.1498s/100 iters), loss = 0.789138
I1022 21:43:08.937595  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.789137 (* 1 = 0.789137 loss)
I1022 21:43:08.937602  3959 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1022 21:43:28.889257  3959 solver.cpp:330] Iteration 9400, Testing net (#0)
I1022 21:43:30.849608  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:43:31.468271  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.943359
I1022 21:43:31.468327  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.16789 (* 1 = 0.16789 loss)
I1022 21:43:31.670601  3959 solver.cpp:218] Iteration 9400 (4.3989 iter/s, 22.733s/100 iters), loss = 0.00796545
I1022 21:43:31.670631  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00796448 (* 1 = 0.00796448 loss)
I1022 21:43:31.670639  3959 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1022 21:43:51.819753  3959 solver.cpp:218] Iteration 9500 (4.96301 iter/s, 20.1491s/100 iters), loss = 0.00180714
I1022 21:43:51.819782  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0018062 (* 1 = 0.0018062 loss)
I1022 21:43:51.819789  3959 sgd_solver.cpp:105] Iteration 9500, lr = 0.01
I1022 21:44:01.921151  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:44:11.770736  3959 solver.cpp:330] Iteration 9600, Testing net (#0)
I1022 21:44:13.730767  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:44:14.350723  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.742188
I1022 21:44:14.350776  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.869342 (* 1 = 0.869342 loss)
I1022 21:44:14.552796  3959 solver.cpp:218] Iteration 9600 (4.3989 iter/s, 22.733s/100 iters), loss = 0.00365677
I1022 21:44:14.552826  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0036558 (* 1 = 0.0036558 loss)
I1022 21:44:14.552832  3959 sgd_solver.cpp:105] Iteration 9600, lr = 0.01
I1022 21:44:34.697069  3959 solver.cpp:218] Iteration 9700 (4.96421 iter/s, 20.1442s/100 iters), loss = 0.0952773
I1022 21:44:34.697212  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0952764 (* 1 = 0.0952764 loss)
I1022 21:44:34.697221  3959 sgd_solver.cpp:105] Iteration 9700, lr = 0.01
I1022 21:44:54.642467  3959 solver.cpp:330] Iteration 9800, Testing net (#0)
I1022 21:44:56.600240  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:44:57.221298  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.9375
I1022 21:44:57.221359  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.185101 (* 1 = 0.185101 loss)
I1022 21:44:57.423162  3959 solver.cpp:218] Iteration 9800 (4.40027 iter/s, 22.7259s/100 iters), loss = 0.195382
I1022 21:44:57.423192  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195381 (* 1 = 0.195381 loss)
I1022 21:44:57.423197  3959 sgd_solver.cpp:105] Iteration 9800, lr = 0.01
I1022 21:45:15.972460  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:45:17.561739  3959 solver.cpp:218] Iteration 9900 (4.96561 iter/s, 20.1385s/100 iters), loss = 0.0808739
I1022 21:45:17.561770  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.080873 (* 1 = 0.080873 loss)
I1022 21:45:17.561777  3959 sgd_solver.cpp:105] Iteration 9900, lr = 0.01
I1022 21:45:37.496064  3959 solver.cpp:330] Iteration 10000, Testing net (#0)
I1022 21:45:39.453529  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:45:40.075059  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914062
I1022 21:45:40.075121  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.240351 (* 1 = 0.240351 loss)
I1022 21:45:40.277038  3959 solver.cpp:218] Iteration 10000 (4.40234 iter/s, 22.7152s/100 iters), loss = 0.433962
I1022 21:45:40.277073  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.433961 (* 1 = 0.433961 loss)
I1022 21:45:40.277079  3959 sgd_solver.cpp:46] MultiStep Status: Iteration 10000, step = 1
I1022 21:45:40.277083  3959 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1022 21:46:00.409924  3959 solver.cpp:218] Iteration 10100 (4.96702 iter/s, 20.1328s/100 iters), loss = 0.021919
I1022 21:46:00.410056  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0219179 (* 1 = 0.0219179 loss)
I1022 21:46:00.410064  3959 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1022 21:46:20.343451  3959 solver.cpp:330] Iteration 10200, Testing net (#0)
I1022 21:46:22.263851  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:46:22.922798  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.927734
I1022 21:46:22.922860  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.253404 (* 1 = 0.253404 loss)
I1022 21:46:23.124663  3959 solver.cpp:218] Iteration 10200 (4.40246 iter/s, 22.7146s/100 iters), loss = 0.0162266
I1022 21:46:23.124696  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0162256 (* 1 = 0.0162256 loss)
I1022 21:46:23.124701  3959 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1022 21:46:29.797312  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:46:43.253499  3959 solver.cpp:218] Iteration 10300 (4.96802 iter/s, 20.1288s/100 iters), loss = 0.00369401
I1022 21:46:43.253648  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00369303 (* 1 = 0.00369303 loss)
I1022 21:46:43.253656  3959 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1022 21:47:03.183907  3959 solver.cpp:330] Iteration 10400, Testing net (#0)
I1022 21:47:05.104310  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:47:05.763952  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.958984
I1022 21:47:05.764017  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.117594 (* 1 = 0.117594 loss)
I1022 21:47:05.966240  3959 solver.cpp:218] Iteration 10400 (4.40285 iter/s, 22.7125s/100 iters), loss = 0.0211622
I1022 21:47:05.966271  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0211613 (* 1 = 0.0211613 loss)
I1022 21:47:05.966279  3959 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1022 21:47:26.098969  3959 solver.cpp:218] Iteration 10500 (4.96706 iter/s, 20.1326s/100 iters), loss = 0.00382148
I1022 21:47:26.099087  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00382059 (* 1 = 0.00382059 loss)
I1022 21:47:26.099095  3959 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1022 21:47:41.025746  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:47:46.039549  3959 solver.cpp:330] Iteration 10600, Testing net (#0)
I1022 21:47:47.958853  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:47:48.619375  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.955078
I1022 21:47:48.619436  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.129276 (* 1 = 0.129276 loss)
I1022 21:47:48.821563  3959 solver.cpp:218] Iteration 10600 (4.40094 iter/s, 22.7224s/100 iters), loss = 0.0180693
I1022 21:47:48.821595  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0180684 (* 1 = 0.0180684 loss)
I1022 21:47:48.821602  3959 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1022 21:48:08.950167  3959 solver.cpp:218] Iteration 10700 (4.96808 iter/s, 20.1285s/100 iters), loss = 0.00235011
I1022 21:48:08.950295  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0023492 (* 1 = 0.0023492 loss)
I1022 21:48:08.950304  3959 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1022 21:48:28.887280  3959 solver.cpp:330] Iteration 10800, Testing net (#0)
I1022 21:48:30.804219  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:48:31.466670  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.957031
I1022 21:48:31.466732  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.113792 (* 1 = 0.113792 loss)
I1022 21:48:31.668748  3959 solver.cpp:218] Iteration 10800 (4.40172 iter/s, 22.7184s/100 iters), loss = 0.000460997
I1022 21:48:31.668790  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000460096 (* 1 = 0.000460096 loss)
I1022 21:48:31.668797  3959 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1022 21:48:51.797247  3959 solver.cpp:218] Iteration 10900 (4.9681 iter/s, 20.1284s/100 iters), loss = 0.00800179
I1022 21:48:51.797397  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00800088 (* 1 = 0.00800088 loss)
I1022 21:48:51.797408  3959 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1022 21:48:54.846843  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:49:11.729734  3959 solver.cpp:330] Iteration 11000, Testing net (#0)
I1022 21:49:13.608361  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:49:14.308586  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.964844
I1022 21:49:14.308648  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0923689 (* 1 = 0.0923689 loss)
I1022 21:49:14.510579  3959 solver.cpp:218] Iteration 11000 (4.40274 iter/s, 22.7131s/100 iters), loss = 0.0998346
I1022 21:49:14.510610  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0998336 (* 1 = 0.0998336 loss)
I1022 21:49:14.510617  3959 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1022 21:49:34.642922  3959 solver.cpp:218] Iteration 11100 (4.96715 iter/s, 20.1323s/100 iters), loss = 0.0824107
I1022 21:49:34.643064  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0824098 (* 1 = 0.0824098 loss)
I1022 21:49:34.643074  3959 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1022 21:49:54.577517  3959 solver.cpp:330] Iteration 11200, Testing net (#0)
I1022 21:49:56.455874  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:49:57.156997  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.982422
I1022 21:49:57.157058  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0639393 (* 1 = 0.0639393 loss)
I1022 21:49:57.359176  3959 solver.cpp:218] Iteration 11200 (4.40217 iter/s, 22.7161s/100 iters), loss = 0.00532428
I1022 21:49:57.359210  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00532344 (* 1 = 0.00532344 loss)
I1022 21:49:57.359217  3959 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1022 21:50:08.863482  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:50:17.499398  3959 solver.cpp:218] Iteration 11300 (4.96521 iter/s, 20.1401s/100 iters), loss = 0.0177697
I1022 21:50:17.499429  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0177689 (* 1 = 0.0177689 loss)
I1022 21:50:17.499436  3959 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1022 21:50:37.426060  3959 solver.cpp:330] Iteration 11400, Testing net (#0)
I1022 21:50:39.303303  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:50:40.004863  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 21:50:40.004925  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.049077 (* 1 = 0.049077 loss)
I1022 21:50:40.206495  3959 solver.cpp:218] Iteration 11400 (4.40393 iter/s, 22.707s/100 iters), loss = 0.0460504
I1022 21:50:40.206527  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0460496 (* 1 = 0.0460496 loss)
I1022 21:50:40.206533  3959 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1022 21:51:00.330883  3959 solver.cpp:218] Iteration 11500 (4.96912 iter/s, 20.1243s/100 iters), loss = 0.00179608
I1022 21:51:00.330916  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00179519 (* 1 = 0.00179519 loss)
I1022 21:51:00.330924  3959 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1022 21:51:20.080278  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:51:20.259341  3959 solver.cpp:330] Iteration 11600, Testing net (#0)
I1022 21:51:22.137378  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:51:22.839488  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.976562
I1022 21:51:22.839548  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0668507 (* 1 = 0.0668507 loss)
I1022 21:51:23.041371  3959 solver.cpp:218] Iteration 11600 (4.40327 iter/s, 22.7104s/100 iters), loss = 0.00602774
I1022 21:51:23.041404  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00602685 (* 1 = 0.00602685 loss)
I1022 21:51:23.041410  3959 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1022 21:51:43.175724  3959 solver.cpp:218] Iteration 11700 (4.96666 iter/s, 20.1343s/100 iters), loss = 0.106639
I1022 21:51:43.175756  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106638 (* 1 = 0.106638 loss)
I1022 21:51:43.175763  3959 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1022 21:52:03.113896  3959 solver.cpp:330] Iteration 11800, Testing net (#0)
I1022 21:52:04.953229  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:52:05.693315  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 21:52:05.693374  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0466764 (* 1 = 0.0466764 loss)
I1022 21:52:05.895715  3959 solver.cpp:218] Iteration 11800 (4.40143 iter/s, 22.7199s/100 iters), loss = 0.019704
I1022 21:52:05.895758  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0197032 (* 1 = 0.0197032 loss)
I1022 21:52:05.895766  3959 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1022 21:52:26.033329  3959 solver.cpp:218] Iteration 11900 (4.96586 iter/s, 20.1375s/100 iters), loss = 0.0227054
I1022 21:52:26.033361  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0227046 (* 1 = 0.0227046 loss)
I1022 21:52:26.033368  3959 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1022 21:52:33.916256  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:52:45.975034  3959 solver.cpp:330] Iteration 12000, Testing net (#0)
I1022 21:52:47.812995  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:52:48.555048  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.970703
I1022 21:52:48.555102  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0741716 (* 1 = 0.0741716 loss)
I1022 21:52:48.756753  3959 solver.cpp:218] Iteration 12000 (4.40076 iter/s, 22.7233s/100 iters), loss = 0.0231915
I1022 21:52:48.756784  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0231906 (* 1 = 0.0231906 loss)
I1022 21:52:48.756790  3959 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1022 21:53:08.890336  3959 solver.cpp:218] Iteration 12100 (4.96685 iter/s, 20.1335s/100 iters), loss = 0.00766696
I1022 21:53:08.890485  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0076661 (* 1 = 0.0076661 loss)
I1022 21:53:08.890493  3959 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1022 21:53:28.826467  3959 solver.cpp:330] Iteration 12200, Testing net (#0)
I1022 21:53:30.664518  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:53:31.407301  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.992188
I1022 21:53:31.407362  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0438616 (* 1 = 0.0438616 loss)
I1022 21:53:31.609340  3959 solver.cpp:218] Iteration 12200 (4.40164 iter/s, 22.7188s/100 iters), loss = 0.0101107
I1022 21:53:31.609371  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0101098 (* 1 = 0.0101098 loss)
I1022 21:53:31.609378  3959 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1022 21:53:47.746752  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:53:51.753844  3959 solver.cpp:218] Iteration 12300 (4.96415 iter/s, 20.1444s/100 iters), loss = 0.0654679
I1022 21:53:51.753876  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0654671 (* 1 = 0.0654671 loss)
I1022 21:53:51.753883  3959 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1022 21:54:11.689726  3959 solver.cpp:330] Iteration 12400, Testing net (#0)
I1022 21:54:13.526351  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:54:14.268872  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 21:54:14.268939  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0536687 (* 1 = 0.0536687 loss)
I1022 21:54:14.471050  3959 solver.cpp:218] Iteration 12400 (4.40197 iter/s, 22.7171s/100 iters), loss = 0.0123043
I1022 21:54:14.471081  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123034 (* 1 = 0.0123034 loss)
I1022 21:54:14.471087  3959 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1022 21:54:34.605322  3959 solver.cpp:218] Iteration 12500 (4.96668 iter/s, 20.1342s/100 iters), loss = 0.0113358
I1022 21:54:34.605473  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.011335 (* 1 = 0.011335 loss)
I1022 21:54:34.605496  3959 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1022 21:54:54.543565  3959 solver.cpp:330] Iteration 12600, Testing net (#0)
I1022 21:54:56.342641  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:54:57.121824  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.992188
I1022 21:54:57.121891  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0470179 (* 1 = 0.0470179 loss)
I1022 21:54:57.323771  3959 solver.cpp:218] Iteration 12600 (4.40175 iter/s, 22.7182s/100 iters), loss = 0.140077
I1022 21:54:57.323807  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140077 (* 1 = 0.140077 loss)
I1022 21:54:57.323813  3959 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1022 21:55:01.780591  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:55:17.455473  3959 solver.cpp:218] Iteration 12700 (4.96731 iter/s, 20.1316s/100 iters), loss = 0.000761061
I1022 21:55:17.455621  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000760208 (* 1 = 0.000760208 loss)
I1022 21:55:17.455631  3959 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1022 21:55:37.388156  3959 solver.cpp:330] Iteration 12800, Testing net (#0)
I1022 21:55:39.186035  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:55:39.967456  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 21:55:39.967525  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0372327 (* 1 = 0.0372327 loss)
I1022 21:55:40.169437  3959 solver.cpp:218] Iteration 12800 (4.40262 iter/s, 22.7138s/100 iters), loss = 0.0399523
I1022 21:55:40.169467  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0399515 (* 1 = 0.0399515 loss)
I1022 21:55:40.169473  3959 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1022 21:56:00.420236  3959 solver.cpp:218] Iteration 12900 (4.9381 iter/s, 20.2507s/100 iters), loss = 0.00845438
I1022 21:56:00.420383  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00845349 (* 1 = 0.00845349 loss)
I1022 21:56:00.420393  3959 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1022 21:56:13.251039  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:56:20.532351  3959 solver.cpp:330] Iteration 13000, Testing net (#0)
I1022 21:56:22.328274  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:56:23.110065  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 21:56:23.110128  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0502988 (* 1 = 0.0502988 loss)
I1022 21:56:23.312075  3959 solver.cpp:218] Iteration 13000 (4.36841 iter/s, 22.8916s/100 iters), loss = 0.00138935
I1022 21:56:23.312113  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00138848 (* 1 = 0.00138848 loss)
I1022 21:56:23.312119  3959 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1022 21:56:43.438859  3959 solver.cpp:218] Iteration 13100 (4.96852 iter/s, 20.1267s/100 iters), loss = 0.00620087
I1022 21:56:43.439038  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00620003 (* 1 = 0.00620003 loss)
I1022 21:56:43.439049  3959 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1022 21:57:03.365628  3959 solver.cpp:330] Iteration 13200, Testing net (#0)
I1022 21:57:05.160971  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:57:05.943480  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.992188
I1022 21:57:05.943534  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0449099 (* 1 = 0.0449099 loss)
I1022 21:57:06.145380  3959 solver.cpp:218] Iteration 13200 (4.40407 iter/s, 22.7063s/100 iters), loss = 0.000688369
I1022 21:57:06.145413  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00068752 (* 1 = 0.00068752 loss)
I1022 21:57:06.145421  3959 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1022 21:57:26.267102  3959 solver.cpp:218] Iteration 13300 (4.96978 iter/s, 20.1216s/100 iters), loss = 0.00327212
I1022 21:57:26.267230  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00327122 (* 1 = 0.00327122 loss)
I1022 21:57:26.267248  3959 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1022 21:57:27.099589  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:57:46.190383  3959 solver.cpp:330] Iteration 13400, Testing net (#0)
I1022 21:57:47.947206  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:57:48.767429  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.972656
I1022 21:57:48.767488  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0703309 (* 1 = 0.0703309 loss)
I1022 21:57:48.969465  3959 solver.cpp:218] Iteration 13400 (4.40486 iter/s, 22.7022s/100 iters), loss = 0.00285518
I1022 21:57:48.969497  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00285434 (* 1 = 0.00285434 loss)
I1022 21:57:48.969504  3959 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1022 21:58:09.097179  3959 solver.cpp:218] Iteration 13500 (4.9683 iter/s, 20.1276s/100 iters), loss = 0.0279647
I1022 21:58:09.097326  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0279639 (* 1 = 0.0279639 loss)
I1022 21:58:09.097334  3959 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1022 21:58:29.022953  3959 solver.cpp:330] Iteration 13600, Testing net (#0)
I1022 21:58:30.778506  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:58:31.599182  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 21:58:31.599246  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.05013 (* 1 = 0.05013 loss)
I1022 21:58:31.801412  3959 solver.cpp:218] Iteration 13600 (4.4045 iter/s, 22.704s/100 iters), loss = 0.0115556
I1022 21:58:31.801445  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0115548 (* 1 = 0.0115548 loss)
I1022 21:58:31.801450  3959 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1022 21:58:40.883970  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:58:51.927883  3959 solver.cpp:218] Iteration 13700 (4.9686 iter/s, 20.1264s/100 iters), loss = 0.00965748
I1022 21:58:51.927916  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00965654 (* 1 = 0.00965654 loss)
I1022 21:58:51.927923  3959 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1022 21:59:11.854398  3959 solver.cpp:330] Iteration 13800, Testing net (#0)
I1022 21:59:13.609741  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:59:14.430768  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 21:59:14.430824  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.041835 (* 1 = 0.041835 loss)
I1022 21:59:14.632521  3959 solver.cpp:218] Iteration 13800 (4.4044 iter/s, 22.7045s/100 iters), loss = 0.0175389
I1022 21:59:14.632552  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175379 (* 1 = 0.0175379 loss)
I1022 21:59:14.632560  3959 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1022 21:59:34.752104  3959 solver.cpp:218] Iteration 13900 (4.9703 iter/s, 20.1195s/100 iters), loss = 0.00636042
I1022 21:59:34.752146  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0063595 (* 1 = 0.0063595 loss)
I1022 21:59:34.752153  3959 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1022 21:59:52.279497  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:59:54.673025  3959 solver.cpp:330] Iteration 14000, Testing net (#0)
I1022 21:59:56.427067  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 21:59:57.249438  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 21:59:57.249498  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0416944 (* 1 = 0.0416944 loss)
I1022 21:59:57.451915  3959 solver.cpp:218] Iteration 14000 (4.40534 iter/s, 22.6997s/100 iters), loss = 0.0109826
I1022 21:59:57.451952  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109817 (* 1 = 0.0109817 loss)
I1022 21:59:57.451963  3959 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1022 22:00:17.579856  3959 solver.cpp:218] Iteration 14100 (4.96824 iter/s, 20.1279s/100 iters), loss = 0.210456
I1022 22:00:17.579900  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210455 (* 1 = 0.210455 loss)
I1022 22:00:17.579907  3959 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1022 22:00:37.506027  3959 solver.cpp:330] Iteration 14200, Testing net (#0)
I1022 22:00:39.223353  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:00:40.082828  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 22:00:40.082895  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0434703 (* 1 = 0.0434703 loss)
I1022 22:00:40.285331  3959 solver.cpp:218] Iteration 14200 (4.40424 iter/s, 22.7054s/100 iters), loss = 0.001024
I1022 22:00:40.285365  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00102307 (* 1 = 0.00102307 loss)
I1022 22:00:40.285373  3959 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1022 22:01:00.415233  3959 solver.cpp:218] Iteration 14300 (4.96776 iter/s, 20.1298s/100 iters), loss = 0.00874566
I1022 22:01:00.415276  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0087447 (* 1 = 0.0087447 loss)
I1022 22:01:00.415282  3959 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1022 22:01:06.075608  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:01:20.344614  3959 solver.cpp:330] Iteration 14400, Testing net (#0)
I1022 22:01:22.062173  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:01:22.923612  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:01:22.923667  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0382548 (* 1 = 0.0382548 loss)
I1022 22:01:23.125584  3959 solver.cpp:218] Iteration 14400 (4.4033 iter/s, 22.7103s/100 iters), loss = 0.0046177
I1022 22:01:23.125617  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00461674 (* 1 = 0.00461674 loss)
I1022 22:01:23.125623  3959 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1022 22:01:43.248375  3959 solver.cpp:218] Iteration 14500 (4.96951 iter/s, 20.1227s/100 iters), loss = 0.00280704
I1022 22:01:43.248409  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00280608 (* 1 = 0.00280608 loss)
I1022 22:01:43.248415  3959 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1022 22:02:03.173202  3959 solver.cpp:330] Iteration 14600, Testing net (#0)
I1022 22:02:04.888352  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:02:05.749490  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:02:05.749544  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0431232 (* 1 = 0.0431232 loss)
I1022 22:02:05.951557  3959 solver.cpp:218] Iteration 14600 (4.40469 iter/s, 22.7031s/100 iters), loss = 0.00744484
I1022 22:02:05.951591  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00744386 (* 1 = 0.00744386 loss)
I1022 22:02:05.951596  3959 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1022 22:02:19.864878  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:02:26.080271  3959 solver.cpp:218] Iteration 14700 (4.96805 iter/s, 20.1286s/100 iters), loss = 0.0206977
I1022 22:02:26.080302  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0206967 (* 1 = 0.0206967 loss)
I1022 22:02:26.080310  3959 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1022 22:02:46.008610  3959 solver.cpp:330] Iteration 14800, Testing net (#0)
I1022 22:02:47.722929  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:02:48.585075  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:02:48.585129  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0493969 (* 1 = 0.0493969 loss)
I1022 22:02:48.787222  3959 solver.cpp:218] Iteration 14800 (4.40395 iter/s, 22.7069s/100 iters), loss = 0.00661754
I1022 22:02:48.787255  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00661657 (* 1 = 0.00661657 loss)
I1022 22:02:48.787261  3959 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1022 22:03:08.916360  3959 solver.cpp:218] Iteration 14900 (4.96794 iter/s, 20.1291s/100 iters), loss = 0.00300374
I1022 22:03:08.916405  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00300276 (* 1 = 0.00300276 loss)
I1022 22:03:08.916412  3959 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1022 22:03:28.846999  3959 solver.cpp:330] Iteration 15000, Testing net (#0)
I1022 22:03:30.523947  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:03:31.423529  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:03:31.423593  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0510184 (* 1 = 0.0510184 loss)
I1022 22:03:31.625465  3959 solver.cpp:218] Iteration 15000 (4.40354 iter/s, 22.709s/100 iters), loss = 0.00585694
I1022 22:03:31.625496  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00585596 (* 1 = 0.00585596 loss)
I1022 22:03:31.625502  3959 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1022 22:03:33.667232  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:03:51.750171  3959 solver.cpp:218] Iteration 15100 (4.96904 iter/s, 20.1246s/100 iters), loss = 0.00858127
I1022 22:03:51.750212  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00858029 (* 1 = 0.00858029 loss)
I1022 22:03:51.750219  3959 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1022 22:04:11.672603  3959 solver.cpp:330] Iteration 15200, Testing net (#0)
I1022 22:04:13.348732  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:04:14.249788  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.992188
I1022 22:04:14.249855  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0398603 (* 1 = 0.0398603 loss)
I1022 22:04:14.451966  3959 solver.cpp:218] Iteration 15200 (4.40496 iter/s, 22.7017s/100 iters), loss = 0.00635369
I1022 22:04:14.451998  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0063527 (* 1 = 0.0063527 loss)
I1022 22:04:14.452004  3959 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1022 22:04:34.578652  3959 solver.cpp:218] Iteration 15300 (4.96855 iter/s, 20.1266s/100 iters), loss = 0.0171727
I1022 22:04:34.578685  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171717 (* 1 = 0.0171717 loss)
I1022 22:04:34.578691  3959 sgd_solver.cpp:105] Iteration 15300, lr = 0.001
I1022 22:04:45.068392  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:04:54.509362  3959 solver.cpp:330] Iteration 15400, Testing net (#0)
I1022 22:04:56.184120  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:04:57.085561  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:04:57.085628  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0602426 (* 1 = 0.0602426 loss)
I1022 22:04:57.287685  3959 solver.cpp:218] Iteration 15400 (4.40355 iter/s, 22.7089s/100 iters), loss = 0.000924889
I1022 22:04:57.287729  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000923885 (* 1 = 0.000923885 loss)
I1022 22:04:57.287736  3959 sgd_solver.cpp:105] Iteration 15400, lr = 0.001
I1022 22:05:17.415252  3959 solver.cpp:218] Iteration 15500 (4.96833 iter/s, 20.1275s/100 iters), loss = 0.000211589
I1022 22:05:17.415390  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000210604 (* 1 = 0.000210604 loss)
I1022 22:05:17.415398  3959 sgd_solver.cpp:105] Iteration 15500, lr = 0.001
I1022 22:05:37.341078  3959 solver.cpp:330] Iteration 15600, Testing net (#0)
I1022 22:05:39.016178  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:05:39.919473  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 22:05:39.919533  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0501502 (* 1 = 0.0501502 loss)
I1022 22:05:40.121423  3959 solver.cpp:218] Iteration 15600 (4.40413 iter/s, 22.706s/100 iters), loss = 0.000629637
I1022 22:05:40.121455  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00062865 (* 1 = 0.00062865 loss)
I1022 22:05:40.121461  3959 sgd_solver.cpp:105] Iteration 15600, lr = 0.001
I1022 22:05:58.859767  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:06:00.246246  3959 solver.cpp:218] Iteration 15700 (4.96901 iter/s, 20.1247s/100 iters), loss = 0.00668342
I1022 22:06:00.246280  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00668243 (* 1 = 0.00668243 loss)
I1022 22:06:00.246287  3959 sgd_solver.cpp:105] Iteration 15700, lr = 0.001
I1022 22:06:20.169347  3959 solver.cpp:330] Iteration 15800, Testing net (#0)
I1022 22:06:21.806129  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:06:22.748625  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:06:22.748685  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0394675 (* 1 = 0.0394675 loss)
I1022 22:06:22.950526  3959 solver.cpp:218] Iteration 15800 (4.40447 iter/s, 22.7042s/100 iters), loss = 0.00153854
I1022 22:06:22.950561  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00153756 (* 1 = 0.00153756 loss)
I1022 22:06:22.950567  3959 sgd_solver.cpp:105] Iteration 15800, lr = 0.001
I1022 22:06:43.077522  3959 solver.cpp:218] Iteration 15900 (4.96847 iter/s, 20.1269s/100 iters), loss = 0.00415538
I1022 22:06:43.077646  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00415443 (* 1 = 0.00415443 loss)
I1022 22:06:43.077664  3959 sgd_solver.cpp:105] Iteration 15900, lr = 0.001
I1022 22:07:03.004439  3959 solver.cpp:330] Iteration 16000, Testing net (#0)
I1022 22:07:04.639391  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:07:05.580605  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:07:05.580663  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0496949 (* 1 = 0.0496949 loss)
I1022 22:07:05.782984  3959 solver.cpp:218] Iteration 16000 (4.40426 iter/s, 22.7053s/100 iters), loss = 0.0048275
I1022 22:07:05.783016  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00482654 (* 1 = 0.00482654 loss)
I1022 22:07:05.783022  3959 sgd_solver.cpp:105] Iteration 16000, lr = 0.001
I1022 22:07:12.652997  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:07:25.913257  3959 solver.cpp:218] Iteration 16100 (4.96766 iter/s, 20.1302s/100 iters), loss = 0.00125002
I1022 22:07:25.913358  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00124906 (* 1 = 0.00124906 loss)
I1022 22:07:25.913367  3959 sgd_solver.cpp:105] Iteration 16100, lr = 0.001
I1022 22:07:45.842934  3959 solver.cpp:330] Iteration 16200, Testing net (#0)
I1022 22:07:47.477818  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:07:48.419831  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:07:48.419898  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0343256 (* 1 = 0.0343256 loss)
I1022 22:07:48.621394  3959 solver.cpp:218] Iteration 16200 (4.40374 iter/s, 22.708s/100 iters), loss = 0.0258237
I1022 22:07:48.621426  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0258228 (* 1 = 0.0258228 loss)
I1022 22:07:48.621433  3959 sgd_solver.cpp:105] Iteration 16200, lr = 0.001
I1022 22:08:08.746695  3959 solver.cpp:218] Iteration 16300 (4.96889 iter/s, 20.1252s/100 iters), loss = 0.079064
I1022 22:08:08.746870  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0790631 (* 1 = 0.0790631 loss)
I1022 22:08:08.746881  3959 sgd_solver.cpp:105] Iteration 16300, lr = 0.001
I1022 22:08:23.864953  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:08:28.672948  3959 solver.cpp:330] Iteration 16400, Testing net (#0)
I1022 22:08:30.306835  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:08:31.250124  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.982422
I1022 22:08:31.250186  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.051651 (* 1 = 0.051651 loss)
I1022 22:08:31.452291  3959 solver.cpp:218] Iteration 16400 (4.40424 iter/s, 22.7054s/100 iters), loss = 0.00226771
I1022 22:08:31.452332  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00226677 (* 1 = 0.00226677 loss)
I1022 22:08:31.452339  3959 sgd_solver.cpp:105] Iteration 16400, lr = 0.001
I1022 22:08:51.578758  3959 solver.cpp:218] Iteration 16500 (4.96861 iter/s, 20.1264s/100 iters), loss = 0.0740206
I1022 22:08:51.578900  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0740197 (* 1 = 0.0740197 loss)
I1022 22:08:51.578909  3959 sgd_solver.cpp:105] Iteration 16500, lr = 0.001
I1022 22:09:11.505290  3959 solver.cpp:330] Iteration 16600, Testing net (#0)
I1022 22:09:13.101615  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:09:14.082100  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:09:14.082157  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0411577 (* 1 = 0.0411577 loss)
I1022 22:09:14.284476  3959 solver.cpp:218] Iteration 16600 (4.40422 iter/s, 22.7055s/100 iters), loss = 0.00509697
I1022 22:09:14.284507  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00509606 (* 1 = 0.00509606 loss)
I1022 22:09:14.284513  3959 sgd_solver.cpp:105] Iteration 16600, lr = 0.001
I1022 22:09:34.411950  3959 solver.cpp:218] Iteration 16700 (4.96835 iter/s, 20.1274s/100 iters), loss = 0.0028379
I1022 22:09:34.412098  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00283701 (* 1 = 0.00283701 loss)
I1022 22:09:34.412107  3959 sgd_solver.cpp:105] Iteration 16700, lr = 0.001
I1022 22:09:37.859527  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:09:54.343035  3959 solver.cpp:330] Iteration 16800, Testing net (#0)
I1022 22:09:55.939513  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:09:56.919989  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 22:09:56.920058  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0460902 (* 1 = 0.0460902 loss)
I1022 22:09:57.121553  3959 solver.cpp:218] Iteration 16800 (4.40346 iter/s, 22.7094s/100 iters), loss = 0.000991018
I1022 22:09:57.121594  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000990127 (* 1 = 0.000990127 loss)
I1022 22:09:57.121601  3959 sgd_solver.cpp:105] Iteration 16800, lr = 0.001
I1022 22:10:17.247428  3959 solver.cpp:218] Iteration 16900 (4.96875 iter/s, 20.1258s/100 iters), loss = 0.00698658
I1022 22:10:17.247540  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0069857 (* 1 = 0.0069857 loss)
I1022 22:10:17.247550  3959 sgd_solver.cpp:105] Iteration 16900, lr = 0.001
I1022 22:10:37.173813  3959 solver.cpp:330] Iteration 17000, Testing net (#0)
I1022 22:10:38.769239  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:10:39.751029  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 22:10:39.751078  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0421133 (* 1 = 0.0421133 loss)
I1022 22:10:39.952986  3959 solver.cpp:218] Iteration 17000 (4.40424 iter/s, 22.7054s/100 iters), loss = 0.000371617
I1022 22:10:39.953016  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000370749 (* 1 = 0.000370749 loss)
I1022 22:10:39.953023  3959 sgd_solver.cpp:105] Iteration 17000, lr = 0.001
I1022 22:10:51.654026  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:11:00.085706  3959 solver.cpp:218] Iteration 17100 (4.96706 iter/s, 20.1326s/100 iters), loss = 0.0196673
I1022 22:11:00.085738  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0196665 (* 1 = 0.0196665 loss)
I1022 22:11:00.085744  3959 sgd_solver.cpp:105] Iteration 17100, lr = 0.001
I1022 22:11:20.015699  3959 solver.cpp:330] Iteration 17200, Testing net (#0)
I1022 22:11:21.610440  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:11:22.592687  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.992188
I1022 22:11:22.592825  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0328057 (* 1 = 0.0328057 loss)
I1022 22:11:22.794963  3959 solver.cpp:218] Iteration 17200 (4.40351 iter/s, 22.7092s/100 iters), loss = 0.00210795
I1022 22:11:22.794994  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00210709 (* 1 = 0.00210709 loss)
I1022 22:11:22.795001  3959 sgd_solver.cpp:105] Iteration 17200, lr = 0.001
I1022 22:11:42.926154  3959 solver.cpp:218] Iteration 17300 (4.96744 iter/s, 20.1311s/100 iters), loss = 0.00146904
I1022 22:11:42.926187  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00146822 (* 1 = 0.00146822 loss)
I1022 22:11:42.926194  3959 sgd_solver.cpp:105] Iteration 17300, lr = 0.001
I1022 22:12:02.857553  3959 solver.cpp:330] Iteration 17400, Testing net (#0)
I1022 22:12:04.413686  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:12:05.434373  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:12:05.434430  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.040809 (* 1 = 0.040809 loss)
I1022 22:12:05.457329  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:12:05.636294  3959 solver.cpp:218] Iteration 17400 (4.40334 iter/s, 22.7101s/100 iters), loss = 0.106885
I1022 22:12:05.636328  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106884 (* 1 = 0.106884 loss)
I1022 22:12:05.636335  3959 sgd_solver.cpp:105] Iteration 17400, lr = 0.001
I1022 22:12:25.756139  3959 solver.cpp:218] Iteration 17500 (4.97024 iter/s, 20.1198s/100 iters), loss = 0.223588
I1022 22:12:25.756170  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.223587 (* 1 = 0.223587 loss)
I1022 22:12:25.756177  3959 sgd_solver.cpp:105] Iteration 17500, lr = 0.001
I1022 22:12:45.679903  3959 solver.cpp:330] Iteration 17600, Testing net (#0)
I1022 22:12:47.235201  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:12:48.256870  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:12:48.256932  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0331075 (* 1 = 0.0331075 loss)
I1022 22:12:48.458782  3959 solver.cpp:218] Iteration 17600 (4.40479 iter/s, 22.7026s/100 iters), loss = 0.00813141
I1022 22:12:48.458827  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00813061 (* 1 = 0.00813061 loss)
I1022 22:12:48.458834  3959 sgd_solver.cpp:105] Iteration 17600, lr = 0.001
I1022 22:13:08.591990  3959 solver.cpp:218] Iteration 17700 (4.96694 iter/s, 20.1331s/100 iters), loss = 0.0927686
I1022 22:13:08.592023  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0927678 (* 1 = 0.0927678 loss)
I1022 22:13:08.592031  3959 sgd_solver.cpp:105] Iteration 17700, lr = 0.001
I1022 22:13:16.671656  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:13:28.524788  3959 solver.cpp:330] Iteration 17800, Testing net (#0)
I1022 22:13:30.079601  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:13:31.102017  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:13:31.102077  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0457679 (* 1 = 0.0457679 loss)
I1022 22:13:31.304150  3959 solver.cpp:218] Iteration 17800 (4.40295 iter/s, 22.7121s/100 iters), loss = 0.106294
I1022 22:13:31.304183  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106293 (* 1 = 0.106293 loss)
I1022 22:13:31.304189  3959 sgd_solver.cpp:105] Iteration 17800, lr = 0.001
I1022 22:13:51.431303  3959 solver.cpp:218] Iteration 17900 (4.96843 iter/s, 20.1271s/100 iters), loss = 0.0266732
I1022 22:13:51.431447  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0266724 (* 1 = 0.0266724 loss)
I1022 22:13:51.431455  3959 sgd_solver.cpp:105] Iteration 17900, lr = 0.001
I1022 22:14:11.359755  3959 solver.cpp:330] Iteration 18000, Testing net (#0)
I1022 22:14:12.913739  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:14:13.937335  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:14:13.937388  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0396186 (* 1 = 0.0396186 loss)
I1022 22:14:14.139217  3959 solver.cpp:218] Iteration 18000 (4.40379 iter/s, 22.7077s/100 iters), loss = 0.0223846
I1022 22:14:14.139250  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0223838 (* 1 = 0.0223838 loss)
I1022 22:14:14.139256  3959 sgd_solver.cpp:105] Iteration 18000, lr = 0.001
I1022 22:14:30.664507  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:14:34.263981  3959 solver.cpp:218] Iteration 18100 (4.96902 iter/s, 20.1247s/100 iters), loss = 0.0117059
I1022 22:14:34.264014  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117051 (* 1 = 0.0117051 loss)
I1022 22:14:34.264020  3959 sgd_solver.cpp:105] Iteration 18100, lr = 0.001
I1022 22:14:54.185068  3959 solver.cpp:330] Iteration 18200, Testing net (#0)
I1022 22:14:55.701778  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:14:56.762425  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:14:56.762490  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0438605 (* 1 = 0.0438605 loss)
I1022 22:14:56.964473  3959 solver.cpp:218] Iteration 18200 (4.40521 iter/s, 22.7004s/100 iters), loss = 0.0106748
I1022 22:14:56.964504  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.010674 (* 1 = 0.010674 loss)
I1022 22:14:56.964511  3959 sgd_solver.cpp:105] Iteration 18200, lr = 0.001
I1022 22:15:17.092778  3959 solver.cpp:218] Iteration 18300 (4.96815 iter/s, 20.1282s/100 iters), loss = 0.00344302
I1022 22:15:17.092921  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0034422 (* 1 = 0.0034422 loss)
I1022 22:15:17.092931  3959 sgd_solver.cpp:105] Iteration 18300, lr = 0.001
I1022 22:15:37.020180  3959 solver.cpp:330] Iteration 18400, Testing net (#0)
I1022 22:15:38.535423  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:15:39.596598  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 22:15:39.596657  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0460658 (* 1 = 0.0460658 loss)
I1022 22:15:39.798436  3959 solver.cpp:218] Iteration 18400 (4.40423 iter/s, 22.7055s/100 iters), loss = 0.00526559
I1022 22:15:39.798467  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00526474 (* 1 = 0.00526474 loss)
I1022 22:15:39.798475  3959 sgd_solver.cpp:105] Iteration 18400, lr = 0.001
I1022 22:15:44.454156  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:15:59.926743  3959 solver.cpp:218] Iteration 18500 (4.96815 iter/s, 20.1282s/100 iters), loss = 0.00204866
I1022 22:15:59.926890  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00204784 (* 1 = 0.00204784 loss)
I1022 22:15:59.926899  3959 sgd_solver.cpp:105] Iteration 18500, lr = 0.001
I1022 22:16:19.852458  3959 solver.cpp:330] Iteration 18600, Testing net (#0)
I1022 22:16:21.367084  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:16:22.429592  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:16:22.429654  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0363995 (* 1 = 0.0363995 loss)
I1022 22:16:22.631680  3959 solver.cpp:218] Iteration 18600 (4.40437 iter/s, 22.7047s/100 iters), loss = 0.0367098
I1022 22:16:22.631726  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.036709 (* 1 = 0.036709 loss)
I1022 22:16:22.631733  3959 sgd_solver.cpp:105] Iteration 18600, lr = 0.001
I1022 22:16:42.750982  3959 solver.cpp:218] Iteration 18700 (4.97038 iter/s, 20.1192s/100 iters), loss = 0.0144132
I1022 22:16:42.751148  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144123 (* 1 = 0.0144123 loss)
I1022 22:16:42.751157  3959 sgd_solver.cpp:105] Iteration 18700, lr = 0.001
I1022 22:16:55.654863  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:17:02.673889  3959 solver.cpp:330] Iteration 18800, Testing net (#0)
I1022 22:17:04.187433  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:17:05.250855  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:17:05.250915  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0435106 (* 1 = 0.0435106 loss)
I1022 22:17:05.453233  3959 solver.cpp:218] Iteration 18800 (4.40489 iter/s, 22.702s/100 iters), loss = 0.180898
I1022 22:17:05.453263  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.180897 (* 1 = 0.180897 loss)
I1022 22:17:05.453269  3959 sgd_solver.cpp:105] Iteration 18800, lr = 0.001
I1022 22:17:25.579919  3959 solver.cpp:218] Iteration 18900 (4.96855 iter/s, 20.1266s/100 iters), loss = 0.00309364
I1022 22:17:25.580068  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00309281 (* 1 = 0.00309281 loss)
I1022 22:17:25.580077  3959 sgd_solver.cpp:105] Iteration 18900, lr = 0.001
I1022 22:17:45.510151  3959 solver.cpp:330] Iteration 19000, Testing net (#0)
I1022 22:17:46.985472  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:17:48.086571  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 22:17:48.086637  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0430438 (* 1 = 0.0430438 loss)
I1022 22:17:48.289139  3959 solver.cpp:218] Iteration 19000 (4.40354 iter/s, 22.709s/100 iters), loss = 0.0231474
I1022 22:17:48.289172  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0231465 (* 1 = 0.0231465 loss)
I1022 22:17:48.289180  3959 sgd_solver.cpp:105] Iteration 19000, lr = 0.001
I1022 22:18:08.420370  3959 solver.cpp:218] Iteration 19100 (4.96743 iter/s, 20.1311s/100 iters), loss = 0.00102637
I1022 22:18:08.420516  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00102549 (* 1 = 0.00102549 loss)
I1022 22:18:08.420524  3959 sgd_solver.cpp:105] Iteration 19100, lr = 0.001
I1022 22:18:09.455700  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:18:28.349865  3959 solver.cpp:330] Iteration 19200, Testing net (#0)
I1022 22:18:29.824611  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:18:30.926968  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:18:30.927031  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0496404 (* 1 = 0.0496404 loss)
I1022 22:18:31.128784  3959 solver.cpp:218] Iteration 19200 (4.40369 iter/s, 22.7082s/100 iters), loss = 0.0319072
I1022 22:18:31.128826  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0319063 (* 1 = 0.0319063 loss)
I1022 22:18:31.128834  3959 sgd_solver.cpp:105] Iteration 19200, lr = 0.001
I1022 22:18:51.250494  3959 solver.cpp:218] Iteration 19300 (4.96978 iter/s, 20.1216s/100 iters), loss = 0.0257124
I1022 22:18:51.250636  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0257115 (* 1 = 0.0257115 loss)
I1022 22:18:51.250646  3959 sgd_solver.cpp:105] Iteration 19300, lr = 0.001
I1022 22:19:11.172350  3959 solver.cpp:330] Iteration 19400, Testing net (#0)
I1022 22:19:12.647001  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:19:13.749572  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.988281
I1022 22:19:13.749629  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0516615 (* 1 = 0.0516615 loss)
I1022 22:19:13.951422  3959 solver.cpp:218] Iteration 19400 (4.40514 iter/s, 22.7007s/100 iters), loss = 0.0006525
I1022 22:19:13.951457  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000651603 (* 1 = 0.000651603 loss)
I1022 22:19:13.951464  3959 sgd_solver.cpp:105] Iteration 19400, lr = 0.001
I1022 22:19:23.437481  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:19:34.081720  3959 solver.cpp:218] Iteration 19500 (4.96766 iter/s, 20.1302s/100 iters), loss = 0.070877
I1022 22:19:34.081754  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0708761 (* 1 = 0.0708761 loss)
I1022 22:19:34.081761  3959 sgd_solver.cpp:105] Iteration 19500, lr = 0.001
I1022 22:19:54.010031  3959 solver.cpp:330] Iteration 19600, Testing net (#0)
I1022 22:19:55.483526  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:19:56.587221  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.990234
I1022 22:19:56.587281  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0374527 (* 1 = 0.0374527 loss)
I1022 22:19:56.789268  3959 solver.cpp:218] Iteration 19600 (4.40384 iter/s, 22.7075s/100 iters), loss = 0.0332494
I1022 22:19:56.789296  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0332485 (* 1 = 0.0332485 loss)
I1022 22:19:56.789304  3959 sgd_solver.cpp:105] Iteration 19600, lr = 0.001
I1022 22:20:16.916437  3959 solver.cpp:218] Iteration 19700 (4.96843 iter/s, 20.1271s/100 iters), loss = 0.00101074
I1022 22:20:16.916481  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00100983 (* 1 = 0.00100983 loss)
I1022 22:20:16.916487  3959 sgd_solver.cpp:105] Iteration 19700, lr = 0.001
I1022 22:20:34.656157  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:20:36.849360  3959 solver.cpp:330] Iteration 19800, Testing net (#0)
I1022 22:20:38.285320  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:20:39.427076  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:20:39.427137  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0541442 (* 1 = 0.0541442 loss)
I1022 22:20:39.629052  3959 solver.cpp:218] Iteration 19800 (4.40286 iter/s, 22.7125s/100 iters), loss = 0.00966702
I1022 22:20:39.629087  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00966612 (* 1 = 0.00966612 loss)
I1022 22:20:39.629093  3959 sgd_solver.cpp:105] Iteration 19800, lr = 0.001
I1022 22:20:59.749943  3959 solver.cpp:218] Iteration 19900 (4.96998 iter/s, 20.1208s/100 iters), loss = 0.000177088
I1022 22:20:59.749974  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00017618 (* 1 = 0.00017618 loss)
I1022 22:20:59.749979  3959 sgd_solver.cpp:105] Iteration 19900, lr = 0.001
I1022 22:21:19.671458  3959 solver.cpp:330] Iteration 20000, Testing net (#0)
I1022 22:21:21.106168  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:21:22.247877  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:21:22.247938  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0422693 (* 1 = 0.0422693 loss)
I1022 22:21:22.449941  3959 solver.cpp:218] Iteration 20000 (4.4053 iter/s, 22.6999s/100 iters), loss = 0.0137167
I1022 22:21:22.449971  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0137158 (* 1 = 0.0137158 loss)
I1022 22:21:22.449977  3959 sgd_solver.cpp:46] MultiStep Status: Iteration 20000, step = 2
I1022 22:21:22.449980  3959 sgd_solver.cpp:105] Iteration 20000, lr = 0.0001
I1022 22:21:42.580476  3959 solver.cpp:218] Iteration 20100 (4.9676 iter/s, 20.1304s/100 iters), loss = 0.0116153
I1022 22:21:42.580507  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116144 (* 1 = 0.0116144 loss)
I1022 22:21:42.580514  3959 sgd_solver.cpp:105] Iteration 20100, lr = 0.0001
I1022 22:21:48.444414  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:22:02.513728  3959 solver.cpp:330] Iteration 20200, Testing net (#0)
I1022 22:22:03.948495  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:22:05.091194  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:22:05.091253  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.046517 (* 1 = 0.046517 loss)
I1022 22:22:05.292979  3959 solver.cpp:218] Iteration 20200 (4.40288 iter/s, 22.7124s/100 iters), loss = 0.00200734
I1022 22:22:05.293009  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00200641 (* 1 = 0.00200641 loss)
I1022 22:22:05.293016  3959 sgd_solver.cpp:105] Iteration 20200, lr = 0.0001
I1022 22:22:25.424899  3959 solver.cpp:218] Iteration 20300 (4.96726 iter/s, 20.1318s/100 iters), loss = 0.00872447
I1022 22:22:25.424933  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00872355 (* 1 = 0.00872355 loss)
I1022 22:22:25.424940  3959 sgd_solver.cpp:105] Iteration 20300, lr = 0.0001
I1022 22:22:45.356608  3959 solver.cpp:330] Iteration 20400, Testing net (#0)
I1022 22:22:46.790263  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:22:47.934495  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:22:47.934557  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0439519 (* 1 = 0.0439519 loss)
I1022 22:22:48.136310  3959 solver.cpp:218] Iteration 20400 (4.40309 iter/s, 22.7113s/100 iters), loss = 0.0203095
I1022 22:22:48.136342  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0203086 (* 1 = 0.0203086 loss)
I1022 22:22:48.136348  3959 sgd_solver.cpp:105] Iteration 20400, lr = 0.0001
I1022 22:23:02.251351  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:23:08.263993  3959 solver.cpp:218] Iteration 20500 (4.9683 iter/s, 20.1276s/100 iters), loss = 0.00397566
I1022 22:23:08.264036  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00397474 (* 1 = 0.00397474 loss)
I1022 22:23:08.264044  3959 sgd_solver.cpp:105] Iteration 20500, lr = 0.0001
I1022 22:23:28.190448  3959 solver.cpp:330] Iteration 20600, Testing net (#0)
I1022 22:23:29.587107  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:23:30.768043  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:23:30.768097  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0446798 (* 1 = 0.0446798 loss)
I1022 22:23:30.969913  3959 solver.cpp:218] Iteration 20600 (4.40416 iter/s, 22.7058s/100 iters), loss = 0.00102613
I1022 22:23:30.969944  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00102522 (* 1 = 0.00102522 loss)
I1022 22:23:30.969950  3959 sgd_solver.cpp:105] Iteration 20600, lr = 0.0001
I1022 22:23:51.098361  3959 solver.cpp:218] Iteration 20700 (4.96811 iter/s, 20.1284s/100 iters), loss = 0.00466445
I1022 22:23:51.098393  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00466353 (* 1 = 0.00466353 loss)
I1022 22:23:51.098400  3959 sgd_solver.cpp:105] Iteration 20700, lr = 0.0001
I1022 22:24:11.028898  3959 solver.cpp:330] Iteration 20800, Testing net (#0)
I1022 22:24:12.423331  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:24:13.605196  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:24:13.605239  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0439147 (* 1 = 0.0439147 loss)
I1022 22:24:13.807227  3959 solver.cpp:218] Iteration 20800 (4.40358 iter/s, 22.7088s/100 iters), loss = 0.00513976
I1022 22:24:13.807262  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00513885 (* 1 = 0.00513885 loss)
I1022 22:24:13.807270  3959 sgd_solver.cpp:105] Iteration 20800, lr = 0.0001
I1022 22:24:16.245772  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:24:33.937314  3959 solver.cpp:218] Iteration 20900 (4.96771 iter/s, 20.13s/100 iters), loss = 0.0107113
I1022 22:24:33.937345  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107104 (* 1 = 0.0107104 loss)
I1022 22:24:33.937351  3959 sgd_solver.cpp:105] Iteration 20900, lr = 0.0001
I1022 22:24:53.865530  3959 solver.cpp:330] Iteration 21000, Testing net (#0)
I1022 22:24:55.259950  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:24:56.442383  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:24:56.442446  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0450948 (* 1 = 0.0450948 loss)
I1022 22:24:56.644249  3959 solver.cpp:218] Iteration 21000 (4.40396 iter/s, 22.7069s/100 iters), loss = 0.000469375
I1022 22:24:56.644280  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000468476 (* 1 = 0.000468476 loss)
I1022 22:24:56.644286  3959 sgd_solver.cpp:105] Iteration 21000, lr = 0.0001
I1022 22:25:16.767599  3959 solver.cpp:218] Iteration 21100 (4.96937 iter/s, 20.1233s/100 iters), loss = 0.00232343
I1022 22:25:16.767632  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00232253 (* 1 = 0.00232253 loss)
I1022 22:25:16.767637  3959 sgd_solver.cpp:105] Iteration 21100, lr = 0.0001
I1022 22:25:27.457393  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:25:36.690448  3959 solver.cpp:330] Iteration 21200, Testing net (#0)
I1022 22:25:38.083892  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:25:39.267750  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:25:39.267808  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0455558 (* 1 = 0.0455558 loss)
I1022 22:25:39.469919  3959 solver.cpp:218] Iteration 21200 (4.40485 iter/s, 22.7022s/100 iters), loss = 0.00621743
I1022 22:25:39.469951  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00621653 (* 1 = 0.00621653 loss)
I1022 22:25:39.469959  3959 sgd_solver.cpp:105] Iteration 21200, lr = 0.0001
I1022 22:25:59.595875  3959 solver.cpp:218] Iteration 21300 (4.96873 iter/s, 20.1259s/100 iters), loss = 0.0063153
I1022 22:25:59.596015  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00631441 (* 1 = 0.00631441 loss)
I1022 22:25:59.596025  3959 sgd_solver.cpp:105] Iteration 21300, lr = 0.0001
I1022 22:26:19.525773  3959 solver.cpp:330] Iteration 21400, Testing net (#0)
I1022 22:26:20.880980  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:26:22.102562  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:26:22.102622  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0455139 (* 1 = 0.0455139 loss)
I1022 22:26:22.304882  3959 solver.cpp:218] Iteration 21400 (4.40358 iter/s, 22.7088s/100 iters), loss = 0.00137476
I1022 22:26:22.304915  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00137387 (* 1 = 0.00137387 loss)
I1022 22:26:22.304922  3959 sgd_solver.cpp:105] Iteration 21400, lr = 0.0001
I1022 22:26:41.248222  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:26:42.435024  3959 solver.cpp:218] Iteration 21500 (4.9677 iter/s, 20.1301s/100 iters), loss = 0.0433211
I1022 22:26:42.435066  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0433202 (* 1 = 0.0433202 loss)
I1022 22:26:42.435073  3959 sgd_solver.cpp:105] Iteration 21500, lr = 0.0001
I1022 22:27:02.363530  3959 solver.cpp:330] Iteration 21600, Testing net (#0)
I1022 22:27:03.718103  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:27:04.941031  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:27:04.941090  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0478369 (* 1 = 0.0478369 loss)
I1022 22:27:05.142885  3959 solver.cpp:218] Iteration 21600 (4.40378 iter/s, 22.7078s/100 iters), loss = 0.0254607
I1022 22:27:05.142917  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0254598 (* 1 = 0.0254598 loss)
I1022 22:27:05.142925  3959 sgd_solver.cpp:105] Iteration 21600, lr = 0.0001
I1022 22:27:25.266268  3959 solver.cpp:218] Iteration 21700 (4.96936 iter/s, 20.1233s/100 iters), loss = 0.000635622
I1022 22:27:25.266391  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000634729 (* 1 = 0.000634729 loss)
I1022 22:27:25.266400  3959 sgd_solver.cpp:105] Iteration 21700, lr = 0.0001
I1022 22:27:45.191231  3959 solver.cpp:330] Iteration 21800, Testing net (#0)
I1022 22:27:46.544701  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:27:47.767884  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:27:47.767946  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0453159 (* 1 = 0.0453159 loss)
I1022 22:27:47.969600  3959 solver.cpp:218] Iteration 21800 (4.40468 iter/s, 22.7032s/100 iters), loss = 0.00276166
I1022 22:27:47.969645  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00276076 (* 1 = 0.00276076 loss)
I1022 22:27:47.969653  3959 sgd_solver.cpp:105] Iteration 21800, lr = 0.0001
I1022 22:27:55.041743  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:28:08.099944  3959 solver.cpp:218] Iteration 21900 (4.96765 iter/s, 20.1302s/100 iters), loss = 0.0162318
I1022 22:28:08.100101  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0162309 (* 1 = 0.0162309 loss)
I1022 22:28:08.100121  3959 sgd_solver.cpp:105] Iteration 21900, lr = 0.0001
I1022 22:28:28.026877  3959 solver.cpp:330] Iteration 22000, Testing net (#0)
I1022 22:28:29.379600  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:28:30.604048  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:28:30.604106  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0486597 (* 1 = 0.0486597 loss)
I1022 22:28:30.806310  3959 solver.cpp:218] Iteration 22000 (4.40409 iter/s, 22.7062s/100 iters), loss = 0.00384887
I1022 22:28:30.806351  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00384797 (* 1 = 0.00384797 loss)
I1022 22:28:30.806358  3959 sgd_solver.cpp:105] Iteration 22000, lr = 0.0001
I1022 22:28:50.933303  3959 solver.cpp:218] Iteration 22100 (4.96848 iter/s, 20.1269s/100 iters), loss = 0.0829082
I1022 22:28:50.933408  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0829073 (* 1 = 0.0829073 loss)
I1022 22:28:50.933416  3959 sgd_solver.cpp:105] Iteration 22100, lr = 0.0001
I1022 22:29:06.458636  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:29:10.865267  3959 solver.cpp:330] Iteration 22200, Testing net (#0)
I1022 22:29:12.181094  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:29:13.442741  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:29:13.442802  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0480054 (* 1 = 0.0480054 loss)
I1022 22:29:13.644531  3959 solver.cpp:218] Iteration 22200 (4.40314 iter/s, 22.7111s/100 iters), loss = 0.0189796
I1022 22:29:13.644563  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0189787 (* 1 = 0.0189787 loss)
I1022 22:29:13.644570  3959 sgd_solver.cpp:105] Iteration 22200, lr = 0.0001
I1022 22:29:33.766234  3959 solver.cpp:218] Iteration 22300 (4.96978 iter/s, 20.1216s/100 iters), loss = 0.00341844
I1022 22:29:33.766377  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00341755 (* 1 = 0.00341755 loss)
I1022 22:29:33.766387  3959 sgd_solver.cpp:105] Iteration 22300, lr = 0.0001
I1022 22:29:53.689513  3959 solver.cpp:330] Iteration 22400, Testing net (#0)
I1022 22:29:55.005029  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:29:56.267725  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:29:56.267796  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0481772 (* 1 = 0.0481772 loss)
I1022 22:29:56.470019  3959 solver.cpp:218] Iteration 22400 (4.40459 iter/s, 22.7036s/100 iters), loss = 0.000240742
I1022 22:29:56.470052  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000239878 (* 1 = 0.000239878 loss)
I1022 22:29:56.470058  3959 sgd_solver.cpp:105] Iteration 22400, lr = 0.0001
I1022 22:30:16.598047  3959 solver.cpp:218] Iteration 22500 (4.96822 iter/s, 20.1279s/100 iters), loss = 0.00676349
I1022 22:30:16.598222  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00676262 (* 1 = 0.00676262 loss)
I1022 22:30:16.598232  3959 sgd_solver.cpp:105] Iteration 22500, lr = 0.0001
I1022 22:30:20.246206  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:30:36.528133  3959 solver.cpp:330] Iteration 22600, Testing net (#0)
I1022 22:30:37.841624  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:30:39.105275  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:30:39.105332  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0490988 (* 1 = 0.0490988 loss)
I1022 22:30:39.307750  3959 solver.cpp:218] Iteration 22600 (4.40345 iter/s, 22.7095s/100 iters), loss = 0.004421
I1022 22:30:39.307785  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00442014 (* 1 = 0.00442014 loss)
I1022 22:30:39.307790  3959 sgd_solver.cpp:105] Iteration 22600, lr = 0.0001
I1022 22:30:59.436005  3959 solver.cpp:218] Iteration 22700 (4.96816 iter/s, 20.1282s/100 iters), loss = 0.0403688
I1022 22:30:59.436107  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0403679 (* 1 = 0.0403679 loss)
I1022 22:30:59.436115  3959 sgd_solver.cpp:105] Iteration 22700, lr = 0.0001
I1022 22:31:19.363657  3959 solver.cpp:330] Iteration 22800, Testing net (#0)
I1022 22:31:20.676652  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:31:21.941326  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:31:21.941386  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0502102 (* 1 = 0.0502102 loss)
I1022 22:31:22.143137  3959 solver.cpp:218] Iteration 22800 (4.40393 iter/s, 22.707s/100 iters), loss = 0.00120841
I1022 22:31:22.143179  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00120756 (* 1 = 0.00120756 loss)
I1022 22:31:22.143187  3959 sgd_solver.cpp:105] Iteration 22800, lr = 0.0001
I1022 22:31:34.041609  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:31:42.269330  3959 solver.cpp:218] Iteration 22900 (4.96867 iter/s, 20.1261s/100 iters), loss = 0.00331546
I1022 22:31:42.269361  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0033146 (* 1 = 0.0033146 loss)
I1022 22:31:42.269368  3959 sgd_solver.cpp:105] Iteration 22900, lr = 0.0001
I1022 22:32:02.193099  3959 solver.cpp:330] Iteration 23000, Testing net (#0)
I1022 22:32:03.468295  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:32:04.769986  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:32:04.770141  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0485329 (* 1 = 0.0485329 loss)
I1022 22:32:04.972230  3959 solver.cpp:218] Iteration 23000 (4.40474 iter/s, 22.7028s/100 iters), loss = 0.0224599
I1022 22:32:04.972260  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.022459 (* 1 = 0.022459 loss)
I1022 22:32:04.972266  3959 sgd_solver.cpp:105] Iteration 23000, lr = 0.0001
I1022 22:32:25.097137  3959 solver.cpp:218] Iteration 23100 (4.96899 iter/s, 20.1248s/100 iters), loss = 0.0011313
I1022 22:32:25.097172  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00113044 (* 1 = 0.00113044 loss)
I1022 22:32:25.097177  3959 sgd_solver.cpp:105] Iteration 23100, lr = 0.0001
I1022 22:32:45.027050  3959 solver.cpp:330] Iteration 23200, Testing net (#0)
I1022 22:32:46.301028  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:32:47.603880  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:32:47.603936  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0472021 (* 1 = 0.0472021 loss)
I1022 22:32:47.806253  3959 solver.cpp:218] Iteration 23200 (4.40354 iter/s, 22.709s/100 iters), loss = 0.00417864
I1022 22:32:47.806284  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00417778 (* 1 = 0.00417778 loss)
I1022 22:32:47.806291  3959 sgd_solver.cpp:105] Iteration 23200, lr = 0.0001
I1022 22:32:47.833945  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:33:07.940990  3959 solver.cpp:218] Iteration 23300 (4.96656 iter/s, 20.1347s/100 iters), loss = 0.00783773
I1022 22:33:07.941025  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00783688 (* 1 = 0.00783688 loss)
I1022 22:33:07.941045  3959 sgd_solver.cpp:105] Iteration 23300, lr = 0.0001
I1022 22:33:27.879830  3959 solver.cpp:330] Iteration 23400, Testing net (#0)
I1022 22:33:29.152446  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:33:30.456408  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.986328
I1022 22:33:30.456467  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0472054 (* 1 = 0.0472054 loss)
I1022 22:33:30.658350  3959 solver.cpp:218] Iteration 23400 (4.40194 iter/s, 22.7173s/100 iters), loss = 0.00109819
I1022 22:33:30.658385  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00109736 (* 1 = 0.00109736 loss)
I1022 22:33:30.658394  3959 sgd_solver.cpp:105] Iteration 23400, lr = 0.0001
I1022 22:33:50.776443  3959 solver.cpp:218] Iteration 23500 (4.97067 iter/s, 20.118s/100 iters), loss = 0.0510915
I1022 22:33:50.776475  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0510907 (* 1 = 0.0510907 loss)
I1022 22:33:50.776484  3959 sgd_solver.cpp:105] Iteration 23500, lr = 0.0001
I1022 22:33:59.250983  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:34:10.694977  3959 solver.cpp:330] Iteration 23600, Testing net (#0)
I1022 22:34:11.967675  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:34:13.272327  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:34:13.272379  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0478431 (* 1 = 0.0478431 loss)
I1022 22:34:13.474328  3959 solver.cpp:218] Iteration 23600 (4.40571 iter/s, 22.6978s/100 iters), loss = 0.00333746
I1022 22:34:13.474364  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00333664 (* 1 = 0.00333664 loss)
I1022 22:34:13.474372  3959 sgd_solver.cpp:105] Iteration 23600, lr = 0.0001
I1022 22:34:33.600018  3959 solver.cpp:218] Iteration 23700 (4.9688 iter/s, 20.1256s/100 iters), loss = 0.00807197
I1022 22:34:33.600143  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00807114 (* 1 = 0.00807114 loss)
I1022 22:34:33.600164  3959 sgd_solver.cpp:105] Iteration 23700, lr = 0.0001
I1022 22:34:53.525599  3959 solver.cpp:330] Iteration 23800, Testing net (#0)
I1022 22:34:54.759467  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:34:56.102326  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.982422
I1022 22:34:56.102382  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0530534 (* 1 = 0.0530534 loss)
I1022 22:34:56.304867  3959 solver.cpp:218] Iteration 23800 (4.40438 iter/s, 22.7047s/100 iters), loss = 0.0776071
I1022 22:34:56.304903  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0776063 (* 1 = 0.0776063 loss)
I1022 22:34:56.304911  3959 sgd_solver.cpp:105] Iteration 23800, lr = 0.0001
I1022 22:35:13.034934  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:35:16.433560  3959 solver.cpp:218] Iteration 23900 (4.96805 iter/s, 20.1286s/100 iters), loss = 0.0109936
I1022 22:35:16.433591  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109928 (* 1 = 0.0109928 loss)
I1022 22:35:16.433598  3959 sgd_solver.cpp:105] Iteration 23900, lr = 0.0001
I1022 22:35:36.359436  3959 solver.cpp:330] Iteration 24000, Testing net (#0)
I1022 22:35:37.594503  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:35:38.937104  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:35:38.937152  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0482705 (* 1 = 0.0482705 loss)
I1022 22:35:39.138993  3959 solver.cpp:218] Iteration 24000 (4.40425 iter/s, 22.7053s/100 iters), loss = 0.00191179
I1022 22:35:39.139024  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00191095 (* 1 = 0.00191095 loss)
I1022 22:35:39.139031  3959 sgd_solver.cpp:105] Iteration 24000, lr = 0.0001
I1022 22:35:59.259021  3959 solver.cpp:218] Iteration 24100 (4.97019 iter/s, 20.1199s/100 iters), loss = 0.00145412
I1022 22:35:59.259166  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00145326 (* 1 = 0.00145326 loss)
I1022 22:35:59.259173  3959 sgd_solver.cpp:105] Iteration 24100, lr = 0.0001
I1022 22:36:19.180883  3959 solver.cpp:330] Iteration 24200, Testing net (#0)
I1022 22:36:20.414647  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:36:21.757731  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:36:21.757788  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0488349 (* 1 = 0.0488349 loss)
I1022 22:36:21.959751  3959 solver.cpp:218] Iteration 24200 (4.40518 iter/s, 22.7005s/100 iters), loss = 0.00698646
I1022 22:36:21.959792  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00698561 (* 1 = 0.00698561 loss)
I1022 22:36:21.959800  3959 sgd_solver.cpp:105] Iteration 24200, lr = 0.0001
I1022 22:36:26.817049  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:36:42.087748  3959 solver.cpp:218] Iteration 24300 (4.96823 iter/s, 20.1279s/100 iters), loss = 0.0059774
I1022 22:36:42.087867  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00597655 (* 1 = 0.00597655 loss)
I1022 22:36:42.087875  3959 sgd_solver.cpp:105] Iteration 24300, lr = 0.0001
I1022 22:37:02.017416  3959 solver.cpp:330] Iteration 24400, Testing net (#0)
I1022 22:37:03.250046  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:37:04.594810  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:37:04.594867  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0476489 (* 1 = 0.0476489 loss)
I1022 22:37:04.796885  3959 solver.cpp:218] Iteration 24400 (4.40354 iter/s, 22.709s/100 iters), loss = 0.0103773
I1022 22:37:04.796918  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0103764 (* 1 = 0.0103764 loss)
I1022 22:37:04.796924  3959 sgd_solver.cpp:105] Iteration 24400, lr = 0.0001
I1022 22:37:24.922761  3959 solver.cpp:218] Iteration 24500 (4.96875 iter/s, 20.1258s/100 iters), loss = 0.00426355
I1022 22:37:24.922873  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00426268 (* 1 = 0.00426268 loss)
I1022 22:37:24.922880  3959 sgd_solver.cpp:105] Iteration 24500, lr = 0.0001
I1022 22:37:38.030692  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:37:44.848992  3959 solver.cpp:330] Iteration 24600, Testing net (#0)
I1022 22:37:46.043812  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:37:47.426548  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:37:47.426605  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0486017 (* 1 = 0.0486017 loss)
I1022 22:37:47.628522  3959 solver.cpp:218] Iteration 24600 (4.4042 iter/s, 22.7056s/100 iters), loss = 0.0548031
I1022 22:37:47.628556  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0548023 (* 1 = 0.0548023 loss)
I1022 22:37:47.628563  3959 sgd_solver.cpp:105] Iteration 24600, lr = 0.0001
I1022 22:38:07.748538  3959 solver.cpp:218] Iteration 24700 (4.9702 iter/s, 20.1199s/100 iters), loss = 0.0013133
I1022 22:38:07.748646  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00131244 (* 1 = 0.00131244 loss)
I1022 22:38:07.748654  3959 sgd_solver.cpp:105] Iteration 24700, lr = 0.0001
I1022 22:38:27.668136  3959 solver.cpp:330] Iteration 24800, Testing net (#0)
I1022 22:38:28.861734  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:38:30.245301  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:38:30.245362  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0508939 (* 1 = 0.0508939 loss)
I1022 22:38:30.447321  3959 solver.cpp:218] Iteration 24800 (4.40555 iter/s, 22.6986s/100 iters), loss = 0.0216379
I1022 22:38:30.447355  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.021637 (* 1 = 0.021637 loss)
I1022 22:38:30.447361  3959 sgd_solver.cpp:105] Iteration 24800, lr = 0.0001
I1022 22:38:50.576467  3959 solver.cpp:218] Iteration 24900 (4.96794 iter/s, 20.1291s/100 iters), loss = 0.0268301
I1022 22:38:50.576594  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0268292 (* 1 = 0.0268292 loss)
I1022 22:38:50.576603  3959 sgd_solver.cpp:105] Iteration 24900, lr = 0.0001
I1022 22:38:52.010668  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:39:10.507526  3959 solver.cpp:330] Iteration 25000, Testing net (#0)
I1022 22:39:11.700984  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:39:13.084712  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:39:13.084771  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0502464 (* 1 = 0.0502464 loss)
I1022 22:39:13.287114  3959 solver.cpp:218] Iteration 25000 (4.40326 iter/s, 22.7105s/100 iters), loss = 0.0140301
I1022 22:39:13.287148  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140293 (* 1 = 0.0140293 loss)
I1022 22:39:13.287158  3959 sgd_solver.cpp:105] Iteration 25000, lr = 0.0001
I1022 22:39:33.417450  3959 solver.cpp:218] Iteration 25100 (4.96765 iter/s, 20.1302s/100 iters), loss = 0.00147151
I1022 22:39:33.417587  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00147069 (* 1 = 0.00147069 loss)
I1022 22:39:33.417595  3959 sgd_solver.cpp:105] Iteration 25100, lr = 0.0001
I1022 22:39:53.345942  3959 solver.cpp:330] Iteration 25200, Testing net (#0)
I1022 22:39:54.538761  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:39:55.924494  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:39:55.924553  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0500999 (* 1 = 0.0500999 loss)
I1022 22:39:56.126276  3959 solver.cpp:218] Iteration 25200 (4.40361 iter/s, 22.7086s/100 iters), loss = 0.000751755
I1022 22:39:56.126308  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000750932 (* 1 = 0.000750932 loss)
I1022 22:39:56.126317  3959 sgd_solver.cpp:105] Iteration 25200, lr = 0.0001
I1022 22:40:05.810781  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:40:16.250442  3959 solver.cpp:218] Iteration 25300 (4.96917 iter/s, 20.1241s/100 iters), loss = 0.0446334
I1022 22:40:16.250474  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446326 (* 1 = 0.0446326 loss)
I1022 22:40:16.250481  3959 sgd_solver.cpp:105] Iteration 25300, lr = 0.0001
I1022 22:40:36.172585  3959 solver.cpp:330] Iteration 25400, Testing net (#0)
I1022 22:40:37.327311  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:40:38.749773  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:40:38.749830  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0486247 (* 1 = 0.0486247 loss)
I1022 22:40:38.951838  3959 solver.cpp:218] Iteration 25400 (4.40503 iter/s, 22.7013s/100 iters), loss = 0.103138
I1022 22:40:38.951869  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103137 (* 1 = 0.103137 loss)
I1022 22:40:38.951875  3959 sgd_solver.cpp:105] Iteration 25400, lr = 0.0001
I1022 22:40:59.077921  3959 solver.cpp:218] Iteration 25500 (4.9687 iter/s, 20.126s/100 iters), loss = 0.0156506
I1022 22:40:59.077955  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0156497 (* 1 = 0.0156497 loss)
I1022 22:40:59.077961  3959 sgd_solver.cpp:105] Iteration 25500, lr = 0.0001
I1022 22:41:17.016700  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:41:19.006219  3959 solver.cpp:330] Iteration 25600, Testing net (#0)
I1022 22:41:20.159651  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:41:21.583559  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:41:21.583617  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0490515 (* 1 = 0.0490515 loss)
I1022 22:41:21.785904  3959 solver.cpp:218] Iteration 25600 (4.40375 iter/s, 22.7079s/100 iters), loss = 0.00108916
I1022 22:41:21.785934  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00108827 (* 1 = 0.00108827 loss)
I1022 22:41:21.785941  3959 sgd_solver.cpp:105] Iteration 25600, lr = 0.0001
I1022 22:41:41.908834  3959 solver.cpp:218] Iteration 25700 (4.96948 iter/s, 20.1228s/100 iters), loss = 0.00660245
I1022 22:41:41.908865  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00660156 (* 1 = 0.00660156 loss)
I1022 22:41:41.908871  3959 sgd_solver.cpp:105] Iteration 25700, lr = 0.0001
I1022 22:42:01.835289  3959 solver.cpp:330] Iteration 25800, Testing net (#0)
I1022 22:42:02.987967  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:42:04.412773  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:42:04.412832  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0466495 (* 1 = 0.0466495 loss)
I1022 22:42:04.614449  3959 solver.cpp:218] Iteration 25800 (4.40421 iter/s, 22.7055s/100 iters), loss = 0.184869
I1022 22:42:04.614478  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.184868 (* 1 = 0.184868 loss)
I1022 22:42:04.614485  3959 sgd_solver.cpp:105] Iteration 25800, lr = 0.0001
I1022 22:42:24.733017  3959 solver.cpp:218] Iteration 25900 (4.97055 iter/s, 20.1185s/100 iters), loss = 0.00145884
I1022 22:42:24.733049  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00145794 (* 1 = 0.00145794 loss)
I1022 22:42:24.733055  3959 sgd_solver.cpp:105] Iteration 25900, lr = 0.0001
I1022 22:42:30.796492  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:42:44.651583  3959 solver.cpp:330] Iteration 26000, Testing net (#0)
I1022 22:42:45.803529  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:42:47.229156  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:42:47.229220  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0476449 (* 1 = 0.0476449 loss)
I1022 22:42:47.431535  3959 solver.cpp:218] Iteration 26000 (4.40559 iter/s, 22.6984s/100 iters), loss = 0.000346892
I1022 22:42:47.431567  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00034597 (* 1 = 0.00034597 loss)
I1022 22:42:47.431573  3959 sgd_solver.cpp:105] Iteration 26000, lr = 0.0001
I1022 22:43:07.557404  3959 solver.cpp:218] Iteration 26100 (4.96875 iter/s, 20.1258s/100 iters), loss = 0.00532482
I1022 22:43:07.557435  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00532391 (* 1 = 0.00532391 loss)
I1022 22:43:07.557441  3959 sgd_solver.cpp:105] Iteration 26100, lr = 0.0001
I1022 22:43:27.484578  3959 solver.cpp:330] Iteration 26200, Testing net (#0)
I1022 22:43:28.598891  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:43:30.061883  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:43:30.061944  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0479594 (* 1 = 0.0479594 loss)
I1022 22:43:30.264487  3959 solver.cpp:218] Iteration 26200 (4.40393 iter/s, 22.707s/100 iters), loss = 0.00618133
I1022 22:43:30.264524  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00618042 (* 1 = 0.00618042 loss)
I1022 22:43:30.264533  3959 sgd_solver.cpp:105] Iteration 26200, lr = 0.0001
I1022 22:43:44.782189  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:43:50.395027  3959 solver.cpp:218] Iteration 26300 (4.9676 iter/s, 20.1305s/100 iters), loss = 0.0209876
I1022 22:43:50.395059  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0209866 (* 1 = 0.0209866 loss)
I1022 22:43:50.395066  3959 sgd_solver.cpp:105] Iteration 26300, lr = 0.0001
I1022 22:44:10.322717  3959 solver.cpp:330] Iteration 26400, Testing net (#0)
I1022 22:44:11.436931  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:44:12.900632  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:44:12.900696  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0491773 (* 1 = 0.0491773 loss)
I1022 22:44:13.102548  3959 solver.cpp:218] Iteration 26400 (4.40384 iter/s, 22.7074s/100 iters), loss = 0.0198855
I1022 22:44:13.102579  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0198845 (* 1 = 0.0198845 loss)
I1022 22:44:13.102586  3959 sgd_solver.cpp:105] Iteration 26400, lr = 0.0001
I1022 22:44:33.224459  3959 solver.cpp:218] Iteration 26500 (4.96973 iter/s, 20.1218s/100 iters), loss = 0.00563225
I1022 22:44:33.224494  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00563132 (* 1 = 0.00563132 loss)
I1022 22:44:33.224501  3959 sgd_solver.cpp:105] Iteration 26500, lr = 0.0001
I1022 22:44:53.144696  3959 solver.cpp:330] Iteration 26600, Testing net (#0)
I1022 22:44:54.257372  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:44:55.721150  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:44:55.721210  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.049521 (* 1 = 0.049521 loss)
I1022 22:44:55.923254  3959 solver.cpp:218] Iteration 26600 (4.40554 iter/s, 22.6987s/100 iters), loss = 0.000443923
I1022 22:44:55.923285  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000442997 (* 1 = 0.000442997 loss)
I1022 22:44:55.923291  3959 sgd_solver.cpp:105] Iteration 26600, lr = 0.0001
I1022 22:44:58.566507  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:45:16.047643  3959 solver.cpp:218] Iteration 26700 (4.96912 iter/s, 20.1243s/100 iters), loss = 0.0263641
I1022 22:45:16.047685  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0263631 (* 1 = 0.0263631 loss)
I1022 22:45:16.047691  3959 sgd_solver.cpp:105] Iteration 26700, lr = 0.0001
I1022 22:45:35.972810  3959 solver.cpp:330] Iteration 26800, Testing net (#0)
I1022 22:45:37.084591  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:45:38.549741  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:45:38.549799  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0486657 (* 1 = 0.0486657 loss)
I1022 22:45:38.752243  3959 solver.cpp:218] Iteration 26800 (4.40441 iter/s, 22.7045s/100 iters), loss = 0.0331977
I1022 22:45:38.752274  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0331968 (* 1 = 0.0331968 loss)
I1022 22:45:38.752281  3959 sgd_solver.cpp:105] Iteration 26800, lr = 0.0001
I1022 22:45:58.877152  3959 solver.cpp:218] Iteration 26900 (4.96899 iter/s, 20.1248s/100 iters), loss = 0.0069671
I1022 22:45:58.877194  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00696617 (* 1 = 0.00696617 loss)
I1022 22:45:58.877202  3959 sgd_solver.cpp:105] Iteration 26900, lr = 0.0001
I1022 22:46:09.771981  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:46:18.804435  3959 solver.cpp:330] Iteration 27000, Testing net (#0)
I1022 22:46:19.878456  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:46:21.381563  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:46:21.381624  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0489431 (* 1 = 0.0489431 loss)
I1022 22:46:21.583688  3959 solver.cpp:218] Iteration 27000 (4.40404 iter/s, 22.7064s/100 iters), loss = 0.0450201
I1022 22:46:21.583719  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0450192 (* 1 = 0.0450192 loss)
I1022 22:46:21.583724  3959 sgd_solver.cpp:105] Iteration 27000, lr = 0.0001
I1022 22:46:41.702764  3959 solver.cpp:218] Iteration 27100 (4.97043 iter/s, 20.119s/100 iters), loss = 0.00471951
I1022 22:46:41.702942  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00471858 (* 1 = 0.00471858 loss)
I1022 22:46:41.702951  3959 sgd_solver.cpp:105] Iteration 27100, lr = 0.0001
I1022 22:47:01.620846  3959 solver.cpp:330] Iteration 27200, Testing net (#0)
I1022 22:47:02.693920  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:47:04.198021  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:47:04.198081  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0464358 (* 1 = 0.0464358 loss)
I1022 22:47:04.400308  3959 solver.cpp:218] Iteration 27200 (4.4058 iter/s, 22.6973s/100 iters), loss = 0.000936538
I1022 22:47:04.400341  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000935618 (* 1 = 0.000935618 loss)
I1022 22:47:04.400346  3959 sgd_solver.cpp:105] Iteration 27200, lr = 0.0001
I1022 22:47:23.545472  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:47:24.526644  3959 solver.cpp:218] Iteration 27300 (4.96864 iter/s, 20.1263s/100 iters), loss = 0.00246565
I1022 22:47:24.526675  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00246473 (* 1 = 0.00246473 loss)
I1022 22:47:24.526682  3959 sgd_solver.cpp:105] Iteration 27300, lr = 0.0001
I1022 22:47:44.453680  3959 solver.cpp:330] Iteration 27400, Testing net (#0)
I1022 22:47:45.525703  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:47:47.030910  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:47:47.030961  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0483889 (* 1 = 0.0483889 loss)
I1022 22:47:47.233446  3959 solver.cpp:218] Iteration 27400 (4.40398 iter/s, 22.7067s/100 iters), loss = 0.015034
I1022 22:47:47.233479  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015033 (* 1 = 0.015033 loss)
I1022 22:47:47.233485  3959 sgd_solver.cpp:105] Iteration 27400, lr = 0.0001
I1022 22:48:07.360723  3959 solver.cpp:218] Iteration 27500 (4.9684 iter/s, 20.1272s/100 iters), loss = 0.000421841
I1022 22:48:07.360864  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000420916 (* 1 = 0.000420916 loss)
I1022 22:48:07.360873  3959 sgd_solver.cpp:105] Iteration 27500, lr = 0.0001
I1022 22:48:27.287245  3959 solver.cpp:330] Iteration 27600, Testing net (#0)
I1022 22:48:28.358656  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:48:29.865411  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:48:29.865466  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0459896 (* 1 = 0.0459896 loss)
I1022 22:48:30.067339  3959 solver.cpp:218] Iteration 27600 (4.40404 iter/s, 22.7064s/100 iters), loss = 0.00143568
I1022 22:48:30.067375  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00143478 (* 1 = 0.00143478 loss)
I1022 22:48:30.067383  3959 sgd_solver.cpp:105] Iteration 27600, lr = 0.0001
I1022 22:48:37.536615  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:48:50.189004  3959 solver.cpp:218] Iteration 27700 (4.96979 iter/s, 20.1216s/100 iters), loss = 0.0170319
I1022 22:48:50.189036  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.017031 (* 1 = 0.017031 loss)
I1022 22:48:50.189043  3959 sgd_solver.cpp:105] Iteration 27700, lr = 0.0001
I1022 22:49:10.110615  3959 solver.cpp:330] Iteration 27800, Testing net (#0)
I1022 22:49:11.144857  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:49:12.688051  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:49:12.688110  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0471613 (* 1 = 0.0471613 loss)
I1022 22:49:12.890156  3959 solver.cpp:218] Iteration 27800 (4.40508 iter/s, 22.7011s/100 iters), loss = 0.00661552
I1022 22:49:12.890197  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00661461 (* 1 = 0.00661461 loss)
I1022 22:49:12.890204  3959 sgd_solver.cpp:105] Iteration 27800, lr = 0.0001
I1022 22:49:33.013919  3959 solver.cpp:218] Iteration 27900 (4.96927 iter/s, 20.1237s/100 iters), loss = 0.0126509
I1022 22:49:33.013950  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.01265 (* 1 = 0.01265 loss)
I1022 22:49:33.013957  3959 sgd_solver.cpp:105] Iteration 27900, lr = 0.0001
I1022 22:49:48.736328  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:49:52.939678  3959 solver.cpp:330] Iteration 28000, Testing net (#0)
I1022 22:49:53.972805  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:49:55.517603  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:49:55.517658  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.046807 (* 1 = 0.046807 loss)
I1022 22:49:55.720099  3959 solver.cpp:218] Iteration 28000 (4.40411 iter/s, 22.7061s/100 iters), loss = 0.00206615
I1022 22:49:55.720130  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00206525 (* 1 = 0.00206525 loss)
I1022 22:49:55.720137  3959 sgd_solver.cpp:105] Iteration 28000, lr = 0.0001
I1022 22:50:15.843580  3959 solver.cpp:218] Iteration 28100 (4.96934 iter/s, 20.1234s/100 iters), loss = 0.00231652
I1022 22:50:15.843611  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00231563 (* 1 = 0.00231563 loss)
I1022 22:50:15.843617  3959 sgd_solver.cpp:105] Iteration 28100, lr = 0.0001
I1022 22:50:35.767762  3959 solver.cpp:330] Iteration 28200, Testing net (#0)
I1022 22:50:36.799716  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:50:38.344666  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:50:38.344719  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0467241 (* 1 = 0.0467241 loss)
I1022 22:50:38.546751  3959 solver.cpp:218] Iteration 28200 (4.40469 iter/s, 22.7031s/100 iters), loss = 0.0299145
I1022 22:50:38.546782  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0299136 (* 1 = 0.0299136 loss)
I1022 22:50:38.546789  3959 sgd_solver.cpp:105] Iteration 28200, lr = 0.0001
I1022 22:50:58.663852  3959 solver.cpp:218] Iteration 28300 (4.97092 iter/s, 20.117s/100 iters), loss = 0.0009243
I1022 22:50:58.663884  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000923424 (* 1 = 0.000923424 loss)
I1022 22:50:58.663892  3959 sgd_solver.cpp:105] Iteration 28300, lr = 0.0001
I1022 22:51:02.514081  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:51:18.583375  3959 solver.cpp:330] Iteration 28400, Testing net (#0)
I1022 22:51:19.615841  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:51:21.161175  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:51:21.161219  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0477766 (* 1 = 0.0477766 loss)
I1022 22:51:21.363320  3959 solver.cpp:218] Iteration 28400 (4.40541 iter/s, 22.6994s/100 iters), loss = 0.003261
I1022 22:51:21.363355  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00326011 (* 1 = 0.00326011 loss)
I1022 22:51:21.363363  3959 sgd_solver.cpp:105] Iteration 28400, lr = 0.0001
I1022 22:51:41.487538  3959 solver.cpp:218] Iteration 28500 (4.96916 iter/s, 20.1241s/100 iters), loss = 0.014417
I1022 22:51:41.487570  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144161 (* 1 = 0.0144161 loss)
I1022 22:51:41.487576  3959 sgd_solver.cpp:105] Iteration 28500, lr = 0.0001
I1022 22:52:01.414485  3959 solver.cpp:330] Iteration 28600, Testing net (#0)
I1022 22:52:02.408638  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:52:03.991096  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:52:03.991160  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0458466 (* 1 = 0.0458466 loss)
I1022 22:52:04.193269  3959 solver.cpp:218] Iteration 28600 (4.40419 iter/s, 22.7056s/100 iters), loss = 0.0897109
I1022 22:52:04.193303  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0897101 (* 1 = 0.0897101 loss)
I1022 22:52:04.193311  3959 sgd_solver.cpp:105] Iteration 28600, lr = 0.0001
I1022 22:52:16.294961  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:52:24.318691  3959 solver.cpp:218] Iteration 28700 (4.96886 iter/s, 20.1253s/100 iters), loss = 0.000767022
I1022 22:52:24.318723  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00076614 (* 1 = 0.00076614 loss)
I1022 22:52:24.318729  3959 sgd_solver.cpp:105] Iteration 28700, lr = 0.0001
I1022 22:52:44.241638  3959 solver.cpp:330] Iteration 28800, Testing net (#0)
I1022 22:52:45.235226  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:52:46.819057  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:52:46.819114  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0471688 (* 1 = 0.0471688 loss)
I1022 22:52:47.020963  3959 solver.cpp:218] Iteration 28800 (4.40486 iter/s, 22.7022s/100 iters), loss = 0.00238249
I1022 22:52:47.020994  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00238162 (* 1 = 0.00238162 loss)
I1022 22:52:47.021000  3959 sgd_solver.cpp:105] Iteration 28800, lr = 0.0001
I1022 22:53:07.141252  3959 solver.cpp:218] Iteration 28900 (4.97013 iter/s, 20.1202s/100 iters), loss = 0.0625064
I1022 22:53:07.141286  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0625055 (* 1 = 0.0625055 loss)
I1022 22:53:07.141293  3959 sgd_solver.cpp:105] Iteration 28900, lr = 0.0001
I1022 22:53:27.061703  3959 solver.cpp:330] Iteration 29000, Testing net (#0)
I1022 22:53:28.053095  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:53:29.637975  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:53:29.638031  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0460628 (* 1 = 0.0460628 loss)
I1022 22:53:29.839887  3959 solver.cpp:218] Iteration 29000 (4.40557 iter/s, 22.6985s/100 iters), loss = 0.0698113
I1022 22:53:29.839920  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0698104 (* 1 = 0.0698104 loss)
I1022 22:53:29.839926  3959 sgd_solver.cpp:105] Iteration 29000, lr = 0.0001
I1022 22:53:30.268677  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:53:49.965773  3959 solver.cpp:218] Iteration 29100 (4.96875 iter/s, 20.1258s/100 iters), loss = 0.00793382
I1022 22:53:49.965806  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00793292 (* 1 = 0.00793292 loss)
I1022 22:53:49.965812  3959 sgd_solver.cpp:105] Iteration 29100, lr = 0.0001
I1022 22:54:09.891142  3959 solver.cpp:330] Iteration 29200, Testing net (#0)
I1022 22:54:10.881734  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:54:12.467870  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:54:12.467931  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0487835 (* 1 = 0.0487835 loss)
I1022 22:54:12.670075  3959 solver.cpp:218] Iteration 29200 (4.40447 iter/s, 22.7042s/100 iters), loss = 0.0246763
I1022 22:54:12.670106  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0246754 (* 1 = 0.0246754 loss)
I1022 22:54:12.670112  3959 sgd_solver.cpp:105] Iteration 29200, lr = 0.0001
I1022 22:54:32.792488  3959 solver.cpp:218] Iteration 29300 (4.9696 iter/s, 20.1223s/100 iters), loss = 0.00886248
I1022 22:54:32.792520  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00886159 (* 1 = 0.00886159 loss)
I1022 22:54:32.792526  3959 sgd_solver.cpp:105] Iteration 29300, lr = 0.0001
I1022 22:54:41.470868  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:54:52.718812  3959 solver.cpp:330] Iteration 29400, Testing net (#0)
I1022 22:54:53.671816  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:54:55.295390  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:54:55.295449  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0496603 (* 1 = 0.0496603 loss)
I1022 22:54:55.497228  3959 solver.cpp:218] Iteration 29400 (4.40438 iter/s, 22.7047s/100 iters), loss = 0.0115805
I1022 22:54:55.497258  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0115796 (* 1 = 0.0115796 loss)
I1022 22:54:55.497264  3959 sgd_solver.cpp:105] Iteration 29400, lr = 0.0001
I1022 22:55:15.615329  3959 solver.cpp:218] Iteration 29500 (4.97067 iter/s, 20.118s/100 iters), loss = 0.00223588
I1022 22:55:15.615501  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00223498 (* 1 = 0.00223498 loss)
I1022 22:55:15.615510  3959 sgd_solver.cpp:105] Iteration 29500, lr = 0.0001
I1022 22:55:35.531240  3959 solver.cpp:330] Iteration 29600, Testing net (#0)
I1022 22:55:36.483748  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:55:38.108395  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:55:38.108456  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0489259 (* 1 = 0.0489259 loss)
I1022 22:55:38.310492  3959 solver.cpp:218] Iteration 29600 (4.40627 iter/s, 22.6949s/100 iters), loss = 0.000699855
I1022 22:55:38.310523  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000698987 (* 1 = 0.000698987 loss)
I1022 22:55:38.310530  3959 sgd_solver.cpp:105] Iteration 29600, lr = 0.0001
I1022 22:55:55.241621  3967 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:55:58.437158  3959 solver.cpp:218] Iteration 29700 (4.96855 iter/s, 20.1266s/100 iters), loss = 0.00080575
I1022 22:55:58.437192  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000804874 (* 1 = 0.000804874 loss)
I1022 22:55:58.437201  3959 sgd_solver.cpp:105] Iteration 29700, lr = 0.0001
I1022 22:56:18.362967  3959 solver.cpp:330] Iteration 29800, Testing net (#0)
I1022 22:56:19.314057  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:56:20.939115  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:56:20.939175  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0487929 (* 1 = 0.0487929 loss)
I1022 22:56:21.141335  3959 solver.cpp:218] Iteration 29800 (4.40449 iter/s, 22.7041s/100 iters), loss = 0.0051207
I1022 22:56:21.141369  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00511986 (* 1 = 0.00511986 loss)
I1022 22:56:21.141376  3959 sgd_solver.cpp:105] Iteration 29800, lr = 0.0001
I1022 22:56:41.264350  3959 solver.cpp:218] Iteration 29900 (4.96946 iter/s, 20.1229s/100 iters), loss = 0.0328839
I1022 22:56:41.264492  3959 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.032883 (* 1 = 0.032883 loss)
I1022 22:56:41.264502  3959 sgd_solver.cpp:105] Iteration 29900, lr = 0.0001
I1022 22:57:01.189504  3959 solver.cpp:447] Snapshotting to binary proto file xn/English_orange/snapshot/res20/res20_penlu_alpha0.25_etanostudy_2study_2decay_iter_30000.caffemodel
I1022 22:57:01.199203  3959 sgd_solver.cpp:273] Snapshotting solver state to binary proto file xn/English_orange/snapshot/res20/res20_penlu_alpha0.25_etanostudy_2study_2decay_iter_30000.solverstate
I1022 22:57:01.255764  3959 solver.cpp:310] Iteration 30000, loss = 0.0281737
I1022 22:57:01.255808  3959 solver.cpp:330] Iteration 30000, Testing net (#0)
I1022 22:57:02.206045  3968 data_layer.cpp:73] Restarting data prefetching from start.
I1022 22:57:03.832949  3959 solver.cpp:397]     Test net output #0: Accuracy1 = 0.984375
I1022 22:57:03.833006  3959 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.0459304 (* 1 = 0.0459304 loss)
I1022 22:57:03.833012  3959 solver.cpp:315] Optimization Done.
I1022 22:57:03.833015  3959 caffe.cpp:259] Optimization Done.
